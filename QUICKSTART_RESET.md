# Quick Start: Cluster Reset & Deployment

## TL;DR
```bash
# On masternode (192.168.4.63)
cd /srv/monitoring_data/VMStation
git fetch && git pull

# Reset cluster (removes all K8s config/network, preserves SSH)
./deploy.sh reset

# Deploy fresh cluster
./deploy.sh

# Verify
kubectl get nodes
kubectl get pods --all-namespaces
```

## What Just Happened?

### Reset Phase
1. âœ… Confirmed operation
2. âœ… Detected running nodes
3. âœ… Cordoned all nodes
4. âœ… Drained all pods (120s timeout)
5. âœ… Reset all workers (serial)
6. âœ… Reset control plane
7. âœ… Verified SSH preserved
8. âœ… Verified ethernet preserved
9. âœ… Validated clean state

### Deploy Phase
1. âœ… System prep (all nodes)
2. âœ… Network fix (kernel/sysctl)
3. âœ… Cluster spinup (kubeadm init/join)
4. âœ… Jellyfin deployment
5. âœ… Monitoring stack

## Common Workflows

### Failed Deployment â†’ Recovery
```bash
./deploy.sh reset
./deploy.sh
```

### Network Issues â†’ Clean Slate
```bash
./deploy.sh reset  # Removes all CNI/network state
./deploy.sh        # Fresh networking
```

### Testing Deployments
```bash
./deploy.sh reset
./deploy.sh
# Test...
./deploy.sh reset  # Clean for next test
```

### Graceful Shutdown
```bash
./deploy.sh spindown  # Scale down, don't power off
# Later...
./deploy.sh reset     # Full clean
./deploy.sh           # Fresh start
```

## Safety Checks

After reset, manually verify:
```bash
# No K8s config
ssh root@192.168.4.61 'ls /etc/kubernetes'  # Should fail

# No K8s interfaces
ssh root@192.168.4.61 'ip link | grep cni'  # Nothing

# SSH works
ssh root@192.168.4.61 uptime  # Should connect

# Physical interfaces intact
ssh root@192.168.4.61 'ip link | grep eth'  # Shows your interfaces
```

## What Gets Removed

ğŸ—‘ï¸ `/etc/kubernetes/*` - All K8s config  
ğŸ—‘ï¸ `/var/lib/kubelet/*` - Kubelet state  
ğŸ—‘ï¸ `/var/lib/etcd/*` - etcd data (control plane)  
ğŸ—‘ï¸ `/etc/cni/net.d/*` - CNI config  
ğŸ—‘ï¸ `/var/lib/cni/*` - CNI state  
ğŸ—‘ï¸ `/run/flannel/*` - Flannel state  
ğŸ—‘ï¸ `flannel.1, cni0, calico*` - K8s network interfaces  
ğŸ—‘ï¸ iptables rules - K8s firewall rules  
ğŸ—‘ï¸ Container state - Running pods/containers  

## What Stays

âœ… SSH keys - `/root/.ssh/authorized_keys`  
âœ… Physical interfaces - `eth0`, `ens160`, etc.  
âœ… Container runtime - containerd binary  
âœ… System config - `/etc/sysctl.conf`, etc.  
âœ… User data - Home directories  

## Troubleshooting

### Reset hangs
```bash
# Check kubelet
ssh root@<node> 'systemctl status kubelet'

# Manual reset
ssh root@<node> 'kubeadm reset --force'

# Re-run
./deploy.sh reset
```

### Deploy fails
```bash
# Check logs
kubectl logs -n kube-system <pod-name>

# Reset and retry
./deploy.sh reset
./deploy.sh
```

### Network issues persist
```bash
# Manual interface cleanup
ssh root@<node> 'ip link delete flannel.1; ip link delete cni0'

# Full reset
./deploy.sh reset
./deploy.sh
```

## Verification Commands

```bash
# Cluster status
kubectl get nodes
kubectl get pods --all-namespaces

# Network connectivity
kubectl run test --image=busybox --rm -it -- ping 8.8.8.8

# DNS resolution
kubectl run test --image=busybox --rm -it -- nslookup kubernetes.default

# Monitoring access
curl http://192.168.4.63:30300  # Grafana
curl http://192.168.4.63:30090  # Prometheus

# Jellyfin access
curl http://192.168.4.61:30096  # Jellyfin
```

## Full Documentation

- **User Guide**: `docs/CLUSTER_RESET_GUIDE.md`
- **Role Docs**: `ansible/roles/cluster-reset/README.md`
- **Summary**: `RESET_ENHANCEMENT_SUMMARY.md`

## Questions?

1. Read the full guides above
2. Check playbook output for errors
3. Review node logs: `ssh root@<node> 'journalctl -xe'`
4. Run with debug: `ansible-playbook -vvv ...`

## That's It!

You now have:
- âœ… One-command cluster reset
- âœ… One-command fresh deployment
- âœ… Safe operations (SSH/ethernet preserved)
- âœ… Comprehensive validation
- âœ… Clear documentation

Happy cluster managing! ğŸš€
