---
# Kubernetes Cluster Setup Playbook
# Sets up monitoring_nodes as control plane and other nodes as workers
- name: Setup Kubernetes cluster with monitoring_nodes as control plane
  hosts: all
  become: true
  vars:
    kubernetes_version: "1.29"
    pod_network_cidr: "10.244.0.0/16"
    
  pre_tasks:
    - name: Remove bad kubernetes-upstream repo and clean yum/dnf cache (preflight)
      shell: |
        rm -f /etc/yum.repos.d/kubernetes-upstream.repo || true
        if command -v dnf >/dev/null 2>&1; then
          dnf clean all || true
        else
          yum clean all || true
        fi
      when: ansible_os_family == 'RedHat'
      ignore_errors: yes

  tasks:
    - name: Update apt cache
      apt:
        update_cache: yes
      when: ansible_os_family == 'Debian'

    - name: Install required packages
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gpg
        state: present
      when: ansible_os_family == 'Debian'

    - name: Add Kubernetes GPG key
      apt_key:
        url: https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/Release.key
        state: present
      when: ansible_os_family == 'Debian'

    - name: Add Kubernetes repository
      apt_repository:
        repo: "deb https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/ /"
        state: present
        filename: kubernetes
      when: ansible_os_family == 'Debian'

    - name: Update apt cache after adding repository
      apt:
        update_cache: yes
      when: ansible_os_family == 'Debian'

    - name: Install Kubernetes packages
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
      when: ansible_os_family == 'Debian'

    - name: Remove user-added kubernetes-upstream repo file if present (prevents broken metadata)
      file:
        path: /etc/yum.repos.d/kubernetes-upstream.repo
        state: absent
      when: ansible_os_family == 'RedHat'
      ignore_errors: yes

    - name: Ensure yum-utils is installed on RHEL-family systems
      package:
        name: yum-utils
        state: present
      when: ansible_os_family == 'RedHat'

    - name: Determine RHEL major version
      set_fact:
        rhel_major: "{{ ansible_distribution_major_version | int }}"
      when: ansible_os_family == 'RedHat'
    - block:
        - name: Add Kubernetes yum repository for RHEL-family systems (EL9/EL8)
          yum_repository:
            name: kubernetes
            description: Kubernetes repo
            baseurl: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/rpm/"
            gpgcheck: yes
            repo_gpgcheck: yes
            gpgkey: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/rpm/Release.key"
            enabled: yes
        - name: Update yum cache on RHEL-family systems (EL9/EL8)
          command: yum makecache -q
        - name: Install Kubernetes and containerd packages on RHEL-family systems (EL9/EL8)
          package:
            name:
              - kubelet
              - kubeadm
              - kubectl
              - containerd
            state: present
      when: ansible_os_family == 'RedHat' and (rhel_major | int) < 10

    - block:
        - name: Install required RHEL 10+ packages for Kubernetes
          package:
            name:
              - curl
              - wget
              - conntrack-tools
              - socat
              - iproute-tc
              - iptables
            state: present
          ignore_errors: yes
        
        - name: Fallback install conntrack if conntrack-tools not available
          package:
            name: conntrack
            state: present
          ignore_errors: yes
            
        - name: Add Docker CE repo for containerd (RHEL 10+ fallback)
          shell: |
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo
            elif command -v wget >/dev/null 2>&1; then
              wget -q -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo
            else
              dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo || true
            fi
          args:
            creates: /etc/yum.repos.d/docker-ce.repo
        
        - name: Ensure Docker CE repo file has correct permissions
          file:
            path: /etc/yum.repos.d/docker-ce.repo
            mode: '0644'
            owner: root
            group: root
        
        - name: Refresh package cache (dnf)
          command: dnf makecache --refresh
          
        - name: Try install containerd.io (RHEL 10+)
          package:
            name: containerd.io
            state: present
          register: containerd_install
          ignore_errors: yes
          
        - name: Fallback install containerd package name (containerd) if containerd.io not available
          package:
            name: containerd
            state: present
          when: containerd_install is failed
        - name: Ensure containerd directory exists
          file:
            path: /etc/containerd
            state: directory
            
        - name: Generate default containerd configuration (RHEL 10+)
          shell: containerd config default > /etc/containerd/config.toml
          args:
            creates: /etc/containerd/config.toml
          ignore_errors: yes
          
        - name: Configure containerd to use systemd cgroup driver (RHEL 10+)
          replace:
            path: /etc/containerd/config.toml
            regexp: 'SystemdCgroup = false'
            replace: 'SystemdCgroup = true'
          when: 
            - ansible_os_family == 'RedHat' 
            - (rhel_major | int) >= 10
          ignore_errors: yes
          
        - name: Start and enable containerd (RHEL 10+)
          systemd:
            name: containerd
            state: restarted
            enabled: yes
            daemon_reload: yes
        - name: Get latest stable Kubernetes version for RHEL 10+ (curl/wget fallback)
          shell: |
            set -o pipefail
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL "https://dl.k8s.io/release/stable-{{ kubernetes_version }}.txt" || true
            elif command -v wget >/dev/null 2>&1; then
              wget -qO- "https://dl.k8s.io/release/stable-{{ kubernetes_version }}.txt" || true
            else
              echo "";
            fi
          args:
            executable: /bin/bash
          register: k8s_stable_version
          changed_when: false
          ignore_errors: yes
          
        - name: Set Kubernetes version for download
          set_fact:
            k8s_download_version: >-
              {{ (k8s_stable_version.stdout | default('') ).strip() if (k8s_stable_version is defined and k8s_stable_version.stdout is defined and (k8s_stable_version.stdout | trim) != '') else 'v' + kubernetes_version + '.0' }}
            
        - name: Download kubeadm binary (RHEL 10+ fallback)
          get_url:
            url: "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubeadm"
            dest: /usr/bin/kubeadm
            mode: '0755'
            owner: root
            group: root
            validate_certs: false
            use_proxy: false
          register: kubeadm_download
          retries: 3
          delay: 5
          until: kubeadm_download is succeeded
          failed_when: false
          
        - name: Download kubeadm binary using shell fallback (RHEL 10+ urllib3 compatibility)
          shell: |
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubeadm" -o /usr/bin/kubeadm
            elif command -v wget >/dev/null 2>&1; then
              wget -q -O /usr/bin/kubeadm "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubeadm"
            else
              echo "Neither curl nor wget available for download"
              exit 1
            fi
            chmod 0755 /usr/bin/kubeadm
            chown root:root /usr/bin/kubeadm
          args:
            creates: /usr/bin/kubeadm
          when: kubeadm_download is failed and ('cert_file' in kubeadm_download.msg or 'urllib3' in kubeadm_download.msg)
          retries: 3
          delay: 5
          
        - name: Verify kubeadm binary was downloaded successfully
          stat:
            path: /usr/bin/kubeadm
          register: kubeadm_binary_check
          failed_when: not kubeadm_binary_check.stat.exists
          
        - name: Ensure kubeadm binary permissions are correct
          file:
            path: /usr/bin/kubeadm
            mode: '0755'
            owner: root
            group: root
          
        - name: Download kubectl binary (RHEL 10+ fallback)
          get_url:
            url: "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubectl"
            dest: /usr/bin/kubectl
            mode: '0755'
            owner: root
            group: root
            validate_certs: false
            use_proxy: false
          register: kubectl_download
          retries: 3
          delay: 5
          until: kubectl_download is succeeded
          failed_when: false
          
        - name: Download kubectl binary using shell fallback (RHEL 10+ urllib3 compatibility)
          shell: |
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubectl" -o /usr/bin/kubectl
            elif command -v wget >/dev/null 2>&1; then
              wget -q -O /usr/bin/kubectl "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubectl"
            else
              echo "Neither curl nor wget available for download"
              exit 1
            fi
            chmod 0755 /usr/bin/kubectl
            chown root:root /usr/bin/kubectl
          args:
            creates: /usr/bin/kubectl
          when: kubectl_download is failed and ('cert_file' in kubectl_download.msg or 'urllib3' in kubectl_download.msg)
          retries: 3
          delay: 5
          
        - name: Verify kubectl binary was downloaded successfully
          stat:
            path: /usr/bin/kubectl
          register: kubectl_binary_check
          failed_when: not kubectl_binary_check.stat.exists
          
        - name: Ensure kubectl binary permissions are correct
          file:
            path: /usr/bin/kubectl
            mode: '0755'
            owner: root
            group: root
          
        - name: Download kubelet binary (RHEL 10+ fallback)
          get_url:
            url: "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubelet"
            dest: /usr/bin/kubelet
            mode: '0755'
            owner: root
            group: root
            validate_certs: false
            use_proxy: false
          register: kubelet_download
          retries: 3
          delay: 5
          until: kubelet_download is succeeded
          failed_when: false
          
        - name: Download kubelet binary using shell fallback (RHEL 10+ urllib3 compatibility)
          shell: |
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubelet" -o /usr/bin/kubelet
            elif command -v wget >/dev/null 2>&1; then
              wget -q -O /usr/bin/kubelet "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubelet"
            else
              echo "Neither curl nor wget available for download"
              exit 1
            fi
            chmod 0755 /usr/bin/kubelet
            chown root:root /usr/bin/kubelet
          args:
            creates: /usr/bin/kubelet
          when: kubelet_download is failed and ('cert_file' in kubelet_download.msg or 'urllib3' in kubelet_download.msg)
          retries: 3
          delay: 5
          
        - name: Verify kubelet binary was downloaded successfully
          stat:
            path: /usr/bin/kubelet
          register: kubelet_binary_check
          failed_when: not kubelet_binary_check.stat.exists
          
        - name: Ensure kubelet binary permissions are correct
          file:
            path: /usr/bin/kubelet
            mode: '0755'
            owner: root
            group: root
        - name: Download crictl binary (RHEL 10+ fallback)
          get_url:
            url: "https://github.com/kubernetes-sigs/cri-tools/releases/download/v{{ kubernetes_version }}.0/crictl-v{{ kubernetes_version }}.0-linux-amd64.tar.gz"
            dest: /tmp/crictl.tar.gz
            validate_certs: false
            use_proxy: false
          register: crictl_download
          retries: 3
          delay: 5
          until: crictl_download is succeeded
          failed_when: false
          
        - name: Download crictl binary using shell fallback (RHEL 10+ urllib3 compatibility)
          shell: |
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL "https://github.com/kubernetes-sigs/cri-tools/releases/download/v{{ kubernetes_version }}.0/crictl-v{{ kubernetes_version }}.0-linux-amd64.tar.gz" -o /tmp/crictl.tar.gz
            elif command -v wget >/dev/null 2>&1; then
              wget -q -O /tmp/crictl.tar.gz "https://github.com/kubernetes-sigs/cri-tools/releases/download/v{{ kubernetes_version }}.0/crictl-v{{ kubernetes_version }}.0-linux-amd64.tar.gz"
            else
              echo "Neither curl nor wget available for download"
              exit 1
            fi
          args:
            creates: /tmp/crictl.tar.gz
          when: crictl_download is failed and ('cert_file' in crictl_download.msg or 'urllib3' in crictl_download.msg)
          retries: 3
          delay: 5
          
        - name: Extract and install crictl
          unarchive:
            src: /tmp/crictl.tar.gz
            dest: /usr/bin/
            remote_src: yes
            creates: /usr/bin/crictl
            
        - name: Ensure crictl is executable
          file:
            path: /usr/bin/crictl
            mode: '0755'
            owner: root
            group: root
            
        - name: Clean up crictl download
          file:
            path: /tmp/crictl.tar.gz
            state: absent
        - name: Install conntrack package (RHEL 10+ fallback)
          package:
            name: conntrack-tools
            state: present
          ignore_errors: yes
        - name: Install conntrack if conntrack-tools not available (RHEL 10+)
          package:
            name: conntrack
            state: present
          ignore_errors: yes
        - name: Create kubeadm-compatible kubelet systemd unit (RHEL 10+ fallback)
          copy:
            dest: /etc/systemd/system/kubelet.service
            content: |
              [Unit]
              Description=kubelet: The Kubernetes Node Agent
              Documentation=https://kubernetes.io/docs/
              Wants=network-online.target
              After=network-online.target containerd.service
              Requires=containerd.service

              [Service]
              Type=notify
              EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
              EnvironmentFile=-/etc/sysconfig/kubelet
              # KUBELET_KUBEADM_ARGS is written by kubeadm when joining
              ExecStart=/usr/bin/kubelet $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
              Restart=always
              RestartSec=10
              StartLimitBurst=3
              StartLimitInterval=60

              [Install]
              WantedBy=multi-user.target
              
        - name: Create kubelet dropin directory
          file:
            path: /etc/systemd/system/kubelet.service.d
            state: directory
            mode: '0755'
            
        - name: Create kubeadm kubelet dropin (join-compatible configuration)
          copy:
            dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
            content: |
              # Note: This dropin only works with kubeadm and kubelet v1.11+
              # Join-compatible kubelet configuration - lets kubeadm manage all configuration during join
              [Service]
              Environment="KUBELET_KUBECONFIG_ARGS="
              Environment="KUBELET_CONFIG_ARGS="
              # This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
              EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
              # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
              # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
              EnvironmentFile=-/etc/sysconfig/kubelet
              ExecStart=
              ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
              
        - name: Ensure /etc/sysconfig directory exists (RHEL 10+)
          file:
            path: /etc/sysconfig
            state: directory
            mode: '0755'
            
        - name: Ensure /etc/sysconfig/kubelet exists with systemd cgroup driver (RHEL 10+)
          copy:
            dest: /etc/sysconfig/kubelet
            content: |
              # Kubelet environment
              KUBELET_EXTRA_ARGS=--cgroup-driver=systemd --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock
            mode: '0644'
            
        - name: Reload systemd after kubelet unit and sysconfig changes (RHEL 10+)
          systemd:
            daemon_reload: yes
            
        - name: Enable kubelet service (RHEL 10+)
          systemd:
            name: kubelet
            enabled: yes
            
        - name: Verify kubelet binary works
          command: /usr/bin/kubelet --version
          register: kubelet_version_check
          changed_when: false
          
        - name: Display kubelet version
          debug:
            msg: "Kubelet version: {{ kubelet_version_check.stdout }}"
      when: ansible_os_family == 'RedHat' and (rhel_major | int) >= 10

    - name: Ensure SELinux is permissive on RHEL-family systems (required for some container setups)
      selinux:
        state: permissive
        policy: targeted
      when: ansible_os_family == 'RedHat' and (ansible_selinux is defined and ansible_selinux.status != 'Disabled')

    - name: Configure firewall for Kubernetes on RHEL systems
      block:
        - name: Check if firewalld is running
          systemd:
            name: firewalld
          register: firewalld_status
          ignore_errors: yes
          
        - name: Open Kubernetes ports in firewalld (control plane)
          firewalld:
            port: "{{ item }}"
            permanent: yes
            state: enabled
            immediate: yes
          loop:
            - "6443/tcp"    # Kubernetes API server
            - "2379-2380/tcp"  # etcd server client API
            - "10250/tcp"   # kubelet API
            - "10259/tcp"   # kube-scheduler
            - "10257/tcp"   # kube-controller-manager
          when: 
            - ansible_os_family == 'RedHat'
            - firewalld_status.status.ActiveState == 'active'
            - inventory_hostname in groups['monitoring_nodes']
          ignore_errors: yes
          
        - name: Open Kubernetes worker ports in firewalld
          firewalld:
            port: "{{ item }}"
            permanent: yes
            state: enabled
            immediate: yes
          loop:
            - "10250/tcp"   # kubelet API
            - "30000-32767/tcp"  # NodePort services
          when: 
            - ansible_os_family == 'RedHat'
            - firewalld_status.status.ActiveState == 'active'
            - inventory_hostname in groups['storage_nodes'] or inventory_hostname in groups['compute_nodes']
          ignore_errors: yes
          
        - name: Open CNI ports (Flannel) in firewalld
          firewalld:
            port: "{{ item }}"
            permanent: yes
            state: enabled
            immediate: yes
          loop:
            - "8285/udp"    # Flannel VXLAN
            - "8472/udp"    # Flannel VXLAN
          when: 
            - ansible_os_family == 'RedHat'
            - firewalld_status.status.ActiveState == 'active'
          ignore_errors: yes
      when: ansible_os_family == 'RedHat'

    - name: Hold Kubernetes packages
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl
      when: ansible_os_family == 'Debian'

    - name: Disable swap permanently
      replace:
        path: /etc/fstab
        regexp: '^([^#].*?\sswap\s.*)$'
        replace: '# \1'

    - name: Disable swap immediately
      command: swapoff -a
      changed_when: false

    - name: Load required kernel modules
      modprobe:
        name: "{{ item }}"
      loop:
        - overlay
        - br_netfilter

    - name: Create kernel modules config
      copy:
        content: |
          overlay
          br_netfilter
        dest: /etc/modules-load.d/k8s.conf

    - name: Set sysctl parameters for Kubernetes
      sysctl:
        name: "{{ item.key }}"
        value: "{{ item.value }}"
        state: present
        reload: yes
      loop:
        - { key: 'net.bridge.bridge-nf-call-iptables', value: '1' }
        - { key: 'net.bridge.bridge-nf-call-ip6tables', value: '1' }
        - { key: 'net.ipv4.ip_forward', value: '1' }

    - name: Install containerd
      apt:
        name: containerd
        state: present
      when: ansible_os_family == 'Debian'

    - name: Create containerd configuration directory
      file:
        path: /etc/containerd
        state: directory

    - name: Check if containerd binary exists
      stat:
        path: /usr/bin/containerd
      register: containerd_bin

    - name: Generate containerd configuration
      shell: containerd config default
      register: containerd_config
      when: containerd_bin.stat.exists

    - name: Write containerd configuration
      copy:
        content: "{{ containerd_config.stdout }}"
        dest: /etc/containerd/config.toml
      when: containerd_bin.stat.exists

    - name: Configure containerd to use systemd cgroup driver
      replace:
        path: /etc/containerd/config.toml
        regexp: 'SystemdCgroup = false'
        replace: 'SystemdCgroup = true'
      when: containerd_bin.stat.exists

    - name: Restart and enable containerd
      systemd:
        name: containerd
        state: restarted
        enabled: yes
      when: containerd_bin.stat.exists

    - name: Gather service facts
      service_facts:

    - name: Restart and enable kubelet (only if service exists)
      systemd:
        name: kubelet
        state: restarted
        enabled: yes
      when: "'kubelet.service' in ansible_facts.services"
      register: kubelet_restart_result
      ignore_errors: yes

    - name: Handle kubelet restart failure with comprehensive recovery
      block:
        - name: Collect initial diagnostics for kubelet failure
          shell: |
            echo "=== Kubelet Failure Diagnostics ==="
            echo "System: $(uname -a)"
            echo "Kubelet status:"
            systemctl status kubelet --no-pager || true
            echo "Recent kubelet logs:"
            journalctl -u kubelet -n 20 --no-pager || true
            echo "Container runtime status:"
            systemctl status containerd --no-pager || true
            echo "Kubelet configuration:"
            ls -la /etc/systemd/system/kubelet.service.d/ || true
            cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf || true
          register: initial_diagnostics
          ignore_errors: yes

        - name: Display initial diagnostics
          debug:
            var: initial_diagnostics.stdout_lines

        - name: Stop kubelet for clean restart
          systemd:
            name: kubelet
            state: stopped
          ignore_errors: yes

        - name: Verify container runtime is operational
          block:
            - name: Check containerd status and restart if needed
              systemd:
                name: containerd
                state: restarted
                enabled: yes
              register: containerd_restart
              ignore_errors: yes

            - name: Verify containerd socket accessibility
              shell: |
                if [ -S /run/containerd/containerd.sock ]; then
                  echo "containerd socket exists"
                  ls -la /run/containerd/containerd.sock
                else
                  echo "containerd socket missing"
                  exit 1
                fi
              register: containerd_socket_check
              ignore_errors: yes

            - name: Test container runtime connectivity
              shell: |
                # Test if containerd is responding
                timeout 10s ctr version || echo "containerd not responding"
              register: ctr_test
              ignore_errors: yes

        - name: Clear comprehensive kubelet state
          shell: |
            # Remove kubelet state that might prevent startup
            rm -rf /var/lib/kubelet/pki/kubelet.crt || true
            rm -rf /var/lib/kubelet/pki/kubelet.key || true
            rm -rf /var/lib/kubelet/config.yaml || true
            # Clear any leftover pod manifests that might cause conflicts
            rm -rf /etc/kubernetes/manifests/*.yaml || true
            # Clear kubelet cache and temporary files
            rm -rf /var/lib/kubelet/pods/* || true
            rm -rf /var/lib/kubelet/cache/* || true
            # Reset kubelet configuration if corrupted
            rm -rf /var/lib/kubelet/kubeconfig || true
            # Remove any existing systemd drop-in files that might have deprecated flags
            rm -f /etc/systemd/system/kubelet.service.d/10-kubeadm.conf || true
            # Remove any existing sysconfig/kubelet that might have deprecated flags  
            rm -f /etc/sysconfig/kubelet || true
          ignore_errors: yes

        # Removed: Create minimal kubelet config to allow startup (control plane)
        # This task was causing kubeadm init conflicts by creating a config.yaml that interferes
        # with kubeadm's initialization process. Let kubeadm manage the config.yaml during init.
        # The config.yaml will be properly created by kubeadm init and referenced post-init.

        # Removed: Create minimal kubelet config to allow startup (worker nodes - CA agnostic)
        # This task was causing kubeadm join failures by creating a config.yaml that conflicts
        # with kubeadm's bootstrap process. Let kubeadm manage the config.yaml during join.
        # The config.yaml will be properly created by kubeadm join and referenced post-join.

        - name: Regenerate kubelet service configuration
          block:
            - name: Create kubelet service drop-in directory
              file:
                path: /etc/systemd/system/kubelet.service.d
                state: directory
                mode: '0755'

            - name: Check if kubelet.conf exists for recovery mode config decision
              stat:
                path: /etc/kubernetes/kubelet.conf
              register: recovery_kubelet_conf_check

            - name: Generate enhanced kubelet service configuration (recovery mode - joined node)
              copy:
                content: |
                  # Note: This dropin only works with kubeadm and kubelet v1.11+
                  [Service]
                  Environment="KUBELET_KUBECONFIG_ARGS=--kubeconfig=/etc/kubernetes/kubelet.conf"
                  Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
                  # This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
                  EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
                  # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
                  # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
                  EnvironmentFile=-/etc/sysconfig/kubelet
                  ExecStart=
                  ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
                  Restart=always
                  StartLimitInterval=0
                  RestartSec=10
                dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
                mode: '0644'
              when: recovery_kubelet_conf_check.stat.exists

            - name: Generate enhanced kubelet service configuration (recovery mode - pre-join)
              copy:
                content: |
                  # Note: This dropin only works with kubeadm and kubelet v1.11+
                  [Service]
                  Environment="KUBELET_KUBECONFIG_ARGS="
                  Environment="KUBELET_CONFIG_ARGS="
                  # This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
                  EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
                  # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
                  # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
                  EnvironmentFile=-/etc/sysconfig/kubelet
                  ExecStart=
                  ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
                  Restart=always
                  StartLimitInterval=0
                  RestartSec=10
                dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
                mode: '0644'
              when: not recovery_kubelet_conf_check.stat.exists
              
            - name: Reload systemd daemon after updating drop-in configuration
              systemd:
                daemon_reload: yes
                
            # Removed: Create kubelet kubeadm flags file to prevent environment variable warnings
            # This task was causing kubeadm join failures by creating a static kubeadm-flags.env
            # that conflicts with kubeadm's bootstrap process. Let kubeadm manage this file during join.
            # The kubelet systemd service will still reference this file via EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
            # but kubeadm will create it with the correct join-specific parameters.

            - name: Ensure /etc/sysconfig directory exists
              file:
                path: /etc/sysconfig
                state: directory
                mode: '0755'
                
            - name: Ensure clean /etc/sysconfig/kubelet without deprecated flags
              copy:
                dest: /etc/sysconfig/kubelet
                content: |
                  # Kubelet environment - no deprecated flags like --network-plugin
                  KUBELET_EXTRA_ARGS=--cgroup-driver=systemd --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock
                mode: '0644'

        - name: Ensure CNI infrastructure is available
          block:
            - name: Create CNI directories
              file:
                path: "{{ item }}"
                state: directory
                owner: root
                group: root
                mode: '0755'
              loop:
                - /opt/cni/bin
                - /etc/cni/net.d
                - /var/lib/cni/networks
                - /var/lib/cni/results
              ignore_errors: yes

            - name: Check if basic CNI plugins exist
              stat:
                path: /opt/cni/bin/bridge
              register: cni_plugins_check
              ignore_errors: yes

            - name: Download essential CNI plugins if missing
              unarchive:
                src: "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz"
                dest: /opt/cni/bin
                remote_src: yes
                owner: root
                group: root
                mode: '0755'
                creates: /opt/cni/bin/bridge
              when: not cni_plugins_check.stat.exists
              ignore_errors: yes
              retries: 2
              delay: 5

        - name: Verify kernel modules are loaded
          shell: |
            # Ensure required kernel modules are loaded
            modprobe overlay || true
            modprobe br_netfilter || true
            # Verify modules are loaded
            lsmod | grep -E "(overlay|br_netfilter)" || echo "Kernel modules check"
          ignore_errors: yes

        - name: Handle systemd start rate limiting
          block:
            - name: Reset systemd failure state for kubelet
              shell: |
                # Reset any systemd failure counters that might prevent restart
                systemctl reset-failed kubelet || true
                # Clear systemd rate limiting for kubelet service
                systemctl stop kubelet || true
                sleep 2
              ignore_errors: yes

            - name: Check for systemd rate limiting issues
              shell: |
                # Check if kubelet is being rate limited
                systemctl status kubelet | grep -i "start request repeated too quickly" || echo "No rate limiting detected"
              register: rate_limit_check
              ignore_errors: yes

            - name: Apply additional rate limiting fixes if detected
              shell: |
                # If rate limiting is detected, reset the service completely
                if echo "{{ rate_limit_check.stdout }}" | grep -qi "start request repeated too quickly"; then
                  echo "Rate limiting detected, applying comprehensive reset"
                  systemctl stop kubelet || true
                  systemctl reset-failed kubelet || true
                  # Wait longer to ensure systemd internal state clears
                  sleep 5
                else
                  echo "No rate limiting issues detected"
                fi
              when: rate_limit_check.stdout is defined
              ignore_errors: yes

        - name: Reload systemd daemon and dependencies
          systemd:
            daemon_reload: yes

        - name: Wait for system to stabilize after configuration changes
          pause:
            seconds: 8

        - name: Attempt kubelet restart after comprehensive cleanup
          systemd:
            name: kubelet
            state: started
            enabled: yes
          register: kubelet_recovery_result
          ignore_errors: yes

        - name: Collect post-recovery diagnostics
          shell: |
            echo "=== Post-Recovery Diagnostics ==="
            echo "Kubelet status after recovery:"
            systemctl status kubelet --no-pager || true
            echo "Recent kubelet logs after recovery:"
            journalctl -u kubelet -n 10 --no-pager || true
            echo "Container runtime connectivity:"
            ctr version || echo "containerd not accessible"
          register: post_recovery_diagnostics
          ignore_errors: yes

        - name: Display comprehensive recovery status
          debug:
            msg: |
              Kubelet recovery attempt: {{ 'SUCCESS' if kubelet_recovery_result is succeeded else 'FAILED' }}
              
              Recovery actions taken:
              - Container runtime restart: {{ 'OK' if containerd_restart is succeeded else 'FAILED' }}
              - Containerd socket check: {{ 'OK' if containerd_socket_check is succeeded else 'FAILED' }}
              - Service configuration regenerated
              - Kubelet state cleared
              - Kernel modules verified
              
              {% if kubelet_recovery_result is failed %}
              Troubleshooting steps:
              1. Check logs: journalctl -u kubelet -f
              2. Check container runtime: systemctl status containerd
              3. Verify network: ss -ltnp | grep :10250
              4. Check node configuration: /var/lib/kubelet/config.yaml
              {% endif %}

        - name: Save diagnostic logs for manual review
          copy:
            content: |
              {{ initial_diagnostics.stdout }}
              
              === Post-Recovery Diagnostics ===
              {{ post_recovery_diagnostics.stdout }}
            dest: "/tmp/kubelet-recovery-{{ inventory_hostname }}-{{ ansible_date_time.epoch }}.log"
          ignore_errors: yes

      when: 
        - "'kubelet.service' in ansible_facts.services"
        - kubelet_restart_result is defined
        - kubelet_restart_result.failed is defined
        - kubelet_restart_result.failed


# Control plane initialization (only on monitoring_nodes)
- name: Initialize Kubernetes control plane
  hosts: monitoring_nodes
  become: true
  vars:
    pod_network_cidr: "10.244.0.0/16"
  tasks:
    - name: Check if cluster is already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeconfig

    - name: Preflight detect listeners on API port 6443
      shell: |
        set -o pipefail
        (ss -ltnp '( sport = :6443 )' 2>/dev/null || true) || (ss -ltnp 2>/dev/null | grep ':6443' || true)
      args:
        executable: /bin/bash
      register: port_6443
      changed_when: false

    - name: Show port-6443 listener (if any)
      debug:
        var: port_6443.stdout_lines
      when: port_6443.stdout != ''

    - name: Attempt to free port 6443 if occupied by kube components
      block:
        - name: Stop kubelet to prevent static pod recreation
          systemd:
            name: kubelet
            state: stopped
          ignore_errors: true

        - name: Stop kube-apiserver systemd unit if present
          systemd:
            name: kube-apiserver
            state: stopped
          ignore_errors: true

        - name: Move kube-apiserver static manifest out of manifests (if present)
          shell: |
            if [ -f /etc/kubernetes/manifests/kube-apiserver.yaml ]; then
              mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/kube-apiserver.yaml.backup || true
            fi
          ignore_errors: true

        - name: Kill any remaining kube-apiserver processes
          shell: pkill -f kube-apiserver || true
          ignore_errors: true

        - name: Re-check port 6443 after cleanup
          shell: ss -ltnp 2>/dev/null | grep ':6443' || true
          register: port_6443_after
          changed_when: false

        - name: Fail if port 6443 still occupied after attempts to free it
          fail:
            msg: |
              Port 6443 is still in use after attempting to stop kubelet/kube-apiserver and remove static manifests.
              Manual intervention required. Run the following on the target node to investigate:
                sudo ss -ltnp | grep ':6443'
                sudo ps -ef | grep kube-apiserver
                sudo systemctl status kubelet kube-apiserver
              If it's safe, stop the owning service or kill the process, or remove the static manifest under /etc/kubernetes/manifests.
          when: port_6443_after.stdout != ''
      when: port_6443.stdout != '' and not kubeconfig.stat.exists

    - name: Ensure /etc/kubernetes directory structure exists
      file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: '0755'
      loop:
        - /etc/kubernetes
        - /etc/kubernetes/manifests
        - /var/lib/kubelet

    - name: Initialize Kubernetes cluster
      command: >
        kubeadm init 
        --pod-network-cidr={{ pod_network_cidr }}
        --apiserver-advertise-address={{ ansible_default_ipv4.address }}
        --control-plane-endpoint={{ ansible_default_ipv4.address }}
        --apiserver-cert-extra-sans={{ ansible_default_ipv4.address }},localhost,127.0.0.1
        --upload-certs
      when: not kubeconfig.stat.exists
      register: kubeadm_init

    - name: Validate certificates generated by kubeadm init
      block:
        - name: Check if API server certificate exists and is valid
          shell: |
            if [ ! -f /etc/kubernetes/pki/apiserver.crt ]; then
              echo "ERROR: API server certificate not found"
              exit 1
            fi
            
            # Check certificate validity
            if ! openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -checkend 86400; then
              echo "ERROR: API server certificate expires within 24 hours"
              exit 1
            fi
            
            # Check certificate contains proper SANs
            SANS=$(openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text | grep -A1 "Subject Alternative Name" | grep -E "IP Address|DNS" || true)
            if [[ "$SANS" != *"{{ ansible_default_ipv4.address }}"* ]]; then
              echo "WARNING: API server certificate may not contain required IP SAN: {{ ansible_default_ipv4.address }}"
            fi
            
            echo "✓ API server certificate is valid"
          register: cert_validation
          failed_when: cert_validation.rc != 0
          when: kubeadm_init is defined and kubeadm_init.changed

        - name: Display certificate validation results
          debug:
            var: cert_validation.stdout_lines
          when: cert_validation is defined

      when: inventory_hostname in groups['monitoring_nodes']

    - name: Verify kubelet configuration after kubeadm init
      block:
        - name: Check if kubelet config file exists
          stat:
            path: /var/lib/kubelet/config.yaml
          register: kubelet_config_check

        - name: Check if kubelet kubeadm flags file exists  
          stat:
            path: /var/lib/kubelet/kubeadm-flags.env
          register: kubelet_flags_check

        - name: Restart and enable kubelet (only if service exists)
          systemd:
            name: kubelet
            state: restarted
            enabled: yes
          when: "'kubelet.service' in ansible_facts.services"
          register: kubelet_post_init_restart
          ignore_errors: yes

        - name: Collect initial diagnostics for kubelet failure
          shell: |
            echo "=== Kubelet Failure Diagnostics ==="
            echo "System: $(uname -a)"
            echo "Kubelet status:"
            systemctl status kubelet --no-pager || true
            echo "Recent kubelet logs:"
            journalctl -u kubelet -n 20 --no-pager || true
            echo "Container runtime status:"
            systemctl status containerd --no-pager || true
            echo "Kubelet configuration:"
            ls -la /etc/systemd/system/kubelet.service.d/ || true
            cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf || true
          register: initial_diagnostics
          ignore_errors: yes
          when: kubelet_post_init_restart is defined and kubelet_post_init_restart.failed

        - name: Display initial diagnostics
          debug:
            var: initial_diagnostics.stdout_lines
          when: kubelet_post_init_restart is defined and kubelet_post_init_restart.failed

        - name: Attempt kubelet restart after comprehensive cleanup
          systemd:
            name: kubelet
            state: restarted
            enabled: yes
          register: kubelet_recovery_result
          ignore_errors: yes
          when: kubelet_post_init_restart is defined and kubelet_post_init_restart.failed

      when: kubeadm_init is defined and not (kubeadm_init.failed | default(false))

    - name: Create .kube directory for root
      file:
        path: /root/.kube
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Copy admin.conf to root's kubeconfig
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        owner: root
        group: root
        mode: '0644'
        remote_src: yes

    - name: Copy custom Flannel manifest to control plane
      copy:
        src: "{{ playbook_dir }}/templates/kube-flannel-allnodes.yml"
        dest: /tmp/kube-flannel-allnodes.yml
        mode: '0644'

    - name: Delete existing Flannel DaemonSet if present (to handle immutable selector)
      shell: kubectl delete daemonset kube-flannel-ds -n kube-flannel --ignore-not-found=true
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      ignore_errors: yes

    - name: Install Flannel CNI (custom manifest - all nodes)
      shell: kubectl apply -f /tmp/kube-flannel-allnodes.yml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Ensure any existing tokens are cleaned up before creating new one
      shell: |
        echo "Cleaning up any existing bootstrap tokens..."
        kubeadm token list | awk 'NR>1 {print $1}' | xargs -r -I {} kubeadm token delete {} || true
      ignore_errors: yes

    - name: Get join command with fresh token (24h TTL)
      shell: kubeadm token create --print-join-command --ttl 24h
      register: join_command
      retries: 3
      delay: 5
      until: join_command.rc == 0

    - name: Validate join command contains required components
      fail:
        msg: "Generated join command is invalid or incomplete: {{ join_command.stdout }}"
      when: not (join_command.stdout | regex_search('kubeadm join.*--token.*--discovery-token-ca-cert-hash'))

    - name: Enhanced join command and certificate validation
      block:
        - name: Verify CA certificate is accessible for join operations
          shell: |
            if [ ! -f /etc/kubernetes/pki/ca.crt ]; then
              echo "ERROR: CA certificate not found - worker nodes cannot validate API server"
              exit 1
            fi
            
            # Test that the ca.crt can be read and is valid
            if ! openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -checkend 86400; then
              echo "ERROR: CA certificate is invalid or expires within 24 hours"
              exit 1
            fi
            
            # Verify the join command contains a valid ca-cert-hash
            CA_HASH=$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //')
            if [[ "{{ join_command.stdout }}" != *"sha256:$CA_HASH"* ]]; then
              echo "WARNING: Join command CA hash may not match actual CA certificate"
              echo "Expected hash: sha256:$CA_HASH"
              echo "Join command: {{ join_command.stdout }}"
            else
              echo "✓ Join command CA hash matches CA certificate"
            fi
            
            echo "✓ CA certificate is valid and accessible for worker joins"
          register: ca_cert_validation
          failed_when: ca_cert_validation.rc != 0

        - name: Display certificate validation results  
          debug:
            var: ca_cert_validation.stdout_lines

    - name: Save join command to file with timestamp
      copy:
        content: |
          #!/bin/bash
          # Generated: {{ ansible_date_time.iso8601 }}
          # Valid for 24 hours from generation time
          # Control plane: {{ inventory_hostname }}:6443
          {{ join_command.stdout }}
        dest: /tmp/kubeadm-join.sh
        mode: '0755'

    - name: Fetch join command to local machine
      fetch:
        src: /tmp/kubeadm-join.sh
        dest: /tmp/kubeadm-join.sh
        flat: yes

# Join worker nodes
- name: Join worker nodes to cluster
  hosts: storage_nodes:compute_nodes
  become: true
  tasks:
    - name: Check if node is already joined
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: Validate kubelet.conf if it exists (enhanced post-spindown recovery check)
      block:
        - name: Test kubelet.conf validity and cluster connectivity
          shell: |
            if [ -f /etc/kubernetes/kubelet.conf ]; then
              # Check if the kubeconfig contains cluster server info
              if grep -q "server:" /etc/kubernetes/kubelet.conf && grep -q "certificate-authority-data:" /etc/kubernetes/kubelet.conf; then
                # Additional check: try to use the kubeconfig to verify connectivity
                if timeout 10s kubectl --kubeconfig=/etc/kubernetes/kubelet.conf cluster-info >/dev/null 2>&1; then
                  echo "valid-and-connected"
                else
                  echo "valid-but-disconnected"
                fi
              else
                echo "invalid"
              fi
            else
              echo "missing"
            fi
          register: kubelet_conf_validation
          changed_when: false

        - name: Force rejoin if kubelet.conf is invalid, disconnected, or references old cluster
          set_fact:
            kubelet_conf: 
              stat:
                exists: false
            force_rejoin_reason: "{{ kubelet_conf_validation.stdout }}"
          when: kubelet_conf_validation.stdout != "valid-and-connected"

        - name: Display kubelet.conf validation result
          debug:
            msg: |
              Kubelet configuration validation on {{ inventory_hostname }}:
              - File exists: {{ kubelet_conf.stat.exists }}
              - Validation result: {{ kubelet_conf_validation.stdout }}
              {% if kubelet_conf_validation.stdout != "valid-and-connected" %}
              - Action: Will trigger rejoin process (reason: {{ force_rejoin_reason | default('unknown') }})
              {% else %}
              - Action: Node appears properly joined, skipping rejoin process
              {% endif %}

      when: kubelet_conf.stat.exists

    - name: Clean up stale CNI state on worker nodes (prevent cert-manager conflicts)
      block:
        - name: Stop and disable kubelet if running (to prevent CNI conflicts)
          systemd:
            name: kubelet
            state: stopped
            enabled: no
          ignore_errors: yes

        - name: Remove any existing CNI network interfaces (cni0, cbr0, flannel.1)
          shell: |
            # Remove cni0 interface if it exists
            if ip link show cni0 2>/dev/null; then
              echo "Removing existing cni0 interface"
              ip link set cni0 down || true
              ip link delete cni0 || true
            fi
            
            # Remove cbr0 interface if it exists  
            if ip link show cbr0 2>/dev/null; then
              echo "Removing existing cbr0 interface"
              ip link set cbr0 down || true
              ip link delete cbr0 || true
            fi
            
            # Remove flannel.1 interface if it exists
            if ip link show flannel.1 2>/dev/null; then
              echo "Removing existing flannel.1 interface"
              ip link set flannel.1 down || true
              ip link delete flannel.1 || true
            fi
          ignore_errors: yes
          register: cni_cleanup_result

        - name: Clear existing CNI configuration files (preserve directory structure and flannel config)
          shell: |
            # Backup flannel configuration if it exists
            if [ -f /etc/cni/net.d/10-flannel.conflist ]; then
              cp /etc/cni/net.d/10-flannel.conflist /tmp/10-flannel.conflist.backup
            fi
            
            # Remove any existing CNI configuration but preserve directories
            rm -rf /etc/cni/net.d/* || true
            rm -f /opt/cni/bin/flannel || true
            # Clear CNI plugin cache but preserve directories
            rm -rf /var/lib/cni/networks/* || true
            rm -rf /var/lib/cni/results/* || true
            
            # Restore flannel configuration if backup exists
            if [ -f /tmp/10-flannel.conflist.backup ]; then
              mv /tmp/10-flannel.conflist.backup /etc/cni/net.d/10-flannel.conflist
            fi
            
            # Ensure CNI directories exist with proper permissions after cleanup
            mkdir -p /opt/cni/bin /etc/cni/net.d /var/lib/cni/networks /var/lib/cni/results || true
            chmod 755 /opt/cni/bin /etc/cni/net.d /var/lib/cni/networks /var/lib/cni/results || true
            chown root:root /opt/cni/bin /etc/cni/net.d /var/lib/cni/networks /var/lib/cni/results || true
          ignore_errors: yes

        - name: Clear kubelet CNI state
          shell: |
            # Remove any existing kubelet CNI state
            rm -rf /var/lib/kubelet/pods/* || true
            rm -rf /var/lib/kubelet/plugins_registry/* || true
          ignore_errors: yes

        - name: Display CNI cleanup results
          debug:
            msg: |
              CNI cleanup completed on {{ inventory_hostname }}:
              {{ cni_cleanup_result.stdout_lines | default(['No interfaces to remove']) | join('\n') }}
      
      when: not kubelet_conf.stat.exists

    - name: Install CNI plugins and configuration on worker nodes (required for kubelet)
      block:
        - name: Create CNI directories on worker nodes
          file:
            path: "{{ item }}"
            state: directory
            owner: root
            group: root
            mode: '0755'
          loop:
            - /opt/cni/bin
            - /etc/cni/net.d
            - /var/lib/cni/networks
            - /var/lib/cni/results

        - name: Validate CNI directory permissions before download
          shell: |
            # Verify directories exist and are writable
            for dir in /opt/cni/bin /etc/cni/net.d /var/lib/cni/networks /var/lib/cni/results; do
              if [ ! -d "$dir" ]; then
                echo "Creating missing directory: $dir"
                mkdir -p "$dir" || exit 1
                chmod 755 "$dir" || exit 1
                chown root:root "$dir" || exit 1
              fi
              if [ ! -w "$dir" ]; then
                echo "Fixing permissions for directory: $dir"
                chmod 755 "$dir" || exit 1
                chown root:root "$dir" || exit 1
              fi
              echo "Directory $dir is ready (permissions: $(stat -c '%a' "$dir"), owner: $(stat -c '%U:%G' "$dir"))"
            done
          register: cni_dir_validation
          failed_when: false

        - name: Display CNI directory validation results
          debug:
            var: cni_dir_validation.stdout_lines

        - name: Set Flannel CNI destination path per node type
          set_fact:
            flannel_cni_dest: /opt/cni/bin/flannel

        - name: Check if valid Flannel binary already exists (preserve pre-downloaded binaries)
          shell: |
            if [ -f "{{ flannel_cni_dest }}" ] && [ -x "{{ flannel_cni_dest }}" ]; then
              size=$(stat -c%s "{{ flannel_cni_dest }}" 2>/dev/null || echo 0)
              if [ "$size" -gt 1000000 ]; then
                if file "{{ flannel_cni_dest }}" | grep -q "ELF.*executable"; then
                  echo "valid_existing_binary"
                  echo "Found existing valid Flannel binary ($size bytes) - skipping cleanup and download"
                  exit 0
                else
                  echo "invalid_format"
                fi
              else
                echo "too_small"
              fi
            elif [ -d "{{ flannel_cni_dest }}" ]; then
              echo "is_directory"
            elif [ -f "{{ flannel_cni_dest }}" ]; then
              echo "not_executable"
            else
              echo "missing"
            fi
          register: flannel_existing_check
          changed_when: false

        - name: Set skip download flag if valid binary exists
          set_fact:
            skip_flannel_download: "{{ flannel_existing_check.stdout_lines[0] == 'valid_existing_binary' }}"

        - name: Display existing Flannel binary status
          debug:
            msg: |
              Existing Flannel binary check: {{ flannel_existing_check.stdout_lines[0] }}
              {% if skip_flannel_download %}
              ✓ Valid Flannel binary already exists - skipping cleanup and download
              {% else %}
              ⚠ Flannel binary needs cleanup/download ({{ flannel_existing_check.stdout_lines[0] }})
              {% endif %}

        - name: Clean up any incorrect Flannel CNI state (fix directory conflicts)
          shell: |
            if [ -d "{{ flannel_cni_dest }}" ]; then
              echo "Found flannel directory at {{ flannel_cni_dest }}, removing it..."
              rm -rf "{{ flannel_cni_dest }}"
            fi
            
            if [ -f "{{ flannel_cni_dest }}" ] && [ ! -x "{{ flannel_cni_dest }}" ]; then
              echo "Found non-executable flannel file at {{ flannel_cni_dest }}, removing it..."
              rm -f "{{ flannel_cni_dest }}"
            fi
            
            if [ -f "{{ flannel_cni_dest }}" ]; then
              size=$(stat -c%s "{{ flannel_cni_dest }}" 2>/dev/null || echo 0)
              if [ "$size" -lt 1000000 ]; then
                echo "Found suspiciously small flannel file ($size bytes), removing it..."
                rm -f "{{ flannel_cni_dest }}"
              else
                echo "Valid flannel binary found at {{ flannel_cni_dest }} ($size bytes), keeping it"
              fi
            fi
          register: flannel_cleanup_result
          changed_when: "'removing' in flannel_cleanup_result.stdout"
          when: not (skip_flannel_download | default(false))

        - name: Display Flannel cleanup results
          debug:
            var: flannel_cleanup_result.stdout_lines
          when: 
            - not (skip_flannel_download | default(false))
            - flannel_cleanup_result is defined
            - flannel_cleanup_result.stdout != ""

        - name: Pre-download validation for Flannel CNI binary
          shell: |
            # Ensure target directory exists and is writable
            target_dir="$(dirname "{{ flannel_cni_dest }}")"
            if [ ! -d "$target_dir" ]; then
              echo "Creating target directory: $target_dir"
              mkdir -p "$target_dir" || exit 1
              chmod 755 "$target_dir" || exit 1
              chown root:root "$target_dir" || exit 1
            fi
            
            if [ ! -w "$target_dir" ]; then
              echo "Target directory not writable, fixing permissions: $target_dir"
              chmod 755 "$target_dir" || exit 1
              chown root:root "$target_dir" || exit 1
            fi
            
            # Test write access by creating a temporary file
            test_file="$target_dir/.write_test_$$"
            if echo "test" > "$test_file" 2>/dev/null; then
              rm -f "$test_file"
              echo "Target directory is writable: $target_dir"
            else
              echo "ERROR: Cannot write to target directory: $target_dir"
              ls -la "$target_dir" || true
              exit 1
            fi
          register: flannel_pre_download_check
          failed_when: flannel_pre_download_check.rc != 0
          when: not (skip_flannel_download | default(false))

        - name: Ensure download tools are available for Flannel installation
          package:
            name: 
              - curl
              - wget
            state: present
          ignore_errors: yes
          register: download_tools_install
          when: not (skip_flannel_download | default(false))

        - name: Download and install Flannel CNI plugin binary (primary method)
          get_url:
            url: "https://github.com/flannel-io/flannel/releases/download/v0.25.2/flanneld-amd64"
            dest: "{{ flannel_cni_dest }}"
            mode: '0755'
            owner: root
            group: root
            timeout: 120
            validate_certs: false
            use_proxy: false
          register: flannel_download_primary
          retries: 3
          delay: 10
          until: flannel_download_primary is succeeded
          failed_when: false
          when: not (skip_flannel_download | default(false))

        - name: Clean up partial download if primary method failed
          file:
            path: "{{ flannel_cni_dest }}"
            state: absent
          when: 
            - not (skip_flannel_download | default(false))
            - flannel_download_primary is defined
            - flannel_download_primary is failed
          
        - name: Add delay before first fallback to avoid server overwhelm
          pause:
            seconds: 5
          when: 
            - not (skip_flannel_download | default(false))
            - flannel_download_primary is defined
            - flannel_download_primary is failed

        - name: Download Flannel CNI plugin binary using curl fallback (first fallback)
          shell: |
            curl -fsSL --connect-timeout 30 --max-time 300 --retry 3 --retry-delay 5 \
              "https://github.com/flannel-io/flannel/releases/download/v0.25.2/flanneld-amd64" \
              -o "{{ flannel_cni_dest }}"
            chmod 0755 "{{ flannel_cni_dest }}"
            chown root:root "{{ flannel_cni_dest }}"
          when: 
            - not (skip_flannel_download | default(false))
            - flannel_download_primary is defined
            - flannel_download_primary is failed
          register: flannel_download_curl
          failed_when: false
          retries: 2
          delay: 15

        - name: Clean up partial download if curl method failed
          file:
            path: "{{ flannel_cni_dest }}"
            state: absent
          when: 
            - not (skip_flannel_download | default(false))
            - flannel_download_primary is defined
            - flannel_download_primary is failed
            - flannel_download_curl is defined
            - flannel_download_curl is failed
            
        - name: Add delay before second fallback to avoid server overwhelm
          pause:
            seconds: 5
          when: 
            - not (skip_flannel_download | default(false))
            - flannel_download_primary is defined
            - flannel_download_primary is failed
            - (flannel_download_curl is defined and (flannel_download_curl is failed or flannel_download_curl is skipped))

        - name: Download Flannel CNI plugin binary using wget fallback (second fallback)
          shell: |
            wget --connect-timeout=30 --read-timeout=300 --tries=3 --waitretry=5 \
              -q -O "{{ flannel_cni_dest }}" \
              "https://github.com/flannel-io/flannel/releases/download/v0.25.2/flanneld-amd64"
            chmod 0755 "{{ flannel_cni_dest }}"
            chown root:root "{{ flannel_cni_dest }}"
          when: 
            - not (skip_flannel_download | default(false))
            - flannel_download_primary is defined
            - flannel_download_primary is failed
            - (flannel_download_curl is defined and (flannel_download_curl is failed or flannel_download_curl is skipped))
          register: flannel_download_wget
          failed_when: false
          retries: 2
          delay: 15

        - name: Clean up partial download if wget method failed
          file:
            path: "{{ flannel_cni_dest }}"
            state: absent
          when: 
            - not (skip_flannel_download | default(false))
            - flannel_download_primary is defined
            - flannel_download_primary is failed
            - (flannel_download_curl is defined and (flannel_download_curl is failed or flannel_download_curl is skipped))
            - (flannel_download_wget is defined and (flannel_download_wget is failed or flannel_download_wget is skipped))
            
        - name: Add delay before alternative version fallback
          pause:
            seconds: 5
          when: 
            - not (skip_flannel_download | default(false))
            - flannel_download_primary is defined
            - flannel_download_primary is failed
            - (flannel_download_curl is defined and (flannel_download_curl is failed or flannel_download_curl is skipped))
            - (flannel_download_wget is defined and (flannel_download_wget is failed or flannel_download_wget is skipped))

        - name: Alternative Flannel binary source (third fallback - different version if needed)
          shell: |
            # Try older version if current version fails
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL --connect-timeout 30 --max-time 300 --retry 2 \
                "https://github.com/flannel-io/flannel/releases/download/v0.24.2/flanneld-amd64" \
                -o "{{ flannel_cni_dest }}" || \
              curl -fsSL --connect-timeout 30 --max-time 300 --retry 2 \
                "https://github.com/flannel-io/flannel/releases/download/v0.23.0/flanneld-amd64" \
                -o "{{ flannel_cni_dest }}"
            elif command -v wget >/dev/null 2>&1; then
              wget --connect-timeout=30 --read-timeout=300 --tries=2 \
                -q -O "{{ flannel_cni_dest }}" \
                "https://github.com/flannel-io/flannel/releases/download/v0.24.2/flanneld-amd64" || \
              wget --connect-timeout=30 --read-timeout=300 --tries=2 \
                -q -O "{{ flannel_cni_dest }}" \
                "https://github.com/flannel-io/flannel/releases/download/v0.23.0/flanneld-amd64"
            fi
            chmod 0755 "{{ flannel_cni_dest }}"
            chown root:root "{{ flannel_cni_dest }}"
          when: 
            - not (skip_flannel_download | default(false))
            - flannel_download_primary is defined
            - flannel_download_primary is failed
            - (flannel_download_curl is defined and (flannel_download_curl is failed or flannel_download_curl is skipped))
            - (flannel_download_wget is defined and (flannel_download_wget is failed or flannel_download_wget is skipped))
          register: flannel_download_alt
          failed_when: false

        - name: Verify Flannel CNI plugin binary was downloaded successfully
          stat:
            path: "{{ flannel_cni_dest }}"
          register: flannel_binary_check

        - name: Collect Flannel download diagnostics if verification fails
          block:
            - name: Show download attempt results
              debug:
                msg: |
                  Flannel download diagnostics for {{ inventory_hostname }}:
                  {% if skip_flannel_download | default(false) %}
                  Status: SKIPPED - Valid pre-existing binary found
                  Pre-existing binary check: {{ flannel_existing_check.stdout_lines[0] | default('unknown') }}
                  {% else %}
                  Primary method: {{ 'SUCCESS' if flannel_download_primary is succeeded else 'FAILED - ' + (flannel_download_primary.msg | default('Unknown error')) if flannel_download_primary is defined else 'NOT ATTEMPTED' }}
                  Curl fallback: {{ 'SUCCESS' if flannel_download_curl is succeeded else ('FAILED - ' + (flannel_download_curl.stderr | default('Unknown error'))) if flannel_download_curl is defined and flannel_download_curl is not skipped else 'SKIPPED' }}
                  Wget fallback: {{ 'SUCCESS' if flannel_download_wget is succeeded else ('FAILED - ' + (flannel_download_wget.stderr | default('Unknown error'))) if flannel_download_wget is defined and flannel_download_wget is not skipped else 'SKIPPED' }}
                  Alt version: {{ 'SUCCESS' if flannel_download_alt is succeeded else ('FAILED - ' + (flannel_download_alt.stderr | default('Unknown error'))) if flannel_download_alt is defined and flannel_download_alt is not skipped else 'SKIPPED' }}
                  {% endif %}
                  
                  Network connectivity test:
            
            - name: Test network connectivity to GitHub
              uri:
                url: "https://github.com"
                method: HEAD
                timeout: 30
                validate_certs: false
              register: github_connectivity
              failed_when: false
              when: not (skip_flannel_download | default(false))
              
            - name: Test DNS resolution
              shell: nslookup github.com || true
              register: dns_test
              changed_when: false
              when: not (skip_flannel_download | default(false))
              
            - name: Show network diagnostics
              debug:
                msg: |
                  {% if not (skip_flannel_download | default(false)) %}
                  GitHub connectivity: {{ 'OK' if github_connectivity.status is defined and github_connectivity.status == 200 else 'FAILED' }}
                  DNS resolution: {{ 'OK' if 'github.com' in dns_test.stdout else 'FAILED' }}
                  Available tools: {{ 'curl' if ansible_facts['packages']['curl'] is defined else 'no curl' }}, {{ 'wget' if ansible_facts['packages']['wget'] is defined else 'no wget' }}
                  {% else %}
                  Network diagnostics skipped - using pre-existing binary
                  {% endif %}
                  
            - name: List current /opt/cni/bin contents
              shell: ls -la /opt/cni/bin/ || echo "Directory does not exist"
              register: cni_dir_contents
              changed_when: false
              
            - name: Check directory permissions and disk space
              shell: |
                echo "=== Directory Permissions ==="
                ls -la /opt/cni/ || echo "/opt/cni not found"
                ls -la /opt/cni/bin/ || echo "/opt/cni/bin not found"
                echo "=== Directory Ownership ==="
                stat /opt/cni/bin/ || echo "/opt/cni/bin stat failed"
                echo "=== Disk Space ==="
                df -h /opt/cni/bin/ || echo "df failed"
                echo "=== Write Test ==="
                touch /opt/cni/bin/.write_test && rm -f /opt/cni/bin/.write_test && echo "Write test: PASSED" || echo "Write test: FAILED"
              register: dir_diagnostics
              changed_when: false
              
            - name: Show CNI directory contents
              debug:
                var: cni_dir_contents.stdout_lines
                
            - name: Show directory permission diagnostics
              debug:
                var: dir_diagnostics.stdout_lines
                
          when: not flannel_binary_check.stat.exists
          
        - name: Fail if Flannel binary still missing after all attempts
          fail:
            msg: |
              {% if skip_flannel_download | default(false) %}
              Flannel binary validation failed on {{ inventory_hostname }}.
              A pre-existing binary was detected but verification still shows it as missing.
              This indicates the binary was removed or corrupted after the initial check.
              Manual installation may be required.
              
              Pre-existing binary status: {{ flannel_existing_check.stdout_lines[0] | default('unknown') }}
              {% else %}
              Failed to download Flannel CNI plugin binary to {{ flannel_cni_dest }} on {{ inventory_hostname }}.
              All download methods failed. Check network connectivity and GitHub access.
              Manual installation may be required.
              {% endif %}
              
              Troubleshooting steps:
              1. Verify target directory permissions: ls -la $(dirname {{ flannel_cni_dest }})
              2. Check available disk space: df -h $(dirname {{ flannel_cni_dest }})
              {% if not (skip_flannel_download | default(false)) %}
              3. Test network connectivity: curl -I https://github.com
              {% endif %}
              4. Manual installation command:
                 ssh {{ inventory_hostname }} 'curl -fsSL https://github.com/flannel-io/flannel/releases/download/v0.25.2/flanneld-amd64 -o {{ flannel_cni_dest }} && chmod 755 {{ flannel_cni_dest }}'
              
              Common causes:
              - Directory permissions issues (ensure /opt/cni/bin is writable by root)
              - Insufficient disk space
              {% if not (skip_flannel_download | default(false)) %}
              - Network connectivity problems
              - SELinux or security policies blocking downloads
              {% else %}
              - Pre-downloaded binary was corrupted or removed
              - File system issues affecting the binary
              {% endif %}
          when: not flannel_binary_check.stat.exists

        - name: Ensure Flannel CNI plugin binary permissions are correct
          file:
            path: "{{ flannel_cni_dest }}"
            mode: '0755'
            owner: root
            group: root
          when: flannel_binary_check.stat.exists

        - name: Final validation of Flannel CNI binary installation
          block:
            - name: Verify Flannel binary is properly installed and executable
              shell: |
                if [ ! -f "{{ flannel_cni_dest }}" ]; then
                  echo "ERROR: Flannel binary not found at {{ flannel_cni_dest }}"
                  exit 1
                fi
                
                if [ ! -x "{{ flannel_cni_dest }}" ]; then
                  echo "ERROR: Flannel binary at {{ flannel_cni_dest }} is not executable"
                  exit 1
                fi
                
                size=$(stat -c%s "{{ flannel_cni_dest }}" 2>/dev/null || echo 0)
                if [ "$size" -lt 1000000 ]; then
                  echo "ERROR: Flannel binary at {{ flannel_cni_dest }} is too small ($size bytes)"
                  exit 1
                fi
                
                if ! file "{{ flannel_cni_dest }}" | grep -q "ELF.*executable"; then
                  echo "ERROR: Flannel binary at {{ flannel_cni_dest }} is not a valid ELF executable"
                  exit 1
                fi
                
                echo "SUCCESS: Flannel binary properly installed at {{ flannel_cni_dest }} ($size bytes)"
                echo "File info: $(file "{{ flannel_cni_dest }}")"
              register: flannel_final_validation
              
            - name: Display Flannel binary validation results
              debug:
                var: flannel_final_validation.stdout_lines

        - name: Protect Flannel binary before CNI plugins installation
          shell: |
            # Backup flannel binary if it exists to prevent corruption during CNI plugins extraction
            if [ -f "{{ flannel_cni_dest }}" ]; then
              cp "{{ flannel_cni_dest }}" "/tmp/flannel-backup-$$"
              echo "Flannel binary backed up to /tmp/flannel-backup-$$"
            else
              echo "WARNING: No Flannel binary found to backup"
              exit 1
            fi
          register: flannel_backup_result

        - name: Download and install additional CNI plugins on worker nodes
          unarchive:
            src: "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz"
            dest: /opt/cni/bin
            remote_src: yes
            owner: root
            group: root
            mode: '0755'
            creates: /opt/cni/bin/bridge
          register: cni_plugins_download
          retries: 3
          delay: 10
          until: cni_plugins_download is succeeded

        - name: Restore and verify Flannel binary after CNI plugins installation
          shell: |
            # Check if Flannel binary still exists and is valid
            if [ -f "{{ flannel_cni_dest }}" ] && [ -x "{{ flannel_cni_dest }}" ]; then
              size=$(stat -c%s "{{ flannel_cni_dest }}" 2>/dev/null || echo 0)
              if [ "$size" -gt 1000000 ]; then
                echo "Flannel binary survived CNI plugins installation ($size bytes)"
                # Clean up backup
                rm -f /tmp/flannel-backup-*
                exit 0
              fi
            fi
            
            echo "Flannel binary was corrupted or removed during CNI plugins installation, restoring from backup..."
            
            # Find and restore backup
            backup_file=$(ls -1 /tmp/flannel-backup-* 2>/dev/null | head -1)
            if [ -n "$backup_file" ] && [ -f "$backup_file" ]; then
              cp "$backup_file" "{{ flannel_cni_dest }}"
              chmod 755 "{{ flannel_cni_dest }}"
              chown root:root "{{ flannel_cni_dest }}"
              rm -f /tmp/flannel-backup-*
              echo "Flannel binary restored from backup"
            else
              echo "ERROR: No backup found to restore Flannel binary"
              exit 1
            fi
          register: flannel_restore_result
          changed_when: "'restored' in flannel_restore_result.stdout"

        - name: Display Flannel protection results
          debug:
            msg: |
              Flannel binary protection:
              - Backup: {{ flannel_backup_result.stdout | default('Failed') }}
              - Restore: {{ flannel_restore_result.stdout | default('Not needed') }}

        - name: Ensure /run/flannel directory exists on worker nodes
          file:
            path: /run/flannel
            state: directory
            owner: root
            group: root
            mode: '0755'

        - name: Create Flannel subnet configuration for worker nodes
          copy:
            content: |
              {
                "Network": "10.244.0.0/16",
                "EnableNFTables": false,
                "Backend": {
                  "Type": "vxlan"
                }
              }
            dest: /run/flannel/subnet.env
            owner: root
            group: root
            mode: '0644'

        - name: Create basic CNI configuration for worker nodes
          copy:
            content: |
              {
                "name": "cni0",
                "cniVersion": "0.3.1",
                "plugins": [
                  {
                    "type": "flannel",
                    "delegate": {
                      "hairpinMode": true,
                      "isDefaultGateway": true
                    }
                  },
                  {
                    "type": "portmap",
                    "capabilities": {
                      "portMappings": true
                    }
                  }
                ]
              }
            dest: /etc/cni/net.d/10-flannel.conflist
            owner: root
            group: root
            mode: '0644'

        - name: Verify CNI plugin installation on worker nodes
          stat:
            path: "{{ item }}"
          register: cni_files_check
          failed_when: not cni_files_check.stat.exists
          loop:
            - /opt/cni/bin/flannel
            - /opt/cni/bin/bridge
            - /opt/cni/bin/portmap
            - /etc/cni/net.d/10-flannel.conflist

        - name: Display CNI installation completion
          debug:
            msg: |
              ✓ CNI plugins and configuration installed on worker node {{ inventory_hostname }}
              - Flannel CNI plugin: /opt/cni/bin/flannel
              - Standard CNI plugins: /opt/cni/bin/
              - CNI configuration: /etc/cni/net.d/10-flannel.conflist
              
              Worker nodes now have the necessary CNI infrastructure without running Flannel daemon.

      when: not kubelet_conf.stat.exists

    - name: Pre-join validation for worker nodes
      block:
        - name: Verify containerd is running
          systemd:
            name: containerd
          register: containerd_status
          failed_when: containerd_status.status.ActiveState != 'active'
          
        - name: Verify kubelet binary exists and is executable
          stat:
            path: /usr/bin/kubelet
          register: kubelet_binary
          failed_when: not kubelet_binary.stat.exists or not kubelet_binary.stat.executable
          
        - name: Verify kubeadm binary exists and is executable
          stat:
            path: /usr/bin/kubeadm
          register: kubeadm_binary
          failed_when: not kubeadm_binary.stat.exists or not kubeadm_binary.stat.executable
          
        - name: Test kubeadm preflight checks (non-fatal; requires bootstrap token on control plane)
          command: kubeadm join phase preflight --dry-run || true
          register: preflight_check
          changed_when: false
          failed_when: false
          ignore_errors: yes
          
        - name: Display preflight results
          debug:
            msg: "Preflight check result: {{ preflight_check.rc }} - {{ preflight_check.stderr if preflight_check.rc != 0 else 'PASSED' }}"
            
        - name: Verify required kernel modules are loaded
          command: lsmod
          register: loaded_modules
          changed_when: false
          
        - name: Check for required kernel modules
          fail:
            msg: "Required kernel module {{ item }} is not loaded"
          when: item not in loaded_modules.stdout
          loop:
            - br_netfilter
            - overlay
          ignore_errors: yes
          
        - name: Check container runtime socket
          stat:
            path: /var/run/containerd/containerd.sock
          register: containerd_socket
          failed_when: not containerd_socket.stat.exists
          
        - name: Test container runtime connectivity
          command: crictl version
          register: crictl_test
          changed_when: false
          ignore_errors: yes
          
        - name: Display container runtime test
          debug:
            msg: "Container runtime test: {{ 'PASSED' if crictl_test.rc == 0 else 'FAILED - ' + crictl_test.stderr }}"
            
      when: not kubelet_conf.stat.exists

    - name: Validate join command freshness and content before use
      block:
        - name: Check if join command file exists and is recent
          stat:
            path: /tmp/kubeadm-join.sh
          register: join_file_stat
          delegate_to: localhost

        - name: Validate join command age and content
          shell: |
            if [ ! -f /tmp/kubeadm-join.sh ]; then
              echo "missing"
              exit 0
            fi
            
            # Check if file is older than 23 hours (tokens expire in 24h)
            if [ $(find /tmp/kubeadm-join.sh -mmin +1380 | wc -l) -gt 0 ]; then
              echo "expired"
              exit 0
            fi
            
            # Check if the join command contains required elements
            if grep -q "kubeadm join.*--token.*--discovery-token-ca-cert-hash" /tmp/kubeadm-join.sh; then
              echo "valid"
            else
              echo "malformed"
            fi
          register: join_command_validation
          delegate_to: localhost
          changed_when: false

        - name: Fail if join command is not available or expired
          fail:
            msg: |
              Join command issue detected: {{ join_command_validation.stdout }}
              - missing: Join command file not found
              - expired: Join command is older than 23 hours (tokens expire in 24h)
              - malformed: Join command doesn't contain required token/hash elements
              
              To resolve:
              1. Ensure the control plane is running
              2. Re-run the setup_cluster playbook to generate a fresh join command
              3. Or manually run: kubeadm token create --print-join-command on control plane
          when: join_command_validation.stdout != "valid"

      when: not kubelet_conf.stat.exists

    - name: Copy join command to worker nodes
      copy:
        src: /tmp/kubeadm-join.sh
        dest: /tmp/kubeadm-join.sh
        mode: '0755'
      when: not kubelet_conf.stat.exists

    - name: Backup existing /etc/kubernetes if ca.crt exists to avoid kubeadm preflight error
      shell: |
        if [ -f /etc/kubernetes/pki/ca.crt ]; then
          ts=$(date +%s)
          mv /etc/kubernetes /etc/kubernetes.backup.$ts || true
          echo "backed_up=/etc/kubernetes.backup.$ts"
        else
          echo "backed_up=none"
        fi
      register: backup_k8s
      changed_when: "'backed_up=' in backup_k8s.stdout and 'none' not in backup_k8s.stdout"
      failed_when: false
      when: not kubelet_conf.stat.exists

    - name: Prepare kubelet service for cluster join
      block:
        - name: Enable kubelet service before cluster join
          systemd:
            name: kubelet
            enabled: yes
            daemon_reload: yes
          
        - name: Stop kubelet service if running (to prevent port conflicts)
          systemd:
            name: kubelet
            state: stopped
          ignore_errors: yes

      when: not kubelet_conf.stat.exists

    - name: Clear kubelet config.yaml before join to allow kubeadm to create its own
      file:
        path: /var/lib/kubelet/config.yaml
        state: absent
      when: not kubelet_conf.stat.exists

    - name: Clean up any stale join state (post-spindown recovery)
      shell: |
        # Remove any leftover join artifacts that might conflict
        rm -f /etc/kubernetes/kubelet.conf.backup.* 2>/dev/null || true
        rm -f /var/lib/kubelet/kubeadm-flags.env 2>/dev/null || true
        
        # If this was triggered by invalid kubelet.conf, remove it to force fresh join
        if [ "{{ force_rejoin_reason | default('') }}" != "" ]; then
          echo "Removing invalid kubelet.conf (reason: {{ force_rejoin_reason | default('unknown') }})"
          rm -f /etc/kubernetes/kubelet.conf || true
          rm -f /etc/kubernetes/pki/ca.crt || true
        fi
        
        echo "Pre-join cleanup completed"
      when: not kubelet_conf.stat.exists
      changed_when: true
      
    - name: Join worker nodes to cluster with enhanced timeout handling
      block:
        - name: Pre-join kubelet readiness verification
          block:
            - name: Ensure kubelet service is stopped before join
              systemd:
                name: kubelet
                state: stopped
              ignore_errors: yes
              
            - name: Verify containerd socket is available
              wait_for:
                path: /var/run/containerd/containerd.sock
                timeout: 30
                
            - name: Optimize kubelet configuration for join
              shell: |
                # Clear any kubelet cache that might slow startup
                rm -rf /var/lib/kubelet/plugins_registry/* || true
                rm -rf /var/lib/kubelet/pods/* || true
                
                # Ensure kubelet state directory has optimal permissions
                chmod 755 /var/lib/kubelet || true
              register: kubelet_optimization
              ignore_errors: yes

              
            - name: Pre-warm container runtime (essential containers only)
              shell: |
                # Only pull pause container if not already present - avoid delays
                crictl images | grep pause || crictl pull registry.k8s.io/pause:3.9 || true
              register: runtime_precheck
              ignore_errors: yes
            
            - name: Verify CNI configuration exists before join attempt
              block:
                - name: Check if flannel CNI configuration exists
                  stat:
                    path: /etc/cni/net.d/10-flannel.conflist
                  register: cni_config_check
                
                - name: Recreate flannel CNI configuration if missing
                  copy:
                    content: |
                      {
                        "name": "cni0",
                        "cniVersion": "0.3.1",
                        "plugins": [
                          {
                            "type": "flannel",
                            "delegate": {
                              "hairpinMode": true,
                              "isDefaultGateway": true
                            }
                          },
                          {
                            "type": "portmap",
                            "capabilities": {
                              "portMappings": true
                            }
                          }
                        ]
                      }
                    dest: /etc/cni/net.d/10-flannel.conflist
                    owner: root
                    group: root
                    mode: '0644'
                  when: not cni_config_check.stat.exists
                
                - name: Verify containerd can read CNI configuration
                  shell: |
                    # Test that containerd can parse the CNI configuration
                    timeout 10s crictl info | grep -A5 '"Network"' || echo "CNI info not available yet"
                  register: cni_runtime_check
                  ignore_errors: yes
                  failed_when: false
              
          when: not kubelet_conf.stat.exists

        - name: Final CNI readiness check before join
          block:
            - name: Ensure CNI configuration is present and valid
              stat:
                path: /etc/cni/net.d/10-flannel.conflist
              register: final_cni_check
              failed_when: not final_cni_check.stat.exists
            
            - name: Check Flannel DaemonSet status on control plane
              shell: |
                # Check if we can reach the control plane and Flannel pods
                kubectl --kubeconfig=/tmp/admin.conf get daemonset,pods -n kube-flannel -l app=flannel -o wide 2>/dev/null || echo "Unable to check Flannel status - control plane may not be ready"
              register: flannel_status_check
              ignore_errors: yes
              failed_when: false
              delegate_to: "{{ groups['monitoring_nodes'][0] }}"
              run_once: true
            
            - name: Check CNI plugins availability on this node
              shell: |
                echo "CNI Plugin Status:"
                if [ -d /opt/cni/bin ]; then
                  ls -la /opt/cni/bin/ 2>/dev/null | grep -E "(flannel|bridge|portmap)" || echo "CNI plugins missing"
                  
                  echo ""
                  echo "Flannel Binary Validation:"
                  if [ -f /opt/cni/bin/flannel ]; then
                    if [ -x /opt/cni/bin/flannel ]; then
                      size=$(stat -c%s /opt/cni/bin/flannel 2>/dev/null || echo 0)
                      if [ "$size" -gt 1000000 ]; then
                        echo "✅ Flannel binary is valid: executable file ($size bytes)"
                      else
                        echo "❌ Flannel binary is too small: $size bytes (corrupted)"
                      fi
                    else
                      echo "❌ Flannel binary exists but is not executable"
                    fi
                  elif [ -d /opt/cni/bin/flannel ]; then
                    echo "❌ CRITICAL: /opt/cni/bin/flannel is a directory, not a binary file!"
                    echo "   This will cause CNI plugin failures. Requires cleanup and reinstallation."
                  else
                    echo "❌ Flannel binary not found at /opt/cni/bin/flannel"
                  fi
                else
                  echo "❌ CNI bin directory /opt/cni/bin does not exist"
                fi
                
                echo ""
                echo "CNI Configuration:"
                if [ -f /etc/cni/net.d/10-flannel.conflist ]; then
                  cat /etc/cni/net.d/10-flannel.conflist 2>/dev/null || echo "Cannot read flannel config"
                else
                  echo "Flannel config missing"
                fi
                echo ""
                echo "CNI Network Interfaces:"
                ip link show | grep -E "(cni0|cbr0|flannel)" || echo "No CNI interfaces (expected for worker nodes)"
              register: node_cni_status
              ignore_errors: yes
              failed_when: false
            
            - name: Check containerd CNI configuration
              shell: |
                echo "Containerd CNI Status:"
                if [ -f /etc/containerd/config.toml ]; then
                  grep -n "cni" /etc/containerd/config.toml 2>/dev/null || echo "No CNI config in containerd.toml"
                else
                  echo "containerd config.toml missing"
                fi
                echo ""
                echo "Containerd Socket:"
                ls -l /run/containerd/containerd.sock 2>/dev/null || echo "containerd socket missing"
              register: containerd_cni_status
              ignore_errors: yes
              failed_when: false
            
            - name: Analyze CNI runtime status
              set_fact:
                cni_has_real_network: "{{ 'flannel' in (cni_runtime_check.stdout | default('')) or 'bridge' in (cni_runtime_check.stdout | default('')) }}"
                cni_only_loopback: "{{ 'loopback' in (cni_runtime_check.stdout | default('')) and 'flannel' not in (cni_runtime_check.stdout | default('')) }}"
            
            - name: Check Flannel binary status for enhanced remediation
              shell: |
                if [ -f /opt/cni/bin/flannel ] && [ -x /opt/cni/bin/flannel ]; then
                  size=$(stat -c%s /opt/cni/bin/flannel 2>/dev/null || echo 0)
                  if [ "$size" -gt 1000000 ]; then
                    echo "flannel_binary_ok"
                  else
                    echo "flannel_binary_corrupted"
                  fi
                elif [ -d /opt/cni/bin/flannel ]; then
                  echo "flannel_is_directory"
                else
                  echo "flannel_binary_missing"
                fi
              register: flannel_binary_status_check
              ignore_errors: yes
              failed_when: false
            
            - name: Set enhanced CNI status facts
              set_fact:
                flannel_binary_ok: "{{ flannel_binary_status_check.stdout == 'flannel_binary_ok' }}"
                flannel_binary_missing: "{{ flannel_binary_status_check.stdout in ['flannel_binary_missing', 'flannel_binary_corrupted', 'flannel_is_directory'] }}"
                flannel_is_directory: "{{ flannel_binary_status_check.stdout == 'flannel_is_directory' }}"
                needs_flannel_reinstall: "{{ cni_only_loopback | bool or flannel_binary_status_check.stdout in ['flannel_binary_missing', 'flannel_binary_corrupted', 'flannel_is_directory'] }}"

            - name: Apply Flannel remediation if needed
              block:
                - name: Fix Flannel binary issues on worker node if needed
                  shell: |
                    echo "Flannel binary status: {{ flannel_binary_status_check.stdout }}"
                    
                    # Remove invalid flannel state
                    if [ -d /opt/cni/bin/flannel ]; then
                      echo "Removing flannel directory..."
                      rm -rf /opt/cni/bin/flannel
                    elif [ -f /opt/cni/bin/flannel ] && [ ! -x /opt/cni/bin/flannel ]; then
                      echo "Removing non-executable flannel file..."
                      rm -f /opt/cni/bin/flannel
                    fi
                    
                    # Download flannel binary if missing or corrupted
                    if [ ! -f /opt/cni/bin/flannel ] || [ ! -x /opt/cni/bin/flannel ]; then
                      echo "Downloading Flannel binary..."
                      if curl -fsSL --connect-timeout 30 --max-time 300 --retry 2 \
                        "https://github.com/flannel-io/flannel/releases/download/v0.25.2/flanneld-amd64" \
                        -o /opt/cni/bin/flannel; then
                        chmod 755 /opt/cni/bin/flannel
                        chown root:root /opt/cni/bin/flannel
                        echo "Flannel binary downloaded successfully"
                      else
                        echo "ERROR: Failed to download Flannel binary"
                        exit 1
                      fi
                    fi
                    
                    # Verify the binary is now OK
                    if [ -f /opt/cni/bin/flannel ] && [ -x /opt/cni/bin/flannel ]; then
                      size=$(stat -c%s /opt/cni/bin/flannel 2>/dev/null || echo 0)
                      echo "Flannel binary verification: $size bytes"
                      if [ "$size" -gt 1000000 ]; then
                        echo "SUCCESS: Flannel binary is now properly installed"
                      else
                        echo "ERROR: Flannel binary is still too small after download"
                        exit 1
                      fi
                    else
                      echo "ERROR: Flannel binary is still not properly installed"
                      exit 1
                    fi
                  when: flannel_binary_missing | bool
                  register: flannel_binary_remediation
                  ignore_errors: yes

                - name: Reapply Flannel on control plane if CNI only shows loopback
                  shell: |
                    echo "Reapplying Flannel CNI to resolve loopback-only issue..."
                    kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /tmp/kube-flannel-allnodes.yml
                    echo "Waiting for Flannel DaemonSet to become ready..."
                    kubectl --kubeconfig=/etc/kubernetes/admin.conf rollout status daemonset/kube-flannel-ds -n kube-flannel --timeout=120s || echo "Flannel rollout may still be in progress"
                  delegate_to: "{{ groups['monitoring_nodes'][0] }}"
                  run_once: true
                  ignore_errors: yes
                  
                - name: Wait for CNI to stabilize after Flannel reapplication
                  wait_for:
                    timeout: 30
                  
                - name: Re-check CNI runtime after remediation
                  shell: |
                    timeout 10s crictl info | grep -A5 '"Network"' || echo "CNI info still not available"
                  register: cni_runtime_recheck
                  ignore_errors: yes
                  failed_when: false
              when: needs_flannel_reinstall | bool
            
            - name: Display comprehensive CNI readiness status
              debug:
                msg: |
                  === CNI Configuration Status ===
                  - Flannel config exists: {{ final_cni_check.stat.exists }}
                  - CNI runtime shows real network: {{ cni_has_real_network | default(false) }}
                  - CNI only shows loopback: {{ cni_only_loopback | default(true) }}
                  - Flannel binary status: {{ flannel_binary_status_check.stdout | default('not checked') }}
                  
                  === Control Plane Flannel Status ===
                  {{ flannel_status_check.stdout | default('Not checked') }}
                  
                  === Node CNI Infrastructure ===
                  {{ node_cni_status.stdout | default('Not checked') }}
                  
                  === Containerd CNI Status ===
                  {{ containerd_cni_status.stdout | default('Not checked') }}
                  
                  === CNI Runtime Check ===
                  {% if cni_runtime_recheck is defined %}
                  After remediation: {{ cni_runtime_recheck.stdout | default('No output') }}
                  {% else %}
                  Initial check: {{ cni_runtime_check.stdout | default('not performed') }}
                  {% endif %}
                  
                  {% if flannel_binary_remediation is defined and flannel_binary_remediation.changed %}
                  === Flannel Binary Remediation ===
                  {{ flannel_binary_remediation.stdout | default('Remediation attempted') }}
                  
                  {% endif %}
                  {% if not (flannel_binary_ok | default(false)) and (flannel_is_directory | default(false)) %}
                  🚨 CRITICAL ISSUE: Flannel binary is a directory, not an executable file!
                  
                  This is the root cause of the kubelet join timeout. The CNI plugin cannot execute
                  because /opt/cni/bin/flannel is a directory instead of the Flannel binary.
                  
                  Resolution: The remediation above should have fixed this. If join still fails:
                  1. SSH to the worker node
                  2. Run: rm -rf /opt/cni/bin/flannel
                  3. Run: curl -fsSL https://github.com/flannel-io/flannel/releases/download/v0.25.2/flanneld-amd64 -o /opt/cni/bin/flannel
                  4. Run: chmod 755 /opt/cni/bin/flannel
                  5. Retry kubelet join
                  
                  {% elif cni_only_loopback | default(true) or not (flannel_binary_ok | default(false)) %}
                  ⚠️  WARNING: CNI issues detected:
                  - CNI runtime only shows loopback: {{ cni_only_loopback | default('unknown') }}
                  - Flannel binary OK: {{ flannel_binary_ok | default('unknown') }}
                  
                  This means:
                  - No real network plugin (Flannel) is active
                  - Pod networking may not function properly  
                  - kubelet join may succeed but pods will use only loopback
                  
                  Recommended actions:
                  1. Verify Flannel DaemonSet is running on control plane
                  2. Check Flannel pod logs for errors
                  3. Ensure worker nodes have CNI plugins in /opt/cni/bin/
                  4. Verify CNI configuration in /etc/cni/net.d/
                  {% else %}
                  ✅ CNI runtime shows active network plugin - Ready for kubelet join.
                  {% endif %}
          when: not kubelet_conf.stat.exists

        - name: Pre-join certificate and connectivity validation
          block:
            - name: Test API server certificate accessibility from worker node
              shell: |
                # Extract the join target from join command
                JOIN_TARGET=$(grep -E "^kubeadm join" /tmp/kubeadm-join.sh | head -1 | awk '{print $3}' | sed 's/[[:space:]]*$//')
                API_SERVER=$(echo "$JOIN_TARGET" | sed 's/:.*$//')
                
                echo "Testing connectivity to API server: $API_SERVER"
                
                # Test basic connectivity
                if ! ping -c 2 "$API_SERVER" >/dev/null 2>&1; then
                  echo "ERROR: Cannot ping API server at $API_SERVER"
                  exit 1
                fi
                
                # Test API server port accessibility
                if ! timeout 10 bash -c "</dev/tcp/$API_SERVER/6443" 2>/dev/null; then
                  echo "ERROR: API server port 6443 not accessible at $API_SERVER"
                  exit 1
                fi
                
                # Test TLS connection to API server (should get 403 without creds)
                HTTP_CODE=$(curl -k -s -o /dev/null -w "%{http_code}" "https://$JOIN_TARGET/healthz" || echo "000")
                if [[ "$HTTP_CODE" != "401" && "$HTTP_CODE" != "403" ]]; then
                  echo "WARNING: Unexpected HTTP response from API server: $HTTP_CODE"
                  echo "Expected 401/403 (auth required), got $HTTP_CODE"
                else
                  echo "✓ API server is responding correctly at $JOIN_TARGET"
                fi
                
                # Validate the CA cert hash in join command is accessible
                CA_HASH=$(grep -o 'sha256:[a-f0-9]\{64\}' /tmp/kubeadm-join.sh | head -1)
                if [ -z "$CA_HASH" ]; then
                  echo "ERROR: No valid CA certificate hash found in join command"
                  exit 1
                else
                  echo "✓ CA certificate hash found in join command: $CA_HASH"
                fi
                
                echo "✓ Pre-join validation successful - API server is accessible"
              register: pre_join_validation
              failed_when: pre_join_validation.rc != 0
              when: not kubelet_conf.stat.exists

            - name: Display pre-join validation results
              debug:
                var: pre_join_validation.stdout_lines
              when: pre_join_validation is defined
          
        - name: Attempt to join cluster (primary attempt with reasonable timeout)
          shell: |
            # Join with standard timeout - performance issues resolved
            timeout 600 /tmp/kubeadm-join.sh \
              --v=5 \
              --node-name={{ inventory_hostname }} \
              --skip-phases=addon/kube-proxy
          register: join_result_1
          failed_when: false
          when: not kubelet_conf.stat.exists
          
        - name: Wait and retry if first attempt failed or timed out
          block:
            - name: Analyze first attempt failure
              debug:
                msg: |
                  First join attempt failed:
                  Return Code: {{ join_result_1.rc | default('unknown') }}
                  Stderr: {{ join_result_1.stderr | default('no stderr') | truncate(500) }}
                  Stdout: {{ join_result_1.stdout | default('no stdout') | truncate(300) }}
                  
            - name: Detect kubelet-start specific failures
              block:
                - name: Check if failure is kubelet-start phase timeout
                  set_fact:
                    is_kubelet_start_timeout: "{{ 'kubelet-start' in (join_result_1.stderr | default('')) and 'timed out waiting for the condition' in (join_result_1.stderr | default('')) }}"
                    
                - name: Apply kubelet-start specific recovery if detected
                  shell: |
                    echo "Detected kubelet-start phase timeout, applying recovery"
                    
                    # Stop kubelet service
                    systemctl stop kubelet || true
                    
                    # Remove problematic kubelet state only
                    rm -rf /var/lib/kubelet/pki/* || true
                    
                    # Quick systemd reload
                    systemctl daemon-reload
                  when: is_kubelet_start_timeout | default(false)
                  register: kubelet_start_recovery
                  ignore_errors: yes
                  
            - name: Check kubelet status for diagnostic information
              shell: |
                echo "=== Kubelet Status ==="
                systemctl status kubelet --no-pager -l || true
                echo "=== Kubelet Logs (last 10 lines) ==="
                journalctl -u kubelet --no-pager -l -n 10 || true
                echo "=== Kubelet Health Endpoint ==="
                curl -s http://localhost:10248/healthz || echo "Health endpoint not available"
              register: kubelet_diagnostic
              ignore_errors: yes
              
            - name: Display kubelet diagnostic information  
              debug:
                msg: "{{ kubelet_diagnostic.stdout }}"
              when: kubelet_diagnostic.stdout is defined
                  
            - name: Wait before retry
              pause:
                seconds: 60
                
            - name: Enhanced kubelet service preparation for retry
              block:
                - name: Clear problematic kubelet state for retry
                  shell: |
                    # Stop kubelet service
                    systemctl stop kubelet || true
                    
                    # Clear only problematic kubelet state - keep essential files
                    rm -f /var/lib/kubelet/kubeadm-flags.env || true
                    rm -f /var/lib/kubelet/config.yaml || true
                    
                    # Ensure kubelet directories have correct permissions
                    chmod -R 755 /var/lib/kubelet || true
                    chown -R root:root /var/lib/kubelet || true
                  ignore_errors: yes
                  
                - name: Reset kubeadm state for clean retry
                  shell: |
                    # Reset kubeadm state without removing containerd setup
                    kubeadm reset -f || true
                    # Only remove kubernetes-specific configs, preserve containerd
                    rm -rf /etc/kubernetes/kubelet.conf /etc/kubernetes/pki/ || true
                  ignore_errors: yes
              
            - name: Verify containerd is ready before retry
              wait_for:
                path: /var/run/containerd/containerd.sock
                timeout: 30

            - name: Ensure /etc/sysconfig directory exists for retry
              file:
                path: /etc/sysconfig
                state: directory
                mode: '0755'
                
            - name: Recreate clean sysconfig/kubelet for retry attempt
              copy:
                dest: /etc/sysconfig/kubelet
                content: |
                  # Kubelet environment - no deprecated flags like --network-plugin
                  KUBELET_EXTRA_ARGS=--cgroup-driver=systemd --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock
                mode: '0644'
                
            - name: Update systemd drop-in to clean format for retry attempt
              copy:
                dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
                content: |
                  # Note: This dropin only works with kubeadm and kubelet v1.11+
                  # Join-compatible kubelet configuration - lets kubeadm manage all configuration during join
                  [Service]
                  Environment="KUBELET_KUBECONFIG_ARGS="
                  Environment="KUBELET_CONFIG_ARGS="
                  # This is a file that "kubeadm init" and "kubeadm join" generates at runtime, 
                  # populating the KUBELET_KUBEADM_ARGS variable dynamically
                  EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
                  # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
                  # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
                  EnvironmentFile=-/etc/sysconfig/kubelet
                  ExecStart=
                  ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
                mode: '0644'
                
            - name: Reload systemd daemon after updating drop-in
              systemd:
                daemon_reload: yes
                
            - name: Attempt to join cluster (retry with reasonable timeout)
              shell: |
                # Retry join with reasonable timeout - performance optimized
                timeout 900 /tmp/kubeadm-join.sh \
                  --v=5 \
                  --node-name={{ inventory_hostname }} \
                  --skip-phases=addon/kube-proxy \
                  --ignore-preflight-errors=DirAvailable--var-lib-etcd
              register: join_result_2
              failed_when: false
              
          when: join_result_1 is defined and (join_result_1.rc | default(0)) != 0
          
        - name: Set final join result
          set_fact:
            join_result: "{{ join_result_2 if join_result_2 is defined else join_result_1 }}"
            
      when: not kubelet_conf.stat.exists

    - name: Enhanced error collection and diagnostics
      block:
        - name: Extract join target from script (for connectivity checks) - robust parsing
          shell: |
            # More robust extraction that handles comments and variations in join script format
            if [ ! -f /tmp/kubeadm-join.sh ]; then
              echo "ERROR: Join script not found"
              exit 1
            fi
            
            # Extract the API server address from the join command, handling various formats
            grep -E "^kubeadm join" /tmp/kubeadm-join.sh | head -1 | awk '{print $3}' | sed 's/[[:space:]]*$//'
          register: join_target
          changed_when: false
          failed_when: false
          
        - name: Test API server reachability from node
          uri:
            url: "https://{{ join_target.stdout }}/healthz"
            validate_certs: no
            timeout: 10
          register: api_health
          failed_when: false
          changed_when: false
          
        - name: Test basic network connectivity to control plane
          shell: |
            # Extract IP from join target (remove port if present)
            TARGET_IP=$(echo "{{ join_target.stdout }}" | sed 's/:.*$//')
            if [ -n "$TARGET_IP" ] && [ "$TARGET_IP" != "ERROR:" ]; then
              ping -c 3 "$TARGET_IP"
            else
              echo "Cannot ping - invalid target IP: {{ join_target.stdout }}"
              exit 1
            fi
          register: ping_test
          failed_when: false
          changed_when: false
          
        - name: Collect enhanced system diagnostics
          shell: |
            echo "=== System Information ==="
            uname -a
            echo "=== Container Runtime Status ==="
            systemctl status containerd --no-pager || true
            crictl version || true
            crictl info || true
            echo "=== Kubelet Status ==="
            systemctl status kubelet --no-pager || true
            echo "=== CNI Configuration Status ==="
            echo "CNI Config Files:"
            ls -la /etc/cni/net.d/ || true
            echo "Flannel Config Content:"
            cat /etc/cni/net.d/10-flannel.conflist 2>/dev/null || echo "Flannel config missing"
            echo "CNI Plugins:"
            ls -la /opt/cni/bin/ | head -10 || true
            echo "=== Network Configuration ==="
            ip addr show || true
            ip route show || true
            echo "=== Firewall Status ==="
            if command -v firewall-cmd >/dev/null; then
              firewall-cmd --list-all || true
            fi
            if command -v iptables >/dev/null; then
              iptables -L -n || true
            fi
            echo "=== SELinux Status ==="
            if command -v getenforce >/dev/null; then
              getenforce || true
            fi
            echo "=== Kernel Modules ==="
            lsmod | grep -E "(br_netfilter|overlay)" || true
            echo "=== Disk Space ==="
            df -h || true
            echo "=== Memory ==="
            free -m || true
          register: system_diagnostics
          changed_when: false
          failed_when: false
          
        - name: Save enhanced diagnostics to file
          copy:
            content: |
              Join Result: {{ join_result }}
              API Health: {{ api_health }}
              Ping Test: {{ ping_test }}
              System Diagnostics:
              {{ system_diagnostics.stdout }}
            dest: /tmp/join-diagnostics.log
            
        - name: Save kubelet journal to a file for fetch
          shell: "journalctl -u kubelet -n 500 --no-pager > /tmp/kubelet-journal.log || true"
          changed_when: false
          
        - name: Save containerd journal to a file for fetch
          shell: "journalctl -u containerd -n 500 --no-pager > /tmp/containerd-journal.log || true"
          changed_when: false
          
        - name: Save kubeadm audit logs if they exist
          shell: "find /var/log -name '*kubeadm*' -exec cat {} \\; > /tmp/kubeadm-audit.log 2>/dev/null || echo 'No kubeadm logs found'"
          changed_when: false
          
        - name: Ensure debug logs directory exists locally
          file:
            path: ./debug_logs
            state: directory
          delegate_to: localhost
          become: no
          
        - name: Fetch comprehensive diagnostic logs
          fetch:
            src: "{{ item.src }}"
            dest: "{{ item.dest }}"
            flat: yes
          loop:
            - { src: "/tmp/join-diagnostics.log", dest: "./debug_logs/{{ inventory_hostname }}-join-diagnostics.log" }
            - { src: "/tmp/kubelet-journal.log", dest: "./debug_logs/{{ inventory_hostname }}-kubelet-journal.log" }
            - { src: "/tmp/containerd-journal.log", dest: "./debug_logs/{{ inventory_hostname }}-containerd-journal.log" }
            - { src: "/tmp/kubeadm-audit.log", dest: "./debug_logs/{{ inventory_hostname }}-kubeadm-audit.log" }
          ignore_errors: yes
      when: not kubelet_conf.stat.exists and (join_result is defined and (join_result.rc | default(0)) != 0)

    - name: Post-join verification
      block:
        - name: Verify node joined successfully
          stat:
            path: /etc/kubernetes/kubelet.conf
          register: final_kubelet_conf

        - name: Wait for kubelet to be ready
          systemd:
            name: kubelet
          register: kubelet_final_status
          retries: 12
          delay: 15
          until: kubelet_final_status.status.ActiveState == 'active'
          
        - name: Display join success message
          debug:
            msg: "✓ Node {{ inventory_hostname }} successfully joined the Kubernetes cluster!"
          when: final_kubelet_conf.stat.exists
          
        - name: Display join failure message
          fail:
            msg: "✗ Node {{ inventory_hostname }} failed to join the cluster. Check debug logs for details."
          when: not final_kubelet_conf.stat.exists and (join_result is defined and (join_result.rc | default(0)) != 0)
          
      when: not kubelet_conf.stat.exists

    - name: Verify worker node kubelet configuration after join
      block:
        - name: Check if kubelet config file exists after join
          stat:
            path: /var/lib/kubelet/config.yaml
          register: worker_kubelet_config_check

        - name: Check if worker kubelet kubeconfig exists
          stat:
            path: /etc/kubernetes/kubelet.conf
          register: worker_kubelet_kubeconfig_check

        - name: Verify kubelet is using kubeadm-generated configuration after join
          block:
            - name: Check that kubeadm created the necessary files
              stat:
                path: "{{ item }}"
              register: kubeadm_files
              failed_when: not kubeadm_files.stat.exists
              loop:
                - /etc/kubernetes/kubelet.conf
                - /var/lib/kubelet/config.yaml
                - /var/lib/kubelet/kubeadm-flags.env
              
            - name: Update kubelet systemd config after successful join
              copy:
                dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
                content: |
                  # Note: This dropin only works with kubeadm and kubelet v1.11+
                  [Service]
                  Environment="KUBELET_KUBECONFIG_ARGS=--kubeconfig=/etc/kubernetes/kubelet.conf"
                  Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
                  # This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
                  EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
                  # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
                  # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
                  EnvironmentFile=-/etc/sysconfig/kubelet
                  ExecStart=
                  ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
                  # Post-join config - no bootstrap config dependency
              register: post_join_config_update
              when: worker_kubelet_kubeconfig_check.stat.exists

        - name: Reload systemd daemon after kubelet config update
          systemd:
            daemon_reload: yes
          when: post_join_config_update is changed

        - name: Restart and enable kubelet on worker (only if join was successful)
          systemd:
            name: kubelet
            state: restarted
            enabled: yes
          when: "'kubelet.service' in ansible_facts.services"
          register: worker_kubelet_restart
          ignore_errors: yes

        - name: Handle worker kubelet startup failure (simplified recovery)
          block:
            - name: Check if worker kubelet kubeconfig is missing (post-spindown recovery)
              stat:
                path: /etc/kubernetes/kubelet.conf
              register: worker_kubelet_kubeconfig_missing

            - name: Display worker recovery status
              debug:
                msg: |
                  Worker kubelet recovery needed: {{ worker_kubelet_kubeconfig_missing.stat.exists == false }}
                  {% if worker_kubelet_kubeconfig_missing.stat.exists == false %}
                  Action required: Worker node needs to rejoin the cluster
                  The node must be reset and rejoined using kubeadm join command
                  {% endif %}

            - name: Set rejoin flag if kubeconfig is missing
              set_fact:
                worker_needs_rejoin: true
              when: worker_kubelet_kubeconfig_missing.stat.exists == false

          when: worker_kubelet_restart is defined and worker_kubelet_restart.failed

        - name: Worker kubelet diagnostics if restart failed
          shell: |
            echo "=== Worker Node Kubelet Diagnostics ==="
            echo "System: $(uname -a)"
            echo "Kubelet status:"
            systemctl status kubelet --no-pager || true
            echo "Recent kubelet logs:"
            journalctl -u kubelet -n 20 --no-pager || true
            echo "Container runtime status:"
            systemctl status containerd --no-pager || true
            echo "Kubelet configuration files:"
            ls -la /var/lib/kubelet/ || true
            ls -la /etc/kubernetes/ || true
          register: worker_diagnostics
          ignore_errors: yes
          when: (worker_kubelet_restart is defined and (worker_kubelet_restart.failed | default(false))) or (worker_kubelet_restart_retry is defined and (worker_kubelet_restart_retry.failed | default(false)))

        - name: Display worker kubelet diagnostics
          debug:
            var: worker_diagnostics.stdout_lines
          when: (worker_kubelet_restart is defined and (worker_kubelet_restart.failed | default(false))) or (worker_kubelet_restart_retry is defined and (worker_kubelet_restart_retry.failed | default(false)))

      when: kubelet_conf.stat.exists or (join_result is defined and (join_result.rc | default(0)) == 0)

    - name: Setup static manifests directory on worker nodes (for cert-manager compatibility)
      block:
        - name: Create /etc/kubernetes/manifests directory on worker nodes
          file:
            path: /etc/kubernetes/manifests
            state: directory
            owner: root
            group: root
            mode: '0755'
          
        - name: Display manifests directory setup completion
          debug:
            msg: "✓ Created /etc/kubernetes/manifests directory on worker node {{ inventory_hostname }} for cert-manager compatibility"
            
      when: kubelet_conf.stat.exists or (join_result is defined and (join_result.rc | default(0)) == 0)

    - name: Remove join command file
      file:
        path: /tmp/kubeadm-join.sh
        state: absent
