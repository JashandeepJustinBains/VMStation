---
# Subsite 03: Monitoring stack deploy (scaffold)
# This playbook should deploy Prometheus, Grafana, Loki, Promtail and exporters.
# It performs pre-checks and prints remediation steps if resources/permissions are missing.
- hosts: monitoring_nodes
  gather_facts: false
  connection: local
  vars:
    storage_paths: "{{ storage_paths | default({}) }}"
  tasks:
    - name: Check if kubectl is available
      ansible.builtin.command:
        cmd: kubectl version --client
      register: kubectl_check
      failed_when: false
      changed_when: false
      check_mode: false

    - name: Fail with remediation if kubectl not available
      ansible.builtin.fail:
        msg: |
          kubectl is required for Kubernetes operations. Install it with:
          
          For Ubuntu/Debian:
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          
          For RHEL/CentOS/Fedora:
          sudo dnf install -y kubectl
          
          Or download from: https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/
      when: kubectl_check.rc != 0

    - name: Check for monitoring namespace
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Namespace
        name: "{{ monitoring_namespace | default('monitoring') }}"
      register: ns_info
      failed_when: false

    - name: Explain how to create monitoring namespace if missing
      ansible.builtin.debug:
        msg: |
          Monitoring namespace '{{ monitoring_namespace | default('monitoring') }}' not found. To create it, run:
          
          kubectl create namespace {{ monitoring_namespace | default('monitoring') }}
          
          Or apply this manifest:
          ---
          apiVersion: v1
          kind: Namespace
          metadata:
            name: {{ monitoring_namespace | default('monitoring') }}
            labels:
              name: {{ monitoring_namespace | default('monitoring') }}
      when: ns_info.resources is defined and (ns_info.resources | length == 0)

    - name: Check for Prometheus Operator CRDs (ServiceMonitor)
      kubernetes.core.k8s_info:
        api_version: apiextensions.k8s.io/v1
        kind: CustomResourceDefinition
        name: servicemonitors.monitoring.coreos.com
      register: servicemonitor_crd
      failed_when: false

    - name: Check for other Prometheus Operator CRDs
      kubernetes.core.k8s_info:
        api_version: apiextensions.k8s.io/v1
        kind: CustomResourceDefinition
        name: "{{ item }}"
      register: prometheus_crds
      failed_when: false
      loop:
        - prometheuses.monitoring.coreos.com
        - prometheusrules.monitoring.coreos.com
        - alertmanagers.monitoring.coreos.com
        - podmonitors.monitoring.coreos.com

    - name: Print install instructions for Prometheus operator if CRDs missing
      ansible.builtin.debug:
        msg: |
          Prometheus Operator CRDs not detected. To install using Helm:
          
          # Add Helm repositories
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update
          
          # Install kube-prometheus-stack (includes Prometheus, Grafana, AlertManager)
          helm install kube-prom-stack prometheus-community/kube-prometheus-stack \
            -n {{ monitoring_namespace | default('monitoring') }} --create-namespace \
            --set prometheus.service.type=NodePort \
            --set prometheus.service.nodePort=30090 \
            --set grafana.service.type=NodePort \
            --set grafana.service.nodePort=30300 \
            --set alertmanager.service.type=NodePort \
            --set alertmanager.service.nodePort=30903
          
          # Install Loki stack separately
          helm install loki-stack grafana/loki-stack \
            -n {{ monitoring_namespace | default('monitoring') }} \
            --set loki.service.type=NodePort \
            --set loki.service.nodePort=31100
      when: servicemonitor_crd.resources is defined and (servicemonitor_crd.resources | length == 0)

    - name: Check if Helm is available
      ansible.builtin.command:
        cmd: helm version --short
      register: helm_check
      failed_when: false
      changed_when: false
      check_mode: false

    - name: Suggest Helm installation if not available
      ansible.builtin.debug:
        msg: |
          Helm is recommended for managing monitoring stack. Install it with:
          
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          
          Or download from: https://github.com/helm/helm/releases
      when: helm_check.rc != 0

- hosts: all
  gather_facts: true
  vars:
    storage_paths: "{{ storage_paths | default({}) }}"
  tasks:
    - name: Check if monitoring data directory exists
      ansible.builtin.stat:
        path: "{{ monit_root | default('/srv/monitoring_data') }}"
      register: monit_dir_check
      become: true

    - name: Provide remediation for missing monitoring data directory
      ansible.builtin.debug:
        msg: |
          Monitoring data directory {{ monit_root | default('/srv/monitoring_data') }} does not exist. To create it, run on each host:
          
          sudo mkdir -p {{ monit_root | default('/srv/monitoring_data') }}
          sudo chown root:root {{ monit_root | default('/srv/monitoring_data') }}
          sudo chmod 755 {{ monit_root | default('/srv/monitoring_data') }}
          
          For monitoring nodes, you may also need subdirectories:
          sudo mkdir -p {{ monit_root | default('/srv/monitoring_data') }}/grafana {{ monit_root | default('/srv/monitoring_data') }}/prometheus {{ monit_root | default('/srv/monitoring_data') }}/loki
          sudo chmod 755 {{ monit_root | default('/srv/monitoring_data') }}/grafana {{ monit_root | default('/srv/monitoring_data') }}/prometheus {{ monit_root | default('/srv/monitoring_data') }}/loki
          
          Why this is needed: Persistent volumes and monitoring services need this directory for data storage.
      when: not monit_dir_check.stat.exists

    - name: Check monitoring data directory permissions
      ansible.builtin.stat:
        path: "{{ monit_root | default('/srv/monitoring_data') }}"
      register: monit_perms_check
      become: true
      when: monit_dir_check.stat.exists

    - name: Report on monitoring directory permissions
      ansible.builtin.debug:
        msg: |
          Monitoring directory permissions:
          Path: {{ monit_root | default('/srv/monitoring_data') }}
          Owner: {{ monit_perms_check.stat.pw_name | default('unknown') }}:{{ monit_perms_check.stat.gr_name | default('unknown') }}
          Mode: {{ monit_perms_check.stat.mode | default('unknown') }}
          
          If containers fail to write, you may need to adjust permissions:
          sudo chown -R 65534:65534 {{ monit_root | default('/srv/monitoring_data') }}/prometheus  # nobody user for Prometheus
          sudo chown -R 472:472 {{ monit_root | default('/srv/monitoring_data') }}/grafana         # grafana user
          sudo chmod -R 755 {{ monit_root | default('/srv/monitoring_data') }}
      when: monit_dir_check.stat.exists

    - name: Check node exporter port availability
      ansible.builtin.wait_for:
        port: 9100
        host: "{{ ansible_default_ipv4.address }}"
        timeout: 3
        state: started
      register: node_exporter_check
      failed_when: false
      changed_when: false

    - name: Provide node exporter installation instructions if not running
      ansible.builtin.debug:
        msg: |
          Node Exporter is not running on port 9100. To install it:
          
          # Manual installation
          wget https://github.com/prometheus/node_exporter/releases/download/v1.6.1/node_exporter-1.6.1.linux-amd64.tar.gz
          tar xvfz node_exporter-1.6.1.linux-amd64.tar.gz
          sudo cp node_exporter-1.6.1.linux-amd64/node_exporter /usr/local/bin/
          sudo chmod +x /usr/local/bin/node_exporter
          
          # Create systemd service
          sudo tee /etc/systemd/system/node_exporter.service > /dev/null <<EOF
          [Unit]
          Description=Node Exporter
          After=network.target
          
          [Service]
          User=nobody
          Group=nobody
          Type=simple
          ExecStart=/usr/local/bin/node_exporter
          
          [Install]
          WantedBy=multi-user.target
          EOF
          
          # Enable and start
          sudo systemctl daemon-reload
          sudo systemctl enable node_exporter
          sudo systemctl start node_exporter
          
          Why this is needed: Node Exporter provides host-level metrics for Prometheus.
      when: node_exporter_check.failed | default(false)

    - name: Check SELinux contexts for monitoring directories
      ansible.builtin.command:
        cmd: ls -Zd {{ monit_root | default('/srv/monitoring_data') }}
      register: selinux_context_check
      failed_when: false
      changed_when: false
      become: true
      when: monit_dir_check.stat.exists

    - name: Provide SELinux remediation if needed
      ansible.builtin.debug:
        msg: |
          Current SELinux context for {{ monit_root | default('/srv/monitoring_data') }}: {{ selinux_context_check.stdout | default('N/A') }}
          
          If containers fail due to SELinux denials, set appropriate contexts:
          
          sudo setsebool -P container_manage_cgroup on
          sudo semanage fcontext -a -t container_file_t "{{ monit_root | default('/srv/monitoring_data') }}(/.*)?"
          sudo restorecon -Rv {{ monit_root | default('/srv/monitoring_data') }}
          
          Or set permissive mode for containers:
          sudo setsebool -P container_use_cephfs on
          
    - name: Optional; gather PV hostPath info and prepare safe remediation commands
      block:
        - name: Set defaults for remediation flags
          ansible.builtin.set_fact:
            monitoring_apply_fixes: "{{ monitoring_apply_fixes | default(false) }}"
            monitoring_allow_fs_change: "{{ monitoring_allow_fs_change | default(false) }}"
            monitoring_allow_helm_upgrade: "{{ monitoring_allow_helm_upgrade | default(false) }}"

        - name: Get grafana PVC bound PV name (read-only)
          ansible.builtin.command:
            cmd: kubectl -n monitoring get pvc kube-prometheus-stack-grafana -o jsonpath='{.spec.volumeName}'
          register: grafana_pvname
          failed_when: false
          changed_when: false

        - name: Get loki PVC bound PV name (read-only)
          ansible.builtin.command:
            cmd: kubectl -n monitoring get pvc storage-loki-stack-0 -o jsonpath='{.spec.volumeName}'
          register: loki_pvname
          failed_when: false
          changed_when: false

        - name: Fetch PV hostPath for grafana (read-only)
          ansible.builtin.command:
            cmd: |
              if [ -n "${grafana_pvname.stdout}" ]; then
                kubectl get pv ${grafana_pvname.stdout} -o jsonpath='{.spec.local.path}{.spec.hostPath.path}{"\n"}' || true
              else
                echo ""
              fi
          register: grafana_hostpath
          failed_when: false
          changed_when: false

        - name: Fetch PV hostPath for loki (read-only)
          ansible.builtin.command:
            cmd: |
              if [ -n "${loki_pvname.stdout}" ]; then
                kubectl get pv ${loki_pvname.stdout} -o jsonpath='{.spec.local.path}{.spec.hostPath.path}{"\n"}' || true
              else
                echo ""
              fi
          register: loki_hostpath
          failed_when: false
          changed_when: false

        - name: Show operator-only filesystem remediation commands (do NOT run automatically)
          ansible.builtin.debug:
            msg: |
              Grafana PV -> {{ grafana_pvname.stdout | default('') }}
              Grafana hostPath -> {{ grafana_hostpath.stdout | default('') }}
              Suggested operator-only chown (run on the node hosting the PV):
              operator-only: modifies node filesystem
              sudo chown -R 472:472 {{ grafana_hostpath.stdout | default('<hostPath-from-pv>') }} && sudo chmod -R 0755 {{ grafana_hostpath.stdout | default('<hostPath-from-pv>') }}

              Loki PV -> {{ loki_pvname.stdout | default('') }}
              Loki hostPath -> {{ loki_hostpath.stdout | default('') }}
              Suggested operator-only chown (identify loki UID from logs before running):
              operator-only: modifies node filesystem
              sudo chown -R <loki-uid>:<loki-uid> {{ loki_hostpath.stdout | default('<hostPath-from-pv>') }} && sudo chmod -R 0755 {{ loki_hostpath.stdout | default('<hostPath-from-pv>') }}

        - name: Show helm dry-run suggestion for Loki fix (read-only)
          ansible.builtin.debug:
            msg: |
              To test removing an invalid 'max_retries' key from Loki values do:
              cat > loki-fix-values.yaml <<'EOF'
              loki:
                config:
                  table_manager:
                    retention_deletes_enabled: true
                    retention_period: 168h
              EOF
              Then run a dry-run upgrade:
              helm -n monitoring upgrade --reuse-values loki-stack grafana/loki-stack -f loki-fix-values.yaml --dry-run

        - name: Run helm dry-run for Loki (conditional)
          ansible.builtin.command:
            cmd: helm -n monitoring upgrade --reuse-values loki-stack grafana/loki-stack -f loki-fix-values.yaml --dry-run
          register: loki_helm_dryrun
          failed_when: false
          changed_when: false
          when: monitoring_allow_helm_upgrade | bool

        - name: Optionally perform helm upgrade for Loki (operator must enable)
          ansible.builtin.command:
            cmd: helm -n monitoring upgrade --reuse-values loki-stack grafana/loki-stack -f loki-fix-values.yaml
          register: loki_helm_upgrade
          failed_when: false
          changed_when: false
          when: monitoring_apply_fixes | bool and monitoring_allow_helm_upgrade | bool

        - name: Restart grafana pods to re-run init-chown (conditional)
          ansible.builtin.command:
            cmd: kubectl -n monitoring delete pod -l app.kubernetes.io/name=grafana,app.kubernetes.io/instance=kube-prometheus-stack
          register: grafana_restart
          failed_when: false
          changed_when: false
          when: monitoring_apply_fixes | bool

      when: 
        - selinux_context_check is defined
        - selinux_context_check.rc is defined  
        - selinux_context_check.rc == 0
        - ansible_selinux is defined 
        - ansible_selinux.status == "enabled"

- hosts: monitoring_nodes
  gather_facts: false
  connection: local
  tasks:
    - name: Final monitoring stack deployment summary
      ansible.builtin.debug:
        msg: |
          === Monitoring Stack Pre-Check Summary ===
          
          Namespace Check: {{ 'READY' if (ns_info.resources is defined and ns_info.resources | length > 0) else 'NEEDS CREATION' }}
          Prometheus Operator CRDs: {{ 'INSTALLED' if (servicemonitor_crd.resources is defined and servicemonitor_crd.resources | length > 0) else 'MISSING' }}
          kubectl Available: {{ 'YES' if kubectl_check.rc == 0 else 'NO' }}
          Helm Available: {{ 'YES' if helm_check.rc == 0 else 'NO' }}
          
          Next steps:
          1. Create namespace: kubectl create namespace {{ monitoring_namespace | default('monitoring') }}
          2. Install monitoring stack using Helm commands shown above
          3. Verify installation: kubectl get pods -n {{ monitoring_namespace | default('monitoring') }}
          4. Access services via NodePort (see documentation)
          
          This playbook follows non-destructive principles and provides CLI commands
          instead of making system changes automatically.
