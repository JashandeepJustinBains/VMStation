{
  "metadata": {
    "generated": "2025-09-16T21:31:54.149Z",
    "repositoryPath": "/home/runner/work/VMStation/VMStation",
    "nodeCount": 135,
    "edgeCount": 574,
    "version": "1.0.0"
  },
  "nodes": [
    {
      "id": "efc0b58e",
      "path": "scripts/README.md",
      "type": "doc",
      "title": "README.md",
      "summary": "This directory contains operational scripts for VMStation infrastructure management.",
      "sizeBytes": 7750,
      "tags": [
        "scripts",
        "kubernetes",
        "monitoring",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "585e2b00",
      "path": "scripts/ansible_pre_join_validation.sh",
      "type": "script",
      "title": "ansible_pre_join_validation.sh",
      "summary": "VMStation Pre-Join Validation Script for Ansible Handles crictl permissions and containerd validation properly",
      "sizeBytes": 8144,
      "tags": [
        "scripts",
        "containers",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "7fb7d1a5",
      "path": "scripts/check_cni_bridge_conflict.sh",
      "type": "script",
      "title": "check_cni_bridge_conflict.sh",
      "summary": "Check for CNI Bridge Conflict Issues This script detects if pods are stuck due to CNI bridge IP conflicts Check if we have kubectl access",
      "sizeBytes": 1384,
      "tags": [
        "scripts",
        "kubernetes",
        "networking",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "36bd22c9",
      "path": "scripts/check_coredns_status.sh",
      "type": "script",
      "title": "check_coredns_status.sh",
      "summary": "Quick CoreDNS Status Checker Quickly identify if CoreDNS has the \"Unknown\" status issue after flannel regeneration Color output",
      "sizeBytes": 2174,
      "tags": [
        "scripts",
        "kubernetes",
        "networking",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "37f70066",
      "path": "scripts/comprehensive_worker_setup.sh",
      "type": "script",
      "title": "comprehensive_worker_setup.sh",
      "summary": "VMStation Comprehensive Worker Node Manual Setup This script provides a complete manual installation and configuration of all necessary components for worker nodes when the automated Ansible process encounters issues.",
      "sizeBytes": 14166,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "deployment",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "3128233f",
      "path": "scripts/diagnose_remaining_pod_issues.sh",
      "type": "script",
      "title": "diagnose_remaining_pod_issues.sh",
      "summary": "Diagnose Remaining Pod Issues This script analyzes specific pod failures mentioned in the problem statement Color output",
      "sizeBytes": 6642,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "jellyfin",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "d3a169d7",
      "path": "scripts/enhanced_kubeadm_join.sh",
      "type": "script",
      "title": "enhanced_kubeadm_join.sh",
      "summary": "VMStation Enhanced Kubeadm Join Process Comprehensive join process with prerequisite validation and robust error handling Color codes",
      "sizeBytes": 54420,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.844Z"
    },
    {
      "id": "1e7cdd43",
      "path": "scripts/fix_cluster_communication.sh",
      "type": "script",
      "title": "fix_cluster_communication.sh",
      "summary": "Master Cluster Communication Fix Script This script addresses all the issues identified in the problem statement: 1. kubectl configuration on worker nodes",
      "sizeBytes": 29847,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.844Z"
    },
    {
      "id": "4f42b816",
      "path": "scripts/fix_cluster_dns_configuration.sh",
      "type": "script",
      "title": "fix_cluster_dns_configuration.sh",
      "summary": "VMStation Cluster DNS Configuration Fix Fixes the issue where kubectl uses router gateway (192.168.4.1) instead of CoreDNS This addresses the problem: \"dial tcp: lookup hort on 192.168.4.1:53: no such host\"",
      "sizeBytes": 13063,
      "tags": [
        "scripts",
        "kubernetes",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.844Z"
    },
    {
      "id": "1970801b",
      "path": "scripts/fix_cni_bridge_conflict.sh",
      "type": "script",
      "title": "fix_cni_bridge_conflict.sh",
      "summary": "fix_cni_bridge_conflict.sh Enhanced CNI bridge conflict resolution for VMStation cluster Addresses specific issues with Jellyfin pod IP assignment failures",
      "sizeBytes": 22122,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.844Z"
    },
    {
      "id": "7071b363",
      "path": "scripts/fix_coredns_unknown_status.sh",
      "type": "script",
      "title": "fix_coredns_unknown_status.sh",
      "summary": "Fix CoreDNS Unknown Status After Flannel Regeneration This script addresses the issue where CoreDNS pods show \"Unknown\" status with no IP after flannel pods have been regenerated by deploy-cluster.sh full",
      "sizeBytes": 9118,
      "tags": [
        "scripts",
        "kubernetes",
        "networking",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.844Z"
    },
    {
      "id": "263c2cb9",
      "path": "scripts/fix_flannel_mixed_os.sh",
      "type": "script",
      "title": "fix_flannel_mixed_os.sh",
      "summary": "Fix Flannel CNI Configuration for Mixed OS Environments Addresses specific issues in mixed Windows/Linux environments that can cause pod-to-pod communication failures",
      "sizeBytes": 12074,
      "tags": [
        "scripts",
        "kubernetes",
        "networking",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.844Z"
    },
    {
      "id": "a249d706",
      "path": "scripts/fix_homelab_node_issues.sh",
      "type": "script",
      "title": "fix_homelab_node_issues.sh",
      "summary": "Fix Homelab Node Networking Issues Addresses: Flannel CrashLoopBackOff, kube-proxy crashes, and CNI problems Color output",
      "sizeBytes": 17833,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.844Z"
    },
    {
      "id": "630db1ca",
      "path": "scripts/fix_iptables_compatibility.sh",
      "type": "script",
      "title": "fix_iptables_compatibility.sh",
      "summary": "Fix iptables/nftables Compatibility Issues This script detects and fixes iptables/nftables compatibility problems that can cause kube-proxy to fail with \"incompatible\" errors",
      "sizeBytes": 11473,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.844Z"
    },
    {
      "id": "1ddb9838",
      "path": "scripts/fix_kubelet_systemd_config.sh",
      "type": "script",
      "title": "fix_kubelet_systemd_config.sh",
      "summary": "VMStation Kubelet Systemd Configuration Fix Fixes the \"Assignment outside of section\" error for kubelet systemd drop-in files This script addresses the issue where systemd drop-in files are created with",
      "sizeBytes": 12762,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.844Z"
    },
    {
      "id": "09f42a5d",
      "path": "scripts/fix_nodeport_external_access.sh",
      "type": "script",
      "title": "fix_nodeport_external_access.sh",
      "summary": "VMStation NodePort External Access Fix Fixes external access to NodePort services (like Jellyfin on port 30096) from machines outside the cluster by ensuring proper firewall and iptables rules",
      "sizeBytes": 8788,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.844Z"
    },
    {
      "id": "090d10b0",
      "path": "scripts/fix_remaining_pod_issues.sh",
      "type": "script",
      "title": "fix_remaining_pod_issues.sh",
      "summary": "Fix Remaining VMStation Pod Issues Addresses specific problems after deploy-cluster.sh and fix_homelab_node_issues.sh Focuses on: jellyfin readiness issues and kube-proxy crashloop",
      "sizeBytes": 27016,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "2efbd35b",
      "path": "scripts/fix_worker_kubectl_config.sh",
      "type": "script",
      "title": "fix_worker_kubectl_config.sh",
      "summary": "Fix kubectl Configuration on Worker Nodes Addresses the \"connection refused\" errors when running kubectl on worker nodes This script configures kubectl to communicate with the cluster API server",
      "sizeBytes": 8110,
      "tags": [
        "scripts",
        "kubernetes",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "52a04a1d",
      "path": "scripts/fix_worker_node_cni.sh",
      "type": "script",
      "title": "fix_worker_node_cni.sh",
      "summary": "Fix Worker Node CNI Communication Issues Specifically addresses the issue where pods on the same worker node cannot communicate with each other, as evidenced by \"Destination Host Unreachable\" errors",
      "sizeBytes": 14535,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "b3dc3aae",
      "path": "scripts/gather_worker_diagnostics.sh",
      "type": "script",
      "title": "gather_worker_diagnostics.sh",
      "summary": "VMStation Worker Diagnostics Gathering Script Collects comprehensive diagnostic information from worker nodes Color codes",
      "sizeBytes": 15938,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "21313a3e",
      "path": "scripts/manual_containerd_filesystem_fix.sh",
      "type": "script",
      "title": "manual_containerd_filesystem_fix.sh",
      "summary": "VMStation Manual Containerd Filesystem Fix Aggressive containerd configuration and filesystem initialization fix This script addresses persistent containerd image filesystem initialization issues",
      "sizeBytes": 17239,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "1b255a42",
      "path": "scripts/quick_join_diagnostics.sh",
      "type": "script",
      "title": "quick_join_diagnostics.sh",
      "summary": "VMStation Quick Join Diagnostics Rapid diagnostic script for kubeadm join issues Color codes",
      "sizeBytes": 9454,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "7a273252",
      "path": "scripts/reset_cni_bridge.sh",
      "type": "script",
      "title": "reset_cni_bridge.sh",
      "summary": "Reset CNI Bridge for VMStation Kubernetes Cluster This script resets the CNI bridge to align with proper kube-flannel, kube-proxy, and CoreDNS configuration Addresses the specific issue where cni0 has an IP address that conflicts with the expected 10.244.0.0/16 subnet",
      "sizeBytes": 7894,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "ad0a12c7",
      "path": "scripts/reset_cni_bridge_minimal.sh",
      "type": "script",
      "title": "reset_cni_bridge_minimal.sh",
      "summary": "VMStation CNI Bridge Reset Script Fixes CNI bridge IP conflicts that prevent pod creation Colors",
      "sizeBytes": 7276,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "09b572dc",
      "path": "scripts/run_network_diagnosis.sh",
      "type": "script",
      "title": "run_network_diagnosis.sh",
      "summary": "VMStation Network Diagnosis Runner Quick wrapper script to run the automated network diagnosis playbook Color codes",
      "sizeBytes": 3734,
      "tags": [
        "scripts",
        "kubernetes",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "4f2a3f41",
      "path": "scripts/setup_static_ips_and_dns.sh",
      "type": "script",
      "title": "setup_static_ips_and_dns.sh",
      "summary": "VMStation Static IP Assignment and DNS Subdomain Setup This script ensures critical Kubernetes components have static IPs and sets up homelab.com DNS Color codes",
      "sizeBytes": 13720,
      "tags": [
        "scripts",
        "kubernetes",
        "monitoring",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "0f552648",
      "path": "scripts/smoke-test.sh",
      "type": "script",
      "title": "smoke-test.sh",
      "summary": "VMStation Kubernetes Cluster Smoke Test Quick validation script for cluster health Colors for output",
      "sizeBytes": 6713,
      "tags": [
        "scripts",
        "kubernetes",
        "monitoring",
        "networking",
        "jellyfin",
        "deployment",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "b804c80b",
      "path": "scripts/test_enhanced_join_functionality.sh",
      "type": "script",
      "title": "test_enhanced_join_functionality.sh",
      "summary": "VMStation Enhanced Worker Join Preflight Tests Validates the enhanced join process functionality Color codes",
      "sizeBytes": 7555,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "7daa8953",
      "path": "scripts/test_static_ips_and_dns_integration.sh",
      "type": "script",
      "title": "test_static_ips_and_dns_integration.sh",
      "summary": "VMStation Static IP and DNS Setup Integration Test Tests the integration without requiring a running cluster Color codes",
      "sizeBytes": 3534,
      "tags": [
        "scripts",
        "networking",
        "jellyfin",
        "deployment",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "7c606adf",
      "path": "scripts/validate_cluster_communication.sh",
      "type": "script",
      "title": "validate_cluster_communication.sh",
      "summary": "Validate Cluster Communication and NodePort Services This script validates that all aspects of cluster communication work correctly including kubectl access, NodePort services, and inter-node connectivity",
      "sizeBytes": 11489,
      "tags": [
        "scripts",
        "kubernetes",
        "networking",
        "jellyfin",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "1cd4e1f5",
      "path": "scripts/validate_deployment_fixes.sh",
      "type": "script",
      "title": "validate_deployment_fixes.sh",
      "summary": "VMStation Post-Deployment Validation Script Run this after deploying the cluster to verify all fixes are working Check if we have kubectl access",
      "sizeBytes": 5737,
      "tags": [
        "scripts",
        "kubernetes",
        "monitoring",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.845Z"
    },
    {
      "id": "b428256d",
      "path": "scripts/validate_join_prerequisites.sh",
      "type": "script",
      "title": "validate_join_prerequisites.sh",
      "summary": "VMStation Kubernetes Join Prerequisites Validator Comprehensive validation before attempting kubeadm join to prevent standalone mode Color codes",
      "sizeBytes": 13524,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.846Z"
    },
    {
      "id": "ac18a0b8",
      "path": "scripts/validate_mixed_os_flannel.sh",
      "type": "script",
      "title": "validate_mixed_os_flannel.sh",
      "summary": "Validation script for Flannel download fix in mixed OS environments This script validates that both RHEL10 and Debian nodes can download Flannel CNI plugin Color codes",
      "sizeBytes": 7786,
      "tags": [
        "scripts",
        "networking",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.846Z"
    },
    {
      "id": "ca8ac5f9",
      "path": "scripts/validate_network_prerequisites.sh",
      "type": "script",
      "title": "validate_network_prerequisites.sh",
      "summary": "VMStation Network Prerequisites Validation Validates network configuration before deploying Jellyfin and other pods Prevents common CNI bridge conflicts and mixed OS compatibility issues",
      "sizeBytes": 12910,
      "tags": [
        "scripts",
        "kubernetes",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.846Z"
    },
    {
      "id": "27edfbfd",
      "path": "scripts/validate_nodeport_external_access.sh",
      "type": "script",
      "title": "validate_nodeport_external_access.sh",
      "summary": "VMStation NodePort External Access Validation Tests external access to NodePort services to validate the fix Color output",
      "sizeBytes": 9068,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.846Z"
    },
    {
      "id": "52dfbbac",
      "path": "scripts/validate_pod_connectivity.sh",
      "type": "script",
      "title": "validate_pod_connectivity.sh",
      "summary": "Validate Pod-to-Pod CNI Communication Tests the exact scenario from the problem statement: - Debug pod on storagenodet3500 trying to reach Jellyfin pod",
      "sizeBytes": 10599,
      "tags": [
        "scripts",
        "kubernetes",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.846Z"
    },
    {
      "id": "62721f19",
      "path": "scripts/validate_pod_health.sh",
      "type": "script",
      "title": "validate_pod_health.sh",
      "summary": "Validate VMStation Pod Fixes Quick validation script to check if the pod issues have been resolved Color output",
      "sizeBytes": 5495,
      "tags": [
        "scripts",
        "kubernetes",
        "networking",
        "jellyfin",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.846Z"
    },
    {
      "id": "6ea2bc0f",
      "path": "scripts/validate_post_wipe_functionality.sh",
      "type": "script",
      "title": "validate_post_wipe_functionality.sh",
      "summary": "VMStation Post-Wipe Worker Join Validation Test This script validates that the enhanced post-wipe worker join functionality is working correctly Remove set -e to prevent premature exit on arithmetic operations",
      "sizeBytes": 6716,
      "tags": [
        "scripts",
        "kubernetes",
        "deployment",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.846Z"
    },
    {
      "id": "5e0bdb07",
      "path": "scripts/validate_static_ips_and_dns.sh",
      "type": "script",
      "title": "validate_static_ips_and_dns.sh",
      "summary": "VMStation Static IP and DNS Validation Script Tests the static IP assignments and DNS subdomain functionality Color codes",
      "sizeBytes": 11901,
      "tags": [
        "scripts",
        "kubernetes",
        "monitoring",
        "networking",
        "jellyfin",
        "deployment",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.846Z"
    },
    {
      "id": "59ecfc80",
      "path": "scripts/validate_systemd_dropins.sh",
      "type": "script",
      "title": "validate_systemd_dropins.sh",
      "summary": "VMStation Systemd Drop-in Validator Validates and ensures proper formatting of systemd drop-in files This script prevents \"Assignment outside of section\" errors by ensuring",
      "sizeBytes": 11555,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.846Z"
    },
    {
      "id": "563e9b03",
      "path": "scripts/vmstation_status.sh",
      "type": "script",
      "title": "vmstation_status.sh",
      "summary": "VMStation Deployment Status and Troubleshooting Summary Provides a comprehensive overview of cluster status and recommended actions Color output",
      "sizeBytes": 5293,
      "tags": [
        "scripts",
        "kubernetes",
        "monitoring",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.846Z"
    },
    {
      "id": "6564808c",
      "path": "scripts/worker_node_join_remediation.sh",
      "type": "script",
      "title": "worker_node_join_remediation.sh",
      "summary": "VMStation Worker Node Join Remediation General purpose remediation script for worker node join issues Color codes",
      "sizeBytes": 8849,
      "tags": [
        "scripts",
        "kubernetes",
        "containers",
        "networking",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.846Z"
    },
    {
      "id": "8c19061a",
      "path": "docs/AGGRESSIVE_NODE_RESET.md",
      "type": "doc",
      "title": "AGGRESSIVE_NODE_RESET.md",
      "summary": "The enhanced kubeadm join script was failing with persistent containerd filesystem initialization errors:",
      "sizeBytes": 4310,
      "tags": [
        "documentation",
        "kubernetes",
        "containers",
        "networking",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "094a0932",
      "path": "docs/CNI_BRIDGE_FIX.md",
      "type": "doc",
      "title": "CNI_BRIDGE_FIX.md",
      "summary": "Kubernetes pods get stuck in \"ContainerCreating\" state with errors like:",
      "sizeBytes": 3437,
      "tags": [
        "documentation",
        "kubernetes",
        "containers",
        "networking",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "f0840b04",
      "path": "docs/CNI_BRIDGE_RESET.md",
      "type": "doc",
      "title": "CNI_BRIDGE_RESET.md",
      "summary": "Jellyfin pod (or other pods) stuck in ContainerCreating state with error:",
      "sizeBytes": 1772,
      "tags": [
        "documentation",
        "kubernetes",
        "containers",
        "networking",
        "jellyfin",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "8facf1bd",
      "path": "docs/COREDNS_MASTERNODE_ENFORCEMENT.md",
      "type": "doc",
      "title": "COREDNS_MASTERNODE_ENFORCEMENT.md",
      "summary": "CoreDNS pods were being scheduled on the \"homelab\" worker node (192.168.4.62) instead of staying on the \"masternode\" control-plane node (192.168.4.63), causing networking instability and DNS resolution issues.",
      "sizeBytes": 2719,
      "tags": [
        "documentation",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "fcb85303",
      "path": "docs/COREDNS_UNKNOWN_STATUS_FIX.md",
      "type": "doc",
      "title": "COREDNS_UNKNOWN_STATUS_FIX.md",
      "summary": "After running `deploy.sh full` and regenerating flannel pods, CoreDNS pods may show \"Unknown\" status with no IP address assigned. This prevents DNS resolution in the cluster and causes other pods to remain stuck in \"ContainerCreating\" or \"Pending\" states.",
      "sizeBytes": 5065,
      "tags": [
        "documentation",
        "kubernetes",
        "networking",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "0665ba5c",
      "path": "docs/ENHANCED_JOIN_PROCESS.md",
      "type": "doc",
      "title": "ENHANCED_JOIN_PROCESS.md",
      "summary": "This document describes the enhanced kubeadm join process implemented to address persistent issues with worker nodes falling back to \"standalone mode\" instead of properly joining the cluster.",
      "sizeBytes": 11186,
      "tags": [
        "documentation",
        "kubernetes",
        "containers",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "20493e93",
      "path": "docs/FIXES_APPLIED.md",
      "type": "doc",
      "title": "FIXES_APPLIED.md",
      "summary": "This document describes the fixes applied to resolve DNS resolution, monitoring access, and homelab node stability issues.",
      "sizeBytes": 3449,
      "tags": [
        "documentation",
        "monitoring",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "e8e06adf",
      "path": "docs/HOMELAB_NODE_FIXES.md",
      "type": "doc",
      "title": "HOMELAB_NODE_FIXES.md",
      "summary": "After multiple runs of `deploy.sh full`, several critical networking issues were observed:",
      "sizeBytes": 4355,
      "tags": [
        "documentation",
        "kubernetes",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "13b742f3",
      "path": "docs/JELLYFIN_NETWORKING_TROUBLESHOOT.md",
      "type": "doc",
      "title": "JELLYFIN_NETWORKING_TROUBLESHOOT.md",
      "summary": "This guide addresses the specific issue where Jellyfin pods fail to get IP addresses and remain stuck in \"ContainerCreating\" state, often accompanied by kube-flannel and kube-proxy crashloopbackoff issues.",
      "sizeBytes": 5408,
      "tags": [
        "documentation",
        "kubernetes",
        "containers",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "2267ad9a",
      "path": "docs/KUBELET_SYSTEMD_CONFIG_FIX.md",
      "type": "doc",
      "title": "KUBELET_SYSTEMD_CONFIG_FIX.md",
      "summary": "This document describes the fix for the \"Assignment outside of section\" systemd error that occurs during VMStation Kubernetes worker node joins.",
      "sizeBytes": 5200,
      "tags": [
        "documentation",
        "kubernetes",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "121dca56",
      "path": "docs/MANUAL_CLUSTER_TROUBLESHOOTING.md",
      "type": "doc",
      "title": "MANUAL_CLUSTER_TROUBLESHOOTING.md",
      "summary": "This guide provides comprehensive troubleshooting steps for worker node join failures, focusing on the enhanced remediation capabilities added to VMStation.",
      "sizeBytes": 15200,
      "tags": [
        "documentation",
        "kubernetes",
        "containers",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "044d8702",
      "path": "docs/POST_WIPE_WORKER_JOIN.md",
      "type": "doc",
      "title": "POST_WIPE_WORKER_JOIN.md",
      "summary": "This document describes the enhanced worker join process specifically designed to handle workers that have been reset using `aggressive_worker_wipe_preserve_storage.sh` or similar aggressive cleanup procedures.",
      "sizeBytes": 8604,
      "tags": [
        "documentation",
        "kubernetes",
        "containers",
        "networking",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "e7010005",
      "path": "docs/README.md",
      "type": "doc",
      "title": "README.md",
      "summary": "Welcome to the reorganized documentation for your homelab/self-hosting server stack.",
      "sizeBytes": 865,
      "tags": [
        "documentation",
        "monitoring",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "53177eca",
      "path": "docs/RHEL10_TROUBLESHOOTING.md",
      "type": "doc",
      "title": "RHEL10_TROUBLESHOOTING.md",
      "summary": "This guide addresses the specific issue where RHEL 10 compute nodes (192.168.4.62) fail to join the Kubernetes cluster during the \"TASK [Join worker nodes to cluster]\" step.",
      "sizeBytes": 19601,
      "tags": [
        "documentation",
        "kubernetes",
        "containers",
        "networking",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "40ee73d0",
      "path": "docs/ansible_vault_quickstart.md",
      "type": "doc",
      "title": "ansible_vault_quickstart.md",
      "summary": "This guide helps you securely manage secrets and sync your configuration repo across your desktop and server machines.",
      "sizeBytes": 1255,
      "tags": [
        "documentation",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "88482a18",
      "path": "docs/cluster-communication-fixes.md",
      "type": "doc",
      "title": "cluster-communication-fixes.md",
      "summary": "This document describes the fixes for Kubernetes cluster communication issues, addressing the problems described in the issue where worker nodes cannot properly communicate with the cluster.",
      "sizeBytes": 5933,
      "tags": [
        "documentation",
        "kubernetes",
        "containers",
        "networking",
        "jellyfin",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "797dca0d",
      "path": "docs/cni-pod-communication-fix.md",
      "type": "doc",
      "title": "cni-pod-communication-fix.md",
      "summary": "This document describes the solution for the CNI communication issue where pods cannot reach each other, specifically addressing the problem where a debug pod (10.244.0.20) on storagenodet3500 cannot ping the Jellyfin pod (10.244.0.19) on the same node with \"Destination Host Unreachable\" errors.",
      "sizeBytes": 6358,
      "tags": [
        "documentation",
        "kubernetes",
        "containers",
        "networking",
        "jellyfin",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "19990fc4",
      "path": "docs/containerd-timeout-fix.md",
      "type": "doc",
      "title": "containerd-timeout-fix.md",
      "summary": "Worker nodes were failing during Kubernetes cluster join with the error:",
      "sizeBytes": 2802,
      "tags": [
        "documentation",
        "containers",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.840Z"
    },
    {
      "id": "378a3809",
      "path": "docs/devices/Catalyst3650V02_Switch.md",
      "type": "doc",
      "title": "Catalyst3650V02_Switch.md",
      "summary": "- Managed switch for VLANs, QoS, and network segmentation",
      "sizeBytes": 335,
      "tags": [
        "documentation",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "c1dd3153",
      "path": "docs/devices/MiniPC_Monitoring_Ansible.md",
      "type": "doc",
      "title": "MiniPC_Monitoring_Ansible.md",
      "summary": "This guide describes how to use Ansible to set up Prometheus, Grafana, and Loki on your MiniPC.",
      "sizeBytes": 1123,
      "tags": [
        "documentation",
        "containers",
        "monitoring",
        "deployment",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "37f609a0",
      "path": "docs/devices/R430_Compute.md",
      "type": "doc",
      "title": "R430_Compute.md",
      "summary": "- Main compute node for cluster workloads",
      "sizeBytes": 384,
      "tags": [
        "documentation",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "28982728",
      "path": "docs/devices/T3500_NAS.md",
      "type": "doc",
      "title": "T3500_NAS.md",
      "summary": "- Dedicated NAS (Network Attached Storage)",
      "sizeBytes": 371,
      "tags": [
        "documentation",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "9b321fa0",
      "path": "docs/devices/reset_debian_nodes.md",
      "type": "doc",
      "title": "reset_debian_nodes.md",
      "summary": "This guide provides an Ansible playbook to remove non-system apps and custom network settings from your Debian nodes (MiniPC, T3500).",
      "sizeBytes": 3565,
      "tags": [
        "documentation",
        "containers",
        "monitoring",
        "deployment",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "f087a852",
      "path": "docs/dns-fix-guide.md",
      "type": "doc",
      "title": "dns-fix-guide.md",
      "summary": "When running `kubectl version --short`, you get the error:",
      "sizeBytes": 4073,
      "tags": [
        "documentation",
        "kubernetes",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "e00aacd0",
      "path": "docs/fix-jellyfin-pod-deletion.md",
      "type": "doc",
      "title": "fix-jellyfin-pod-deletion.md",
      "summary": "This fix addresses the issue where previous PR changes were unnecessarily deleting healthy jellyfin pods and causing flannel pod crashes.",
      "sizeBytes": 1308,
      "tags": [
        "documentation",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "0eb8fc77",
      "path": "docs/fix_cluster_communication.md",
      "type": "doc",
      "title": "fix_cluster_communication.md",
      "summary": "Path: `scripts/fix_cluster_communication.sh`",
      "sizeBytes": 10151,
      "tags": [
        "documentation",
        "kubernetes",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "207b4d0f",
      "path": "docs/grafana_datasource_conflict_fix.md",
      "type": "doc",
      "title": "grafana_datasource_conflict_fix.md",
      "summary": "Grafana was experiencing the following error:",
      "sizeBytes": 5096,
      "tags": [
        "documentation",
        "kubernetes",
        "monitoring",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "da894f5f",
      "path": "docs/jellyfin/JELLYFIN_HA_DEPLOYMENT.md",
      "type": "doc",
      "title": "JELLYFIN_HA_DEPLOYMENT.md",
      "summary": "This implementation provides a comprehensive, highly-available Jellyfin media server deployment on Kubernetes with auto-scaling capabilities. It replaces the existing single Podman container setup with a robust, scalable solution optimized for 1080p+ streaming to multiple devices.",
      "sizeBytes": 10594,
      "tags": [
        "documentation",
        "jellyfin",
        "media",
        "kubernetes",
        "monitoring",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "8a5efc5c",
      "path": "docs/jellyfin/MIGRATION_CHECKLIST.md",
      "type": "doc",
      "title": "MIGRATION_CHECKLIST.md",
      "summary": "- [ ] Verify existing Podman Jellyfin is running: `podman ps | grep jellyfin`",
      "sizeBytes": 6515,
      "tags": [
        "documentation",
        "jellyfin",
        "media",
        "kubernetes",
        "monitoring",
        "deployment",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "710930e0",
      "path": "docs/jellyfin/USAGE_EXAMPLE.md",
      "type": "doc",
      "title": "USAGE_EXAMPLE.md",
      "summary": "This example demonstrates how the Jellyfin HA deployment automatically scales based on usage:",
      "sizeBytes": 5431,
      "tags": [
        "documentation",
        "jellyfin",
        "media",
        "kubernetes",
        "monitoring",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "0c29bbcc",
      "path": "docs/jellyfin-cni-bridge-fix.md",
      "type": "doc",
      "title": "jellyfin-cni-bridge-fix.md",
      "summary": "This script addresses the specific issue where Jellyfin pods fail to create with the error:",
      "sizeBytes": 2917,
      "tags": [
        "documentation",
        "kubernetes",
        "containers",
        "networking",
        "jellyfin",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "1e068979",
      "path": "docs/jellyfin-network-fix.md",
      "type": "doc",
      "title": "jellyfin-network-fix.md",
      "summary": "After running the cluster deployment, the Jellyfin pod shows `0/1` ready status despite the container running successfully. The health probes fail with \"no route to host\" errors when trying to connect to the pod IP.",
      "sizeBytes": 1953,
      "tags": [
        "documentation",
        "kubernetes",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "1e57853e",
      "path": "docs/jellyfin-pvc-hostpath-migration.md",
      "type": "doc",
      "title": "jellyfin-pvc-hostpath-migration.md",
      "summary": "The VMStation deployment was experiencing two critical issues:",
      "sizeBytes": 4814,
      "tags": [
        "documentation",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "98b95b67",
      "path": "docs/jellyfin-toleration-fix.md",
      "type": "doc",
      "title": "jellyfin-toleration-fix.md",
      "summary": "The Jellyfin pod deployment was failing with the error:",
      "sizeBytes": 1423,
      "tags": [
        "documentation",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "42b5112f",
      "path": "docs/kubernetes_authorization_modes.md",
      "type": "doc",
      "title": "kubernetes_authorization_modes.md",
      "summary": "VMStation now supports configurable authorization modes for the Kubernetes API server to address RBAC issues that may prevent worker nodes from joining the cluster.",
      "sizeBytes": 4136,
      "tags": [
        "documentation",
        "kubernetes",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "d6e5693f",
      "path": "docs/namespace-termination-resolution.md",
      "type": "doc",
      "title": "namespace-termination-resolution.md",
      "summary": "The `fix-cluster.sh` script was failing with the error:",
      "sizeBytes": 2998,
      "tags": [
        "documentation",
        "kubernetes",
        "networking",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "19863979",
      "path": "docs/network-diagnosis.md",
      "type": "doc",
      "title": "network-diagnosis.md",
      "summary": "This document describes the automated network diagnosis system for troubleshooting inter-pod communication issues in the VMStation Kubernetes cluster.",
      "sizeBytes": 6133,
      "tags": [
        "documentation",
        "kubernetes",
        "networking",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "c914457e",
      "path": "docs/nodeport-external-access-fix.md",
      "type": "doc",
      "title": "nodeport-external-access-fix.md",
      "summary": "External machines (like development desktops) cannot access Kubernetes NodePort services (specifically Jellyfin on port 30096) even after running standard cluster communication fixes. The issue manifests as:",
      "sizeBytes": 6642,
      "tags": [
        "documentation",
        "kubernetes",
        "monitoring",
        "jellyfin",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.841Z"
    },
    {
      "id": "84b873a0",
      "path": "docs/pv_permissions_and_loki_issues.md",
      "type": "doc",
      "title": "pv_permissions_and_loki_issues.md",
      "summary": "This document explains common issues encountered during monitoring stack deployment including Grafana CrashLoopBackOff (PVC/PV permissions), Loki crashes (invalid config), and Grafana Pending state (scheduling constraints). The examples use placeholders like `<pvc-name-here>` and `<pv-hostpath-here>`; replace them with the values from your `kubectl` output.",
      "sizeBytes": 11719,
      "tags": [
        "documentation",
        "kubernetes",
        "monitoring",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.842Z"
    },
    {
      "id": "565c9435",
      "path": "docs/security/cloudflare_tunnel_quickstart.md",
      "type": "doc",
      "title": "cloudflare_tunnel_quickstart.md",
      "summary": "This guide explains how to set up a free Cloudflare Tunnel (Zero Trust) to securely expose your self-hosted applications to the internet.",
      "sizeBytes": 1930,
      "tags": [
        "documentation",
        "deployment",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.842Z"
    },
    {
      "id": "76046802",
      "path": "docs/security/firewall.md",
      "type": "doc",
      "title": "firewall.md",
      "summary": "sudo apt install ufw -y",
      "sizeBytes": 337,
      "tags": [
        "documentation",
        "deployment",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.842Z"
    },
    {
      "id": "a93403c8",
      "path": "docs/security/hashicorp_vault_usage.md",
      "type": "doc",
      "title": "hashicorp_vault_usage.md",
      "summary": "This guide outlines how to use HashiCorp Vault for self-hosted credential management in your homelab, with a TODO for advanced security features.",
      "sizeBytes": 1971,
      "tags": [
        "documentation",
        "deployment",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.842Z"
    },
    {
      "id": "4096f24e",
      "path": "docs/static-ips-and-dns.md",
      "type": "doc",
      "title": "static-ips-and-dns.md",
      "summary": "This document describes the static IP assignments for critical Kubernetes components and the DNS subdomain configuration for the homelab.",
      "sizeBytes": 8979,
      "tags": [
        "documentation",
        "kubernetes",
        "monitoring",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.842Z"
    },
    {
      "id": "0fd6c688",
      "path": "docs/ufw_rules.txt",
      "type": "doc",
      "title": "ufw_rules.txt",
      "summary": "ufw default deny incoming",
      "sizeBytes": 486,
      "tags": [
        "documentation",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.842Z"
    },
    {
      "id": "233067f8",
      "path": "ansible/ansible.cfg",
      "type": "config",
      "title": "ansible.cfg",
      "summary": "[defaults] # Basic configuration",
      "sizeBytes": 785,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "deployment",
        "config"
      ],
      "lastModified": "2025-09-16T21:28:57.836Z"
    },
    {
      "id": "dc5e49f8",
      "path": "ansible/artifacts/arc-network-diagnosis/README.md",
      "type": "doc",
      "title": "README.md",
      "summary": "This directory contains output from the automated network diagnosis playbook.",
      "sizeBytes": 2744,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "networking",
        "deployment",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.836Z"
    },
    {
      "id": "39641f50",
      "path": "ansible/files/dashboard-secret-access.yaml",
      "type": "manifest",
      "title": "dashboard-secret-access.yaml",
      "summary": "--- apiVersion: v1",
      "sizeBytes": 1273,
      "tags": [
        "ansible",
        "automation",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.836Z"
    },
    {
      "id": "6f2e75b0",
      "path": "ansible/files/grafana_dashboards/loki-dashboard.json",
      "type": "config",
      "title": "loki-dashboard.json",
      "summary": "{   \"schemaVersion\": 27,",
      "sizeBytes": 108,
      "tags": [
        "ansible",
        "automation",
        "config"
      ],
      "lastModified": "2025-09-16T21:28:57.836Z"
    },
    {
      "id": "b1bc04a3",
      "path": "ansible/files/grafana_dashboards/node-dashboard.json",
      "type": "config",
      "title": "node-dashboard.json",
      "summary": "{   \"schemaVersion\": 27,",
      "sizeBytes": 108,
      "tags": [
        "ansible",
        "automation",
        "config"
      ],
      "lastModified": "2025-09-16T21:28:57.836Z"
    },
    {
      "id": "1f7744bc",
      "path": "ansible/files/grafana_dashboards/prometheus-dashboard.json",
      "type": "config",
      "title": "prometheus-dashboard.json",
      "summary": "{   \"schemaVersion\": 27,",
      "sizeBytes": 114,
      "tags": [
        "ansible",
        "automation",
        "monitoring",
        "config"
      ],
      "lastModified": "2025-09-16T21:28:57.836Z"
    },
    {
      "id": "8036f074",
      "path": "ansible/files/grafana_datasources/prometheus-datasource.yaml",
      "type": "manifest",
      "title": "prometheus-datasource.yaml",
      "summary": "# NOTE: This file is no longer used in the deployment. # The kube-prometheus-stack Helm chart automatically creates a default Prometheus datasource.",
      "sizeBytes": 477,
      "tags": [
        "ansible",
        "automation",
        "monitoring",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.836Z"
    },
    {
      "id": "607e6e59",
      "path": "ansible/group_vars/all.yml",
      "type": "manifest",
      "title": "all.yml",
      "summary": "# VMStation Ansible Configuration Variables Template # Copy this file to all.yml and customize for your environment.",
      "sizeBytes": 6576,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "containers",
        "monitoring",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.836Z"
    },
    {
      "id": "1e67c2e0",
      "path": "ansible/inventory/hosts.yml",
      "type": "manifest",
      "title": "hosts.yml",
      "summary": "--- # VMStation Kubernetes Cluster Inventory",
      "sizeBytes": 2386,
      "tags": [
        "ansible",
        "automation",
        "containers",
        "networking",
        "jellyfin",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.837Z"
    },
    {
      "id": "4c5a2d8f",
      "path": "ansible/inventory.txt",
      "type": "doc",
      "title": "inventory.txt",
      "summary": "[monitoring_nodes] 192.168.4.63 ansible_user=root ansible_connection=local",
      "sizeBytes": 275,
      "tags": [
        "ansible",
        "automation",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.836Z"
    },
    {
      "id": "39bfddc5",
      "path": "ansible/playbooks/cluster-bootstrap/roles/system-prep/tasks/main.yml",
      "type": "manifest",
      "title": "main.yml",
      "summary": "--- # System preparation tasks for Kubernetes cluster nodes",
      "sizeBytes": 2495,
      "tags": [
        "ansible",
        "automation",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.837Z"
    },
    {
      "id": "fa6b94bd",
      "path": "ansible/playbooks/cluster-bootstrap/simple-bootstrap.yml",
      "type": "manifest",
      "title": "simple-bootstrap.yml",
      "summary": "--- # VMStation Kubernetes Cluster Bootstrap - Simple Version",
      "sizeBytes": 8529,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "monitoring",
        "networking",
        "jellyfin",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.837Z"
    },
    {
      "id": "2e9ce2c1",
      "path": "ansible/playbooks/cluster-bootstrap.yml",
      "type": "manifest",
      "title": "cluster-bootstrap.yml",
      "summary": "--- # VMStation Kubernetes Cluster Bootstrap",
      "sizeBytes": 6695,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "networking",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.837Z"
    },
    {
      "id": "fd7c8bd5",
      "path": "ansible/playbooks/minimal-network-fix.yml",
      "type": "manifest",
      "title": "minimal-network-fix.yml",
      "summary": "--- # VMStation Minimal Network Fix Playbook",
      "sizeBytes": 16636,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "containers",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.837Z"
    },
    {
      "id": "91932608",
      "path": "ansible/playbooks/verify-cluster.yml",
      "type": "manifest",
      "title": "verify-cluster.yml",
      "summary": "--- # VMStation Cluster Verification and Smoke Tests",
      "sizeBytes": 20504,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "monitoring",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.837Z"
    },
    {
      "id": "bcfb5f43",
      "path": "ansible/plays/deploy-apps.yaml",
      "type": "manifest",
      "title": "deploy-apps.yaml",
      "summary": "--- # VMStation Simplified Application Deployment",
      "sizeBytes": 17069,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "monitoring",
        "networking",
        "deployment",
        "troubleshooting",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.837Z"
    },
    {
      "id": "2ac1c05f",
      "path": "ansible/plays/jellyfin.yml",
      "type": "manifest",
      "title": "jellyfin.yml",
      "summary": "--- # Jellyfin Deployment Playbook",
      "sizeBytes": 13414,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.837Z"
    },
    {
      "id": "ca8597d0",
      "path": "ansible/plays/kubernetes/deploy_monitoring.yaml",
      "type": "manifest",
      "title": "deploy_monitoring.yaml",
      "summary": "--- # Deploy monitoring stack (Prometheus, Grafana, Loki) using Helm charts",
      "sizeBytes": 18940,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "containers",
        "monitoring",
        "deployment",
        "troubleshooting",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.838Z"
    },
    {
      "id": "377c4256",
      "path": "ansible/plays/kubernetes/jellyfin-minimal.yml",
      "type": "manifest",
      "title": "jellyfin-minimal.yml",
      "summary": "# ============================================================================ # Minimal Jellyfin Kubernetes Deployment",
      "sizeBytes": 8140,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "monitoring",
        "jellyfin",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.838Z"
    },
    {
      "id": "7ef785f1",
      "path": "ansible/plays/kubernetes/monitoring_validation.yaml",
      "type": "manifest",
      "title": "monitoring_validation.yaml",
      "summary": "--- # Post-deployment validation for Grafana dashboards and datasources",
      "sizeBytes": 4536,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "monitoring",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.838Z"
    },
    {
      "id": "3e468245",
      "path": "ansible/plays/kubernetes/rhel10_setup_fixes.yaml",
      "type": "manifest",
      "title": "rhel10_setup_fixes.yaml",
      "summary": "--- # RHEL 10 Specific Fixes for Kubernetes Cluster Setup",
      "sizeBytes": 9856,
      "tags": [
        "ansible",
        "automation",
        "containers",
        "deployment",
        "troubleshooting",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.838Z"
    },
    {
      "id": "4bd4f2e4",
      "path": "ansible/plays/kubernetes/setup_cert_manager.yaml",
      "type": "manifest",
      "title": "setup_cert_manager.yaml",
      "summary": "--- # Install and configure cert-manager for TLS certificate management",
      "sizeBytes": 20256,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "containers",
        "monitoring",
        "deployment",
        "troubleshooting",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.838Z"
    },
    {
      "id": "2adacfa6",
      "path": "ansible/plays/kubernetes/setup_helm.yaml",
      "type": "manifest",
      "title": "setup_helm.yaml",
      "summary": "--- # Install Helm on the control plane node",
      "sizeBytes": 2025,
      "tags": [
        "ansible",
        "automation",
        "monitoring",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.838Z"
    },
    {
      "id": "5407a29a",
      "path": "ansible/plays/kubernetes/setup_local_path_provisioner.yaml",
      "type": "manifest",
      "title": "setup_local_path_provisioner.yaml",
      "summary": "--- # Deploy custom local-path provisioner with /srv/monitoring_data storage path",
      "sizeBytes": 7259,
      "tags": [
        "ansible",
        "automation",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.838Z"
    },
    {
      "id": "6ec02888",
      "path": "ansible/plays/kubernetes/templates/kube-flannel-allnodes.yml",
      "type": "manifest",
      "title": "kube-flannel-allnodes.yml",
      "summary": "--- # Custom Flannel manifest for VMStation",
      "sizeBytes": 4981,
      "tags": [
        "ansible",
        "automation",
        "networking",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.838Z"
    },
    {
      "id": "b9618ca5",
      "path": "ansible/plays/network-diagnosis.yaml",
      "type": "manifest",
      "title": "network-diagnosis.yaml",
      "summary": "--- # VMStation Inter-Pod Communication Network Diagnosis Playbook",
      "sizeBytes": 13957,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "networking",
        "deployment",
        "troubleshooting",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.838Z"
    },
    {
      "id": "f0bc548f",
      "path": "ansible/plays/setup-cluster.yaml",
      "type": "manifest",
      "title": "setup-cluster.yaml",
      "summary": "--- # VMStation Simplified Kubernetes Cluster Setup",
      "sizeBytes": 109519,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "containers",
        "networking",
        "deployment",
        "troubleshooting",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.838Z"
    },
    {
      "id": "74a84d68",
      "path": "ansible/requirements.yml",
      "type": "manifest",
      "title": "requirements.yml",
      "summary": "# Ansible Collections Requirements for VMStation Kubernetes collections:",
      "sizeBytes": 214,
      "tags": [
        "ansible",
        "automation",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.839Z"
    },
    {
      "id": "b0430b0d",
      "path": "ansible/roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-minimal.yml",
      "type": "manifest",
      "title": "jellyfin-minimal.yml",
      "summary": "# ============================================================================ # Minimal Jellyfin Kubernetes Deployment",
      "sizeBytes": 7793,
      "tags": [
        "ansible",
        "automation",
        "jellyfin",
        "media",
        "kubernetes",
        "monitoring",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.839Z"
    },
    {
      "id": "c782d582",
      "path": "ansible/roles/jellyfin/tasks/main.yml",
      "type": "manifest",
      "title": "main.yml",
      "summary": "--- # Jellyfin Deployment Role - Main Tasks",
      "sizeBytes": 8069,
      "tags": [
        "ansible",
        "automation",
        "jellyfin",
        "media",
        "monitoring",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.839Z"
    },
    {
      "id": "dd358be7",
      "path": "ansible/simple-deploy.yaml",
      "type": "manifest",
      "title": "simple-deploy.yaml",
      "summary": "--- # VMStation Simplified Deployment Playbook",
      "sizeBytes": 9806,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "monitoring",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.839Z"
    },
    {
      "id": "9fa74fe4",
      "path": "ansible/subsites/00-spindown.yaml",
      "type": "manifest",
      "title": "00-spindown.yaml",
      "summary": "--- # Subsite 00: Spin-down / destructive cleanup helper",
      "sizeBytes": 29392,
      "tags": [
        "ansible",
        "automation",
        "kubernetes",
        "containers",
        "monitoring",
        "networking",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.839Z"
    },
    {
      "id": "3d26bfd6",
      "path": "manifests/cni/flannel-minimal.yaml",
      "type": "manifest",
      "title": "flannel-minimal.yaml",
      "summary": "--- # Minimal Flannel CNI Plugin for VMStation Kubernetes Cluster",
      "sizeBytes": 4489,
      "tags": [
        "kubernetes",
        "manifests",
        "networking",
        "containers",
        "deployment",
        "troubleshooting",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.842Z"
    },
    {
      "id": "0a813314",
      "path": "manifests/cni/flannel.yaml",
      "type": "manifest",
      "title": "flannel.yaml",
      "summary": "--- # Flannel CNI Plugin for VMStation Kubernetes Cluster",
      "sizeBytes": 4513,
      "tags": [
        "kubernetes",
        "manifests",
        "networking",
        "containers",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.842Z"
    },
    {
      "id": "4ede3e1b",
      "path": "manifests/jellyfin/jellyfin-minimal.yaml",
      "type": "manifest",
      "title": "jellyfin-minimal.yaml",
      "summary": "--- # Minimal Jellyfin deployment for VMStation",
      "sizeBytes": 2293,
      "tags": [
        "kubernetes",
        "manifests",
        "jellyfin",
        "media",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.842Z"
    },
    {
      "id": "269791b0",
      "path": "manifests/jellyfin/jellyfin.yaml",
      "type": "manifest",
      "title": "jellyfin.yaml",
      "summary": "--- # VMStation Jellyfin Namespace",
      "sizeBytes": 4827,
      "tags": [
        "kubernetes",
        "manifests",
        "jellyfin",
        "media",
        "networking",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "24a2f90a",
      "path": "manifests/monitoring/grafana.yaml",
      "type": "manifest",
      "title": "grafana.yaml",
      "summary": "--- # Grafana ConfigMap for datasources",
      "sizeBytes": 6401,
      "tags": [
        "kubernetes",
        "manifests",
        "monitoring",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "1d93e806",
      "path": "manifests/monitoring/prometheus.yaml",
      "type": "manifest",
      "title": "prometheus.yaml",
      "summary": "--- # VMStation Monitoring Namespace",
      "sizeBytes": 4301,
      "tags": [
        "kubernetes",
        "manifests",
        "monitoring",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "71a6fc0c",
      "path": "manifests/network/coredns-configmap.yaml",
      "type": "manifest",
      "title": "coredns-configmap.yaml",
      "summary": "apiVersion: v1 kind: ConfigMap",
      "sizeBytes": 1197,
      "tags": [
        "kubernetes",
        "manifests",
        "networking",
        "monitoring",
        "jellyfin",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "81cd44f2",
      "path": "manifests/network/coredns-deployment.yaml",
      "type": "manifest",
      "title": "coredns-deployment.yaml",
      "summary": "apiVersion: apps/v1 kind: Deployment",
      "sizeBytes": 2934,
      "tags": [
        "kubernetes",
        "manifests",
        "networking",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "0d91b1c9",
      "path": "manifests/network/coredns-minimal.yaml",
      "type": "manifest",
      "title": "coredns-minimal.yaml",
      "summary": "--- # Minimal CoreDNS Configuration for VMStation",
      "sizeBytes": 4804,
      "tags": [
        "kubernetes",
        "manifests",
        "networking",
        "monitoring",
        "deployment",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "ad0e321a",
      "path": "manifests/network/coredns-service.yaml",
      "type": "manifest",
      "title": "coredns-service.yaml",
      "summary": "apiVersion: v1 kind: Service",
      "sizeBytes": 485,
      "tags": [
        "kubernetes",
        "manifests",
        "networking",
        "monitoring",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "2883003d",
      "path": "manifests/network/kube-proxy-configmap.yaml",
      "type": "manifest",
      "title": "kube-proxy-configmap.yaml",
      "summary": "apiVersion: v1 kind: ConfigMap",
      "sizeBytes": 1348,
      "tags": [
        "kubernetes",
        "manifests",
        "networking",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "80025501",
      "path": "manifests/network/kube-proxy-daemonset.yaml",
      "type": "manifest",
      "title": "kube-proxy-daemonset.yaml",
      "summary": "apiVersion: apps/v1 kind: DaemonSet",
      "sizeBytes": 1792,
      "tags": [
        "kubernetes",
        "manifests",
        "networking",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "0aec39aa",
      "path": "manifests/network/kube-proxy-minimal.yaml",
      "type": "manifest",
      "title": "kube-proxy-minimal.yaml",
      "summary": "--- # Minimal kube-proxy configuration for mixed OS environment",
      "sizeBytes": 3493,
      "tags": [
        "kubernetes",
        "manifests",
        "networking",
        "manifest"
      ],
      "lastModified": "2025-09-16T21:28:57.843Z"
    },
    {
      "id": "04c6e90f",
      "path": "README.md",
      "type": "doc",
      "title": "README.md",
      "summary": "Welcome to VMStation! A home cloud infrastructure built on Kubernetes for scalable, reliable self-hosted services.",
      "sizeBytes": 31457,
      "tags": [
        "kubernetes",
        "containers",
        "monitoring",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "doc"
      ],
      "lastModified": "2025-09-16T21:28:57.836Z"
    },
    {
      "id": "0ff387c7",
      "path": "TODO.md",
      "type": "todo",
      "title": "TODO.md",
      "summary": "# VMStation TODO List This repository uses **kubeadm-based Kubernetes** for production deployments. ## Current Status",
      "sizeBytes": 2541,
      "tags": [
        "kubernetes",
        "containers",
        "monitoring",
        "networking",
        "deployment",
        "troubleshooting",
        "todo"
      ],
      "lastModified": "2025-09-16T21:28:57.836Z"
    },
    {
      "id": "72c7835a",
      "path": "deploy-cluster.sh",
      "type": "script",
      "title": "deploy-cluster.sh",
      "summary": "VMStation Kubernetes Cluster Deployment Script Wrapper for the complete cluster bootstrap process Colors",
      "sizeBytes": 42371,
      "tags": [
        "kubernetes",
        "containers",
        "networking",
        "jellyfin",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.839Z"
    },
    {
      "id": "0ecf3a99",
      "path": "generate_join_command.sh",
      "type": "script",
      "title": "generate_join_command.sh",
      "summary": "VMStation Kubeadm Join Command Generator Generates the exact join command for worker nodes as recommended in problem statement Colors",
      "sizeBytes": 5828,
      "tags": [
        "kubernetes",
        "deployment",
        "troubleshooting",
        "script"
      ],
      "lastModified": "2025-09-16T21:28:57.842Z"
    }
  ],
  "edges": [
    {
      "from": "efc0b58e",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "- **`enhanced_kubeadm_join.sh`** - Enhanced Kubernetes join process with comprehensive validation"
    },
    {
      "from": "efc0b58e",
      "to": "37f70066",
      "relation": "calls",
      "context": "- **`comprehensive_worker_setup.sh`** - Complete worker node setup and configuration"
    },
    {
      "from": "efc0b58e",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "- **`fix_cluster_communication.sh`** - Fixes cluster communication and networking issues"
    },
    {
      "from": "efc0b58e",
      "to": "090d10b0",
      "relation": "calls",
      "context": "- **`fix_remaining_pod_issues.sh`** - Fixes common pod issues including Jellyfin readiness and kube-"
    },
    {
      "from": "efc0b58e",
      "to": "62721f19",
      "relation": "calls",
      "context": "- **`validate_pod_health.sh`** - Validates pod health and cluster status"
    },
    {
      "from": "efc0b58e",
      "to": "563e9b03",
      "relation": "calls",
      "context": "- **`vmstation_status.sh`** - Comprehensive cluster status and diagnostic information"
    },
    {
      "from": "efc0b58e",
      "to": "1970801b",
      "relation": "calls",
      "context": "- **`fix_cni_bridge_conflict.sh`** - Fixes CNI bridge IP conflicts causing ContainerCreating errors"
    },
    {
      "from": "efc0b58e",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "- **`fix_worker_node_cni.sh`** - Fixes worker node CNI communication issues"
    },
    {
      "from": "efc0b58e",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "- **`fix_flannel_mixed_os.sh`** - Fixes Flannel issues in mixed OS environments"
    },
    {
      "from": "efc0b58e",
      "to": "7c606adf",
      "relation": "calls",
      "context": "- **`validate_cluster_communication.sh`** - Validates cluster communication and NodePort services"
    },
    {
      "from": "efc0b58e",
      "to": "3128233f",
      "relation": "calls",
      "context": "- **`diagnose_remaining_pod_issues.sh`** - Analyzes specific pod failures and issues"
    },
    {
      "from": "efc0b58e",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "- **`gather_worker_diagnostics.sh`** - Comprehensive worker node diagnostic collection"
    },
    {
      "from": "efc0b58e",
      "to": "09b572dc",
      "relation": "calls",
      "context": "- **`run_network_diagnosis.sh`** - Network diagnosis and connectivity testing"
    },
    {
      "from": "efc0b58e",
      "to": "36bd22c9",
      "relation": "calls",
      "context": "- **`check_coredns_status.sh`** - CoreDNS status and configuration validation"
    },
    {
      "from": "efc0b58e",
      "to": "b428256d",
      "relation": "calls",
      "context": "- **`validate_join_prerequisites.sh`** - Comprehensive validation before kubeadm join"
    },
    {
      "from": "efc0b58e",
      "to": "6ea2bc0f",
      "relation": "calls",
      "context": "- **`validate_post_wipe_functionality.sh`** - Validates post-wipe worker join functionality"
    },
    {
      "from": "efc0b58e",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "- **`validate_pod_connectivity.sh`** - Tests pod-to-pod CNI communication"
    },
    {
      "from": "efc0b58e",
      "to": "27edfbfd",
      "relation": "calls",
      "context": "- **`validate_nodeport_external_access.sh`** - Validates external access to NodePort services"
    },
    {
      "from": "efc0b58e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh"
    },
    {
      "from": "efc0b58e",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "./scripts/enhanced_kubeadm_join.sh"
    },
    {
      "from": "efc0b58e",
      "to": "37f70066",
      "relation": "calls",
      "context": "./scripts/comprehensive_worker_setup.sh"
    },
    {
      "from": "efc0b58e",
      "to": "563e9b03",
      "relation": "calls",
      "context": "./scripts/vmstation_status.sh"
    },
    {
      "from": "efc0b58e",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "efc0b58e",
      "to": "090d10b0",
      "relation": "calls",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "efc0b58e",
      "to": "1970801b",
      "relation": "calls",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "efc0b58e",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "./scripts/fix_worker_node_cni.sh"
    },
    {
      "from": "efc0b58e",
      "to": "62721f19",
      "relation": "calls",
      "context": "./scripts/validate_pod_health.sh"
    },
    {
      "from": "efc0b58e",
      "to": "7c606adf",
      "relation": "calls",
      "context": "./scripts/validate_cluster_communication.sh"
    },
    {
      "from": "efc0b58e",
      "to": "3128233f",
      "relation": "calls",
      "context": "./scripts/diagnose_remaining_pod_issues.sh"
    },
    {
      "from": "efc0b58e",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "./scripts/gather_worker_diagnostics.sh"
    },
    {
      "from": "efc0b58e",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "| `enhanced_kubeadm_join.sh` | Enhanced Kubernetes join process | 1388 |"
    },
    {
      "from": "efc0b58e",
      "to": "37f70066",
      "relation": "calls",
      "context": "| `comprehensive_worker_setup.sh` | Complete worker node setup | 481 |"
    },
    {
      "from": "efc0b58e",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "| `fix_cluster_communication.sh` | Cluster communication fixes | 705 |"
    },
    {
      "from": "efc0b58e",
      "to": "090d10b0",
      "relation": "calls",
      "context": "| `fix_remaining_pod_issues.sh` | Pod issue remediation | 673 |"
    },
    {
      "from": "efc0b58e",
      "to": "1970801b",
      "relation": "calls",
      "context": "| `fix_cni_bridge_conflict.sh` | CNI bridge IP conflict fixes | 378 |"
    },
    {
      "from": "efc0b58e",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "| `fix_worker_node_cni.sh` | Worker CNI communication fixes | 423 |"
    },
    {
      "from": "efc0b58e",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "| `fix_flannel_mixed_os.sh` | Mixed OS Flannel fixes | 403 |"
    },
    {
      "from": "efc0b58e",
      "to": "7c606adf",
      "relation": "calls",
      "context": "| `validate_cluster_communication.sh` | Cluster communication validation | 339 |"
    },
    {
      "from": "efc0b58e",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "| `gather_worker_diagnostics.sh` | Worker diagnostic collection | 453 |"
    },
    {
      "from": "efc0b58e",
      "to": "b428256d",
      "relation": "calls",
      "context": "| `validate_join_prerequisites.sh` | Pre-join validation | 398 |"
    },
    {
      "from": "efc0b58e",
      "to": "3128233f",
      "relation": "calls",
      "context": "| `diagnose_remaining_pod_issues.sh` | Pod issue diagnosis | 195 |"
    },
    {
      "from": "efc0b58e",
      "to": "62721f19",
      "relation": "calls",
      "context": "| `validate_pod_health.sh` | Pod health validation | 109 |"
    },
    {
      "from": "efc0b58e",
      "to": "607e6e59",
      "relation": "references",
      "context": "- **Configuration**: Falls back to `ansible/group_vars/all.yml`"
    },
    {
      "from": "36bd22c9",
      "to": "7071b363",
      "relation": "calls",
      "context": "echo \"  ./scripts/fix_coredns_unknown_status.sh\""
    },
    {
      "from": "36bd22c9",
      "to": "72c7835a",
      "relation": "calls",
      "context": "echo \"  ./deploy-cluster.sh full && ./scripts/fix_coredns_unknown_status.sh\""
    },
    {
      "from": "36bd22c9",
      "to": "7071b363",
      "relation": "calls",
      "context": "echo \"  ./deploy-cluster.sh full && ./scripts/fix_coredns_unknown_status.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "b428256d",
      "relation": "calls",
      "context": "local validator=\"$script_dir/validate_join_prerequisites.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "21313a3e",
      "relation": "calls",
      "context": "local manual_fix_script=\"$script_dir/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "21313a3e",
      "relation": "calls",
      "context": "error \"   sudo ./scripts/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "local validator=\"$script_dir/validate_systemd_dropins.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "21313a3e",
      "relation": "calls",
      "context": "local manual_fix_script=\"$script_dir/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "21313a3e",
      "relation": "calls",
      "context": "error \"   sudo ./scripts/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "6564808c",
      "relation": "calls",
      "context": "error \"   sudo ./worker_node_join_remediation.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "1b255a42",
      "relation": "calls",
      "context": "error \"   sudo ./scripts/quick_join_diagnostics.sh\""
    },
    {
      "from": "1e7cdd43",
      "to": "630db1ca",
      "relation": "calls",
      "context": "if ! run_fix_script \"fix_iptables_compatibility.sh\" \"iptables/nftables compatibility fix\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "1970801b",
      "relation": "calls",
      "context": "if ! run_fix_script \"fix_cni_bridge_conflict.sh\" \"CNI bridge conflict fix\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "if ! run_fix_script \"fix_worker_node_cni.sh\" \"Worker node CNI communication fix\" \"--node\" \"storageno"
    },
    {
      "from": "1e7cdd43",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "if ! run_fix_script \"fix_flannel_mixed_os.sh\" \"Flannel mixed-OS configuration fix\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "090d10b0",
      "relation": "calls",
      "context": "if ! run_fix_script \"fix_remaining_pod_issues.sh\" \"kube-proxy and pod issues fix\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "if ! run_fix_script \"fix_worker_kubectl_config.sh\" \"kubectl worker node configuration\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "7c606adf",
      "relation": "calls",
      "context": "if ! run_fix_script \"validate_cluster_communication.sh\" \"cluster communication validation\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "if ! run_fix_script \"validate_pod_connectivity.sh\" \"detailed pod-to-pod connectivity validation\"; th"
    },
    {
      "from": "1e7cdd43",
      "to": "09f42a5d",
      "relation": "calls",
      "context": "if ! run_fix_script \"fix_nodeport_external_access.sh\" \"NodePort external access fix\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "7c606adf",
      "relation": "calls",
      "context": "echo \"  ./scripts/validate_cluster_communication.sh\""
    },
    {
      "from": "1e7cdd43",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "echo \"  ./scripts/validate_pod_connectivity.sh\""
    },
    {
      "from": "7071b363",
      "to": "72c7835a",
      "relation": "calls",
      "context": "# after flannel pods have been regenerated by deploy-cluster.sh full"
    },
    {
      "from": "263c2cb9",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "echo \"2. Test pod-to-pod connectivity: ./scripts/validate_pod_connectivity.sh\""
    },
    {
      "from": "a249d706",
      "to": "1970801b",
      "relation": "calls",
      "context": "if [ -f \"./scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "a249d706",
      "to": "1970801b",
      "relation": "calls",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "a249d706",
      "to": "090d10b0",
      "relation": "calls",
      "context": "if [ -f \"./scripts/fix_remaining_pod_issues.sh\" ]; then"
    },
    {
      "from": "a249d706",
      "to": "090d10b0",
      "relation": "calls",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "a249d706",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"4. If Jellyfin shows 0/1 ready, run: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "090d10b0",
      "to": "72c7835a",
      "relation": "calls",
      "context": "# Addresses specific problems after deploy-cluster.sh and fix_homelab_node_issues.sh"
    },
    {
      "from": "090d10b0",
      "to": "a249d706",
      "relation": "calls",
      "context": "# Addresses specific problems after deploy-cluster.sh and fix_homelab_node_issues.sh"
    },
    {
      "from": "090d10b0",
      "to": "1970801b",
      "relation": "calls",
      "context": "if [ -f \"./scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "090d10b0",
      "to": "1970801b",
      "relation": "calls",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "1b255a42",
      "to": "21313a3e",
      "relation": "calls",
      "context": "error \"   sudo ./scripts/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "1b255a42",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "info \"   sudo ./scripts/enhanced_kubeadm_join.sh \\\"<your-join-command>\\\"\""
    },
    {
      "from": "09b572dc",
      "to": "b9618ca5",
      "relation": "references",
      "context": "PLAYBOOK_PATH=\"$REPO_ROOT/ansible/plays/network-diagnosis.yaml\""
    },
    {
      "from": "4f2a3f41",
      "to": "ad0e321a",
      "relation": "references",
      "context": "local coredns_manifest=\"/home/runner/work/VMStation/VMStation/manifests/network/coredns-service.yaml"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "if [ -f \"manifests/network/coredns-configmap.yaml\" ]; then"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "kubectl apply -f manifests/network/coredns-configmap.yaml"
    },
    {
      "from": "4f2a3f41",
      "to": "4096f24e",
      "relation": "references",
      "context": "local doc_file=\"/home/runner/work/VMStation/VMStation/docs/static-ips-and-dns.md\""
    },
    {
      "from": "4f2a3f41",
      "to": "ad0e321a",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/network/coredns-service.yaml`"
    },
    {
      "from": "4f2a3f41",
      "to": "80025501",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/network/kube-proxy-daemonset.yaml`"
    },
    {
      "from": "4f2a3f41",
      "to": "0a813314",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/cni/flannel.yaml`"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/network/coredns-configmap.yaml`"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "kubectl apply -f manifests/network/coredns-configmap.yaml"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "echo \"   CoreDNS config: manifests/network/coredns-configmap.yaml\""
    },
    {
      "from": "4f2a3f41",
      "to": "4096f24e",
      "relation": "references",
      "context": "echo \"   Documentation: docs/static-ips-and-dns.md\""
    },
    {
      "from": "0f552648",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "INVENTORY_FILE=\"ansible/inventory/hosts.yml\""
    },
    {
      "from": "0f552648",
      "to": "91932608",
      "relation": "references",
      "context": "echo \"   - Run full verification: ansible-playbook ansible/playbooks/verify-cluster.yml\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script exists\" \"test -x '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "run_test \"Gather diagnostics script exists\" \"test -x '$PROJECT_ROOT/scripts/gather_worker_diagnostic"
    },
    {
      "from": "b804c80b",
      "to": "1b255a42",
      "relation": "calls",
      "context": "run_test \"Quick diagnostics script exists\" \"test -x '$PROJECT_ROOT/scripts/quick_join_diagnostics.sh"
    },
    {
      "from": "b804c80b",
      "to": "f0bc548f",
      "relation": "references",
      "context": "run_test \"Ansible playbook syntax check\" \"ansible-playbook --syntax-check '$PROJECT_ROOT/ansible/pla"
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script invalid args handling\" \"'$PROJECT_ROOT/scripts/enhanced_kubeadm_join."
    },
    {
      "from": "b804c80b",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "run_test \"Gather diagnostics script help\" \"'$PROJECT_ROOT/scripts/gather_worker_diagnostics.sh' --he"
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script contains crictl fixes\" \"grep -q 'crictl.*config\\|socket.*perm' '$PROJ"
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script contains token refresh\" \"grep -q 'refresh.*token\\|TOKEN_REFRESH' '$PR"
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script contains kubelet validation\" \"grep -q 'validate.*kubelet.*config\\|kub"
    },
    {
      "from": "b804c80b",
      "to": "f0bc548f",
      "relation": "references",
      "context": "run_test \"Ansible playbook contains enhanced error handling\" \"grep -q 'failure.*diagnostics\\|Handle."
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script has comprehensive logging\" \"grep -q 'LOG_FILE\\|log_both\\|tee.*LOG' '$"
    },
    {
      "from": "b804c80b",
      "to": "121dca56",
      "relation": "references",
      "context": "run_test \"Enhanced troubleshooting documentation exists\" \"test -f '$PROJECT_ROOT/docs/MANUAL_CLUSTER"
    },
    {
      "from": "b804c80b",
      "to": "121dca56",
      "relation": "references",
      "context": "run_test \"Documentation contains crictl troubleshooting\" \"grep -q 'crictl.*communication\\|socket.*pe"
    },
    {
      "from": "b804c80b",
      "to": "121dca56",
      "relation": "references",
      "context": "run_test \"Documentation mentions containerd filesystem fixes\" \"grep -q 'invalid capacity 0\\|filesyst"
    },
    {
      "from": "b804c80b",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "run_test \"Gather script collects comprehensive diagnostics\" \"grep -q 'kubelet.*log\\|containerd.*log\\"
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script crictl test simulation\" \"bash -n '$PROJECT_ROOT/scripts/enhanced_kube"
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script has proper error handling\" \"grep -q 'exit 1\\|failed_when.*false\\|erro"
    },
    {
      "from": "b804c80b",
      "to": "1b255a42",
      "relation": "calls",
      "context": "run_test \"Quick diagnostics script syntax check\" \"bash -n '$PROJECT_ROOT/scripts/quick_join_diagnost"
    },
    {
      "from": "b804c80b",
      "to": "f0bc548f",
      "relation": "references",
      "context": "run_test \"Ansible playbook handles socket permissions\" \"grep -q 'containerd.*socket\\|socket.*permiss"
    },
    {
      "from": "b804c80b",
      "to": "f0bc548f",
      "relation": "references",
      "context": "run_test \"Ansible playbook has skip join logic\" \"grep -q 'skip.*join\\|already.*joined\\|KUBELET_ALREA"
    },
    {
      "from": "7daa8953",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "\"[ -x 'scripts/setup_static_ips_and_dns.sh' ]\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "\"[ -x 'scripts/validate_static_ips_and_dns.sh' ]\""
    },
    {
      "from": "7daa8953",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "\"bash -n scripts/setup_static_ips_and_dns.sh\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "\"bash -n scripts/validate_static_ips_and_dns.sh\""
    },
    {
      "from": "7daa8953",
      "to": "4096f24e",
      "relation": "references",
      "context": "\"[ -f 'docs/static-ips-and-dns.md' ]\""
    },
    {
      "from": "7daa8953",
      "to": "4096f24e",
      "relation": "references",
      "context": "\"grep -q 'Static IP Assignment' docs/static-ips-and-dns.md && grep -q 'homelab.com' docs/static-ips-"
    },
    {
      "from": "7daa8953",
      "to": "4096f24e",
      "relation": "references",
      "context": "\"grep -q 'Static IP Assignment' docs/static-ips-and-dns.md && grep -q 'homelab.com' docs/static-ips-"
    },
    {
      "from": "7daa8953",
      "to": "ad0e321a",
      "relation": "references",
      "context": "\"grep -q 'clusterIP: 10.96.0.10' manifests/network/coredns-service.yaml\""
    },
    {
      "from": "7daa8953",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "\"grep -q 'homelab.com:53' manifests/network/coredns-configmap.yaml\""
    },
    {
      "from": "7daa8953",
      "to": "80025501",
      "relation": "references",
      "context": "\"grep -q 'hostNetwork: true' manifests/network/kube-proxy-daemonset.yaml\""
    },
    {
      "from": "7daa8953",
      "to": "0a813314",
      "relation": "references",
      "context": "\"grep -q 'hostNetwork: true' manifests/cni/flannel.yaml\""
    },
    {
      "from": "7daa8953",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "\"grep -q 'setup_static_ips_and_dns.sh' deploy-cluster.sh\""
    },
    {
      "from": "7daa8953",
      "to": "72c7835a",
      "relation": "calls",
      "context": "\"grep -q 'setup_static_ips_and_dns.sh' deploy-cluster.sh\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "\"scripts/validate_static_ips_and_dns.sh --help | grep -q 'Usage:'\""
    },
    {
      "from": "7daa8953",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "\"timeout 10 scripts/setup_static_ips_and_dns.sh verify >/dev/null 2>&1 || true\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "\"scripts/validate_static_ips_and_dns.sh manifests | grep -q 'All tests passed'\""
    },
    {
      "from": "7daa8953",
      "to": "72c7835a",
      "relation": "calls",
      "context": "info \"  1. Deploy a cluster: ./deploy-cluster.sh deploy\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "info \"  2. Run full validation: ./scripts/validate_static_ips_and_dns.sh\""
    },
    {
      "from": "7c606adf",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "echo \"1. Run kubectl configuration fix: ./scripts/fix_worker_kubectl_config.sh\""
    },
    {
      "from": "7c606adf",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"2. Fix kube-proxy issues: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "7c606adf",
      "to": "1970801b",
      "relation": "calls",
      "context": "echo \"3. Fix CNI bridge conflicts: ./scripts/fix_cni_bridge_conflict.sh\""
    },
    {
      "from": "7c606adf",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"3. Run comprehensive fix: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "ac18a0b8",
      "to": "6ec02888",
      "relation": "references",
      "context": "local template_file=\"/home/runner/work/VMStation/VMStation/ansible/plays/kubernetes/templates/kube-f"
    },
    {
      "from": "ac18a0b8",
      "to": "f0bc548f",
      "relation": "references",
      "context": "local playbook_path=\"/home/runner/work/VMStation/VMStation/ansible/plays/setup-cluster.yaml\""
    },
    {
      "from": "ac18a0b8",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if cd \"$(dirname \"$playbook_path\")/../..\" && ansible-playbook --syntax-check -i ansible/inventory.tx"
    },
    {
      "from": "ca8ac5f9",
      "to": "7a273252",
      "relation": "calls",
      "context": "warn \"SOLUTION: Run 'sudo ./scripts/reset_cni_bridge.sh' to fix this issue\""
    },
    {
      "from": "ca8ac5f9",
      "to": "7a273252",
      "relation": "calls",
      "context": "warn \"SOLUTION: Run 'sudo ./scripts/reset_cni_bridge.sh' to fix this issue\""
    },
    {
      "from": "ca8ac5f9",
      "to": "7a273252",
      "relation": "calls",
      "context": "warn \" IMMEDIATE FIX: Run 'sudo ./scripts/reset_cni_bridge.sh'\""
    },
    {
      "from": "ca8ac5f9",
      "to": "7a273252",
      "relation": "calls",
      "context": "echo \"  1. Run sudo ./scripts/reset_cni_bridge.sh if CNI bridge conflicts detected\""
    },
    {
      "from": "ca8ac5f9",
      "to": "1970801b",
      "relation": "calls",
      "context": "echo \"  2. Run ./scripts/fix_cni_bridge_conflict.sh for comprehensive CNI fixes\""
    },
    {
      "from": "ca8ac5f9",
      "to": "a249d706",
      "relation": "calls",
      "context": "echo \"  3. Run ./scripts/fix_homelab_node_issues.sh for node-specific problems\""
    },
    {
      "from": "27edfbfd",
      "to": "09f42a5d",
      "relation": "calls",
      "context": "warn \" Some validations failed. Run fix_nodeport_external_access.sh if needed.\""
    },
    {
      "from": "52dfbbac",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "echo \"  1. sudo ./scripts/fix_worker_node_cni.sh --node storagenodet3500\""
    },
    {
      "from": "52dfbbac",
      "to": "1970801b",
      "relation": "calls",
      "context": "echo \"  2. ./scripts/fix_cni_bridge_conflict.sh\""
    },
    {
      "from": "62721f19",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"Run: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "62721f19",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"Run: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "62721f19",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"1. Run: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "6ea2bc0f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "if grep -q \"detect_post_wipe_state\" scripts/enhanced_kubeadm_join.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "if grep -q \"WORKER_POST_WIPE\" scripts/enhanced_kubeadm_join.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"Detect post-wipe worker state\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"worker_was_wiped\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"control-plane readiness\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"Generate fresh join command for wiped workers\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"ttl=2h\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"Enhanced kubeadm join with post-wipe worker support\" ansible/plays/setup-cluster.yaml; t"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"Post-join validation and verification for wiped workers\" ansible/plays/setup-cluster.yam"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"NOT standalone\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "044d8702",
      "relation": "references",
      "context": "if [ -f docs/POST_WIPE_WORKER_JOIN.md ]; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "calls",
      "context": "# Test 6: Validate deploy-cluster.sh integration"
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "calls",
      "context": "info \"Test 6: Validating deploy-cluster.sh integration...\""
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "calls",
      "context": "if grep -q \"cluster\" deploy-cluster.sh && grep -q \"setup-cluster.yaml\" deploy-cluster.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "calls",
      "context": "if grep -q \"cluster\" deploy-cluster.sh && grep -q \"setup-cluster.yaml\" deploy-cluster.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"cluster\" deploy-cluster.sh && grep -q \"setup-cluster.yaml\" deploy-cluster.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "calls",
      "context": "info \" deploy-cluster.sh integrates with enhanced cluster setup\""
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "calls",
      "context": "error \" deploy-cluster.sh missing cluster setup integration\""
    },
    {
      "from": "6ea2bc0f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "if bash -n scripts/enhanced_kubeadm_join.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if ansible-playbook --syntax-check ansible/plays/setup-cluster.yaml >/dev/null 2>&1; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "if grep -r \"$marker\" ansible/plays/setup-cluster.yaml scripts/enhanced_kubeadm_join.sh >/dev/null 2>"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -r \"$marker\" ansible/plays/setup-cluster.yaml scripts/enhanced_kubeadm_join.sh >/dev/null 2>"
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "calls",
      "context": "info \" Full integration with deploy-cluster.sh cluster deployment\""
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "calls",
      "context": "info \"  2. From master node: ./deploy-cluster.sh cluster\""
    },
    {
      "from": "5e0bdb07",
      "to": "ad0e321a",
      "relation": "references",
      "context": "\"grep -q 'clusterIP: 10.96.0.10' /home/runner/work/VMStation/VMStation/manifests/network/coredns-ser"
    },
    {
      "from": "5e0bdb07",
      "to": "80025501",
      "relation": "references",
      "context": "\"grep -q 'hostNetwork: true' /home/runner/work/VMStation/VMStation/manifests/network/kube-proxy-daem"
    },
    {
      "from": "5e0bdb07",
      "to": "0a813314",
      "relation": "references",
      "context": "\"grep -q 'hostNetwork: true' /home/runner/work/VMStation/VMStation/manifests/cni/flannel.yaml\""
    },
    {
      "from": "5e0bdb07",
      "to": "4096f24e",
      "relation": "references",
      "context": "\"[ -f '/home/runner/work/VMStation/VMStation/docs/static-ips-and-dns.md' ]\""
    },
    {
      "from": "563e9b03",
      "to": "a249d706",
      "relation": "calls",
      "context": "echo \"   ./scripts/fix_homelab_node_issues.sh\""
    },
    {
      "from": "563e9b03",
      "to": "36bd22c9",
      "relation": "calls",
      "context": "echo \"   ./scripts/check_coredns_status.sh\""
    },
    {
      "from": "563e9b03",
      "to": "7071b363",
      "relation": "calls",
      "context": "echo \"   ./scripts/fix_coredns_unknown_status.sh\""
    },
    {
      "from": "563e9b03",
      "to": "72c7835a",
      "relation": "calls",
      "context": "echo \"   ./deploy-cluster.sh apps\""
    },
    {
      "from": "563e9b03",
      "to": "a249d706",
      "relation": "calls",
      "context": "echo \"   ./scripts/fix_homelab_node_issues.sh\""
    },
    {
      "from": "563e9b03",
      "to": "72c7835a",
      "relation": "calls",
      "context": "echo \"   ./deploy-cluster.sh apps\""
    },
    {
      "from": "563e9b03",
      "to": "e8e06adf",
      "relation": "references",
      "context": "note \"For detailed troubleshooting, see: docs/HOMELAB_NODE_FIXES.md\""
    },
    {
      "from": "6564808c",
      "to": "1b255a42",
      "relation": "calls",
      "context": "local diag_script=\"$script_dir/quick_join_diagnostics.sh\""
    },
    {
      "from": "6564808c",
      "to": "21313a3e",
      "relation": "calls",
      "context": "local manual_fix=\"$script_dir/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "6564808c",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "info \"   sudo ./scripts/enhanced_kubeadm_join.sh \\\"<join-command-from-step-1>\\\"\""
    },
    {
      "from": "8c19061a",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"kubeadm join 192.168.4.63:6443 --token abc.123 --discovery-"
    },
    {
      "from": "8c19061a",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "- `scripts/enhanced_kubeadm_join.sh` - Added aggressive_node_reset() function and enhanced cleanup_f"
    },
    {
      "from": "094a0932",
      "to": "7fb7d1a5",
      "relation": "calls",
      "context": "2. **Manual diagnosis**: `./scripts/check_cni_bridge_conflict.sh`"
    },
    {
      "from": "094a0932",
      "to": "1970801b",
      "relation": "calls",
      "context": "3. **Manual fix**: `./scripts/fix_cni_bridge_conflict.sh`"
    },
    {
      "from": "094a0932",
      "to": "a249d706",
      "relation": "calls",
      "context": "- **`fix_homelab_node_issues.sh`**: Now includes CNI bridge conflict detection as Step 0"
    },
    {
      "from": "094a0932",
      "to": "7fb7d1a5",
      "relation": "calls",
      "context": "./scripts/check_cni_bridge_conflict.sh"
    },
    {
      "from": "094a0932",
      "to": "1970801b",
      "relation": "calls",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "f0840b04",
      "to": "7a273252",
      "relation": "calls",
      "context": "sudo ./scripts/reset_cni_bridge.sh"
    },
    {
      "from": "f0840b04",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "- Run `./scripts/validate_network_prerequisites.sh` first to detect issues"
    },
    {
      "from": "8facf1bd",
      "to": "81cd44f2",
      "relation": "references",
      "context": "### 1. `manifests/network/coredns-deployment.yaml`"
    },
    {
      "from": "8facf1bd",
      "to": "a249d706",
      "relation": "calls",
      "context": "### 2. `scripts/fix_homelab_node_issues.sh`"
    },
    {
      "from": "8facf1bd",
      "to": "f0bc548f",
      "relation": "references",
      "context": "### 3. `ansible/plays/setup-cluster.yaml`"
    },
    {
      "from": "fcb85303",
      "to": "36bd22c9",
      "relation": "calls",
      "context": "./scripts/check_coredns_status.sh"
    },
    {
      "from": "fcb85303",
      "to": "7071b363",
      "relation": "calls",
      "context": "./scripts/fix_coredns_unknown_status.sh"
    },
    {
      "from": "fcb85303",
      "to": "7071b363",
      "relation": "calls",
      "context": "- `scripts/fix_coredns_unknown_status.sh` - Comprehensive fix script"
    },
    {
      "from": "fcb85303",
      "to": "36bd22c9",
      "relation": "calls",
      "context": "- `scripts/check_coredns_status.sh` - Quick status checker"
    },
    {
      "from": "fcb85303",
      "to": "f0bc548f",
      "relation": "references",
      "context": "- `ansible/plays/setup-cluster.yaml` - Enhanced cluster setup with CoreDNS validation"
    },
    {
      "from": "0665ba5c",
      "to": "b428256d",
      "relation": "calls",
      "context": "1. **`validate_join_prerequisites.sh`** - Comprehensive system validation"
    },
    {
      "from": "0665ba5c",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "2. **`enhanced_kubeadm_join.sh`** - Robust join process with monitoring"
    },
    {
      "from": "0665ba5c",
      "to": "b428256d",
      "relation": "calls",
      "context": "sudo ./scripts/validate_join_prerequisites.sh 192.168.4.63"
    },
    {
      "from": "0665ba5c",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh kubeadm join 192.168.4.63:6443 --token <token> --discovery-t"
    },
    {
      "from": "0665ba5c",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"kubeadm join <master-ip>:6443 --token <token> --discovery-t"
    },
    {
      "from": "20493e93",
      "to": "a249d706",
      "relation": "calls",
      "context": "**Problem**: After deployment, always had to manually run `./scripts/fix_homelab_node_issues.sh` bec"
    },
    {
      "from": "20493e93",
      "to": "72c7835a",
      "relation": "calls",
      "context": "**Result**: No more manual intervention needed after running `./deploy-cluster.sh deploy`"
    },
    {
      "from": "20493e93",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "20493e93",
      "to": "1cd4e1f5",
      "relation": "calls",
      "context": "./scripts/validate_deployment_fixes.sh"
    },
    {
      "from": "20493e93",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "./scripts/validate_static_ips_and_dns.sh"
    },
    {
      "from": "20493e93",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "20493e93",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "1. `scripts/setup_static_ips_and_dns.sh` - Simplified DNS setup"
    },
    {
      "from": "20493e93",
      "to": "a249d706",
      "relation": "calls",
      "context": "2. `scripts/fix_homelab_node_issues.sh` - Added preventive checks"
    },
    {
      "from": "20493e93",
      "to": "24a2f90a",
      "relation": "references",
      "context": "3. `manifests/monitoring/grafana.yaml` - Fixed nodeSelector"
    },
    {
      "from": "20493e93",
      "to": "1d93e806",
      "relation": "references",
      "context": "4. `manifests/monitoring/prometheus.yaml` - Fixed nodeSelector"
    },
    {
      "from": "20493e93",
      "to": "72c7835a",
      "relation": "calls",
      "context": "5. `deploy-cluster.sh` - Reordered fix execution"
    },
    {
      "from": "20493e93",
      "to": "1cd4e1f5",
      "relation": "calls",
      "context": "6. `scripts/validate_deployment_fixes.sh` - New validation script"
    },
    {
      "from": "e8e06adf",
      "to": "a249d706",
      "relation": "calls",
      "context": "### 1. Enhanced Homelab Node Issue Fix (`scripts/fix_homelab_node_issues.sh`)"
    },
    {
      "from": "e8e06adf",
      "to": "f0bc548f",
      "relation": "references",
      "context": "Updated `ansible/plays/setup-cluster.yaml` to:"
    },
    {
      "from": "e8e06adf",
      "to": "bcfb5f43",
      "relation": "references",
      "context": "Enhanced `ansible/plays/deploy-apps.yaml` to:"
    },
    {
      "from": "e8e06adf",
      "to": "2ac1c05f",
      "relation": "references",
      "context": "Enhanced `ansible/plays/jellyfin.yml` to:"
    },
    {
      "from": "e8e06adf",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "e8e06adf",
      "to": "a249d706",
      "relation": "calls",
      "context": "- `scripts/fix_homelab_node_issues.sh` - Main homelab node issue remediation"
    },
    {
      "from": "e8e06adf",
      "to": "7071b363",
      "relation": "calls",
      "context": "- `scripts/fix_coredns_unknown_status.sh` - CoreDNS-specific fixes"
    },
    {
      "from": "e8e06adf",
      "to": "f0bc548f",
      "relation": "references",
      "context": "- `ansible/plays/setup-cluster.yaml` - Enhanced cluster setup with CoreDNS scheduling"
    },
    {
      "from": "e8e06adf",
      "to": "bcfb5f43",
      "relation": "references",
      "context": "- `ansible/plays/deploy-apps.yaml` - Improved application deployment"
    },
    {
      "from": "e8e06adf",
      "to": "2ac1c05f",
      "relation": "references",
      "context": "- `ansible/plays/jellyfin.yml` - Enhanced Jellyfin deployment"
    },
    {
      "from": "13b742f3",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "./scripts/validate_network_prerequisites.sh"
    },
    {
      "from": "13b742f3",
      "to": "1970801b",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "13b742f3",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "13b742f3",
      "to": "090d10b0",
      "relation": "calls",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "13b742f3",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "./scripts/validate_network_prerequisites.sh"
    },
    {
      "from": "13b742f3",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh net-reset --confirm"
    },
    {
      "from": "13b742f3",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh reset --force"
    },
    {
      "from": "13b742f3",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "13b742f3",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "- `scripts/validate_network_prerequisites.sh` - Pre-deployment validation"
    },
    {
      "from": "13b742f3",
      "to": "1970801b",
      "relation": "calls",
      "context": "- `scripts/fix_cni_bridge_conflict.sh` - CNI bridge conflict resolution"
    },
    {
      "from": "13b742f3",
      "to": "a249d706",
      "relation": "calls",
      "context": "- `scripts/fix_homelab_node_issues.sh` - Mixed OS compatibility fixes"
    },
    {
      "from": "2267ad9a",
      "to": "1ddb9838",
      "relation": "calls",
      "context": "#### 1. `fix_kubelet_systemd_config.sh`"
    },
    {
      "from": "2267ad9a",
      "to": "1ddb9838",
      "relation": "calls",
      "context": "sudo ./scripts/fix_kubelet_systemd_config.sh"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "#### 2. `validate_systemd_dropins.sh`"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "./scripts/validate_systemd_dropins.sh validate kubelet"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "./scripts/validate_systemd_dropins.sh fix kubelet"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "./scripts/validate_systemd_dropins.sh ensure-join 192.168.4.63"
    },
    {
      "from": "2267ad9a",
      "to": "1ddb9838",
      "relation": "calls",
      "context": "sudo ./scripts/fix_kubelet_systemd_config.sh"
    },
    {
      "from": "2267ad9a",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<join-command>\""
    },
    {
      "from": "2267ad9a",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "- `scripts/enhanced_kubeadm_join.sh` - Added systemd validation"
    },
    {
      "from": "2267ad9a",
      "to": "f0bc548f",
      "relation": "references",
      "context": "- `ansible/plays/setup-cluster.yaml` - Added pre-join systemd fixes"
    },
    {
      "from": "2267ad9a",
      "to": "1ddb9838",
      "relation": "calls",
      "context": "- `scripts/fix_kubelet_systemd_config.sh` - New comprehensive fix script"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "- `scripts/validate_systemd_dropins.sh` - New validation script"
    },
    {
      "from": "121dca56",
      "to": "1b255a42",
      "relation": "calls",
      "context": "sudo ./scripts/quick_join_diagnostics.sh"
    },
    {
      "from": "121dca56",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh"
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<your-original-join-command>\""
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<new-join-command>\""
    },
    {
      "from": "121dca56",
      "to": "21313a3e",
      "relation": "calls",
      "context": "sudo ./scripts/manual_containerd_filesystem_fix.sh"
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<new-join-command>\""
    },
    {
      "from": "121dca56",
      "to": "1b255a42",
      "relation": "calls",
      "context": "sudo ./scripts/quick_join_diagnostics.sh [CONTROL_PLANE_IP]"
    },
    {
      "from": "121dca56",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh [CONTROL_PLANE_IP] [WORKER_IPS] [OUTPUT_DIR]"
    },
    {
      "from": "121dca56",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh 192.168.4.63 \"192.168.4.61,192.168.4.62\""
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<join-command>\""
    },
    {
      "from": "121dca56",
      "to": "1b255a42",
      "relation": "calls",
      "context": "sudo ./scripts/quick_join_diagnostics.sh"
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<join-command>\""
    },
    {
      "from": "121dca56",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh"
    },
    {
      "from": "121dca56",
      "to": "0665ba5c",
      "relation": "references",
      "context": "- [Enhanced Join Process Details](./ENHANCED_JOIN_PROCESS.md)"
    },
    {
      "from": "121dca56",
      "to": "044d8702",
      "relation": "references",
      "context": "- [Post-Wipe Worker Join Process](./POST_WIPE_WORKER_JOIN.md)"
    },
    {
      "from": "121dca56",
      "to": "19990fc4",
      "relation": "references",
      "context": "- [containerd Timeout Fix Guide](./containerd-timeout-fix.md)"
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh <join-command>"
    },
    {
      "from": "121dca56",
      "to": "0665ba5c",
      "relation": "references",
      "context": "**For complete resolution, see:** [Enhanced Join Process Documentation](./ENHANCED_JOIN_PROCESS.md)"
    },
    {
      "from": "121dca56",
      "to": "3e468245",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "121dca56",
      "to": "53177eca",
      "relation": "references",
      "context": "1. Check the [RHEL 10 Troubleshooting Guide](RHEL10_TROUBLESHOOTING.md) for OS-specific issues"
    },
    {
      "from": "044d8702",
      "to": "0ecf3a99",
      "relation": "calls",
      "context": "./generate_join_command.sh"
    },
    {
      "from": "044d8702",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"kubeadm join 192.168.4.63:6443 --token <token> --discovery-"
    },
    {
      "from": "044d8702",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"kubeadm join ...\""
    },
    {
      "from": "e7010005",
      "to": "28982728",
      "relation": "references",
      "context": "- [T3500 NAS Server](./devices/T3500_NAS.md)"
    },
    {
      "from": "e7010005",
      "to": "37f609a0",
      "relation": "references",
      "context": "- [R430 Compute Engine](./devices/R430_Compute.md)"
    },
    {
      "from": "e7010005",
      "to": "378a3809",
      "relation": "references",
      "context": "- [Catalyst 3650V02 Managed Switch](./devices/Catalyst3650V02_Switch.md)"
    },
    {
      "from": "e7010005",
      "to": "76046802",
      "relation": "references",
      "context": "- [Security & Firewall](../security/firewall.md)"
    },
    {
      "from": "53177eca",
      "to": "3e468245",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "53177eca",
      "to": "3e468245",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml -v"
    },
    {
      "from": "53177eca",
      "to": "3e468245",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "88482a18",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "**Solution**: Run `./scripts/fix_worker_kubectl_config.sh`"
    },
    {
      "from": "88482a18",
      "to": "090d10b0",
      "relation": "calls",
      "context": "**Solution**: Enhanced `./scripts/fix_remaining_pod_issues.sh` with iptables compatibility fixes"
    },
    {
      "from": "88482a18",
      "to": "630db1ca",
      "relation": "calls",
      "context": "**Solution**: Run `./scripts/fix_iptables_compatibility.sh`"
    },
    {
      "from": "88482a18",
      "to": "1970801b",
      "relation": "calls",
      "context": "**Solution**: Existing `./scripts/fix_cni_bridge_conflict.sh` (enhanced)"
    },
    {
      "from": "88482a18",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "- **File**: `./scripts/fix_cluster_communication.sh`"
    },
    {
      "from": "88482a18",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "- **File**: `./scripts/fix_worker_kubectl_config.sh`"
    },
    {
      "from": "88482a18",
      "to": "630db1ca",
      "relation": "calls",
      "context": "- **File**: `./scripts/fix_iptables_compatibility.sh`"
    },
    {
      "from": "88482a18",
      "to": "7c606adf",
      "relation": "calls",
      "context": "- **File**: `./scripts/validate_cluster_communication.sh`"
    },
    {
      "from": "88482a18",
      "to": "090d10b0",
      "relation": "calls",
      "context": "- **File**: `./scripts/fix_remaining_pod_issues.sh` (enhanced)"
    },
    {
      "from": "88482a18",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "88482a18",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "sudo ./scripts/fix_worker_kubectl_config.sh"
    },
    {
      "from": "88482a18",
      "to": "630db1ca",
      "relation": "calls",
      "context": "sudo ./scripts/fix_iptables_compatibility.sh"
    },
    {
      "from": "88482a18",
      "to": "090d10b0",
      "relation": "calls",
      "context": "sudo ./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "88482a18",
      "to": "7c606adf",
      "relation": "calls",
      "context": "./scripts/validate_cluster_communication.sh"
    },
    {
      "from": "88482a18",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "- `scripts/fix_worker_kubectl_config.sh` - kubectl configuration fix"
    },
    {
      "from": "88482a18",
      "to": "630db1ca",
      "relation": "calls",
      "context": "- `scripts/fix_iptables_compatibility.sh` - iptables compatibility fix"
    },
    {
      "from": "88482a18",
      "to": "7c606adf",
      "relation": "calls",
      "context": "- `scripts/validate_cluster_communication.sh` - comprehensive validation"
    },
    {
      "from": "88482a18",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "- `scripts/fix_cluster_communication.sh` - master fix orchestrator"
    },
    {
      "from": "88482a18",
      "to": "090d10b0",
      "relation": "calls",
      "context": "- `scripts/fix_remaining_pod_issues.sh` - added iptables compatibility"
    },
    {
      "from": "797dca0d",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "### 1. Worker Node CNI Fix (`scripts/fix_worker_node_cni.sh`)"
    },
    {
      "from": "797dca0d",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "sudo ./scripts/fix_worker_node_cni.sh --node storagenodet3500"
    },
    {
      "from": "797dca0d",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "### 2. Pod Connectivity Validation (`scripts/validate_pod_connectivity.sh`)"
    },
    {
      "from": "797dca0d",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "./scripts/validate_pod_connectivity.sh"
    },
    {
      "from": "797dca0d",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "### 3. Flannel Mixed-OS Fix (`scripts/fix_flannel_mixed_os.sh`)"
    },
    {
      "from": "797dca0d",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "./scripts/fix_flannel_mixed_os.sh"
    },
    {
      "from": "797dca0d",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "The main `scripts/fix_cluster_communication.sh` now includes:"
    },
    {
      "from": "797dca0d",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "797dca0d",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "sudo ./scripts/fix_worker_node_cni.sh --node storagenodet3500"
    },
    {
      "from": "797dca0d",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "./scripts/fix_flannel_mixed_os.sh"
    },
    {
      "from": "797dca0d",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "./scripts/validate_pod_connectivity.sh"
    },
    {
      "from": "797dca0d",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "- `scripts/fix_worker_node_cni.sh` - Worker node specific CNI fixes"
    },
    {
      "from": "797dca0d",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "- `scripts/validate_pod_connectivity.sh` - Problem statement scenario validation"
    },
    {
      "from": "797dca0d",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "- `scripts/fix_flannel_mixed_os.sh` - Flannel configuration optimization"
    },
    {
      "from": "797dca0d",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "- `scripts/fix_cluster_communication.sh` - Enhanced with new fixes"
    },
    {
      "from": "19990fc4",
      "to": "f0bc548f",
      "relation": "references",
      "context": "The pre-join validation script in `ansible/plays/setup-cluster.yaml` had insufficient timeout and re"
    },
    {
      "from": "19990fc4",
      "to": "f0bc548f",
      "relation": "references",
      "context": "- `ansible/plays/setup-cluster.yaml` - Enhanced pre-join validation timeouts"
    },
    {
      "from": "f087a852",
      "to": "4f42b816",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cluster_dns_configuration.sh"
    },
    {
      "from": "f087a852",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "f087a852",
      "to": "4f42b816",
      "relation": "calls",
      "context": "The script `fix_cluster_dns_configuration.sh` is now run as part of post-deployment fixes."
    },
    {
      "from": "f087a852",
      "to": "72c7835a",
      "relation": "calls",
      "context": "- Included in `deploy-cluster.sh` as part of post-deployment fixes"
    },
    {
      "from": "e00aacd0",
      "to": "72c7835a",
      "relation": "calls",
      "context": "- `deploy-cluster.sh`: Added pod health checks before jellyfin deletion"
    },
    {
      "from": "e00aacd0",
      "to": "1970801b",
      "relation": "calls",
      "context": "- `scripts/fix_cni_bridge_conflict.sh`: Replaced aggressive flannel deletion with intelligent restar"
    },
    {
      "from": "e00aacd0",
      "to": "090d10b0",
      "relation": "calls",
      "context": "- `scripts/fix_remaining_pod_issues.sh`: Added conditional jellyfin pod handling"
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "## fix_cluster_communication.sh  Enhanced Cluster communication fix orchestration"
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "Path: `scripts/fix_cluster_communication.sh`"
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "This document describes what `fix_cluster_communication.sh` does, its prerequisites, safe usage, the"
    },
    {
      "from": "0eb8fc77",
      "to": "7c606adf",
      "relation": "calls",
      "context": "- `./scripts/validate_cluster_communication.sh`  comprehensive cluster validation"
    },
    {
      "from": "0eb8fc77",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "- `./scripts/validate_pod_connectivity.sh`  detailed pod networking tests"
    },
    {
      "from": "0eb8fc77",
      "to": "630db1ca",
      "relation": "calls",
      "context": "2. `fix_iptables_compatibility.sh`  attempts to resolve iptables / nftables compatibility issues th"
    },
    {
      "from": "0eb8fc77",
      "to": "1970801b",
      "relation": "calls",
      "context": "3. `fix_cni_bridge_conflict.sh`  resolves common CNI bridge conflicts (cni0 vs flannel devices, sta"
    },
    {
      "from": "0eb8fc77",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "4. `fix_worker_node_cni.sh`  specifically targets worker node CNI communication issues"
    },
    {
      "from": "0eb8fc77",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "5. `fix_flannel_mixed_os.sh`  handles Flannel configuration for mixed OS environments"
    },
    {
      "from": "0eb8fc77",
      "to": "090d10b0",
      "relation": "calls",
      "context": "6. `fix_remaining_pod_issues.sh`  targets kube-proxy and pods that are CrashLoopBackOff or failing "
    },
    {
      "from": "0eb8fc77",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "7. `fix_worker_kubectl_config.sh`  repairs kubeconfig/kubectl issues on worker nodes (if present)"
    },
    {
      "from": "0eb8fc77",
      "to": "7c606adf",
      "relation": "calls",
      "context": "9. `validate_cluster_communication.sh`  runs a final validation pass (checks nodes, key pods, and N"
    },
    {
      "from": "0eb8fc77",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "10. `validate_pod_connectivity.sh`  detailed pod connectivity validation"
    },
    {
      "from": "0eb8fc77",
      "to": "09f42a5d",
      "relation": "calls",
      "context": "11. `fix_nodeport_external_access.sh`  specifically addresses NodePort accessibility issues includi"
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "sudo bash scripts/fix_cluster_communication.sh"
    },
    {
      "from": "0eb8fc77",
      "to": "630db1ca",
      "relation": "calls",
      "context": "- Review the helper scripts in `scripts/` (`fix_iptables_compatibility.sh`, `fix_cni_bridge_conflict"
    },
    {
      "from": "0eb8fc77",
      "to": "1970801b",
      "relation": "calls",
      "context": "- Review the helper scripts in `scripts/` (`fix_iptables_compatibility.sh`, `fix_cni_bridge_conflict"
    },
    {
      "from": "0eb8fc77",
      "to": "090d10b0",
      "relation": "calls",
      "context": "- Review the helper scripts in `scripts/` (`fix_iptables_compatibility.sh`, `fix_cni_bridge_conflict"
    },
    {
      "from": "0eb8fc77",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "- Review the helper scripts in `scripts/` (`fix_iptables_compatibility.sh`, `fix_cni_bridge_conflict"
    },
    {
      "from": "0eb8fc77",
      "to": "7c606adf",
      "relation": "calls",
      "context": "- Review the helper scripts in `scripts/` (`fix_iptables_compatibility.sh`, `fix_cni_bridge_conflict"
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "- 2025-09-12: Created documentation for `scripts/fix_cluster_communication.sh`."
    },
    {
      "from": "207b4d0f",
      "to": "8036f074",
      "relation": "references",
      "context": "logger=provisioning t=2025-09-02T18:37:13.902569751Z level=error msg=\"Failed to provision data sourc"
    },
    {
      "from": "207b4d0f",
      "to": "ca8597d0",
      "relation": "references",
      "context": "**File**: `ansible/plays/kubernetes/deploy_monitoring.yaml`"
    },
    {
      "from": "207b4d0f",
      "to": "ca8597d0",
      "relation": "references",
      "context": "**File**: `ansible/plays/kubernetes/deploy_monitoring.yaml`"
    },
    {
      "from": "207b4d0f",
      "to": "8036f074",
      "relation": "references",
      "context": "prometheus-datasource.yaml: \"{{ lookup('file', playbook_dir + '/../../files/grafana_datasources/prom"
    },
    {
      "from": "207b4d0f",
      "to": "8036f074",
      "relation": "references",
      "context": "prometheus-datasource.yaml: \"{{ lookup('file', playbook_dir + '/../../files/grafana_datasources/prom"
    },
    {
      "from": "207b4d0f",
      "to": "7ef785f1",
      "relation": "references",
      "context": "**File**: `ansible/plays/kubernetes/monitoring_validation.yaml`"
    },
    {
      "from": "207b4d0f",
      "to": "ca8597d0",
      "relation": "references",
      "context": "ansible-playbook -i inventory.txt ansible/plays/kubernetes/deploy_monitoring.yaml"
    },
    {
      "from": "207b4d0f",
      "to": "ca8597d0",
      "relation": "references",
      "context": "- `ansible/plays/kubernetes/deploy_monitoring.yaml` - Main fix"
    },
    {
      "from": "207b4d0f",
      "to": "7ef785f1",
      "relation": "references",
      "context": "- `ansible/plays/kubernetes/monitoring_validation.yaml` - Updated validation"
    },
    {
      "from": "207b4d0f",
      "to": "8036f074",
      "relation": "references",
      "context": "- `ansible/files/grafana_datasources/prometheus-datasource.yaml` - Added deprecation notice"
    },
    {
      "from": "da894f5f",
      "to": "607e6e59",
      "relation": "references",
      "context": "- Configurable via `jellyfin_media_path` in `ansible/group_vars/all.yml`"
    },
    {
      "from": "da894f5f",
      "to": "607e6e59",
      "relation": "references",
      "context": "To customize the media directory path, edit `ansible/group_vars/all.yml`:"
    },
    {
      "from": "8a5efc5c",
      "to": "da894f5f",
      "relation": "references",
      "context": "- **Documentation**: `docs/jellyfin/JELLYFIN_HA_DEPLOYMENT.md`"
    },
    {
      "from": "0c29bbcc",
      "to": "269791b0",
      "relation": "references",
      "context": "7. Recreate Jellyfin: `kubectl delete pod -n jellyfin jellyfin && kubectl apply -f manifests/jellyfi"
    },
    {
      "from": "1e068979",
      "to": "1970801b",
      "relation": "calls",
      "context": "2. Apply the CNI bridge fix if needed (using `scripts/fix_cni_bridge_conflict.sh`)"
    },
    {
      "from": "1e068979",
      "to": "1970801b",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "1e068979",
      "to": "269791b0",
      "relation": "references",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml"
    },
    {
      "from": "1e57853e",
      "to": "269791b0",
      "relation": "references",
      "context": "**File**: `manifests/jellyfin/jellyfin.yaml`"
    },
    {
      "from": "1e57853e",
      "to": "269791b0",
      "relation": "references",
      "context": "**Files**: `manifests/jellyfin/jellyfin.yaml`, `ansible/playbooks/verify-cluster.yml`"
    },
    {
      "from": "1e57853e",
      "to": "91932608",
      "relation": "references",
      "context": "**Files**: `manifests/jellyfin/jellyfin.yaml`, `ansible/playbooks/verify-cluster.yml`"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "**File**: `deploy-cluster.sh`"
    },
    {
      "from": "1e57853e",
      "to": "a249d706",
      "relation": "calls",
      "context": "- `scripts/fix_homelab_node_issues.sh` (handles kube-proxy CrashLoopBackOff)"
    },
    {
      "from": "1e57853e",
      "to": "090d10b0",
      "relation": "calls",
      "context": "- `scripts/fix_remaining_pod_issues.sh` (handles various pod issues)"
    },
    {
      "from": "1e57853e",
      "to": "269791b0",
      "relation": "references",
      "context": "**File**: `manifests/jellyfin/jellyfin.yaml`"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "The deploy-cluster.sh script now:"
    },
    {
      "from": "1e57853e",
      "to": "269791b0",
      "relation": "references",
      "context": "1. **`manifests/jellyfin/jellyfin.yaml`** - Complete rewrite from PVC to hostPath"
    },
    {
      "from": "1e57853e",
      "to": "91932608",
      "relation": "references",
      "context": "2. **`ansible/playbooks/verify-cluster.yml`** - Fixed health check endpoint"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "3. **`deploy-cluster.sh`** - Added post-deployment fixes integration"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh --simple deploy"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh --dry-run deploy"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh verify"
    },
    {
      "from": "98b95b67",
      "to": "269791b0",
      "relation": "references",
      "context": "In `manifests/jellyfin/jellyfin.yaml`, line 170 had:"
    },
    {
      "from": "42b5112f",
      "to": "607e6e59",
      "relation": "references",
      "context": "Add these variables to your `ansible/group_vars/all.yml`:"
    },
    {
      "from": "d6e5693f",
      "to": "fd7c8bd5",
      "relation": "references",
      "context": "- `ansible/playbooks/minimal-network-fix.yml` - Enhanced namespace cleanup logic"
    },
    {
      "from": "19863979",
      "to": "09b572dc",
      "relation": "calls",
      "context": "./scripts/run_network_diagnosis.sh"
    },
    {
      "from": "19863979",
      "to": "09b572dc",
      "relation": "calls",
      "context": "./scripts/run_network_diagnosis.sh --verbose"
    },
    {
      "from": "19863979",
      "to": "09b572dc",
      "relation": "calls",
      "context": "./scripts/run_network_diagnosis.sh --check"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml -vv"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook ansible/plays/network-diagnosis.yaml --syntax-check"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml --check"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml -vv"
    },
    {
      "from": "19863979",
      "to": "0eb8fc77",
      "relation": "references",
      "context": "- [Inter-Pod Communication Troubleshooting](docs/fix_cluster_communication.md)"
    },
    {
      "from": "19863979",
      "to": "f087a852",
      "relation": "references",
      "context": "- [DNS Fix Guide](docs/dns-fix-guide.md)"
    },
    {
      "from": "19863979",
      "to": "094a0932",
      "relation": "references",
      "context": "- [CNI Bridge Fix](docs/CNI_BRIDGE_FIX.md)"
    },
    {
      "from": "19863979",
      "to": "fcb85303",
      "relation": "references",
      "context": "- [CoreDNS Unknown Status Fix](docs/COREDNS_UNKNOWN_STATUS_FIX.md)"
    },
    {
      "from": "c914457e",
      "to": "09f42a5d",
      "relation": "calls",
      "context": "#### 1. `scripts/fix_nodeport_external_access.sh`"
    },
    {
      "from": "c914457e",
      "to": "09f42a5d",
      "relation": "calls",
      "context": "sudo ./scripts/fix_nodeport_external_access.sh"
    },
    {
      "from": "c914457e",
      "to": "27edfbfd",
      "relation": "calls",
      "context": "#### 2. `scripts/validate_nodeport_external_access.sh`"
    },
    {
      "from": "c914457e",
      "to": "27edfbfd",
      "relation": "calls",
      "context": "./scripts/validate_nodeport_external_access.sh"
    },
    {
      "from": "c914457e",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "#### Updated: `scripts/fix_cluster_communication.sh`"
    },
    {
      "from": "c914457e",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "c914457e",
      "to": "09f42a5d",
      "relation": "calls",
      "context": "sudo ./scripts/fix_nodeport_external_access.sh"
    },
    {
      "from": "c914457e",
      "to": "27edfbfd",
      "relation": "calls",
      "context": "./scripts/validate_nodeport_external_access.sh"
    },
    {
      "from": "c914457e",
      "to": "09f42a5d",
      "relation": "calls",
      "context": "- `scripts/fix_nodeport_external_access.sh` (new)"
    },
    {
      "from": "c914457e",
      "to": "27edfbfd",
      "relation": "calls",
      "context": "- `scripts/validate_nodeport_external_access.sh` (new)"
    },
    {
      "from": "c914457e",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "- `scripts/fix_cluster_communication.sh` (updated)"
    },
    {
      "from": "84b873a0",
      "to": "207b4d0f",
      "relation": "references",
      "context": "# Or apply manual fix (see docs/grafana_datasource_conflict_fix.md for details)"
    },
    {
      "from": "84b873a0",
      "to": "207b4d0f",
      "relation": "references",
      "context": "For complete fix details, see: `docs/grafana_datasource_conflict_fix.md`"
    },
    {
      "from": "84b873a0",
      "to": "607e6e59",
      "relation": "references",
      "context": "# Option 2: Change monitoring scheduling mode to 'unrestricted' in ansible/group_vars/all.yml"
    },
    {
      "from": "84b873a0",
      "to": "ca8597d0",
      "relation": "references",
      "context": "# Then redeploy: ansible-playbook -i inventory.txt plays/kubernetes/deploy_monitoring.yaml"
    },
    {
      "from": "84b873a0",
      "to": "607e6e59",
      "relation": "references",
      "context": "For permanent fix, configure `monitoring_scheduling_mode` in `ansible/group_vars/all.yml`:"
    },
    {
      "from": "84b873a0",
      "to": "607e6e59",
      "relation": "references",
      "context": "**Grafana Pending**: pods stuck in Pending state cannot be scheduled due to strict node selectors, t"
    },
    {
      "from": "4096f24e",
      "to": "ad0e321a",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/network/coredns-service.yaml`"
    },
    {
      "from": "4096f24e",
      "to": "80025501",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/network/kube-proxy-daemonset.yaml`"
    },
    {
      "from": "4096f24e",
      "to": "0a813314",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/cni/flannel.yaml`"
    },
    {
      "from": "4096f24e",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "- **File**: `/manifests/network/coredns-configmap.yaml`"
    },
    {
      "from": "4096f24e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "4096f24e",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "sudo ./scripts/setup_static_ips_and_dns.sh"
    },
    {
      "from": "4096f24e",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "./scripts/validate_static_ips_and_dns.sh"
    },
    {
      "from": "4096f24e",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "./scripts/validate_static_ips_and_dns.sh static-ips"
    },
    {
      "from": "4096f24e",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "./scripts/validate_static_ips_and_dns.sh dns"
    },
    {
      "from": "4096f24e",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "sudo ./scripts/setup_static_ips_and_dns.sh --verify"
    },
    {
      "from": "4096f24e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "- **Deployment Scripts**: Automatically applied during `deploy-cluster.sh`"
    },
    {
      "from": "dc5e49f8",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml"
    },
    {
      "from": "607e6e59",
      "to": "2e9ce2c1",
      "relation": "references",
      "context": "cluster_bootstrap_mode: false  # Set to true when using cluster-bootstrap.yml"
    },
    {
      "from": "607e6e59",
      "to": "b0430b0d",
      "relation": "references",
      "context": "# Default: kubernetes/jellyfin-minimal.yml (relative to ansible/plays/ directory)"
    },
    {
      "from": "607e6e59",
      "to": "2ac1c05f",
      "relation": "references",
      "context": "# WARNING: When using {{ playbook_dir }}, remember that jellyfin.yml runs from plays/ directory"
    },
    {
      "from": "607e6e59",
      "to": "b0430b0d",
      "relation": "references",
      "context": "#          so use \"{{ playbook_dir }}/../roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-mini"
    },
    {
      "from": "607e6e59",
      "to": "b0430b0d",
      "relation": "references",
      "context": "jellyfin_manifest_path: kubernetes/jellyfin-minimal.yml"
    },
    {
      "from": "39bfddc5",
      "to": "f0bc548f",
      "relation": "references",
      "context": "# Extracted common tasks from the existing setup-cluster.yaml"
    },
    {
      "from": "fa6b94bd",
      "to": "f0bc548f",
      "relation": "references",
      "context": "# Alternative to the complex setup-cluster.yaml for basic kubeadm deployment"
    },
    {
      "from": "fa6b94bd",
      "to": "f0bc548f",
      "relation": "references",
      "context": "include_tasks: ../../plays/setup-cluster.yaml"
    },
    {
      "from": "fa6b94bd",
      "to": "0a813314",
      "relation": "references",
      "context": "src: \"{{ playbook_dir }}/../../manifests/cni/flannel.yaml\""
    },
    {
      "from": "fa6b94bd",
      "to": "1d93e806",
      "relation": "references",
      "context": "- \"{{ playbook_dir }}/../../manifests/monitoring/prometheus.yaml\""
    },
    {
      "from": "fa6b94bd",
      "to": "24a2f90a",
      "relation": "references",
      "context": "- \"{{ playbook_dir }}/../../manifests/monitoring/grafana.yaml\""
    },
    {
      "from": "fa6b94bd",
      "to": "269791b0",
      "relation": "references",
      "context": "src: \"{{ playbook_dir }}/../../manifests/jellyfin/jellyfin.yaml\""
    },
    {
      "from": "fa6b94bd",
      "to": "91932608",
      "relation": "references",
      "context": "- Run verification: ansible-playbook verify-cluster.yml"
    },
    {
      "from": "2e9ce2c1",
      "to": "f0bc548f",
      "relation": "references",
      "context": "# Builds upon existing setup-cluster.yaml with kubeadm-specific enhancements"
    },
    {
      "from": "2e9ce2c1",
      "to": "f0bc548f",
      "relation": "references",
      "context": "import_playbook: ../plays/setup-cluster.yaml"
    },
    {
      "from": "2e9ce2c1",
      "to": "0a813314",
      "relation": "references",
      "context": "src: \"{{ playbook_dir }}/../../manifests/cni/flannel.yaml\""
    },
    {
      "from": "fd7c8bd5",
      "to": "3d26bfd6",
      "relation": "references",
      "context": "flannel_manifest: \"{{ playbook_dir }}/../../manifests/cni/flannel-minimal.yaml\""
    },
    {
      "from": "fd7c8bd5",
      "to": "0d91b1c9",
      "relation": "references",
      "context": "coredns_manifest: \"{{ playbook_dir }}/../../manifests/network/coredns-minimal.yaml\""
    },
    {
      "from": "fd7c8bd5",
      "to": "0aec39aa",
      "relation": "references",
      "context": "kube_proxy_manifest: \"{{ playbook_dir }}/../../manifests/network/kube-proxy-minimal.yaml\""
    },
    {
      "from": "fd7c8bd5",
      "to": "4ede3e1b",
      "relation": "references",
      "context": "jellyfin_manifest: \"{{ playbook_dir }}/../../manifests/jellyfin/jellyfin-minimal.yaml\""
    },
    {
      "from": "91932608",
      "to": "1d93e806",
      "relation": "references",
      "context": "- \"{{ playbook_dir }}/../../manifests/monitoring/prometheus.yaml\""
    },
    {
      "from": "91932608",
      "to": "24a2f90a",
      "relation": "references",
      "context": "- \"{{ playbook_dir }}/../../manifests/monitoring/grafana.yaml\""
    },
    {
      "from": "91932608",
      "to": "269791b0",
      "relation": "references",
      "context": "src: \"{{ playbook_dir }}/../../manifests/jellyfin/jellyfin.yaml\""
    },
    {
      "from": "91932608",
      "to": "1970801b",
      "relation": "calls",
      "context": "if [ -f \"/home/runner/work/VMStation/VMStation/scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "91932608",
      "to": "1970801b",
      "relation": "calls",
      "context": "chmod +x /home/runner/work/VMStation/VMStation/scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "91932608",
      "to": "1970801b",
      "relation": "calls",
      "context": "/home/runner/work/VMStation/VMStation/scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "bcfb5f43",
      "to": "a249d706",
      "relation": "calls",
      "context": "msg: \"CoreDNS is not running - cluster networking is unstable. Run: ./scripts/fix_homelab_node_issue"
    },
    {
      "from": "2ac1c05f",
      "to": "b0430b0d",
      "relation": "references",
      "context": "jellyfin_manifest_path: \"kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "ca8597d0",
      "to": "607e6e59",
      "relation": "references",
      "context": "# group_vars/all.yml or via --extra-vars when running the playbook."
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "calls",
      "context": "if [ -x /root/VMStation/scripts/fix_cni_bridge_conflict.sh ]; then"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "calls",
      "context": "/root/VMStation/scripts/fix_cni_bridge_conflict.sh || true"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "calls",
      "context": "elif [ -x /srv/monitoring_data/VMStation/scripts/fix_cni_bridge_conflict.sh ]; then"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "calls",
      "context": "/srv/monitoring_data/VMStation/scripts/fix_cni_bridge_conflict.sh || true"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "calls",
      "context": "echo \"The deployment will complete and run fix_cni_bridge_conflict.sh automatically.\""
    },
    {
      "from": "f0bc548f",
      "to": "7071b363",
      "relation": "calls",
      "context": "After cluster setup completes, run: ./scripts/fix_coredns_unknown_status.sh"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "calls",
      "context": "The deployment will automatically run fix_cni_bridge_conflict.sh after cluster setup."
    },
    {
      "from": "f0bc548f",
      "to": "b428256d",
      "relation": "calls",
      "context": "- \"../../scripts/validate_join_prerequisites.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "- \"../../scripts/enhanced_kubeadm_join.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "- \"../../scripts/validate_systemd_dropins.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "1ddb9838",
      "relation": "calls",
      "context": "- \"../../scripts/fix_kubelet_systemd_config.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "if [ -f /tmp/validate_systemd_dropins.sh ]; then"
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "if ! /tmp/validate_systemd_dropins.sh validate kubelet; then"
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "/tmp/validate_systemd_dropins.sh fix kubelet"
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "/tmp/validate_systemd_dropins.sh ensure-join \"{{ control_plane_ip }}\""
    },
    {
      "from": "f0bc548f",
      "to": "1ddb9838",
      "relation": "calls",
      "context": "if [ -f /tmp/fix_kubelet_systemd_config.sh ]; then"
    },
    {
      "from": "f0bc548f",
      "to": "1ddb9838",
      "relation": "calls",
      "context": "/tmp/fix_kubelet_systemd_config.sh"
    },
    {
      "from": "f0bc548f",
      "to": "585e2b00",
      "relation": "calls",
      "context": "src: \"../../scripts/ansible_pre_join_validation.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "585e2b00",
      "relation": "calls",
      "context": "dest: \"/tmp/ansible_pre_join_validation.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "585e2b00",
      "relation": "calls",
      "context": "shell: \"/tmp/ansible_pre_join_validation.sh {{ control_plane_ip }} {{ worker_was_wiped }}\""
    },
    {
      "from": "f0bc548f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "/tmp/enhanced_kubeadm_join.sh \"$JOIN_COMMAND\"'"
    },
    {
      "from": "f0bc548f",
      "to": "21313a3e",
      "relation": "calls",
      "context": "sudo ./scripts/manual_containerd_filesystem_fix.sh"
    },
    {
      "from": "f0bc548f",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh"
    },
    {
      "from": "f0bc548f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "Run enhanced join: sudo ./scripts/enhanced_kubeadm_join.sh \"<join-command>\""
    },
    {
      "from": "f0bc548f",
      "to": "b428256d",
      "relation": "calls",
      "context": "- /tmp/validate_join_prerequisites.sh"
    },
    {
      "from": "f0bc548f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "- /tmp/enhanced_kubeadm_join.sh"
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "- /tmp/validate_systemd_dropins.sh"
    },
    {
      "from": "f0bc548f",
      "to": "1ddb9838",
      "relation": "calls",
      "context": "- /tmp/fix_kubelet_systemd_config.sh"
    },
    {
      "from": "c782d582",
      "to": "b0430b0d",
      "relation": "references",
      "context": "jellyfin_manifest_path_var: \"{{ jellyfin_manifest_path | default('kubernetes/jellyfin-minimal.yml') "
    },
    {
      "from": "c782d582",
      "to": "b0430b0d",
      "relation": "references",
      "context": "jellyfin_manifest_path: kubernetes/jellyfin-minimal.yml"
    },
    {
      "from": "c782d582",
      "to": "b0430b0d",
      "relation": "references",
      "context": "jellyfin_manifest_path: \"{{ '{{' }} playbook_dir {{ '}}' }}/../roles/jellyfin/files/ansible/plays/ku"
    },
    {
      "from": "c782d582",
      "to": "b0430b0d",
      "relation": "references",
      "context": "jellyfin_manifest_path: \"{{ playbook_dir | default('.') }}/roles/jellyfin/files/ansible/plays/kubern"
    },
    {
      "from": "dd358be7",
      "to": "f0bc548f",
      "relation": "references",
      "context": "- import_playbook: plays/setup-cluster.yaml"
    },
    {
      "from": "dd358be7",
      "to": "bcfb5f43",
      "relation": "references",
      "context": "- import_playbook: plays/deploy-apps.yaml"
    },
    {
      "from": "dd358be7",
      "to": "2ac1c05f",
      "relation": "references",
      "context": "- import_playbook: plays/jellyfin.yml"
    },
    {
      "from": "24a2f90a",
      "to": "1d93e806",
      "relation": "references",
      "context": "prometheus.yaml: |"
    },
    {
      "from": "04c6e90f",
      "to": "0665ba5c",
      "relation": "references",
      "context": "For detailed information, see: [Enhanced Join Process Documentation](docs/ENHANCED_JOIN_PROCESS.md)"
    },
    {
      "from": "04c6e90f",
      "to": "607e6e59",
      "relation": "references",
      "context": "nano ansible/group_vars/all.yml"
    },
    {
      "from": "04c6e90f",
      "to": "dd358be7",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/simple-deploy.yaml --check"
    },
    {
      "from": "04c6e90f",
      "to": "607e6e59",
      "relation": "references",
      "context": "cp ansible/group_vars/all.yml.template ansible/group_vars/all.yml"
    },
    {
      "from": "04c6e90f",
      "to": "607e6e59",
      "relation": "references",
      "context": "cp ansible/group_vars/all.yml.template ansible/group_vars/all.yml"
    },
    {
      "from": "04c6e90f",
      "to": "607e6e59",
      "relation": "references",
      "context": "# Edit all.yml with your specific settings"
    },
    {
      "from": "04c6e90f",
      "to": "0ecf3a99",
      "relation": "calls",
      "context": "./generate_join_command.sh"
    },
    {
      "from": "04c6e90f",
      "to": "62721f19",
      "relation": "calls",
      "context": "- **Pod health validation**: `./scripts/validate_pod_health.sh` - **NEW!** Quick validation of all p"
    },
    {
      "from": "04c6e90f",
      "to": "090d10b0",
      "relation": "calls",
      "context": "- **Remaining pod fixes**: `./scripts/fix_remaining_pod_issues.sh` - **NEW!** Fixes jellyfin readine"
    },
    {
      "from": "04c6e90f",
      "to": "3128233f",
      "relation": "calls",
      "context": "- **Pod diagnostics**: `./scripts/diagnose_remaining_pod_issues.sh` - **NEW!** Detailed analysis of "
    },
    {
      "from": "04c6e90f",
      "to": "1970801b",
      "relation": "calls",
      "context": "- **CNI bridge conflicts**: `./scripts/fix_cni_bridge_conflict.sh` - **NEW!** Fixes CNI bridge IP co"
    },
    {
      "from": "04c6e90f",
      "to": "7a273252",
      "relation": "calls",
      "context": "- **CNI bridge reset**: `sudo ./scripts/reset_cni_bridge.sh` - **NEW!** Quick reset for \"cni0 alread"
    },
    {
      "from": "04c6e90f",
      "to": "62721f19",
      "relation": "calls",
      "context": "./scripts/validate_pod_health.sh"
    },
    {
      "from": "04c6e90f",
      "to": "3128233f",
      "relation": "calls",
      "context": "./scripts/diagnose_remaining_pod_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "090d10b0",
      "relation": "calls",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "53177eca",
      "relation": "references",
      "context": "# See docs/RHEL10_TROUBLESHOOTING.md for detailed guide"
    },
    {
      "from": "04c6e90f",
      "to": "3e468245",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "04c6e90f",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "36bd22c9",
      "relation": "calls",
      "context": "./scripts/check_coredns_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "7071b363",
      "relation": "calls",
      "context": "./scripts/fix_coredns_unknown_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "./scripts/validate_network_prerequisites.sh"
    },
    {
      "from": "04c6e90f",
      "to": "7a273252",
      "relation": "calls",
      "context": "sudo ./scripts/reset_cni_bridge.sh"
    },
    {
      "from": "04c6e90f",
      "to": "1970801b",
      "relation": "calls",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "04c6e90f",
      "to": "f0840b04",
      "relation": "references",
      "context": "- **Quick CNI reset**: [docs/CNI_BRIDGE_RESET.md](./docs/CNI_BRIDGE_RESET.md) - Fast targeted fix"
    },
    {
      "from": "04c6e90f",
      "to": "f0840b04",
      "relation": "references",
      "context": "- **Quick CNI reset**: [docs/CNI_BRIDGE_RESET.md](./docs/CNI_BRIDGE_RESET.md) - Fast targeted fix"
    },
    {
      "from": "04c6e90f",
      "to": "8facf1bd",
      "relation": "references",
      "context": "- CoreDNS scheduling fixes: [docs/COREDNS_MASTERNODE_ENFORCEMENT.md](./docs/COREDNS_MASTERNODE_ENFOR"
    },
    {
      "from": "04c6e90f",
      "to": "8facf1bd",
      "relation": "references",
      "context": "- CoreDNS scheduling fixes: [docs/COREDNS_MASTERNODE_ENFORCEMENT.md](./docs/COREDNS_MASTERNODE_ENFOR"
    },
    {
      "from": "04c6e90f",
      "to": "fcb85303",
      "relation": "references",
      "context": "- CoreDNS unknown status: [docs/COREDNS_UNKNOWN_STATUS_FIX.md](./docs/COREDNS_UNKNOWN_STATUS_FIX.md)"
    },
    {
      "from": "04c6e90f",
      "to": "fcb85303",
      "relation": "references",
      "context": "- CoreDNS unknown status: [docs/COREDNS_UNKNOWN_STATUS_FIX.md](./docs/COREDNS_UNKNOWN_STATUS_FIX.md)"
    },
    {
      "from": "04c6e90f",
      "to": "563e9b03",
      "relation": "calls",
      "context": "./scripts/vmstation_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "7071b363",
      "relation": "calls",
      "context": "./scripts/fix_coredns_unknown_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "36bd22c9",
      "relation": "calls",
      "context": "./scripts/check_coredns_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "090d10b0",
      "relation": "calls",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "62721f19",
      "relation": "calls",
      "context": "./scripts/validate_pod_health.sh"
    },
    {
      "from": "04c6e90f",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "563e9b03",
      "relation": "calls",
      "context": "./scripts/vmstation_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "e8e06adf",
      "relation": "references",
      "context": "- [docs/HOMELAB_NODE_FIXES.md](./docs/HOMELAB_NODE_FIXES.md) - General cluster issues"
    },
    {
      "from": "04c6e90f",
      "to": "e8e06adf",
      "relation": "references",
      "context": "- [docs/HOMELAB_NODE_FIXES.md](./docs/HOMELAB_NODE_FIXES.md) - General cluster issues"
    },
    {
      "from": "04c6e90f",
      "to": "0c29bbcc",
      "relation": "references",
      "context": "- [docs/jellyfin-cni-bridge-fix.md](./docs/jellyfin-cni-bridge-fix.md) - Jellyfin CNI bridge conflic"
    },
    {
      "from": "04c6e90f",
      "to": "0c29bbcc",
      "relation": "references",
      "context": "- [docs/jellyfin-cni-bridge-fix.md](./docs/jellyfin-cni-bridge-fix.md) - Jellyfin CNI bridge conflic"
    },
    {
      "from": "04c6e90f",
      "to": "72c7835a",
      "relation": "calls",
      "context": "- `./deploy-cluster.sh` - Main deployment script (967 lines)"
    },
    {
      "from": "04c6e90f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "- `scripts/enhanced_kubeadm_join.sh` - Enhanced join process (1388 lines)"
    },
    {
      "from": "04c6e90f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "- `ansible/plays/setup-cluster.yaml` - Essential cluster setup"
    },
    {
      "from": "72c7835a",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "INVENTORY_FILE=\"ansible/inventory/hosts.yml\""
    },
    {
      "from": "72c7835a",
      "to": "91932608",
      "relation": "references",
      "context": "VERIFICATION_PLAYBOOK=\"$PLAYBOOK_DIR/verify-cluster.yml\""
    },
    {
      "from": "72c7835a",
      "to": "2e9ce2c1",
      "relation": "references",
      "context": "MAIN_BOOTSTRAP=\"$PLAYBOOK_DIR/cluster-bootstrap.yml\""
    },
    {
      "from": "72c7835a",
      "to": "fa6b94bd",
      "relation": "references",
      "context": "SIMPLE_BOOTSTRAP=\"$PLAYBOOK_DIR/cluster-bootstrap/simple-bootstrap.yml\""
    },
    {
      "from": "72c7835a",
      "to": "f0bc548f",
      "relation": "references",
      "context": "--simple            Use simple bootstrap (without existing setup-cluster.yaml)"
    },
    {
      "from": "72c7835a",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "if [ -f \"scripts/validate_network_prerequisites.sh\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "chmod +x scripts/validate_network_prerequisites.sh"
    },
    {
      "from": "72c7835a",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "if scripts/validate_network_prerequisites.sh; then"
    },
    {
      "from": "72c7835a",
      "to": "1970801b",
      "relation": "calls",
      "context": "\"scripts/fix_cni_bridge_conflict.sh\"          # Fix CNI bridge conflicts first"
    },
    {
      "from": "72c7835a",
      "to": "a249d706",
      "relation": "calls",
      "context": "\"scripts/fix_homelab_node_issues.sh\"          # Fix homelab node networking"
    },
    {
      "from": "72c7835a",
      "to": "4f42b816",
      "relation": "calls",
      "context": "\"scripts/fix_cluster_dns_configuration.sh\"    # Fix cluster DNS"
    },
    {
      "from": "72c7835a",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "\"scripts/setup_static_ips_and_dns.sh\"         # Setup static IPs"
    },
    {
      "from": "72c7835a",
      "to": "090d10b0",
      "relation": "calls",
      "context": "\"scripts/fix_remaining_pod_issues.sh\"         # Fix remaining issues"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "calls",
      "context": "if [ ! -f \"scripts/smoke-test.sh\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "calls",
      "context": "error \"Smoke test script not found: scripts/smoke-test.sh\""
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "calls",
      "context": "if chmod +x scripts/smoke-test.sh && scripts/smoke-test.sh; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "calls",
      "context": "if chmod +x scripts/smoke-test.sh && scripts/smoke-test.sh; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "calls",
      "context": "if scp scripts/smoke-test.sh ${control_plane_user}@$control_plane_ip:/tmp/; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "calls",
      "context": "if ssh ${control_plane_user}@$control_plane_ip \"chmod +x /tmp/smoke-test.sh && /tmp/smoke-test.sh\"; "
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "calls",
      "context": "if ssh ${control_plane_user}@$control_plane_ip \"chmod +x /tmp/smoke-test.sh && /tmp/smoke-test.sh\"; "
    },
    {
      "from": "72c7835a",
      "to": "80025501",
      "relation": "references",
      "context": "kubectl get daemonset kube-proxy -n kube-system -o yaml > \"$backup_dir/kube-proxy-daemonset.yaml\" 2>"
    },
    {
      "from": "72c7835a",
      "to": "2883003d",
      "relation": "references",
      "context": "kubectl get cm kube-proxy -n kube-system -o yaml > \"$backup_dir/kube-proxy-configmap.yaml\" 2>/dev/nu"
    },
    {
      "from": "72c7835a",
      "to": "81cd44f2",
      "relation": "references",
      "context": "kubectl get deployment coredns -n kube-system -o yaml > \"$backup_dir/coredns-deployment.yaml\" 2>/dev"
    },
    {
      "from": "72c7835a",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "kubectl get configmap coredns -n kube-system -o yaml > \"$backup_dir/coredns-configmap.yaml\" 2>/dev/n"
    },
    {
      "from": "72c7835a",
      "to": "ad0e321a",
      "relation": "references",
      "context": "kubectl get service kube-dns -n kube-system -o yaml > \"$backup_dir/coredns-service.yaml\" 2>/dev/null"
    },
    {
      "from": "72c7835a",
      "to": "2883003d",
      "relation": "references",
      "context": "\"manifests/network/kube-proxy-configmap.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "80025501",
      "relation": "references",
      "context": "\"manifests/network/kube-proxy-daemonset.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "\"manifests/network/coredns-configmap.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "ad0e321a",
      "relation": "references",
      "context": "\"manifests/network/coredns-service.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "81cd44f2",
      "relation": "references",
      "context": "\"manifests/network/coredns-deployment.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "2883003d",
      "relation": "references",
      "context": "if [ -f \"$backup_dir/kube-proxy-configmap.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "2883003d",
      "relation": "references",
      "context": "kubectl apply -f \"$backup_dir/kube-proxy-configmap.yaml\" || warn \"Failed to restore kube-proxy confi"
    },
    {
      "from": "72c7835a",
      "to": "80025501",
      "relation": "references",
      "context": "if [ -f \"$backup_dir/kube-proxy-daemonset.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "80025501",
      "relation": "references",
      "context": "kubectl apply -f \"$backup_dir/kube-proxy-daemonset.yaml\" || warn \"Failed to restore kube-proxy daemo"
    },
    {
      "from": "72c7835a",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "if [ -f \"$backup_dir/coredns-configmap.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "kubectl apply -f \"$backup_dir/coredns-configmap.yaml\" || warn \"Failed to restore coredns configmap\""
    },
    {
      "from": "72c7835a",
      "to": "ad0e321a",
      "relation": "references",
      "context": "if [ -f \"$backup_dir/coredns-service.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "ad0e321a",
      "relation": "references",
      "context": "kubectl apply -f \"$backup_dir/coredns-service.yaml\" || warn \"Failed to restore coredns service\""
    },
    {
      "from": "72c7835a",
      "to": "81cd44f2",
      "relation": "references",
      "context": "if [ -f \"$backup_dir/coredns-deployment.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "81cd44f2",
      "relation": "references",
      "context": "kubectl apply -f \"$backup_dir/coredns-deployment.yaml\" || warn \"Failed to restore coredns deployment"
    }
  ]
}