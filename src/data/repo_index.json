{
  "nodes": [
    {
      "id": "4ac32a78",
      "path": "CHANGELOG.md",
      "type": "doc",
      "title": "VMStation Changelog",
      "summary": "- Network control plane reset functionality (`net-reset` command)",
      "sizeBytes": 1012,
      "tags": [
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.241Z",
      "lines": 25
    },
    {
      "id": "61c1941a",
      "path": "CNI-BRIDGE-FIX-README.md",
      "type": "doc",
      "title": "VMStation CNI Bridge Fix - Enhanced Solution",
      "summary": "Your Kubernetes cluster was experiencing CNI bridge IP conflicts preventing pods from starting:",
      "sizeBytes": 4720,
      "tags": [
        "troubleshooting",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.241Z",
      "lines": 168
    },
    {
      "id": "f0859b7a",
      "path": "NETWORK-DIAGNOSIS-QUICKSTART.md",
      "type": "doc",
      "title": "VMStation Network Diagnosis - Quick Start Guide",
      "summary": "This automated solution addresses the inter-pod communication issues described in the problem statement where:",
      "sizeBytes": 3995,
      "tags": [
        "troubleshooting",
        "guide",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.241Z",
      "lines": 116
    },
    {
      "id": "7036e452",
      "path": "NETWORK_RESET_RUNBOOK.md",
      "type": "doc",
      "title": "VMStation Network Control Plane Reset Runbook",
      "summary": "This runbook provides step-by-step instructions for safely resetting the Kubernetes network control plane components (kube-proxy and CoreDNS) in the VMStation cluster.",
      "sizeBytes": 5310,
      "tags": [
        "troubleshooting"
      ],
      "lastModified": "2025-09-16T22:01:51.241Z",
      "lines": 164
    },
    {
      "id": "f1c06066",
      "path": "Output_for_VSCode.md",
      "type": "doc",
      "title": "container runtime logs (containerd)",
      "summary": "sudo journalctl -u containerd -n 400 --no-pager",
      "sizeBytes": 246341,
      "tags": [],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 1159
    },
    {
      "id": "9c2d9675",
      "path": "QUICK_FIX_GUIDE.md",
      "type": "doc",
      "title": "Quick Fix Guide for CNI Bridge Issue",
      "summary": "Jellyfin pod stuck in `ContainerCreating` with error:",
      "sizeBytes": 1331,
      "tags": [
        "guide",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 53
    },
    {
      "id": "4d7840b3",
      "path": "README-CNI-FIX.md",
      "type": "doc",
      "title": "CNI Pod Communication Issue - Quick Fix",
      "summary": "Pods on the same Kubernetes worker node cannot communicate with each other, showing \"Destination Host Unreachable\" errors. This specifically affects:",
      "sizeBytes": 7125,
      "tags": [
        "troubleshooting",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 167
    },
    {
      "id": "6a7d5d4b",
      "path": "README-cluster.md",
      "type": "doc",
      "title": "VMStation Kubernetes Cluster Deployment Guide",
      "summary": "This guide provides comprehensive instructions for deploying and managing a complete Kubernetes cluster using kubeadm and Ansible.",
      "sizeBytes": 13703,
      "tags": [
        "troubleshooting",
        "guide",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 490
    },
    {
      "id": "04c6e90f",
      "path": "README.md",
      "type": "doc",
      "title": "VMStation Kubernetes Cloud Overview",
      "summary": "Welcome to VMStation! A home cloud infrastructure built on Kubernetes for scalable, reliable self-hosted services.",
      "sizeBytes": 31457,
      "tags": [
        "troubleshooting",
        "guide",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 937
    },
    {
      "id": "ff198a1c",
      "path": "SCRIPT_CONSOLIDATION_GUIDE.md",
      "type": "doc",
      "title": "VMStation Script Consolidation Guide",
      "summary": "This guide documents the consolidation of VMStation scripts performed to clean up redundant, outdated, and deprecated scripts while preserving essential working functionality.",
      "sizeBytes": 6601,
      "tags": [
        "guide",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 143
    },
    {
      "id": "c9daffcc",
      "path": "SIMPLIFIED-DEPLOYMENT.md",
      "type": "doc",
      "title": "VMStation Simplified Deployment",
      "summary": "This document describes the new simplified deployment system for VMStation that replaces the complex previous system.",
      "sizeBytes": 7635,
      "tags": [
        "troubleshooting",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 250
    },
    {
      "id": "761e8cd1",
      "path": "SOLUTION-SUMMARY.md",
      "type": "doc",
      "title": "VMStation Kubernetes Fix - FINAL SOLUTION",
      "summary": "Your Kubernetes cluster had a **CNI bridge IP conflict** that was preventing pods from starting. The error was:",
      "sizeBytes": 3471,
      "tags": [
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 120
    },
    {
      "id": "0ff387c7",
      "path": "TODO.md",
      "type": "doc",
      "title": "VMStation TODO List",
      "summary": "This repository uses **kubeadm-based Kubernetes** for production deployments.",
      "sizeBytes": 2541,
      "tags": [
        "troubleshooting",
        "guide",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 88
    },
    {
      "id": "3fbfd7f8",
      "path": "USAGE_INSTRUCTIONS.md",
      "type": "doc",
      "title": "VMStation Usage Instructions",
      "summary": "If you have aggressively wiped your worker nodes (using `aggressive_worker_wipe_preserve_storage.sh` or similar), VMStation now includes enhanced support for clean worker joins.",
      "sizeBytes": 3883,
      "tags": [
        "troubleshooting",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 136
    },
    {
      "id": "dc5e49f8",
      "path": "ansible/artifacts/arc-network-diagnosis/README.md",
      "type": "doc",
      "title": "VMStation Network Diagnosis Artifacts",
      "summary": "This directory contains output from the automated network diagnosis playbook.",
      "sizeBytes": 2744,
      "tags": [
        "ansible",
        "artifacts",
        "arc-network-diagnosis"
      ],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 72
    },
    {
      "id": "39641f50",
      "path": "ansible/files/dashboard-secret-access.yaml",
      "type": "config",
      "title": "dashboard-secret-access",
      "summary": "--- apiVersion: v1 kind: ServiceAccount",
      "sizeBytes": 1273,
      "tags": [
        "ansible",
        "files"
      ],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 47
    },
    {
      "id": "6f2e75b0",
      "path": "ansible/files/grafana_dashboards/loki-dashboard.json",
      "type": "config",
      "title": "loki-dashboard",
      "summary": "{   \"schemaVersion\": 27,   \"title\": \"Loki Dashboard\",",
      "sizeBytes": 108,
      "tags": [
        "ansible",
        "files",
        "grafana_dashboards"
      ],
      "lastModified": "2025-09-16T22:01:51.242Z",
      "lines": 7
    },
    {
      "id": "b1bc04a3",
      "path": "ansible/files/grafana_dashboards/node-dashboard.json",
      "type": "config",
      "title": "node-dashboard",
      "summary": "{   \"schemaVersion\": 27,   \"title\": \"Node Dashboard\",",
      "sizeBytes": 108,
      "tags": [
        "ansible",
        "files",
        "grafana_dashboards"
      ],
      "lastModified": "2025-09-16T22:01:51.243Z",
      "lines": 7
    },
    {
      "id": "1f7744bc",
      "path": "ansible/files/grafana_dashboards/prometheus-dashboard.json",
      "type": "config",
      "title": "prometheus-dashboard",
      "summary": "{   \"schemaVersion\": 27,   \"title\": \"Prometheus Dashboard\",",
      "sizeBytes": 114,
      "tags": [
        "ansible",
        "files",
        "grafana_dashboards"
      ],
      "lastModified": "2025-09-16T22:01:51.243Z",
      "lines": 7
    },
    {
      "id": "8036f074",
      "path": "ansible/files/grafana_datasources/prometheus-datasource.yaml",
      "type": "config",
      "title": "prometheus-datasource",
      "summary": "# NOTE: This file is no longer used in the deployment. # The kube-prometheus-stack Helm chart automatically creates a default Prometheus datasource. # Manually creating this datasource would cause a c...",
      "sizeBytes": 477,
      "tags": [
        "ansible",
        "files",
        "grafana_datasources"
      ],
      "lastModified": "2025-09-16T22:01:51.243Z",
      "lines": 13
    },
    {
      "id": "607e6e59",
      "path": "ansible/group_vars/all.yml",
      "type": "config",
      "title": "all",
      "summary": "# VMStation Ansible Configuration Variables Template # Copy this file to all.yml and customize for your environment. # IMPORTANT: Never commit `ansible/group_vars/all.yml` with real credentials.",
      "sizeBytes": 6576,
      "tags": [
        "ansible",
        "group_vars"
      ],
      "lastModified": "2025-09-16T22:01:51.243Z",
      "lines": 162
    },
    {
      "id": "5f591fa6",
      "path": "ansible/group_vars/all.yml.template",
      "type": "template",
      "title": "all.yml",
      "summary": "# VMStation Ansible Configuration Variables Template # Copy this file to all.yml and customize for your environment. # IMPORTANT: Never commit `ansible/group_vars/all.yml` with real credentials.",
      "sizeBytes": 6576,
      "tags": [
        "ansible",
        "group_vars"
      ],
      "lastModified": "2025-09-16T22:01:51.243Z",
      "lines": 162
    },
    {
      "id": "e95d5da9",
      "path": "ansible/inventory/group_vars/all.yml.template",
      "type": "template",
      "title": "all.yml",
      "summary": "# VMStation Ansible Configuration Variables Template # Copy this file to all.yml and customize for your environment. # IMPORTANT: Never commit `ansible/group_vars/all.yml` with real credentials.",
      "sizeBytes": 6576,
      "tags": [
        "ansible",
        "inventory",
        "group_vars"
      ],
      "lastModified": "2025-09-16T22:01:51.243Z",
      "lines": 162
    },
    {
      "id": "1e67c2e0",
      "path": "ansible/inventory/hosts.yml",
      "type": "config",
      "title": "hosts",
      "summary": "--- # VMStation Kubernetes Cluster Inventory # Defines the three-node cluster topology as specified in requirements",
      "sizeBytes": 2386,
      "tags": [
        "ansible",
        "inventory"
      ],
      "lastModified": "2025-09-16T22:01:51.243Z",
      "lines": 86
    },
    {
      "id": "39bfddc5",
      "path": "ansible/playbooks/cluster-bootstrap/roles/system-prep/tasks/main.yml",
      "type": "config",
      "title": "main",
      "summary": "--- # System preparation tasks for Kubernetes cluster nodes # Extracted common tasks from the existing setup-cluster.yaml",
      "sizeBytes": 2495,
      "tags": [
        "ansible",
        "playbooks",
        "cluster-bootstrap",
        "roles",
        "system-prep",
        "tasks"
      ],
      "lastModified": "2025-09-16T22:01:51.243Z",
      "lines": 94
    },
    {
      "id": "fa6b94bd",
      "path": "ansible/playbooks/cluster-bootstrap/simple-bootstrap.yml",
      "type": "config",
      "title": "simple-bootstrap",
      "summary": "--- # VMStation Kubernetes Cluster Bootstrap - Simple Version # Alternative to the complex setup-cluster.yaml for basic kubeadm deployment",
      "sizeBytes": 8529,
      "tags": [
        "ansible",
        "playbooks",
        "cluster-bootstrap"
      ],
      "lastModified": "2025-09-16T22:01:51.243Z",
      "lines": 258
    },
    {
      "id": "2e9ce2c1",
      "path": "ansible/playbooks/cluster-bootstrap.yml",
      "type": "config",
      "title": "cluster-bootstrap",
      "summary": "--- # VMStation Kubernetes Cluster Bootstrap # Main playbook for setting up a complete Kubernetes cluster with kubeadm",
      "sizeBytes": 6695,
      "tags": [
        "ansible",
        "playbooks"
      ],
      "lastModified": "2025-09-16T22:01:51.243Z",
      "lines": 209
    },
    {
      "id": "fd7c8bd5",
      "path": "ansible/playbooks/minimal-network-fix.yml",
      "type": "config",
      "title": "minimal-network-fix",
      "summary": "--- # VMStation Minimal Network Fix Playbook # Applies minimal CNI bridge fixes and deploys working network components",
      "sizeBytes": 16636,
      "tags": [
        "ansible",
        "playbooks"
      ],
      "lastModified": "2025-09-16T22:01:51.243Z",
      "lines": 441
    },
    {
      "id": "91932608",
      "path": "ansible/playbooks/verify-cluster.yml",
      "type": "config",
      "title": "verify-cluster",
      "summary": "--- # VMStation Cluster Verification and Smoke Tests # Comprehensive validation of the Kubernetes cluster deployment",
      "sizeBytes": 20504,
      "tags": [
        "ansible",
        "playbooks"
      ],
      "lastModified": "2025-09-16T22:01:51.243Z",
      "lines": 505
    },
    {
      "id": "bcfb5f43",
      "path": "ansible/plays/deploy-apps.yaml",
      "type": "config",
      "title": "deploy-apps",
      "summary": "--- # VMStation Simplified Application Deployment # Deploys essential monitoring and applications",
      "sizeBytes": 17069,
      "tags": [
        "ansible",
        "plays"
      ],
      "lastModified": "2025-09-16T22:01:51.244Z",
      "lines": 500
    },
    {
      "id": "2ac1c05f",
      "path": "ansible/plays/jellyfin.yml",
      "type": "config",
      "title": "jellyfin",
      "summary": "--- # Jellyfin Deployment Playbook # Deploys Jellyfin using the minimal Kubernetes manifest",
      "sizeBytes": 13414,
      "tags": [
        "ansible",
        "plays"
      ],
      "lastModified": "2025-09-16T22:01:51.244Z",
      "lines": 343
    },
    {
      "id": "ca8597d0",
      "path": "ansible/plays/kubernetes/deploy_monitoring.yaml",
      "type": "config",
      "title": "deploy_monitoring",
      "summary": "--- # Deploy monitoring stack (Prometheus, Grafana, Loki) using Helm charts - name: Deploy monitoring stack on Kubernetes",
      "sizeBytes": 18940,
      "tags": [
        "ansible",
        "plays",
        "kubernetes"
      ],
      "lastModified": "2025-09-16T22:01:51.244Z",
      "lines": 520
    },
    {
      "id": "377c4256",
      "path": "ansible/plays/kubernetes/jellyfin-minimal.yml",
      "type": "config",
      "title": "jellyfin-minimal",
      "summary": "# ============================================================================ # Minimal Jellyfin Kubernetes Deployment # ============================================================================",
      "sizeBytes": 8140,
      "tags": [
        "ansible",
        "plays",
        "kubernetes"
      ],
      "lastModified": "2025-09-16T22:01:51.244Z",
      "lines": 296
    },
    {
      "id": "7ef785f1",
      "path": "ansible/plays/kubernetes/monitoring_validation.yaml",
      "type": "config",
      "title": "monitoring_validation",
      "summary": "--- # Post-deployment validation for Grafana dashboards and datasources - name: Validate Grafana provisioning",
      "sizeBytes": 4536,
      "tags": [
        "ansible",
        "plays",
        "kubernetes"
      ],
      "lastModified": "2025-09-16T22:01:51.244Z",
      "lines": 101
    },
    {
      "id": "3e468245",
      "path": "ansible/plays/kubernetes/rhel10_setup_fixes.yaml",
      "type": "config",
      "title": "rhel10_setup_fixes",
      "summary": "--- # RHEL 10 Specific Fixes for Kubernetes Cluster Setup # This playbook addresses common issues with RHEL 10 systems joining Kubernetes clusters",
      "sizeBytes": 9856,
      "tags": [
        "ansible",
        "plays",
        "kubernetes"
      ],
      "lastModified": "2025-09-16T22:01:51.244Z",
      "lines": 291
    },
    {
      "id": "4bd4f2e4",
      "path": "ansible/plays/kubernetes/setup_cert_manager.yaml",
      "type": "config",
      "title": "setup_cert_manager",
      "summary": "--- # Install and configure cert-manager for TLS certificate management - name: Install cert-manager on Kubernetes cluster",
      "sizeBytes": 20256,
      "tags": [
        "ansible",
        "plays",
        "kubernetes"
      ],
      "lastModified": "2025-09-16T22:01:51.244Z",
      "lines": 512
    },
    {
      "id": "2adacfa6",
      "path": "ansible/plays/kubernetes/setup_helm.yaml",
      "type": "config",
      "title": "setup_helm",
      "summary": "--- # Install Helm on the control plane node - name: Install Helm on monitoring_nodes",
      "sizeBytes": 2025,
      "tags": [
        "ansible",
        "plays",
        "kubernetes"
      ],
      "lastModified": "2025-09-16T22:01:51.244Z",
      "lines": 67
    },
    {
      "id": "5407a29a",
      "path": "ansible/plays/kubernetes/setup_local_path_provisioner.yaml",
      "type": "config",
      "title": "setup_local_path_provisioner",
      "summary": "--- # Deploy custom local-path provisioner with /srv/monitoring_data storage path - name: Deploy custom local-path provisioner for VMStation",
      "sizeBytes": 7259,
      "tags": [
        "ansible",
        "plays",
        "kubernetes"
      ],
      "lastModified": "2025-09-16T22:01:51.244Z",
      "lines": 224
    },
    {
      "id": "6ec02888",
      "path": "ansible/plays/kubernetes/templates/kube-flannel-allnodes.yml",
      "type": "config",
      "title": "kube-flannel-allnodes",
      "summary": "--- # Custom Flannel manifest for VMStation # Allows Flannel daemon to run on all nodes (control plane and workers)",
      "sizeBytes": 4981,
      "tags": [
        "ansible",
        "plays",
        "kubernetes",
        "templates"
      ],
      "lastModified": "2025-09-16T22:01:51.244Z",
      "lines": 225
    },
    {
      "id": "b9618ca5",
      "path": "ansible/plays/network-diagnosis.yaml",
      "type": "config",
      "title": "network-diagnosis",
      "summary": "--- # VMStation Inter-Pod Communication Network Diagnosis Playbook # Automates the comprehensive network diagnosis and remediation steps",
      "sizeBytes": 13957,
      "tags": [
        "ansible",
        "plays"
      ],
      "lastModified": "2025-09-16T22:01:51.244Z",
      "lines": 368
    },
    {
      "id": "f0bc548f",
      "path": "ansible/plays/setup-cluster.yaml",
      "type": "config",
      "title": "setup-cluster",
      "summary": "--- # VMStation Simplified Kubernetes Cluster Setup # Replaces the complex 2901-line setup_cluster.yaml with essential functionality",
      "sizeBytes": 109519,
      "tags": [
        "ansible",
        "plays"
      ],
      "lastModified": "2025-09-16T22:01:51.245Z",
      "lines": 2662
    },
    {
      "id": "b6bdf318",
      "path": "ansible/plays/templates/additional-registries.conf.j2",
      "type": "template",
      "title": "additional-registries.conf",
      "summary": "# Additional Container Registries Configuration # Generated by Ansible for VMStation Quay integration # Maintain compatibility with existing unqualified searches",
      "sizeBytes": 803,
      "tags": [
        "ansible",
        "plays",
        "templates"
      ],
      "lastModified": "2025-09-16T22:01:51.245Z",
      "lines": 24
    },
    {
      "id": "502f8253",
      "path": "ansible/plays/templates/network-diagnosis-report.j2",
      "type": "template",
      "title": "network-diagnosis-report",
      "summary": "# VMStation Network Diagnosis Report **Generated:** {{ timestamp }}   **Diagnosis ID:** {{ diagnosis_timestamp }}",
      "sizeBytes": 4184,
      "tags": [
        "ansible",
        "plays",
        "templates"
      ],
      "lastModified": "2025-09-16T22:01:51.245Z",
      "lines": 124
    },
    {
      "id": "74a84d68",
      "path": "ansible/requirements.yml",
      "type": "config",
      "title": "requirements",
      "summary": "# Ansible Collections Requirements for VMStation Kubernetes collections:   - name: kubernetes.core",
      "sizeBytes": 214,
      "tags": [
        "ansible"
      ],
      "lastModified": "2025-09-16T22:01:51.245Z",
      "lines": 8
    },
    {
      "id": "b0430b0d",
      "path": "ansible/roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-minimal.yml",
      "type": "config",
      "title": "jellyfin-minimal",
      "summary": "# ============================================================================ # Minimal Jellyfin Kubernetes Deployment # ============================================================================",
      "sizeBytes": 7793,
      "tags": [
        "ansible",
        "roles",
        "jellyfin",
        "files",
        "plays",
        "kubernetes"
      ],
      "lastModified": "2025-09-16T22:01:51.245Z",
      "lines": 283
    },
    {
      "id": "c782d582",
      "path": "ansible/roles/jellyfin/tasks/main.yml",
      "type": "config",
      "title": "main",
      "summary": "--- # Jellyfin Deployment Role - Main Tasks # Deploys minimal Jellyfin pod using kubernetes.core collection",
      "sizeBytes": 8069,
      "tags": [
        "ansible",
        "roles",
        "jellyfin",
        "tasks"
      ],
      "lastModified": "2025-09-16T22:01:51.245Z",
      "lines": 201
    },
    {
      "id": "dd358be7",
      "path": "ansible/simple-deploy.yaml",
      "type": "config",
      "title": "simple-deploy",
      "summary": "--- # VMStation Simplified Deployment Playbook # Replaces the complex site.yaml with essential functionality only",
      "sizeBytes": 9806,
      "tags": [
        "ansible"
      ],
      "lastModified": "2025-09-16T22:01:51.245Z",
      "lines": 243
    },
    {
      "id": "9fa74fe4",
      "path": "ansible/subsites/00-spindown.yaml",
      "type": "config",
      "title": "00-spindown",
      "summary": "--- # Subsite 00: Spin-down / destructive cleanup helper #",
      "sizeBytes": 29392,
      "tags": [
        "ansible",
        "subsites"
      ],
      "lastModified": "2025-09-16T22:01:51.245Z",
      "lines": 772
    },
    {
      "id": "7b8171b6",
      "path": "copilot-instructions.md",
      "type": "doc",
      "title": "add: quay_username, quay_password, grafana_admin_pass, vault_r430_sudo_password",
      "summary": "```",
      "sizeBytes": 8844,
      "tags": [],
      "lastModified": "2025-09-16T22:01:51.245Z",
      "lines": 135
    },
    {
      "id": "72c7835a",
      "path": "deploy-cluster.sh",
      "type": "script",
      "title": "deploy-cluster",
      "summary": "VMStation Kubernetes Cluster Deployment Script Wrapper for the complete cluster bootstrap process",
      "sizeBytes": 42371,
      "tags": [
        "kubernetes",
        "docker",
        "ansible",
        "jellyfin",
        "networking",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 1076
    },
    {
      "id": "d7ce2015",
      "path": "deploy-single.sh",
      "type": "script",
      "title": "deploy-single",
      "summary": "VMStation Single-Command Kubernetes Deployment Fixes CNI bridge issues and deploys working cluster with minimal manifests",
      "sizeBytes": 10566,
      "tags": [
        "kubernetes",
        "ansible",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 357
    },
    {
      "id": "7a6f331b",
      "path": "diagnose_jellyfin_network.sh",
      "type": "script",
      "title": "diagnose_jellyfin_network",
      "summary": "Diagnose Jellyfin Network Connectivity Issues This script helps identify and diagnose the \"no route to host\" errors that prevent Jellyfin health probes from working",
      "sizeBytes": 9753,
      "tags": [
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 270
    },
    {
      "id": "8c19061a",
      "path": "docs/AGGRESSIVE_NODE_RESET.md",
      "type": "doc",
      "title": "Enhanced Join Process - Aggressive Node Reset",
      "summary": "The enhanced kubeadm join script was failing with persistent containerd filesystem initialization errors:",
      "sizeBytes": 4310,
      "tags": [
        "docs",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 118
    },
    {
      "id": "094a0932",
      "path": "docs/CNI_BRIDGE_FIX.md",
      "type": "doc",
      "title": "CNI Bridge IP Conflict Fix",
      "summary": "Kubernetes pods get stuck in \"ContainerCreating\" state with errors like:",
      "sizeBytes": 3437,
      "tags": [
        "docs",
        "troubleshooting",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 115
    },
    {
      "id": "f0840b04",
      "path": "docs/CNI_BRIDGE_RESET.md",
      "type": "doc",
      "title": "CNI Bridge Reset Quick Fix",
      "summary": "Jellyfin pod (or other pods) stuck in ContainerCreating state with error:",
      "sizeBytes": 1772,
      "tags": [
        "docs",
        "troubleshooting",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 44
    },
    {
      "id": "8facf1bd",
      "path": "docs/COREDNS_MASTERNODE_ENFORCEMENT.md",
      "type": "doc",
      "title": "CoreDNS Masternode Enforcement Fix",
      "summary": "CoreDNS pods were being scheduled on the \"homelab\" worker node (192.168.4.62) instead of staying on the \"masternode\" control-plane node (192.168.4.63), causing networking instability and DNS resolutio...",
      "sizeBytes": 2719,
      "tags": [
        "docs",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 65
    },
    {
      "id": "fcb85303",
      "path": "docs/COREDNS_UNKNOWN_STATUS_FIX.md",
      "type": "doc",
      "title": "CoreDNS \"Unknown\" Status Fix",
      "summary": "After running `deploy.sh full` and regenerating flannel pods, CoreDNS pods may show \"Unknown\" status with no IP address assigned. This prevents DNS resolution in the cluster and causes other pods to r...",
      "sizeBytes": 5065,
      "tags": [
        "docs",
        "troubleshooting",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 161
    },
    {
      "id": "0665ba5c",
      "path": "docs/ENHANCED_JOIN_PROCESS.md",
      "type": "doc",
      "title": "Enhanced Kubernetes Join Process",
      "summary": "This document describes the enhanced kubeadm join process implemented to address persistent issues with worker nodes falling back to \"standalone mode\" instead of properly joining the cluster.",
      "sizeBytes": 11186,
      "tags": [
        "docs",
        "troubleshooting",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 316
    },
    {
      "id": "20493e93",
      "path": "docs/FIXES_APPLIED.md",
      "type": "doc",
      "title": "VMStation DNS and Monitoring Fixes",
      "summary": "This document describes the fixes applied to resolve DNS resolution, monitoring access, and homelab node stability issues.",
      "sizeBytes": 3449,
      "tags": [
        "docs",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 103
    },
    {
      "id": "e8e06adf",
      "path": "docs/HOMELAB_NODE_FIXES.md",
      "type": "doc",
      "title": "Homelab Node Networking Issues Fix",
      "summary": "After multiple runs of `deploy.sh full`, several critical networking issues were observed:",
      "sizeBytes": 4355,
      "tags": [
        "docs",
        "troubleshooting",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 129
    },
    {
      "id": "13b742f3",
      "path": "docs/JELLYFIN_NETWORKING_TROUBLESHOOT.md",
      "type": "doc",
      "title": "VMStation Jellyfin Pod IP Assignment Troubleshooting Guide",
      "summary": "This guide addresses the specific issue where Jellyfin pods fail to get IP addresses and remain stuck in \"ContainerCreating\" state, often accompanied by kube-flannel and kube-proxy crashloopbackoff is...",
      "sizeBytes": 5408,
      "tags": [
        "docs",
        "troubleshooting",
        "guide",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 204
    },
    {
      "id": "2267ad9a",
      "path": "docs/KUBELET_SYSTEMD_CONFIG_FIX.md",
      "type": "doc",
      "title": "Kubelet Systemd Configuration Fix",
      "summary": "This document describes the fix for the \"Assignment outside of section\" systemd error that occurs during VMStation Kubernetes worker node joins.",
      "sizeBytes": 5200,
      "tags": [
        "docs",
        "troubleshooting",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 178
    },
    {
      "id": "121dca56",
      "path": "docs/MANUAL_CLUSTER_TROUBLESHOOTING.md",
      "type": "doc",
      "title": "Enhanced Kubernetes Worker Node Troubleshooting Guide",
      "summary": "This guide provides comprehensive troubleshooting steps for worker node join failures, focusing on the enhanced remediation capabilities added to VMStation.",
      "sizeBytes": 15200,
      "tags": [
        "docs",
        "troubleshooting",
        "guide",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 530
    },
    {
      "id": "044d8702",
      "path": "docs/POST_WIPE_WORKER_JOIN.md",
      "type": "doc",
      "title": "Post-Wipe Worker Join Process",
      "summary": "This document describes the enhanced worker join process specifically designed to handle workers that have been reset using `aggressive_worker_wipe_preserve_storage.sh` or similar aggressive cleanup p...",
      "sizeBytes": 8604,
      "tags": [
        "docs",
        "troubleshooting",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 269
    },
    {
      "id": "e7010005",
      "path": "docs/README.md",
      "type": "doc",
      "title": "VMStation Documentation Index",
      "summary": "Welcome to the reorganized documentation for your homelab/self-hosting server stack.",
      "sizeBytes": 865,
      "tags": [
        "docs",
        "troubleshooting",
        "guide",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.246Z",
      "lines": 20
    },
    {
      "id": "53177eca",
      "path": "docs/RHEL10_TROUBLESHOOTING.md",
      "type": "doc",
      "title": "RHEL 10 Kubernetes Worker Node Join - Troubleshooting Guide",
      "summary": "This guide addresses the specific issue where RHEL 10 compute nodes (192.168.4.62) fail to join the Kubernetes cluster during the \"TASK [Join worker nodes to cluster]\" step.",
      "sizeBytes": 19601,
      "tags": [
        "docs",
        "troubleshooting",
        "guide",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 539
    },
    {
      "id": "40ee73d0",
      "path": "docs/ansible_vault_quickstart.md",
      "type": "doc",
      "title": "Ansible Vault & Git Repo Quickstart",
      "summary": "This guide helps you securely manage secrets and sync your configuration repo across your desktop and server machines.",
      "sizeBytes": 1255,
      "tags": [
        "docs",
        "guide"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 67
    },
    {
      "id": "88482a18",
      "path": "docs/cluster-communication-fixes.md",
      "type": "doc",
      "title": "Cluster Communication Fixes",
      "summary": "This document describes the fixes for Kubernetes cluster communication issues, addressing the problems described in the issue where worker nodes cannot properly communicate with the cluster.",
      "sizeBytes": 5933,
      "tags": [
        "docs",
        "troubleshooting",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 169
    },
    {
      "id": "797dca0d",
      "path": "docs/cni-pod-communication-fix.md",
      "type": "doc",
      "title": "CNI Pod Communication Fix",
      "summary": "This document describes the solution for the CNI communication issue where pods cannot reach each other, specifically addressing the problem where a debug pod (10.244.0.20) on storagenodet3500 cannot ...",
      "sizeBytes": 6358,
      "tags": [
        "docs",
        "troubleshooting",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 181
    },
    {
      "id": "378a3809",
      "path": "docs/devices/Catalyst3650V02_Switch.md",
      "type": "doc",
      "title": "Cisco Catalyst 3650V02 Managed Switch",
      "summary": "- Managed switch for VLANs, QoS, and network segmentation",
      "sizeBytes": 335,
      "tags": [
        "docs",
        "devices",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 15
    },
    {
      "id": "c1dd3153",
      "path": "docs/devices/MiniPC_Monitoring_Ansible.md",
      "type": "doc",
      "title": "Ansible Setup: MiniPC Monitoring Node",
      "summary": "This guide describes how to use Ansible to set up Prometheus, Grafana, and Loki on your MiniPC.",
      "sizeBytes": 1123,
      "tags": [
        "docs",
        "devices",
        "guide",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 52
    },
    {
      "id": "37f609a0",
      "path": "docs/devices/R430_Compute.md",
      "type": "doc",
      "title": "Dell R430 Compute Engine",
      "summary": "- Main compute node for cluster workloads",
      "sizeBytes": 384,
      "tags": [
        "docs",
        "devices",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 20
    },
    {
      "id": "28982728",
      "path": "docs/devices/T3500_NAS.md",
      "type": "doc",
      "title": "Dell T3500 NAS Server",
      "summary": "- Dedicated NAS (Network Attached Storage)",
      "sizeBytes": 371,
      "tags": [
        "docs",
        "devices",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 20
    },
    {
      "id": "9b321fa0",
      "path": "docs/devices/reset_debian_nodes.md",
      "type": "doc",
      "title": "Resetting Debian Nodes (MiniPC & T3500) to Default State",
      "summary": "This guide provides an Ansible playbook to remove non-system apps and custom network settings from your Debian nodes (MiniPC, T3500).",
      "sizeBytes": 3565,
      "tags": [
        "docs",
        "devices",
        "guide"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 131
    },
    {
      "id": "f087a852",
      "path": "docs/dns-fix-guide.md",
      "type": "doc",
      "title": "VMStation Kubernetes DNS Fix Guide",
      "summary": "When running `kubectl version --short`, you get the error:",
      "sizeBytes": 4073,
      "tags": [
        "docs",
        "troubleshooting",
        "guide",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 136
    },
    {
      "id": "e00aacd0",
      "path": "docs/fix-jellyfin-pod-deletion.md",
      "type": "doc",
      "title": "Fix Summary: Prevent Unnecessary Jellyfin Pod Deletion and Flannel Crashes",
      "summary": "This fix addresses the issue where previous PR changes were unnecessarily deleting healthy jellyfin pods and causing flannel pod crashes.",
      "sizeBytes": 1308,
      "tags": [
        "docs",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 23
    },
    {
      "id": "0eb8fc77",
      "path": "docs/fix_cluster_communication.md",
      "type": "doc",
      "title": "from the repository root",
      "summary": "Path: `scripts/fix_cluster_communication.sh`",
      "sizeBytes": 10151,
      "tags": [
        "docs",
        "troubleshooting",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 154
    },
    {
      "id": "207b4d0f",
      "path": "docs/grafana_datasource_conflict_fix.md",
      "type": "doc",
      "title": "Grafana Datasource Conflict Fix",
      "summary": "Grafana was experiencing the following error:",
      "sizeBytes": 5096,
      "tags": [
        "docs",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 149
    },
    {
      "id": "da894f5f",
      "path": "docs/jellyfin/JELLYFIN_HA_DEPLOYMENT.md",
      "type": "doc",
      "title": "Jellyfin High-Availability Kubernetes Deployment",
      "summary": "This implementation provides a comprehensive, highly-available Jellyfin media server deployment on Kubernetes with auto-scaling capabilities. It replaces the existing single Podman container setup wit...",
      "sizeBytes": 10594,
      "tags": [
        "docs",
        "jellyfin",
        "troubleshooting",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 323
    },
    {
      "id": "8a5efc5c",
      "path": "docs/jellyfin/MIGRATION_CHECKLIST.md",
      "type": "doc",
      "title": "Jellyfin Podman to Kubernetes HA Migration Checklist",
      "summary": "- [ ] Verify existing Podman Jellyfin is running: `podman ps | grep jellyfin`",
      "sizeBytes": 6515,
      "tags": [
        "docs",
        "jellyfin",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 179
    },
    {
      "id": "710930e0",
      "path": "docs/jellyfin/USAGE_EXAMPLE.md",
      "type": "doc",
      "title": "Jellyfin HA Deployment Example",
      "summary": "This example demonstrates how the Jellyfin HA deployment automatically scales based on usage:",
      "sizeBytes": 5431,
      "tags": [
        "docs",
        "jellyfin",
        "troubleshooting"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 161
    },
    {
      "id": "0c29bbcc",
      "path": "docs/jellyfin-cni-bridge-fix.md",
      "type": "doc",
      "title": "Fix Jellyfin CNI Bridge IP Conflict",
      "summary": "This script addresses the specific issue where Jellyfin pods fail to create with the error:",
      "sizeBytes": 2917,
      "tags": [
        "docs",
        "troubleshooting",
        "guide",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 73
    },
    {
      "id": "1e068979",
      "path": "docs/jellyfin-network-fix.md",
      "type": "doc",
      "title": "Jellyfin Pod Network Connectivity Fix",
      "summary": "After running the cluster deployment, the Jellyfin pod shows `0/1` ready status despite the container running successfully. The health probes fail with \"no route to host\" errors when trying to connect...",
      "sizeBytes": 1953,
      "tags": [
        "docs",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 61
    },
    {
      "id": "1e57853e",
      "path": "docs/jellyfin-pvc-hostpath-migration.md",
      "type": "doc",
      "title": "VMStation Jellyfin PVC Fix - Implementation Summary",
      "summary": "The VMStation deployment was experiencing two critical issues:",
      "sizeBytes": 4814,
      "tags": [
        "docs",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 128
    },
    {
      "id": "98b95b67",
      "path": "docs/jellyfin-toleration-fix.md",
      "type": "doc",
      "title": "Jellyfin Toleration Fix Verification",
      "summary": "The Jellyfin pod deployment was failing with the error:",
      "sizeBytes": 1423,
      "tags": [
        "docs",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.247Z",
      "lines": 51
    },
    {
      "id": "42b5112f",
      "path": "docs/kubernetes_authorization_modes.md",
      "type": "doc",
      "title": "Kubernetes Authorization Mode Configuration",
      "summary": "VMStation now supports configurable authorization modes for the Kubernetes API server to address RBAC issues that may prevent worker nodes from joining the cluster.",
      "sizeBytes": 4136,
      "tags": [
        "docs",
        "troubleshooting",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 134
    },
    {
      "id": "d6e5693f",
      "path": "docs/namespace-termination-resolution.md",
      "type": "doc",
      "title": "VMStation CNI Namespace Termination Resolution",
      "summary": "The `fix-cluster.sh` script was failing with the error:",
      "sizeBytes": 2998,
      "tags": [
        "docs",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 65
    },
    {
      "id": "19863979",
      "path": "docs/network-diagnosis.md",
      "type": "doc",
      "title": "VMStation Inter-Pod Communication Network Diagnosis",
      "summary": "This document describes the automated network diagnosis system for troubleshooting inter-pod communication issues in the VMStation Kubernetes cluster.",
      "sizeBytes": 6133,
      "tags": [
        "docs",
        "troubleshooting",
        "guide",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 185
    },
    {
      "id": "c914457e",
      "path": "docs/nodeport-external-access-fix.md",
      "type": "doc",
      "title": "NodePort External Access Fix",
      "summary": "External machines (like development desktops) cannot access Kubernetes NodePort services (specifically Jellyfin on port 30096) even after running standard cluster communication fixes. The issue manife...",
      "sizeBytes": 6642,
      "tags": [
        "docs",
        "troubleshooting",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 195
    },
    {
      "id": "84b873a0",
      "path": "docs/pv_permissions_and_loki_issues.md",
      "type": "doc",
      "title": "Check Grafana logs for datasource errors",
      "summary": "This document explains common issues encountered during monitoring stack deployment including Grafana CrashLoopBackOff (PVC/PV permissions), Loki crashes (invalid config), and Grafana Pending state (s...",
      "sizeBytes": 11719,
      "tags": [
        "docs",
        "troubleshooting",
        "guide",
        "fix"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 257
    },
    {
      "id": "565c9435",
      "path": "docs/security/cloudflare_tunnel_quickstart.md",
      "type": "doc",
      "title": "Cloudflare Tunnel (Zero Trust) Quickstart",
      "summary": "This guide explains how to set up a free Cloudflare Tunnel (Zero Trust) to securely expose your self-hosted applications to the internet.",
      "sizeBytes": 1930,
      "tags": [
        "docs",
        "security",
        "guide"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 64
    },
    {
      "id": "76046802",
      "path": "docs/security/firewall.md",
      "type": "doc",
      "title": "Firewall & Security Guide",
      "summary": "```bash",
      "sizeBytes": 337,
      "tags": [
        "docs",
        "security",
        "guide",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 21
    },
    {
      "id": "a93403c8",
      "path": "docs/security/hashicorp_vault_usage.md",
      "type": "doc",
      "title": "HashiCorp Vault Usage Guide",
      "summary": "This guide outlines how to use HashiCorp Vault for self-hosted credential management in your homelab, with a TODO for advanced security features.",
      "sizeBytes": 1971,
      "tags": [
        "docs",
        "security",
        "guide",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 62
    },
    {
      "id": "4096f24e",
      "path": "docs/static-ips-and-dns.md",
      "type": "doc",
      "title": "VMStation Static IP Assignment and DNS Subdomains",
      "summary": "This document describes the static IP assignments for critical Kubernetes components and the DNS subdomain configuration for the homelab.",
      "sizeBytes": 8979,
      "tags": [
        "docs",
        "troubleshooting",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 280
    },
    {
      "id": "311b37fb",
      "path": "fix-cluster.sh",
      "type": "script",
      "title": "fix-cluster",
      "summary": "VMStation One-Command Fix Script Fixes CNI bridge issues and deploys working cluster",
      "sizeBytes": 2781,
      "tags": [
        "kubernetes",
        "ansible",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 85
    },
    {
      "id": "ebb30773",
      "path": "fix_jellyfin_cni_bridge_conflict.sh",
      "type": "script",
      "title": "fix_jellyfin_cni_bridge_conflict",
      "summary": "Fix Jellyfin CNI Bridge IP Conflict and kube-proxy CrashLoopBackOff Issues Addresses the specific issues from the problem statement: 1. Jellyfin pod fails with: \"cni0\" already has an IP address differ...",
      "sizeBytes": 32407,
      "tags": [
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 781
    },
    {
      "id": "9088164c",
      "path": "fix_jellyfin_immediate.sh",
      "type": "script",
      "title": "fix_jellyfin_immediate",
      "summary": "VMStation Immediate Jellyfin Fix Specifically addresses CNI bridge IP conflict preventing Jellyfin pod from starting This script can be run without a full cluster reset",
      "sizeBytes": 10897,
      "tags": [
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 305
    },
    {
      "id": "e3453d63",
      "path": "fix_jellyfin_network_issue.sh",
      "type": "script",
      "title": "fix_jellyfin_network_issue",
      "summary": "Quick remediation and diagnostic script for Jellyfin network issues in this repo - Collects diagnostics from the Jellyfin pod and cluster - Attempts non-destructive cluster-level remediation steps - P...",
      "sizeBytes": 15529,
      "tags": [
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 393
    },
    {
      "id": "c9de76ea",
      "path": "fix_jellyfin_probe.yaml",
      "type": "config",
      "title": "fix_jellyfin_probe",
      "summary": "--- # Fix Jellyfin probe configuration # This patch updates the pod to use the correct health check endpoint",
      "sizeBytes": 2487,
      "tags": [],
      "lastModified": "2025-09-16T22:01:51.248Z",
      "lines": 108
    },
    {
      "id": "0d0c4a60",
      "path": "fix_jellyfin_readiness.sh",
      "type": "script",
      "title": "fix_jellyfin_readiness",
      "summary": "Fix Jellyfin Pod Readiness Issues This script applies minimal changes to fix the probe configuration and ensure proper network connectivity for health checks",
      "sizeBytes": 8463,
      "tags": [
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 244
    },
    {
      "id": "0ecf3a99",
      "path": "generate_join_command.sh",
      "type": "script",
      "title": "generate_join_command",
      "summary": "VMStation Kubeadm Join Command Generator Generates the exact join command for worker nodes as recommended in problem statement",
      "sizeBytes": 5828,
      "tags": [
        "kubernetes",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 178
    },
    {
      "id": "3d26bfd6",
      "path": "manifests/cni/flannel-minimal.yaml",
      "type": "config",
      "title": "flannel-minimal",
      "summary": "--- # Minimal Flannel CNI Plugin for VMStation Kubernetes Cluster # Fixed CNI bridge configuration for proper pod networking",
      "sizeBytes": 4489,
      "tags": [
        "manifests",
        "cni"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 212
    },
    {
      "id": "0a813314",
      "path": "manifests/cni/flannel.yaml",
      "type": "config",
      "title": "flannel",
      "summary": "--- # Flannel CNI Plugin for VMStation Kubernetes Cluster # Pod Subnet: 10.244.0.0/16",
      "sizeBytes": 4513,
      "tags": [
        "manifests",
        "cni"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 218
    },
    {
      "id": "4ede3e1b",
      "path": "manifests/jellyfin/jellyfin-minimal.yaml",
      "type": "config",
      "title": "jellyfin-minimal",
      "summary": "--- # Minimal Jellyfin deployment for VMStation apiVersion: v1",
      "sizeBytes": 2293,
      "tags": [
        "manifests",
        "jellyfin"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 110
    },
    {
      "id": "269791b0",
      "path": "manifests/jellyfin/jellyfin.yaml",
      "type": "config",
      "title": "jellyfin",
      "summary": "--- # VMStation Jellyfin Namespace apiVersion: v1",
      "sizeBytes": 4827,
      "tags": [
        "manifests",
        "jellyfin"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 186
    },
    {
      "id": "597b7bba",
      "path": "manifests/kubeadm-config.yaml.j2",
      "type": "template",
      "title": "kubeadm-config.yaml",
      "summary": "--- # VMStation kubeadm Configuration Template # This template configures a single control-plane Kubernetes cluster",
      "sizeBytes": 1184,
      "tags": [
        "manifests"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 39
    },
    {
      "id": "24a2f90a",
      "path": "manifests/monitoring/grafana.yaml",
      "type": "config",
      "title": "grafana",
      "summary": "--- # Grafana ConfigMap for datasources apiVersion: v1",
      "sizeBytes": 6401,
      "tags": [
        "manifests",
        "monitoring"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 253
    },
    {
      "id": "1d93e806",
      "path": "manifests/monitoring/prometheus.yaml",
      "type": "config",
      "title": "prometheus",
      "summary": "--- # VMStation Monitoring Namespace apiVersion: v1",
      "sizeBytes": 4301,
      "tags": [
        "manifests",
        "monitoring"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 176
    },
    {
      "id": "71a6fc0c",
      "path": "manifests/network/coredns-configmap.yaml",
      "type": "config",
      "title": "coredns-configmap",
      "summary": "apiVersion: v1 kind: ConfigMap metadata:",
      "sizeBytes": 1197,
      "tags": [
        "manifests",
        "network"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 52
    },
    {
      "id": "81cd44f2",
      "path": "manifests/network/coredns-deployment.yaml",
      "type": "config",
      "title": "coredns-deployment",
      "summary": "apiVersion: apps/v1 kind: Deployment metadata:",
      "sizeBytes": 2934,
      "tags": [
        "manifests",
        "network"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 111
    },
    {
      "id": "0d91b1c9",
      "path": "manifests/network/coredns-minimal.yaml",
      "type": "config",
      "title": "coredns-minimal",
      "summary": "--- # Minimal CoreDNS Configuration for VMStation # Simplified for stable operation and quick startup",
      "sizeBytes": 4804,
      "tags": [
        "manifests",
        "network"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 215
    },
    {
      "id": "ad0e321a",
      "path": "manifests/network/coredns-service.yaml",
      "type": "config",
      "title": "coredns-service",
      "summary": "apiVersion: v1 kind: Service metadata:",
      "sizeBytes": 485,
      "tags": [
        "manifests",
        "network"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 26
    },
    {
      "id": "2883003d",
      "path": "manifests/network/kube-proxy-configmap.yaml",
      "type": "config",
      "title": "kube-proxy-configmap",
      "summary": "apiVersion: v1 kind: ConfigMap metadata:",
      "sizeBytes": 1348,
      "tags": [
        "manifests",
        "network"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 54
    },
    {
      "id": "80025501",
      "path": "manifests/network/kube-proxy-daemonset.yaml",
      "type": "config",
      "title": "kube-proxy-daemonset",
      "summary": "apiVersion: apps/v1 kind: DaemonSet metadata:",
      "sizeBytes": 1792,
      "tags": [
        "manifests",
        "network"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 74
    },
    {
      "id": "0aec39aa",
      "path": "manifests/network/kube-proxy-minimal.yaml",
      "type": "config",
      "title": "kube-proxy-minimal",
      "summary": "--- # Minimal kube-proxy configuration for mixed OS environment apiVersion: v1",
      "sizeBytes": 3493,
      "tags": [
        "manifests",
        "network"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 145
    },
    {
      "id": "f157568d",
      "path": "quick_fix_cni_communication.sh",
      "type": "script",
      "title": "quick_fix_cni_communication",
      "summary": "Quick Fix for CNI Pod Communication Issue Addresses the specific issue where pods cannot communicate with each other as described in the problem statement with debug pod failing to reach Jellyfin pod",
      "sizeBytes": 6270,
      "tags": [
        "kubernetes",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 200
    },
    {
      "id": "efc0b58e",
      "path": "scripts/README.md",
      "type": "doc",
      "title": "VMStation Scripts Documentation",
      "summary": "This directory contains operational scripts for VMStation infrastructure management.",
      "sizeBytes": 7750,
      "tags": [
        "scripts",
        "troubleshooting",
        "guide",
        "fix",
        "setup"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 247
    },
    {
      "id": "585e2b00",
      "path": "scripts/ansible_pre_join_validation.sh",
      "type": "script",
      "title": "ansible_pre_join_validation",
      "summary": "VMStation Pre-Join Validation Script for Ansible Handles crictl permissions and containerd validation properly",
      "sizeBytes": 8144,
      "tags": [
        "scripts"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 203
    },
    {
      "id": "7fb7d1a5",
      "path": "scripts/check_cni_bridge_conflict.sh",
      "type": "script",
      "title": "check_cni_bridge_conflict",
      "summary": "Check for CNI Bridge Conflict Issues This script detects if pods are stuck due to CNI bridge IP conflicts Check if we have kubectl access",
      "sizeBytes": 1384,
      "tags": [
        "scripts",
        "kubernetes",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.249Z",
      "lines": 41
    },
    {
      "id": "36bd22c9",
      "path": "scripts/check_coredns_status.sh",
      "type": "script",
      "title": "check_coredns_status",
      "summary": "Quick CoreDNS Status Checker Quickly identify if CoreDNS has the \"Unknown\" status issue after flannel regeneration",
      "sizeBytes": 2174,
      "tags": [
        "scripts",
        "kubernetes",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.250Z",
      "lines": 79
    },
    {
      "id": "37f70066",
      "path": "scripts/comprehensive_worker_setup.sh",
      "type": "script",
      "title": "comprehensive_worker_setup",
      "summary": "VMStation Comprehensive Worker Node Manual Setup This script provides a complete manual installation and configuration of all necessary components for worker nodes when the automated Ansible process e...",
      "sizeBytes": 14166,
      "tags": [
        "scripts",
        "kubernetes",
        "networking",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.250Z",
      "lines": 482
    },
    {
      "id": "3128233f",
      "path": "scripts/diagnose_remaining_pod_issues.sh",
      "type": "script",
      "title": "diagnose_remaining_pod_issues",
      "summary": "Diagnose Remaining Pod Issues This script analyzes specific pod failures mentioned in the problem statement",
      "sizeBytes": 6642,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.250Z",
      "lines": 196
    },
    {
      "id": "d3a169d7",
      "path": "scripts/enhanced_kubeadm_join.sh",
      "type": "script",
      "title": "enhanced_kubeadm_join",
      "summary": "VMStation Enhanced Kubeadm Join Process Comprehensive join process with prerequisite validation and robust error handling",
      "sizeBytes": 54420,
      "tags": [
        "scripts",
        "kubernetes",
        "docker",
        "networking",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.250Z",
      "lines": 1389
    },
    {
      "id": "1e7cdd43",
      "path": "scripts/fix_cluster_communication.sh",
      "type": "script",
      "title": "fix_cluster_communication",
      "summary": "Master Cluster Communication Fix Script This script addresses all the issues identified in the problem statement: 1. kubectl configuration on worker nodes 2. kube-proxy CrashLoopBackOff issues 3. ipta...",
      "sizeBytes": 29847,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin",
        "networking",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.250Z",
      "lines": 706
    },
    {
      "id": "4f42b816",
      "path": "scripts/fix_cluster_dns_configuration.sh",
      "type": "script",
      "title": "fix_cluster_dns_configuration",
      "summary": "VMStation Cluster DNS Configuration Fix Fixes the issue where kubectl uses router gateway (192.168.4.1) instead of CoreDNS This addresses the problem: \"dial tcp: lookup hort on 192.168.4.1:53: no such...",
      "sizeBytes": 13063,
      "tags": [
        "scripts",
        "kubernetes"
      ],
      "lastModified": "2025-09-16T22:01:51.250Z",
      "lines": 392
    },
    {
      "id": "1970801b",
      "path": "scripts/fix_cni_bridge_conflict.sh",
      "type": "script",
      "title": "fix_cni_bridge_conflict",
      "summary": "#!/usr/bin/env bash set -euo pipefail # fix_cni_bridge_conflict.sh",
      "sizeBytes": 22122,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.250Z",
      "lines": 601
    },
    {
      "id": "7071b363",
      "path": "scripts/fix_coredns_unknown_status.sh",
      "type": "script",
      "title": "fix_coredns_unknown_status",
      "summary": "Fix CoreDNS Unknown Status After Flannel Regeneration This script addresses the issue where CoreDNS pods show \"Unknown\" status with no IP after flannel pods have been regenerated by deploy-cluster.sh ...",
      "sizeBytes": 9118,
      "tags": [
        "scripts",
        "kubernetes",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.250Z",
      "lines": 265
    },
    {
      "id": "263c2cb9",
      "path": "scripts/fix_flannel_mixed_os.sh",
      "type": "script",
      "title": "fix_flannel_mixed_os",
      "summary": "Fix Flannel CNI Configuration for Mixed OS Environments Addresses specific issues in mixed Windows/Linux environments that can cause pod-to-pod communication failures",
      "sizeBytes": 12074,
      "tags": [
        "scripts",
        "kubernetes",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.250Z",
      "lines": 404
    },
    {
      "id": "a249d706",
      "path": "scripts/fix_homelab_node_issues.sh",
      "type": "script",
      "title": "fix_homelab_node_issues",
      "summary": "Fix Homelab Node Networking Issues Addresses: Flannel CrashLoopBackOff, kube-proxy crashes, and CNI problems",
      "sizeBytes": 17833,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.250Z",
      "lines": 427
    },
    {
      "id": "630db1ca",
      "path": "scripts/fix_iptables_compatibility.sh",
      "type": "script",
      "title": "fix_iptables_compatibility",
      "summary": "Fix iptables/nftables Compatibility Issues This script detects and fixes iptables/nftables compatibility problems that can cause kube-proxy to fail with \"incompatible\" errors",
      "sizeBytes": 11473,
      "tags": [
        "scripts",
        "kubernetes"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 343
    },
    {
      "id": "1ddb9838",
      "path": "scripts/fix_kubelet_systemd_config.sh",
      "type": "script",
      "title": "fix_kubelet_systemd_config",
      "summary": "VMStation Kubelet Systemd Configuration Fix Fixes the \"Assignment outside of section\" error for kubelet systemd drop-in files This script addresses the issue where systemd drop-in files are created wi...",
      "sizeBytes": 12762,
      "tags": [
        "scripts",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 366
    },
    {
      "id": "09f42a5d",
      "path": "scripts/fix_nodeport_external_access.sh",
      "type": "script",
      "title": "fix_nodeport_external_access",
      "summary": "VMStation NodePort External Access Fix Fixes external access to NodePort services (like Jellyfin on port 30096) from machines outside the cluster by ensuring proper firewall and iptables rules",
      "sizeBytes": 8788,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 243
    },
    {
      "id": "090d10b0",
      "path": "scripts/fix_remaining_pod_issues.sh",
      "type": "script",
      "title": "fix_remaining_pod_issues",
      "summary": "Fix Remaining VMStation Pod Issues Addresses specific problems after deploy-cluster.sh and fix_homelab_node_issues.sh Focuses on: jellyfin readiness issues and kube-proxy crashloop",
      "sizeBytes": 27016,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 699
    },
    {
      "id": "2efbd35b",
      "path": "scripts/fix_worker_kubectl_config.sh",
      "type": "script",
      "title": "fix_worker_kubectl_config",
      "summary": "Fix kubectl Configuration on Worker Nodes Addresses the \"connection refused\" errors when running kubectl on worker nodes This script configures kubectl to communicate with the cluster API server",
      "sizeBytes": 8110,
      "tags": [
        "scripts",
        "kubernetes"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 254
    },
    {
      "id": "52a04a1d",
      "path": "scripts/fix_worker_node_cni.sh",
      "type": "script",
      "title": "fix_worker_node_cni",
      "summary": "Fix Worker Node CNI Communication Issues Specifically addresses the issue where pods on the same worker node cannot communicate with each other, as evidenced by \"Destination Host Unreachable\" errors",
      "sizeBytes": 14535,
      "tags": [
        "scripts",
        "kubernetes",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 424
    },
    {
      "id": "b3dc3aae",
      "path": "scripts/gather_worker_diagnostics.sh",
      "type": "script",
      "title": "gather_worker_diagnostics",
      "summary": "VMStation Worker Diagnostics Gathering Script Collects comprehensive diagnostic information from worker nodes",
      "sizeBytes": 15938,
      "tags": [
        "scripts",
        "kubernetes",
        "docker",
        "networking",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 454
    },
    {
      "id": "21313a3e",
      "path": "scripts/manual_containerd_filesystem_fix.sh",
      "type": "script",
      "title": "manual_containerd_filesystem_fix",
      "summary": "VMStation Manual Containerd Filesystem Fix Aggressive containerd configuration and filesystem initialization fix This script addresses persistent containerd image filesystem initialization issues",
      "sizeBytes": 17239,
      "tags": [
        "scripts",
        "kubernetes",
        "networking",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 521
    },
    {
      "id": "1b255a42",
      "path": "scripts/quick_join_diagnostics.sh",
      "type": "script",
      "title": "quick_join_diagnostics",
      "summary": "VMStation Quick Join Diagnostics Rapid diagnostic script for kubeadm join issues",
      "sizeBytes": 9454,
      "tags": [
        "scripts",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 296
    },
    {
      "id": "7a273252",
      "path": "scripts/reset_cni_bridge.sh",
      "type": "script",
      "title": "reset_cni_bridge",
      "summary": "Reset CNI Bridge for VMStation Kubernetes Cluster This script resets the CNI bridge to align with proper kube-flannel, kube-proxy, and CoreDNS configuration Addresses the specific issue where cni0 has...",
      "sizeBytes": 7894,
      "tags": [
        "scripts",
        "kubernetes",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 246
    },
    {
      "id": "ad0a12c7",
      "path": "scripts/reset_cni_bridge_minimal.sh",
      "type": "script",
      "title": "reset_cni_bridge_minimal",
      "summary": "VMStation CNI Bridge Reset Script Fixes CNI bridge IP conflicts that prevent pod creation",
      "sizeBytes": 7276,
      "tags": [
        "scripts",
        "kubernetes",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 231
    },
    {
      "id": "09b572dc",
      "path": "scripts/run_network_diagnosis.sh",
      "type": "script",
      "title": "run_network_diagnosis",
      "summary": "VMStation Network Diagnosis Runner Quick wrapper script to run the automated network diagnosis playbook",
      "sizeBytes": 3734,
      "tags": [
        "scripts",
        "kubernetes",
        "ansible"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 127
    },
    {
      "id": "4f2a3f41",
      "path": "scripts/setup_static_ips_and_dns.sh",
      "type": "script",
      "title": "setup_static_ips_and_dns",
      "summary": "VMStation Static IP Assignment and DNS Subdomain Setup This script ensures critical Kubernetes components have static IPs and sets up homelab.com DNS",
      "sizeBytes": 13720,
      "tags": [
        "scripts",
        "kubernetes",
        "monitoring",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 402
    },
    {
      "id": "0f552648",
      "path": "scripts/smoke-test.sh",
      "type": "script",
      "title": "smoke-test",
      "summary": "VMStation Kubernetes Cluster Smoke Test Quick validation script for cluster health",
      "sizeBytes": 6713,
      "tags": [
        "scripts",
        "kubernetes",
        "ansible",
        "monitoring",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.251Z",
      "lines": 192
    },
    {
      "id": "b804c80b",
      "path": "scripts/test_enhanced_join_functionality.sh",
      "type": "script",
      "title": "test_enhanced_join_functionality",
      "summary": "VMStation Enhanced Worker Join Preflight Tests Validates the enhanced join process functionality",
      "sizeBytes": 7555,
      "tags": [
        "scripts",
        "ansible",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 184
    },
    {
      "id": "7daa8953",
      "path": "scripts/test_static_ips_and_dns_integration.sh",
      "type": "script",
      "title": "test_static_ips_and_dns_integration",
      "summary": "VMStation Static IP and DNS Setup Integration Test Tests the integration without requiring a running cluster",
      "sizeBytes": 3534,
      "tags": [
        "scripts",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 111
    },
    {
      "id": "7c606adf",
      "path": "scripts/validate_cluster_communication.sh",
      "type": "script",
      "title": "validate_cluster_communication",
      "summary": "Validate Cluster Communication and NodePort Services This script validates that all aspects of cluster communication work correctly including kubectl access, NodePort services, and inter-node connecti...",
      "sizeBytes": 11489,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 340
    },
    {
      "id": "1cd4e1f5",
      "path": "scripts/validate_deployment_fixes.sh",
      "type": "script",
      "title": "validate_deployment_fixes",
      "summary": "VMStation Post-Deployment Validation Script Run this after deploying the cluster to verify all fixes are working",
      "sizeBytes": 5737,
      "tags": [
        "scripts",
        "kubernetes",
        "monitoring",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 158
    },
    {
      "id": "b428256d",
      "path": "scripts/validate_join_prerequisites.sh",
      "type": "script",
      "title": "validate_join_prerequisites",
      "summary": "VMStation Kubernetes Join Prerequisites Validator Comprehensive validation before attempting kubeadm join to prevent standalone mode",
      "sizeBytes": 13524,
      "tags": [
        "scripts",
        "kubernetes",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 399
    },
    {
      "id": "ac18a0b8",
      "path": "scripts/validate_mixed_os_flannel.sh",
      "type": "script",
      "title": "validate_mixed_os_flannel",
      "summary": "Validation script for Flannel download fix in mixed OS environments This script validates that both RHEL10 and Debian nodes can download Flannel CNI plugin",
      "sizeBytes": 7786,
      "tags": [
        "scripts",
        "ansible",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 192
    },
    {
      "id": "ca8ac5f9",
      "path": "scripts/validate_network_prerequisites.sh",
      "type": "script",
      "title": "validate_network_prerequisites",
      "summary": "VMStation Network Prerequisites Validation Validates network configuration before deploying Jellyfin and other pods Prevents common CNI bridge conflicts and mixed OS compatibility issues",
      "sizeBytes": 12910,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 358
    },
    {
      "id": "27edfbfd",
      "path": "scripts/validate_nodeport_external_access.sh",
      "type": "script",
      "title": "validate_nodeport_external_access",
      "summary": "VMStation NodePort External Access Validation Tests external access to NodePort services to validate the fix",
      "sizeBytes": 9068,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 241
    },
    {
      "id": "52dfbbac",
      "path": "scripts/validate_pod_connectivity.sh",
      "type": "script",
      "title": "validate_pod_connectivity",
      "summary": "Validate Pod-to-Pod CNI Communication Tests the exact scenario from the problem statement: - Debug pod on storagenodet3500 trying to reach Jellyfin pod - Validates both internal and external connectiv...",
      "sizeBytes": 10599,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 317
    },
    {
      "id": "62721f19",
      "path": "scripts/validate_pod_health.sh",
      "type": "script",
      "title": "validate_pod_health",
      "summary": "Validate VMStation Pod Fixes Quick validation script to check if the pod issues have been resolved",
      "sizeBytes": 5495,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 150
    },
    {
      "id": "6ea2bc0f",
      "path": "scripts/validate_post_wipe_functionality.sh",
      "type": "script",
      "title": "validate_post_wipe_functionality",
      "summary": "VMStation Post-Wipe Worker Join Validation Test This script validates that the enhanced post-wipe worker join functionality is working correctly Remove set -e to prevent premature exit on arithmetic o...",
      "sizeBytes": 6716,
      "tags": [
        "scripts",
        "ansible",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 199
    },
    {
      "id": "5e0bdb07",
      "path": "scripts/validate_static_ips_and_dns.sh",
      "type": "script",
      "title": "validate_static_ips_and_dns",
      "summary": "VMStation Static IP and DNS Validation Script Tests the static IP assignments and DNS subdomain functionality",
      "sizeBytes": 11901,
      "tags": [
        "scripts",
        "kubernetes",
        "monitoring",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 334
    },
    {
      "id": "59ecfc80",
      "path": "scripts/validate_systemd_dropins.sh",
      "type": "script",
      "title": "validate_systemd_dropins",
      "summary": "VMStation Systemd Drop-in Validator Validates and ensures proper formatting of systemd drop-in files This script prevents \"Assignment outside of section\" errors by ensuring all systemd configuration d...",
      "sizeBytes": 11555,
      "tags": [
        "scripts",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 359
    },
    {
      "id": "563e9b03",
      "path": "scripts/vmstation_status.sh",
      "type": "script",
      "title": "vmstation_status",
      "summary": "VMStation Deployment Status and Troubleshooting Summary Provides a comprehensive overview of cluster status and recommended actions",
      "sizeBytes": 5293,
      "tags": [
        "scripts",
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 174
    },
    {
      "id": "6564808c",
      "path": "scripts/worker_node_join_remediation.sh",
      "type": "script",
      "title": "worker_node_join_remediation",
      "summary": "VMStation Worker Node Join Remediation General purpose remediation script for worker node join issues",
      "sizeBytes": 8849,
      "tags": [
        "scripts",
        "kubernetes",
        "networking",
        "cluster-setup"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 292
    },
    {
      "id": "9553fda2",
      "path": "test_cni_bridge_fix.sh",
      "type": "script",
      "title": "test_cni_bridge_fix",
      "summary": "VMStation CNI Bridge Fix Validation Test This script validates that the CNI bridge fix changes are working correctly Run this on the control plane after applying the fixes",
      "sizeBytes": 8706,
      "tags": [
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.252Z",
      "lines": 247
    },
    {
      "id": "11010dc1",
      "path": "validate-cluster.sh",
      "type": "script",
      "title": "validate-cluster",
      "summary": "VMStation Cluster Validation Script Validates that the CNI bridge fix worked and cluster is healthy",
      "sizeBytes": 9032,
      "tags": [
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.253Z",
      "lines": 283
    },
    {
      "id": "d5c6d418",
      "path": "validate_cni_fix.sh",
      "type": "script",
      "title": "validate_cni_fix",
      "summary": "Validation script for the enhanced CNI bridge conflict fix This script helps test the fix in a live environment",
      "sizeBytes": 5212,
      "tags": [
        "kubernetes",
        "jellyfin",
        "networking"
      ],
      "lastModified": "2025-09-16T22:01:51.253Z",
      "lines": 142
    },
    {
      "id": "f07eb5d6",
      "path": "validate_network_reset.sh",
      "type": "script",
      "title": "validate_network_reset",
      "summary": "VMStation Network Reset Validation Script This script validates the network reset functionality implementation",
      "sizeBytes": 4992,
      "tags": [
        "kubernetes",
        "ansible"
      ],
      "lastModified": "2025-09-16T22:01:51.253Z",
      "lines": 176
    }
  ],
  "edges": [
    {
      "from": "4ac32a78",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "- `deploy-cluster.sh` with new `net-reset` command for surgical network fixes"
    },
    {
      "from": "61c1941a",
      "to": "9088164c",
      "relation": "calls",
      "context": "sudo ./fix_jellyfin_immediate.sh"
    },
    {
      "from": "61c1941a",
      "to": "9088164c",
      "relation": "mentions",
      "context": "sudo ./fix_jellyfin_immediate.sh"
    },
    {
      "from": "61c1941a",
      "to": "311b37fb",
      "relation": "calls",
      "context": "./fix-cluster.sh"
    },
    {
      "from": "61c1941a",
      "to": "311b37fb",
      "relation": "mentions",
      "context": "./fix-cluster.sh"
    },
    {
      "from": "61c1941a",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "### Enhanced Cluster Reset (deploy-cluster.sh)"
    },
    {
      "from": "61c1941a",
      "to": "9088164c",
      "relation": "mentions",
      "context": "Created `fix_jellyfin_immediate.sh` for targeted fixes without full reset:"
    },
    {
      "from": "61c1941a",
      "to": "9088164c",
      "relation": "calls",
      "context": "sudo ./fix_jellyfin_immediate.sh"
    },
    {
      "from": "61c1941a",
      "to": "9088164c",
      "relation": "mentions",
      "context": "sudo ./fix_jellyfin_immediate.sh"
    },
    {
      "from": "61c1941a",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh reset"
    },
    {
      "from": "61c1941a",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh reset"
    },
    {
      "from": "61c1941a",
      "to": "311b37fb",
      "relation": "calls",
      "context": "./fix-cluster.sh"
    },
    {
      "from": "61c1941a",
      "to": "311b37fb",
      "relation": "mentions",
      "context": "./fix-cluster.sh"
    },
    {
      "from": "61c1941a",
      "to": "1970801b",
      "relation": "calls",
      "context": "sudo scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "61c1941a",
      "to": "1970801b",
      "relation": "mentions",
      "context": "sudo scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "f0859b7a",
      "to": "09b572dc",
      "relation": "calls",
      "context": "./scripts/run_network_diagnosis.sh"
    },
    {
      "from": "f0859b7a",
      "to": "09b572dc",
      "relation": "mentions",
      "context": "./scripts/run_network_diagnosis.sh"
    },
    {
      "from": "f0859b7a",
      "to": "09b572dc",
      "relation": "calls",
      "context": "./scripts/run_network_diagnosis.sh --verbose"
    },
    {
      "from": "f0859b7a",
      "to": "09b572dc",
      "relation": "mentions",
      "context": "./scripts/run_network_diagnosis.sh --verbose"
    },
    {
      "from": "f0859b7a",
      "to": "09b572dc",
      "relation": "calls",
      "context": "./scripts/run_network_diagnosis.sh --check"
    },
    {
      "from": "f0859b7a",
      "to": "09b572dc",
      "relation": "mentions",
      "context": "./scripts/run_network_diagnosis.sh --check"
    },
    {
      "from": "f0859b7a",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml"
    },
    {
      "from": "f0859b7a",
      "to": "b9618ca5",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml"
    },
    {
      "from": "7036e452",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh net-reset --dry-run"
    },
    {
      "from": "7036e452",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh net-reset --dry-run"
    },
    {
      "from": "7036e452",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh --dry-run net-reset"
    },
    {
      "from": "7036e452",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh --dry-run net-reset"
    },
    {
      "from": "7036e452",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh net-reset --confirm"
    },
    {
      "from": "7036e452",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh net-reset --confirm"
    },
    {
      "from": "7036e452",
      "to": "2883003d",
      "relation": "mentions",
      "context": "1. `kube-proxy-configmap.yaml` - iptables mode, conservative settings"
    },
    {
      "from": "7036e452",
      "to": "80025501",
      "relation": "mentions",
      "context": "2. `kube-proxy-daemonset.yaml` - with resource limits and tolerations"
    },
    {
      "from": "7036e452",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "3. `coredns-configmap.yaml` - forward to 8.8.8.8, 1.1.1.1"
    },
    {
      "from": "7036e452",
      "to": "ad0e321a",
      "relation": "mentions",
      "context": "4. `coredns-service.yaml` - ClusterIP 10.96.0.10"
    },
    {
      "from": "7036e452",
      "to": "81cd44f2",
      "relation": "mentions",
      "context": "5. `coredns-deployment.yaml` - 2 replicas, system-cluster-critical priority"
    },
    {
      "from": "7036e452",
      "to": "2883003d",
      "relation": "references",
      "context": "kubectl apply -f kube-proxy-configmap.yaml"
    },
    {
      "from": "7036e452",
      "to": "2883003d",
      "relation": "mentions",
      "context": "kubectl apply -f kube-proxy-configmap.yaml"
    },
    {
      "from": "7036e452",
      "to": "80025501",
      "relation": "references",
      "context": "kubectl apply -f kube-proxy-daemonset.yaml"
    },
    {
      "from": "7036e452",
      "to": "80025501",
      "relation": "mentions",
      "context": "kubectl apply -f kube-proxy-daemonset.yaml"
    },
    {
      "from": "7036e452",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "kubectl apply -f coredns-configmap.yaml"
    },
    {
      "from": "7036e452",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "kubectl apply -f coredns-configmap.yaml"
    },
    {
      "from": "7036e452",
      "to": "ad0e321a",
      "relation": "references",
      "context": "kubectl apply -f coredns-service.yaml"
    },
    {
      "from": "7036e452",
      "to": "ad0e321a",
      "relation": "mentions",
      "context": "kubectl apply -f coredns-service.yaml"
    },
    {
      "from": "7036e452",
      "to": "81cd44f2",
      "relation": "references",
      "context": "kubectl apply -f coredns-deployment.yaml"
    },
    {
      "from": "7036e452",
      "to": "81cd44f2",
      "relation": "mentions",
      "context": "kubectl apply -f coredns-deployment.yaml"
    },
    {
      "from": "7036e452",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh reset --force"
    },
    {
      "from": "7036e452",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh reset --force"
    },
    {
      "from": "7036e452",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh net-reset --confirm"
    },
    {
      "from": "7036e452",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh net-reset --confirm"
    },
    {
      "from": "9c2d9675",
      "to": "9088164c",
      "relation": "calls",
      "context": "sudo ./fix_jellyfin_immediate.sh"
    },
    {
      "from": "9c2d9675",
      "to": "9088164c",
      "relation": "mentions",
      "context": "sudo ./fix_jellyfin_immediate.sh"
    },
    {
      "from": "9c2d9675",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh reset"
    },
    {
      "from": "9c2d9675",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh reset"
    },
    {
      "from": "9c2d9675",
      "to": "311b37fb",
      "relation": "calls",
      "context": "./fix-cluster.sh"
    },
    {
      "from": "9c2d9675",
      "to": "311b37fb",
      "relation": "mentions",
      "context": "./fix-cluster.sh"
    },
    {
      "from": "9c2d9675",
      "to": "9553fda2",
      "relation": "calls",
      "context": "./test_cni_bridge_fix.sh"
    },
    {
      "from": "9c2d9675",
      "to": "9553fda2",
      "relation": "mentions",
      "context": "./test_cni_bridge_fix.sh"
    },
    {
      "from": "9c2d9675",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "- `deploy-cluster.sh` - Enhanced reset with CNI cleanup"
    },
    {
      "from": "9c2d9675",
      "to": "91932608",
      "relation": "references",
      "context": "- `ansible/playbooks/verify-cluster.yml` - Corrected URL validation"
    },
    {
      "from": "9c2d9675",
      "to": "91932608",
      "relation": "mentions",
      "context": "- `ansible/playbooks/verify-cluster.yml` - Corrected URL validation"
    },
    {
      "from": "9c2d9675",
      "to": "9088164c",
      "relation": "mentions",
      "context": "- `fix_jellyfin_immediate.sh` - New targeted fix script"
    },
    {
      "from": "9c2d9675",
      "to": "9553fda2",
      "relation": "mentions",
      "context": "- `test_cni_bridge_fix.sh` - Validation test script"
    },
    {
      "from": "9c2d9675",
      "to": "1970801b",
      "relation": "calls",
      "context": "4. Run: `sudo scripts/fix_cni_bridge_conflict.sh`"
    },
    {
      "from": "9c2d9675",
      "to": "1970801b",
      "relation": "mentions",
      "context": "4. Run: `sudo scripts/fix_cni_bridge_conflict.sh`"
    },
    {
      "from": "4d7840b3",
      "to": "4f42b816",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cluster_dns_configuration.sh"
    },
    {
      "from": "4d7840b3",
      "to": "4f42b816",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_cluster_dns_configuration.sh"
    },
    {
      "from": "4d7840b3",
      "to": "ebb30773",
      "relation": "calls",
      "context": "sudo ./fix_jellyfin_cni_bridge_conflict.sh"
    },
    {
      "from": "4d7840b3",
      "to": "ebb30773",
      "relation": "mentions",
      "context": "sudo ./fix_jellyfin_cni_bridge_conflict.sh"
    },
    {
      "from": "4d7840b3",
      "to": "f157568d",
      "relation": "calls",
      "context": "sudo ./quick_fix_cni_communication.sh"
    },
    {
      "from": "4d7840b3",
      "to": "f157568d",
      "relation": "mentions",
      "context": "sudo ./quick_fix_cni_communication.sh"
    },
    {
      "from": "4d7840b3",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "4d7840b3",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "4d7840b3",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "sudo ./scripts/fix_worker_node_cni.sh --node storagenodet3500"
    },
    {
      "from": "4d7840b3",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_worker_node_cni.sh --node storagenodet3500"
    },
    {
      "from": "4d7840b3",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "./scripts/fix_flannel_mixed_os.sh"
    },
    {
      "from": "4d7840b3",
      "to": "263c2cb9",
      "relation": "mentions",
      "context": "./scripts/fix_flannel_mixed_os.sh"
    },
    {
      "from": "4d7840b3",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "./scripts/validate_pod_connectivity.sh"
    },
    {
      "from": "4d7840b3",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "./scripts/validate_pod_connectivity.sh"
    },
    {
      "from": "4d7840b3",
      "to": "797dca0d",
      "relation": "references",
      "context": "For detailed technical information, see: [`docs/cni-pod-communication-fix.md`](docs/cni-pod-communication-fix.md)"
    },
    {
      "from": "4d7840b3",
      "to": "797dca0d",
      "relation": "mentions",
      "context": "For detailed technical information, see: [`docs/cni-pod-communication-fix.md`](docs/cni-pod-communication-fix.md)"
    },
    {
      "from": "4d7840b3",
      "to": "797dca0d",
      "relation": "mentions",
      "context": "For detailed technical information, see: [`docs/cni-pod-communication-fix.md`](docs/cni-pod-communication-fix.md)"
    },
    {
      "from": "4d7840b3",
      "to": "ebb30773",
      "relation": "mentions",
      "context": "- **CNI bridge IP conflicts on worker nodes** (specific fix: fix_jellyfin_cni_bridge_conflict.sh)"
    },
    {
      "from": "4d7840b3",
      "to": "ebb30773",
      "relation": "mentions",
      "context": "The enhanced `fix_jellyfin_cni_bridge_conflict.sh` now addresses the exact scenario from the problem statement:"
    },
    {
      "from": "6a7d5d4b",
      "to": "607e6e59",
      "relation": "references",
      "context": "-  **Runtime configuration ready**: `ansible/group_vars/all.yml` configured with production settings"
    },
    {
      "from": "6a7d5d4b",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "-  **Runtime configuration ready**: `ansible/group_vars/all.yml` configured with production settings"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "2e9ce2c1",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "2e9ce2c1",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/verify-cluster.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "91932608",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/verify-cluster.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/verify-cluster.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "91932608",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/verify-cluster.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml --check --diff"
    },
    {
      "from": "6a7d5d4b",
      "to": "2e9ce2c1",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml --check --diff"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml --check --diff"
    },
    {
      "from": "6a7d5d4b",
      "to": "2e9ce2c1",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml --check --diff"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "2e9ce2c1",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "2e9ce2c1",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "The cluster bootstrap automatically includes the existing `setup-cluster.yaml` which handles:"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "Edit `ansible/inventory/hosts.yml` to customize:"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "Edit `ansible/inventory/hosts.yml` to customize:"
    },
    {
      "from": "6a7d5d4b",
      "to": "607e6e59",
      "relation": "references",
      "context": "Modify `ansible/group_vars/all.yml.template`:"
    },
    {
      "from": "6a7d5d4b",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "Modify `ansible/group_vars/all.yml.template`:"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "6a7d5d4b",
      "to": "3e468245",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "6a7d5d4b",
      "to": "3e468245",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "ansible storage_nodes -i ansible/inventory/hosts.yml -m shell -a \"ls -la /srv/media /var/lib/jellyfin\""
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "ansible storage_nodes -i ansible/inventory/hosts.yml -m shell -a \"ls -la /srv/media /var/lib/jellyfin\""
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/subsites/00-spindown.yaml"
    },
    {
      "from": "6a7d5d4b",
      "to": "9fa74fe4",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/subsites/00-spindown.yaml"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/subsites/00-spindown.yaml"
    },
    {
      "from": "6a7d5d4b",
      "to": "9fa74fe4",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/subsites/00-spindown.yaml"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "2e9ce2c1",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "2e9ce2c1",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory/hosts.yml ansible/playbooks/cluster-bootstrap.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "607e6e59",
      "relation": "references",
      "context": "# In group_vars/all.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# In group_vars/all.yml"
    },
    {
      "from": "6a7d5d4b",
      "to": "53177eca",
      "relation": "mentions",
      "context": "- `docs/RHEL10_TROUBLESHOOTING.md` - RHEL 10 specific issues"
    },
    {
      "from": "6a7d5d4b",
      "to": "8c19061a",
      "relation": "mentions",
      "context": "- `docs/AGGRESSIVE_NODE_RESET.md` - Advanced reset procedures"
    },
    {
      "from": "6a7d5d4b",
      "to": "04c6e90f",
      "relation": "mentions",
      "context": "- `scripts/README.md` - Helper script documentation"
    },
    {
      "from": "6a7d5d4b",
      "to": "91932608",
      "relation": "references",
      "context": "2. Run verification: `ansible-playbook verify-cluster.yml`"
    },
    {
      "from": "6a7d5d4b",
      "to": "91932608",
      "relation": "mentions",
      "context": "2. Run verification: `ansible-playbook verify-cluster.yml`"
    },
    {
      "from": "04c6e90f",
      "to": "0665ba5c",
      "relation": "references",
      "context": "For detailed information, see: [Enhanced Join Process Documentation](docs/ENHANCED_JOIN_PROCESS.md)"
    },
    {
      "from": "04c6e90f",
      "to": "0665ba5c",
      "relation": "mentions",
      "context": "For detailed information, see: [Enhanced Join Process Documentation](docs/ENHANCED_JOIN_PROCESS.md)"
    },
    {
      "from": "04c6e90f",
      "to": "607e6e59",
      "relation": "references",
      "context": "nano ansible/group_vars/all.yml"
    },
    {
      "from": "04c6e90f",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "nano ansible/group_vars/all.yml"
    },
    {
      "from": "04c6e90f",
      "to": "dd358be7",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/simple-deploy.yaml --check"
    },
    {
      "from": "04c6e90f",
      "to": "dd358be7",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/simple-deploy.yaml --check"
    },
    {
      "from": "04c6e90f",
      "to": "e7010005",
      "relation": "references",
      "context": "- See [docs/README.md](./docs/README.md) for the new documentation index."
    },
    {
      "from": "04c6e90f",
      "to": "607e6e59",
      "relation": "references",
      "context": "cp ansible/group_vars/all.yml.template ansible/group_vars/all.yml"
    },
    {
      "from": "04c6e90f",
      "to": "607e6e59",
      "relation": "references",
      "context": "cp ansible/group_vars/all.yml.template ansible/group_vars/all.yml"
    },
    {
      "from": "04c6e90f",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "cp ansible/group_vars/all.yml.template ansible/group_vars/all.yml"
    },
    {
      "from": "04c6e90f",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "cp ansible/group_vars/all.yml.template ansible/group_vars/all.yml"
    },
    {
      "from": "04c6e90f",
      "to": "607e6e59",
      "relation": "references",
      "context": "# Edit all.yml with your specific settings"
    },
    {
      "from": "04c6e90f",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# Edit all.yml with your specific settings"
    },
    {
      "from": "04c6e90f",
      "to": "0ecf3a99",
      "relation": "calls",
      "context": "./generate_join_command.sh"
    },
    {
      "from": "04c6e90f",
      "to": "0ecf3a99",
      "relation": "mentions",
      "context": "./generate_join_command.sh"
    },
    {
      "from": "04c6e90f",
      "to": "62721f19",
      "relation": "calls",
      "context": "- **Pod health validation**: `./scripts/validate_pod_health.sh` - **NEW!** Quick validation of all pod health status"
    },
    {
      "from": "04c6e90f",
      "to": "62721f19",
      "relation": "mentions",
      "context": "- **Pod health validation**: `./scripts/validate_pod_health.sh` - **NEW!** Quick validation of all pod health status"
    },
    {
      "from": "04c6e90f",
      "to": "0d0c4a60",
      "relation": "calls",
      "context": "- **Jellyfin readiness fix**: `./fix_jellyfin_readiness.sh` - **NEW!** Fixes jellyfin probe configuration and readiness issues"
    },
    {
      "from": "04c6e90f",
      "to": "0d0c4a60",
      "relation": "mentions",
      "context": "- **Jellyfin readiness fix**: `./fix_jellyfin_readiness.sh` - **NEW!** Fixes jellyfin probe configuration and readiness issues"
    },
    {
      "from": "04c6e90f",
      "to": "e3453d63",
      "relation": "calls",
      "context": "- **Jellyfin network fix**: `./fix_jellyfin_network_issue.sh` - **NEW!** Fixes \"no route to host\" network connectivity issues"
    },
    {
      "from": "04c6e90f",
      "to": "e3453d63",
      "relation": "mentions",
      "context": "- **Jellyfin network fix**: `./fix_jellyfin_network_issue.sh` - **NEW!** Fixes \"no route to host\" network connectivity issues"
    },
    {
      "from": "04c6e90f",
      "to": "090d10b0",
      "relation": "calls",
      "context": "- **Remaining pod fixes**: `./scripts/fix_remaining_pod_issues.sh` - **NEW!** Fixes jellyfin readiness and kube-proxy crashloop issues"
    },
    {
      "from": "04c6e90f",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "- **Remaining pod fixes**: `./scripts/fix_remaining_pod_issues.sh` - **NEW!** Fixes jellyfin readiness and kube-proxy crashloop issues"
    },
    {
      "from": "04c6e90f",
      "to": "3128233f",
      "relation": "calls",
      "context": "- **Pod diagnostics**: `./scripts/diagnose_remaining_pod_issues.sh` - **NEW!** Detailed analysis of pod failures"
    },
    {
      "from": "04c6e90f",
      "to": "3128233f",
      "relation": "mentions",
      "context": "- **Pod diagnostics**: `./scripts/diagnose_remaining_pod_issues.sh` - **NEW!** Detailed analysis of pod failures"
    },
    {
      "from": "04c6e90f",
      "to": "1970801b",
      "relation": "calls",
      "context": "- **CNI bridge conflicts**: `./scripts/fix_cni_bridge_conflict.sh` - **NEW!** Fixes CNI bridge IP conflicts causing ContainerCreating errors"
    },
    {
      "from": "04c6e90f",
      "to": "1970801b",
      "relation": "mentions",
      "context": "- **CNI bridge conflicts**: `./scripts/fix_cni_bridge_conflict.sh` - **NEW!** Fixes CNI bridge IP conflicts causing ContainerCreating errors"
    },
    {
      "from": "04c6e90f",
      "to": "7a273252",
      "relation": "calls",
      "context": "- **CNI bridge reset**: `sudo ./scripts/reset_cni_bridge.sh` - **NEW!** Quick reset for \"cni0 already has IP address different from 10.244.x.x\" errors"
    },
    {
      "from": "04c6e90f",
      "to": "7a273252",
      "relation": "mentions",
      "context": "- **CNI bridge reset**: `sudo ./scripts/reset_cni_bridge.sh` - **NEW!** Quick reset for \"cni0 already has IP address different from 10.244.x.x\" errors"
    },
    {
      "from": "04c6e90f",
      "to": "62721f19",
      "relation": "calls",
      "context": "./scripts/validate_pod_health.sh"
    },
    {
      "from": "04c6e90f",
      "to": "62721f19",
      "relation": "mentions",
      "context": "./scripts/validate_pod_health.sh"
    },
    {
      "from": "04c6e90f",
      "to": "3128233f",
      "relation": "calls",
      "context": "./scripts/diagnose_remaining_pod_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "3128233f",
      "relation": "mentions",
      "context": "./scripts/diagnose_remaining_pod_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "090d10b0",
      "relation": "calls",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "53177eca",
      "relation": "mentions",
      "context": "# See docs/RHEL10_TROUBLESHOOTING.md for detailed guide"
    },
    {
      "from": "04c6e90f",
      "to": "3e468245",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "04c6e90f",
      "to": "3e468245",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "04c6e90f",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "a249d706",
      "relation": "mentions",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "36bd22c9",
      "relation": "calls",
      "context": "./scripts/check_coredns_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "36bd22c9",
      "relation": "mentions",
      "context": "./scripts/check_coredns_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "7071b363",
      "relation": "calls",
      "context": "./scripts/fix_coredns_unknown_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "7071b363",
      "relation": "mentions",
      "context": "./scripts/fix_coredns_unknown_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "./scripts/validate_network_prerequisites.sh"
    },
    {
      "from": "04c6e90f",
      "to": "ca8ac5f9",
      "relation": "mentions",
      "context": "./scripts/validate_network_prerequisites.sh"
    },
    {
      "from": "04c6e90f",
      "to": "7a273252",
      "relation": "calls",
      "context": "sudo ./scripts/reset_cni_bridge.sh"
    },
    {
      "from": "04c6e90f",
      "to": "7a273252",
      "relation": "mentions",
      "context": "sudo ./scripts/reset_cni_bridge.sh"
    },
    {
      "from": "04c6e90f",
      "to": "1970801b",
      "relation": "calls",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "04c6e90f",
      "to": "1970801b",
      "relation": "mentions",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "04c6e90f",
      "to": "f0840b04",
      "relation": "references",
      "context": "- **Quick CNI reset**: [docs/CNI_BRIDGE_RESET.md](./docs/CNI_BRIDGE_RESET.md) - Fast targeted fix"
    },
    {
      "from": "04c6e90f",
      "to": "f0840b04",
      "relation": "mentions",
      "context": "- **Quick CNI reset**: [docs/CNI_BRIDGE_RESET.md](./docs/CNI_BRIDGE_RESET.md) - Fast targeted fix"
    },
    {
      "from": "04c6e90f",
      "to": "f0840b04",
      "relation": "mentions",
      "context": "- **Quick CNI reset**: [docs/CNI_BRIDGE_RESET.md](./docs/CNI_BRIDGE_RESET.md) - Fast targeted fix"
    },
    {
      "from": "04c6e90f",
      "to": "8facf1bd",
      "relation": "references",
      "context": "- CoreDNS scheduling fixes: [docs/COREDNS_MASTERNODE_ENFORCEMENT.md](./docs/COREDNS_MASTERNODE_ENFORCEMENT.md)"
    },
    {
      "from": "04c6e90f",
      "to": "8facf1bd",
      "relation": "mentions",
      "context": "- CoreDNS scheduling fixes: [docs/COREDNS_MASTERNODE_ENFORCEMENT.md](./docs/COREDNS_MASTERNODE_ENFORCEMENT.md)"
    },
    {
      "from": "04c6e90f",
      "to": "8facf1bd",
      "relation": "mentions",
      "context": "- CoreDNS scheduling fixes: [docs/COREDNS_MASTERNODE_ENFORCEMENT.md](./docs/COREDNS_MASTERNODE_ENFORCEMENT.md)"
    },
    {
      "from": "04c6e90f",
      "to": "fcb85303",
      "relation": "references",
      "context": "- CoreDNS unknown status: [docs/COREDNS_UNKNOWN_STATUS_FIX.md](./docs/COREDNS_UNKNOWN_STATUS_FIX.md)"
    },
    {
      "from": "04c6e90f",
      "to": "fcb85303",
      "relation": "mentions",
      "context": "- CoreDNS unknown status: [docs/COREDNS_UNKNOWN_STATUS_FIX.md](./docs/COREDNS_UNKNOWN_STATUS_FIX.md)"
    },
    {
      "from": "04c6e90f",
      "to": "fcb85303",
      "relation": "mentions",
      "context": "- CoreDNS unknown status: [docs/COREDNS_UNKNOWN_STATUS_FIX.md](./docs/COREDNS_UNKNOWN_STATUS_FIX.md)"
    },
    {
      "from": "04c6e90f",
      "to": "0d0c4a60",
      "relation": "calls",
      "context": "./fix_jellyfin_readiness.sh"
    },
    {
      "from": "04c6e90f",
      "to": "0d0c4a60",
      "relation": "mentions",
      "context": "./fix_jellyfin_readiness.sh"
    },
    {
      "from": "04c6e90f",
      "to": "563e9b03",
      "relation": "calls",
      "context": "./scripts/vmstation_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "563e9b03",
      "relation": "mentions",
      "context": "./scripts/vmstation_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "a249d706",
      "relation": "mentions",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "7071b363",
      "relation": "calls",
      "context": "./scripts/fix_coredns_unknown_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "7071b363",
      "relation": "mentions",
      "context": "./scripts/fix_coredns_unknown_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "36bd22c9",
      "relation": "calls",
      "context": "./scripts/check_coredns_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "36bd22c9",
      "relation": "mentions",
      "context": "./scripts/check_coredns_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "a249d706",
      "relation": "mentions",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "090d10b0",
      "relation": "calls",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "e3453d63",
      "relation": "calls",
      "context": "./fix_jellyfin_network_issue.sh"
    },
    {
      "from": "04c6e90f",
      "to": "e3453d63",
      "relation": "mentions",
      "context": "./fix_jellyfin_network_issue.sh"
    },
    {
      "from": "04c6e90f",
      "to": "62721f19",
      "relation": "calls",
      "context": "./scripts/validate_pod_health.sh"
    },
    {
      "from": "04c6e90f",
      "to": "62721f19",
      "relation": "mentions",
      "context": "./scripts/validate_pod_health.sh"
    },
    {
      "from": "04c6e90f",
      "to": "e3453d63",
      "relation": "calls",
      "context": "./fix_jellyfin_network_issue.sh"
    },
    {
      "from": "04c6e90f",
      "to": "e3453d63",
      "relation": "mentions",
      "context": "./fix_jellyfin_network_issue.sh"
    },
    {
      "from": "04c6e90f",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "a249d706",
      "relation": "mentions",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "04c6e90f",
      "to": "563e9b03",
      "relation": "calls",
      "context": "./scripts/vmstation_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "563e9b03",
      "relation": "mentions",
      "context": "./scripts/vmstation_status.sh"
    },
    {
      "from": "04c6e90f",
      "to": "e8e06adf",
      "relation": "references",
      "context": "- [docs/HOMELAB_NODE_FIXES.md](./docs/HOMELAB_NODE_FIXES.md) - General cluster issues"
    },
    {
      "from": "04c6e90f",
      "to": "e8e06adf",
      "relation": "mentions",
      "context": "- [docs/HOMELAB_NODE_FIXES.md](./docs/HOMELAB_NODE_FIXES.md) - General cluster issues"
    },
    {
      "from": "04c6e90f",
      "to": "e8e06adf",
      "relation": "mentions",
      "context": "- [docs/HOMELAB_NODE_FIXES.md](./docs/HOMELAB_NODE_FIXES.md) - General cluster issues"
    },
    {
      "from": "04c6e90f",
      "to": "0c29bbcc",
      "relation": "references",
      "context": "- [docs/jellyfin-cni-bridge-fix.md](./docs/jellyfin-cni-bridge-fix.md) - Jellyfin CNI bridge conflicts"
    },
    {
      "from": "04c6e90f",
      "to": "0c29bbcc",
      "relation": "mentions",
      "context": "- [docs/jellyfin-cni-bridge-fix.md](./docs/jellyfin-cni-bridge-fix.md) - Jellyfin CNI bridge conflicts"
    },
    {
      "from": "04c6e90f",
      "to": "0c29bbcc",
      "relation": "mentions",
      "context": "- [docs/jellyfin-cni-bridge-fix.md](./docs/jellyfin-cni-bridge-fix.md) - Jellyfin CNI bridge conflicts"
    },
    {
      "from": "04c6e90f",
      "to": "4d7840b3",
      "relation": "references",
      "context": "- [README-CNI-FIX.md](./README-CNI-FIX.md) - CNI networking issues"
    },
    {
      "from": "04c6e90f",
      "to": "4d7840b3",
      "relation": "mentions",
      "context": "- [README-CNI-FIX.md](./README-CNI-FIX.md) - CNI networking issues"
    },
    {
      "from": "04c6e90f",
      "to": "4d7840b3",
      "relation": "mentions",
      "context": "- [README-CNI-FIX.md](./README-CNI-FIX.md) - CNI networking issues"
    },
    {
      "from": "04c6e90f",
      "to": "72c7835a",
      "relation": "calls",
      "context": "- `./deploy-cluster.sh` - Main deployment script (967 lines)"
    },
    {
      "from": "04c6e90f",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "- `./deploy-cluster.sh` - Main deployment script (967 lines)"
    },
    {
      "from": "04c6e90f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "- `scripts/enhanced_kubeadm_join.sh` - Enhanced join process (1388 lines)"
    },
    {
      "from": "04c6e90f",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "- `scripts/enhanced_kubeadm_join.sh` - Enhanced join process (1388 lines)"
    },
    {
      "from": "04c6e90f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "- `ansible/plays/setup-cluster.yaml` - Essential cluster setup"
    },
    {
      "from": "04c6e90f",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "- `ansible/plays/setup-cluster.yaml` - Essential cluster setup"
    },
    {
      "from": "ff198a1c",
      "to": "9553fda2",
      "relation": "calls",
      "context": "- `scripts/test_cni_bridge_fix.sh` - Test script, not core infrastructure"
    },
    {
      "from": "ff198a1c",
      "to": "9553fda2",
      "relation": "mentions",
      "context": "- `scripts/test_cni_bridge_fix.sh` - Test script, not core infrastructure"
    },
    {
      "from": "ff198a1c",
      "to": "b804c80b",
      "relation": "calls",
      "context": "- **Preserved:** `scripts/test_enhanced_join_functionality.sh` - Core join functionality validation"
    },
    {
      "from": "ff198a1c",
      "to": "b804c80b",
      "relation": "mentions",
      "context": "- **Preserved:** `scripts/test_enhanced_join_functionality.sh` - Core join functionality validation"
    },
    {
      "from": "ff198a1c",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "- `deploy-cluster.sh` - Main deployment script (967 lines)"
    },
    {
      "from": "ff198a1c",
      "to": "7a6f331b",
      "relation": "mentions",
      "context": "- `diagnose_jellyfin_network.sh` - Jellyfin network diagnostics"
    },
    {
      "from": "ff198a1c",
      "to": "ebb30773",
      "relation": "mentions",
      "context": "- `fix_jellyfin_cni_bridge_conflict.sh` - Jellyfin-specific CNI bridge fixes (780 lines)"
    },
    {
      "from": "ff198a1c",
      "to": "e3453d63",
      "relation": "mentions",
      "context": "- `fix_jellyfin_network_issue.sh` - Jellyfin network issue remediation"
    },
    {
      "from": "ff198a1c",
      "to": "0d0c4a60",
      "relation": "mentions",
      "context": "- `fix_jellyfin_readiness.sh` - Jellyfin readiness probe fixes"
    },
    {
      "from": "ff198a1c",
      "to": "0ecf3a99",
      "relation": "mentions",
      "context": "- `generate_join_command.sh` - Join command generation"
    },
    {
      "from": "ff198a1c",
      "to": "f157568d",
      "relation": "mentions",
      "context": "- `quick_fix_cni_communication.sh` - Quick CNI communication fixes"
    },
    {
      "from": "ff198a1c",
      "to": "d5c6d418",
      "relation": "mentions",
      "context": "- `validate_cni_fix.sh` - CNI fix validation"
    },
    {
      "from": "ff198a1c",
      "to": "f07eb5d6",
      "relation": "mentions",
      "context": "- `validate_network_reset.sh` - Network reset validation"
    },
    {
      "from": "ff198a1c",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "- `enhanced_kubeadm_join.sh` - Enhanced join process (1388 lines)"
    },
    {
      "from": "ff198a1c",
      "to": "37f70066",
      "relation": "mentions",
      "context": "- `comprehensive_worker_setup.sh` - Complete worker setup (481 lines)"
    },
    {
      "from": "ff198a1c",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "- `fix_cluster_communication.sh` - Cluster communication fixes (705 lines)"
    },
    {
      "from": "ff198a1c",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "- `fix_remaining_pod_issues.sh` - Pod issue remediation (673 lines)"
    },
    {
      "from": "ff198a1c",
      "to": "1970801b",
      "relation": "mentions",
      "context": "- `fix_cni_bridge_conflict.sh` - Generic CNI bridge fixes (378 lines)"
    },
    {
      "from": "ff198a1c",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "- `fix_worker_node_cni.sh` - Worker CNI communication (423 lines)"
    },
    {
      "from": "ff198a1c",
      "to": "263c2cb9",
      "relation": "mentions",
      "context": "- `fix_flannel_mixed_os.sh` - Mixed OS Flannel fixes (403 lines)"
    },
    {
      "from": "ff198a1c",
      "to": "4f42b816",
      "relation": "mentions",
      "context": "- `fix_cluster_dns_configuration.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "7071b363",
      "relation": "mentions",
      "context": "- `fix_coredns_unknown_status.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "a249d706",
      "relation": "mentions",
      "context": "- `fix_homelab_node_issues.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "630db1ca",
      "relation": "mentions",
      "context": "- `fix_iptables_compatibility.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "1ddb9838",
      "relation": "mentions",
      "context": "- `fix_kubelet_systemd_config.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "09f42a5d",
      "relation": "mentions",
      "context": "- `fix_nodeport_external_access.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "2efbd35b",
      "relation": "mentions",
      "context": "- `fix_worker_kubectl_config.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "b3dc3aae",
      "relation": "mentions",
      "context": "- `gather_worker_diagnostics.sh` - Worker diagnostics (453 lines)"
    },
    {
      "from": "ff198a1c",
      "to": "b428256d",
      "relation": "mentions",
      "context": "- `validate_join_prerequisites.sh` - Pre-join validation (398 lines)"
    },
    {
      "from": "ff198a1c",
      "to": "3128233f",
      "relation": "mentions",
      "context": "- `diagnose_remaining_pod_issues.sh` - Pod issue diagnosis"
    },
    {
      "from": "ff198a1c",
      "to": "62721f19",
      "relation": "mentions",
      "context": "- `validate_pod_health.sh` - Pod health validation"
    },
    {
      "from": "ff198a1c",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "- `validate_cluster_communication.sh` - Cluster communication validation"
    },
    {
      "from": "ff198a1c",
      "to": "27edfbfd",
      "relation": "mentions",
      "context": "- `validate_nodeport_external_access.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "- `validate_pod_connectivity.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "6ea2bc0f",
      "relation": "mentions",
      "context": "- `validate_post_wipe_functionality.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "ac18a0b8",
      "relation": "mentions",
      "context": "- `validate_mixed_os_flannel.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "- `validate_systemd_dropins.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "563e9b03",
      "relation": "mentions",
      "context": "- `vmstation_status.sh` - Cluster status and diagnostics"
    },
    {
      "from": "ff198a1c",
      "to": "09b572dc",
      "relation": "mentions",
      "context": "- `run_network_diagnosis.sh` - Network diagnosis wrapper"
    },
    {
      "from": "ff198a1c",
      "to": "36bd22c9",
      "relation": "mentions",
      "context": "- `check_coredns_status.sh` - CoreDNS status checking"
    },
    {
      "from": "ff198a1c",
      "to": "7fb7d1a5",
      "relation": "mentions",
      "context": "- `check_cni_bridge_conflict.sh` - CNI bridge conflict detection"
    },
    {
      "from": "ff198a1c",
      "to": "585e2b00",
      "relation": "mentions",
      "context": "- `ansible_pre_join_validation.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "21313a3e",
      "relation": "mentions",
      "context": "- `manual_containerd_filesystem_fix.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "1b255a42",
      "relation": "mentions",
      "context": "- `quick_join_diagnostics.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "0f552648",
      "relation": "mentions",
      "context": "- `smoke-test.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "b804c80b",
      "relation": "mentions",
      "context": "- `test_enhanced_join_functionality.sh` - Join functionality testing"
    },
    {
      "from": "ff198a1c",
      "to": "6564808c",
      "relation": "mentions",
      "context": "- `worker_node_join_remediation.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "04c6e90f",
      "relation": "mentions",
      "context": "- **scripts/README.md**: Complete rewrite to reflect actual scripts"
    },
    {
      "from": "ff198a1c",
      "to": "04c6e90f",
      "relation": "mentions",
      "context": "- **README.md**: Updated deployment script references from `deploy.sh` to `deploy-cluster.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "- **README.md**: Updated deployment script references from `deploy.sh` to `deploy-cluster.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "- Changed `deploy.sh`  `deploy-cluster.sh` throughout all scripts"
    },
    {
      "from": "ff198a1c",
      "to": "ebb30773",
      "relation": "calls",
      "context": "**After:** Use the actual fix script: `./fix_jellyfin_cni_bridge_conflict.sh` or `./scripts/fix_cni_bridge_conflict.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "1970801b",
      "relation": "calls",
      "context": "**After:** Use the actual fix script: `./fix_jellyfin_cni_bridge_conflict.sh` or `./scripts/fix_cni_bridge_conflict.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "ebb30773",
      "relation": "mentions",
      "context": "**After:** Use the actual fix script: `./fix_jellyfin_cni_bridge_conflict.sh` or `./scripts/fix_cni_bridge_conflict.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "1970801b",
      "relation": "mentions",
      "context": "**After:** Use the actual fix script: `./fix_jellyfin_cni_bridge_conflict.sh` or `./scripts/fix_cni_bridge_conflict.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "62721f19",
      "relation": "calls",
      "context": "**After:** Use validation: `./scripts/validate_pod_health.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "62721f19",
      "relation": "mentions",
      "context": "**After:** Use validation: `./scripts/validate_pod_health.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "72c7835a",
      "relation": "calls",
      "context": "**After:** `./deploy-cluster.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "**After:** `./deploy-cluster.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "62721f19",
      "relation": "calls",
      "context": "- `./scripts/validate_pod_health.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "62721f19",
      "relation": "mentions",
      "context": "- `./scripts/validate_pod_health.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "7c606adf",
      "relation": "calls",
      "context": "- `./scripts/validate_cluster_communication.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "- `./scripts/validate_cluster_communication.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "b428256d",
      "relation": "calls",
      "context": "- `./scripts/validate_join_prerequisites.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "b428256d",
      "relation": "mentions",
      "context": "- `./scripts/validate_join_prerequisites.sh`"
    },
    {
      "from": "ff198a1c",
      "to": "1970801b",
      "relation": "calls",
      "context": "1. **Preserved Functional Diversity**: Both generic (`scripts/fix_cni_bridge_conflict.sh`) and specific (`fix_jellyfin_cni_bridge_conflict.sh`) scripts were kept as they serve different purposes."
    },
    {
      "from": "ff198a1c",
      "to": "1970801b",
      "relation": "mentions",
      "context": "1. **Preserved Functional Diversity**: Both generic (`scripts/fix_cni_bridge_conflict.sh`) and specific (`fix_jellyfin_cni_bridge_conflict.sh`) scripts were kept as they serve different purposes."
    },
    {
      "from": "ff198a1c",
      "to": "ebb30773",
      "relation": "mentions",
      "context": "1. **Preserved Functional Diversity**: Both generic (`scripts/fix_cni_bridge_conflict.sh`) and specific (`fix_jellyfin_cni_bridge_conflict.sh`) scripts were kept as they serve different purposes."
    },
    {
      "from": "c9daffcc",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "- Clean `setup-cluster.yaml` (200 lines) with essential functionality"
    },
    {
      "from": "c9daffcc",
      "to": "dd358be7",
      "relation": "mentions",
      "context": "- Consolidated `simple-deploy.yaml` combining essential features"
    },
    {
      "from": "c9daffcc",
      "to": "dd358be7",
      "relation": "references",
      "context": " simple-deploy.yaml             # Main deployment playbook"
    },
    {
      "from": "c9daffcc",
      "to": "dd358be7",
      "relation": "mentions",
      "context": " simple-deploy.yaml             # Main deployment playbook"
    },
    {
      "from": "c9daffcc",
      "to": "f0bc548f",
      "relation": "references",
      "context": "    setup-cluster.yaml         # Kubernetes cluster setup"
    },
    {
      "from": "c9daffcc",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "    setup-cluster.yaml         # Kubernetes cluster setup"
    },
    {
      "from": "c9daffcc",
      "to": "bcfb5f43",
      "relation": "references",
      "context": "    deploy-apps.yaml           # Application deployment"
    },
    {
      "from": "c9daffcc",
      "to": "bcfb5f43",
      "relation": "mentions",
      "context": "    deploy-apps.yaml           # Application deployment"
    },
    {
      "from": "c9daffcc",
      "to": "2ac1c05f",
      "relation": "references",
      "context": "    jellyfin.yml               # Jellyfin deployment (existing)"
    },
    {
      "from": "c9daffcc",
      "to": "2ac1c05f",
      "relation": "mentions",
      "context": "    jellyfin.yml               # Jellyfin deployment (existing)"
    },
    {
      "from": "c9daffcc",
      "to": "607e6e59",
      "relation": "references",
      "context": " all.yml                     # Configuration"
    },
    {
      "from": "c9daffcc",
      "to": "607e6e59",
      "relation": "mentions",
      "context": " all.yml                     # Configuration"
    },
    {
      "from": "c9daffcc",
      "to": "607e6e59",
      "relation": "references",
      "context": " all.yml.template            # Configuration template"
    },
    {
      "from": "c9daffcc",
      "to": "607e6e59",
      "relation": "mentions",
      "context": " all.yml.template            # Configuration template"
    },
    {
      "from": "c9daffcc",
      "to": "607e6e59",
      "relation": "references",
      "context": "The deployment uses `ansible/group_vars/all.yml` for configuration. If this file doesn't exist, it will be created automatically from the template with sensible defaults."
    },
    {
      "from": "c9daffcc",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "The deployment uses `ansible/group_vars/all.yml` for configuration. If this file doesn't exist, it will be created automatically from the template with sensible defaults."
    },
    {
      "from": "c9daffcc",
      "to": "607e6e59",
      "relation": "references",
      "context": "cp ansible/group_vars/all.yml ansible/group_vars/all.yml.backup"
    },
    {
      "from": "c9daffcc",
      "to": "607e6e59",
      "relation": "references",
      "context": "cp ansible/group_vars/all.yml ansible/group_vars/all.yml.backup"
    },
    {
      "from": "c9daffcc",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "cp ansible/group_vars/all.yml ansible/group_vars/all.yml.backup"
    },
    {
      "from": "c9daffcc",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "cp ansible/group_vars/all.yml ansible/group_vars/all.yml.backup"
    },
    {
      "from": "c9daffcc",
      "to": "607e6e59",
      "relation": "references",
      "context": "Edit `ansible/group_vars/all.yml` to customize:"
    },
    {
      "from": "c9daffcc",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "Edit `ansible/group_vars/all.yml` to customize:"
    },
    {
      "from": "c9daffcc",
      "to": "f0bc548f",
      "relation": "references",
      "context": "ansible-playbook -i inventory.txt plays/setup-cluster.yaml"
    },
    {
      "from": "c9daffcc",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "ansible-playbook -i inventory.txt plays/setup-cluster.yaml"
    },
    {
      "from": "c9daffcc",
      "to": "bcfb5f43",
      "relation": "references",
      "context": "ansible-playbook -i inventory.txt plays/deploy-apps.yaml"
    },
    {
      "from": "c9daffcc",
      "to": "bcfb5f43",
      "relation": "mentions",
      "context": "ansible-playbook -i inventory.txt plays/deploy-apps.yaml"
    },
    {
      "from": "761e8cd1",
      "to": "311b37fb",
      "relation": "calls",
      "context": "./fix-cluster.sh"
    },
    {
      "from": "761e8cd1",
      "to": "311b37fb",
      "relation": "mentions",
      "context": "./fix-cluster.sh"
    },
    {
      "from": "761e8cd1",
      "to": "311b37fb",
      "relation": "calls",
      "context": "./fix-cluster.sh"
    },
    {
      "from": "761e8cd1",
      "to": "311b37fb",
      "relation": "mentions",
      "context": "./fix-cluster.sh"
    },
    {
      "from": "761e8cd1",
      "to": "11010dc1",
      "relation": "calls",
      "context": "./validate-cluster.sh"
    },
    {
      "from": "761e8cd1",
      "to": "11010dc1",
      "relation": "mentions",
      "context": "./validate-cluster.sh"
    },
    {
      "from": "761e8cd1",
      "to": "311b37fb",
      "relation": "mentions",
      "context": "- `fix-cluster.sh` - **MAIN SCRIPT** - Single command to fix everything"
    },
    {
      "from": "761e8cd1",
      "to": "11010dc1",
      "relation": "mentions",
      "context": "- `validate-cluster.sh` - Validates the fix worked"
    },
    {
      "from": "761e8cd1",
      "to": "61c1941a",
      "relation": "mentions",
      "context": "- `CNI-BRIDGE-FIX-README.md` - Detailed explanation"
    },
    {
      "from": "761e8cd1",
      "to": "fd7c8bd5",
      "relation": "references",
      "context": "- `ansible/playbooks/minimal-network-fix.yml` - Ansible playbook approach"
    },
    {
      "from": "761e8cd1",
      "to": "fd7c8bd5",
      "relation": "mentions",
      "context": "- `ansible/playbooks/minimal-network-fix.yml` - Ansible playbook approach"
    },
    {
      "from": "761e8cd1",
      "to": "311b37fb",
      "relation": "calls",
      "context": "1. **Re-run the fix**: `./fix-cluster.sh`"
    },
    {
      "from": "761e8cd1",
      "to": "311b37fb",
      "relation": "mentions",
      "context": "1. **Re-run the fix**: `./fix-cluster.sh`"
    },
    {
      "from": "761e8cd1",
      "to": "11010dc1",
      "relation": "calls",
      "context": "2. **Check validation**: `./validate-cluster.sh`"
    },
    {
      "from": "761e8cd1",
      "to": "11010dc1",
      "relation": "mentions",
      "context": "2. **Check validation**: `./validate-cluster.sh`"
    },
    {
      "from": "761e8cd1",
      "to": "ad0a12c7",
      "relation": "calls",
      "context": "3. **Manual CNI reset**: `sudo ./scripts/reset_cni_bridge_minimal.sh`"
    },
    {
      "from": "761e8cd1",
      "to": "ad0a12c7",
      "relation": "mentions",
      "context": "3. **Manual CNI reset**: `sudo ./scripts/reset_cni_bridge_minimal.sh`"
    },
    {
      "from": "761e8cd1",
      "to": "311b37fb",
      "relation": "calls",
      "context": "After running `./fix-cluster.sh`, you should have:"
    },
    {
      "from": "761e8cd1",
      "to": "311b37fb",
      "relation": "mentions",
      "context": "After running `./fix-cluster.sh`, you should have:"
    },
    {
      "from": "761e8cd1",
      "to": "311b37fb",
      "relation": "calls",
      "context": "**After**: Single command (`./fix-cluster.sh`) that creates a working cluster"
    },
    {
      "from": "761e8cd1",
      "to": "311b37fb",
      "relation": "mentions",
      "context": "**After**: Single command (`./fix-cluster.sh`) that creates a working cluster"
    },
    {
      "from": "3fbfd7f8",
      "to": "044d8702",
      "relation": "references",
      "context": "See [Post-Wipe Worker Join Documentation](docs/POST_WIPE_WORKER_JOIN.md) for detailed information."
    },
    {
      "from": "3fbfd7f8",
      "to": "044d8702",
      "relation": "mentions",
      "context": "See [Post-Wipe Worker Join Documentation](docs/POST_WIPE_WORKER_JOIN.md) for detailed information."
    },
    {
      "from": "3fbfd7f8",
      "to": "f0bc548f",
      "relation": "references",
      "context": "ansible-playbook -i inventory.txt ansible/plays/setup-cluster.yaml"
    },
    {
      "from": "3fbfd7f8",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "ansible-playbook -i inventory.txt ansible/plays/setup-cluster.yaml"
    },
    {
      "from": "dc5e49f8",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml"
    },
    {
      "from": "dc5e49f8",
      "to": "b9618ca5",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml"
    },
    {
      "from": "607e6e59",
      "to": "2e9ce2c1",
      "relation": "references",
      "context": "cluster_bootstrap_mode: false  # Set to true when using cluster-bootstrap.yml"
    },
    {
      "from": "607e6e59",
      "to": "2e9ce2c1",
      "relation": "mentions",
      "context": "cluster_bootstrap_mode: false  # Set to true when using cluster-bootstrap.yml"
    },
    {
      "from": "607e6e59",
      "to": "377c4256",
      "relation": "references",
      "context": "# Default: kubernetes/jellyfin-minimal.yml (relative to ansible/plays/ directory)"
    },
    {
      "from": "607e6e59",
      "to": "377c4256",
      "relation": "mentions",
      "context": "# Default: kubernetes/jellyfin-minimal.yml (relative to ansible/plays/ directory)"
    },
    {
      "from": "607e6e59",
      "to": "2ac1c05f",
      "relation": "references",
      "context": "# WARNING: When using {{ playbook_dir }}, remember that jellyfin.yml runs from plays/ directory"
    },
    {
      "from": "607e6e59",
      "to": "2ac1c05f",
      "relation": "mentions",
      "context": "# WARNING: When using {{ playbook_dir }}, remember that jellyfin.yml runs from plays/ directory"
    },
    {
      "from": "607e6e59",
      "to": "377c4256",
      "relation": "references",
      "context": "#          so use \"{{ playbook_dir }}/../roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "607e6e59",
      "to": "377c4256",
      "relation": "mentions",
      "context": "#          so use \"{{ playbook_dir }}/../roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "607e6e59",
      "to": "377c4256",
      "relation": "references",
      "context": "jellyfin_manifest_path: kubernetes/jellyfin-minimal.yml"
    },
    {
      "from": "607e6e59",
      "to": "377c4256",
      "relation": "mentions",
      "context": "jellyfin_manifest_path: kubernetes/jellyfin-minimal.yml"
    },
    {
      "from": "5f591fa6",
      "to": "607e6e59",
      "relation": "references",
      "context": "# Copy this file to all.yml and customize for your environment."
    },
    {
      "from": "5f591fa6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# Copy this file to all.yml and customize for your environment."
    },
    {
      "from": "5f591fa6",
      "to": "607e6e59",
      "relation": "references",
      "context": "# IMPORTANT: Never commit `ansible/group_vars/all.yml` with real credentials."
    },
    {
      "from": "5f591fa6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# IMPORTANT: Never commit `ansible/group_vars/all.yml` with real credentials."
    },
    {
      "from": "5f591fa6",
      "to": "607e6e59",
      "relation": "references",
      "context": "# This template mirrors the structure of the real `ansible/group_vars/all.yml` used at runtime."
    },
    {
      "from": "5f591fa6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# This template mirrors the structure of the real `ansible/group_vars/all.yml` used at runtime."
    },
    {
      "from": "5f591fa6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# real `all.yml` on-disk. If a task or agent complains about missing credentials, assume those"
    },
    {
      "from": "5f591fa6",
      "to": "607e6e59",
      "relation": "references",
      "context": "# secrets live in `ansible/group_vars/all.yml` (not the template) and will be available to"
    },
    {
      "from": "5f591fa6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# secrets live in `ansible/group_vars/all.yml` (not the template) and will be available to"
    },
    {
      "from": "5f591fa6",
      "to": "2e9ce2c1",
      "relation": "references",
      "context": "cluster_bootstrap_mode: false  # Set to true when using cluster-bootstrap.yml"
    },
    {
      "from": "5f591fa6",
      "to": "2e9ce2c1",
      "relation": "mentions",
      "context": "cluster_bootstrap_mode: false  # Set to true when using cluster-bootstrap.yml"
    },
    {
      "from": "5f591fa6",
      "to": "377c4256",
      "relation": "references",
      "context": "# Default: kubernetes/jellyfin-minimal.yml (relative to ansible/plays/ directory)"
    },
    {
      "from": "5f591fa6",
      "to": "377c4256",
      "relation": "mentions",
      "context": "# Default: kubernetes/jellyfin-minimal.yml (relative to ansible/plays/ directory)"
    },
    {
      "from": "5f591fa6",
      "to": "2ac1c05f",
      "relation": "references",
      "context": "# WARNING: When using {{ playbook_dir }}, remember that jellyfin.yml runs from plays/ directory"
    },
    {
      "from": "5f591fa6",
      "to": "2ac1c05f",
      "relation": "mentions",
      "context": "# WARNING: When using {{ playbook_dir }}, remember that jellyfin.yml runs from plays/ directory"
    },
    {
      "from": "5f591fa6",
      "to": "377c4256",
      "relation": "references",
      "context": "#          so use \"{{ playbook_dir }}/../roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "5f591fa6",
      "to": "377c4256",
      "relation": "mentions",
      "context": "#          so use \"{{ playbook_dir }}/../roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "5f591fa6",
      "to": "377c4256",
      "relation": "references",
      "context": "jellyfin_manifest_path: kubernetes/jellyfin-minimal.yml"
    },
    {
      "from": "5f591fa6",
      "to": "377c4256",
      "relation": "mentions",
      "context": "jellyfin_manifest_path: kubernetes/jellyfin-minimal.yml"
    },
    {
      "from": "5f591fa6",
      "to": "607e6e59",
      "relation": "references",
      "context": "# Set enable_tls: false in all.yml to skip creating TLS certs and installing cert-manager"
    },
    {
      "from": "5f591fa6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# Set enable_tls: false in all.yml to skip creating TLS certs and installing cert-manager"
    },
    {
      "from": "e95d5da9",
      "to": "607e6e59",
      "relation": "references",
      "context": "# Copy this file to all.yml and customize for your environment."
    },
    {
      "from": "e95d5da9",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# Copy this file to all.yml and customize for your environment."
    },
    {
      "from": "e95d5da9",
      "to": "607e6e59",
      "relation": "references",
      "context": "# IMPORTANT: Never commit `ansible/group_vars/all.yml` with real credentials."
    },
    {
      "from": "e95d5da9",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# IMPORTANT: Never commit `ansible/group_vars/all.yml` with real credentials."
    },
    {
      "from": "e95d5da9",
      "to": "607e6e59",
      "relation": "references",
      "context": "# This template mirrors the structure of the real `ansible/group_vars/all.yml` used at runtime."
    },
    {
      "from": "e95d5da9",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# This template mirrors the structure of the real `ansible/group_vars/all.yml` used at runtime."
    },
    {
      "from": "e95d5da9",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# real `all.yml` on-disk. If a task or agent complains about missing credentials, assume those"
    },
    {
      "from": "e95d5da9",
      "to": "607e6e59",
      "relation": "references",
      "context": "# secrets live in `ansible/group_vars/all.yml` (not the template) and will be available to"
    },
    {
      "from": "e95d5da9",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# secrets live in `ansible/group_vars/all.yml` (not the template) and will be available to"
    },
    {
      "from": "e95d5da9",
      "to": "2e9ce2c1",
      "relation": "references",
      "context": "cluster_bootstrap_mode: false  # Set to true when using cluster-bootstrap.yml"
    },
    {
      "from": "e95d5da9",
      "to": "2e9ce2c1",
      "relation": "mentions",
      "context": "cluster_bootstrap_mode: false  # Set to true when using cluster-bootstrap.yml"
    },
    {
      "from": "e95d5da9",
      "to": "377c4256",
      "relation": "references",
      "context": "# Default: kubernetes/jellyfin-minimal.yml (relative to ansible/plays/ directory)"
    },
    {
      "from": "e95d5da9",
      "to": "377c4256",
      "relation": "mentions",
      "context": "# Default: kubernetes/jellyfin-minimal.yml (relative to ansible/plays/ directory)"
    },
    {
      "from": "e95d5da9",
      "to": "2ac1c05f",
      "relation": "references",
      "context": "# WARNING: When using {{ playbook_dir }}, remember that jellyfin.yml runs from plays/ directory"
    },
    {
      "from": "e95d5da9",
      "to": "2ac1c05f",
      "relation": "mentions",
      "context": "# WARNING: When using {{ playbook_dir }}, remember that jellyfin.yml runs from plays/ directory"
    },
    {
      "from": "e95d5da9",
      "to": "377c4256",
      "relation": "references",
      "context": "#          so use \"{{ playbook_dir }}/../roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "e95d5da9",
      "to": "377c4256",
      "relation": "mentions",
      "context": "#          so use \"{{ playbook_dir }}/../roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "e95d5da9",
      "to": "377c4256",
      "relation": "references",
      "context": "jellyfin_manifest_path: kubernetes/jellyfin-minimal.yml"
    },
    {
      "from": "e95d5da9",
      "to": "377c4256",
      "relation": "mentions",
      "context": "jellyfin_manifest_path: kubernetes/jellyfin-minimal.yml"
    },
    {
      "from": "e95d5da9",
      "to": "607e6e59",
      "relation": "references",
      "context": "# Set enable_tls: false in all.yml to skip creating TLS certs and installing cert-manager"
    },
    {
      "from": "e95d5da9",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# Set enable_tls: false in all.yml to skip creating TLS certs and installing cert-manager"
    },
    {
      "from": "39bfddc5",
      "to": "f0bc548f",
      "relation": "references",
      "context": "# Extracted common tasks from the existing setup-cluster.yaml"
    },
    {
      "from": "39bfddc5",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "# Extracted common tasks from the existing setup-cluster.yaml"
    },
    {
      "from": "fa6b94bd",
      "to": "f0bc548f",
      "relation": "references",
      "context": "# Alternative to the complex setup-cluster.yaml for basic kubeadm deployment"
    },
    {
      "from": "fa6b94bd",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "# Alternative to the complex setup-cluster.yaml for basic kubeadm deployment"
    },
    {
      "from": "fa6b94bd",
      "to": "f0bc548f",
      "relation": "references",
      "context": "include_tasks: ../../plays/setup-cluster.yaml"
    },
    {
      "from": "fa6b94bd",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "include_tasks: ../../plays/setup-cluster.yaml"
    },
    {
      "from": "fa6b94bd",
      "to": "0a813314",
      "relation": "references",
      "context": "src: \"{{ playbook_dir }}/../../manifests/cni/flannel.yaml\""
    },
    {
      "from": "fa6b94bd",
      "to": "0a813314",
      "relation": "mentions",
      "context": "src: \"{{ playbook_dir }}/../../manifests/cni/flannel.yaml\""
    },
    {
      "from": "fa6b94bd",
      "to": "1d93e806",
      "relation": "references",
      "context": "- \"{{ playbook_dir }}/../../manifests/monitoring/prometheus.yaml\""
    },
    {
      "from": "fa6b94bd",
      "to": "1d93e806",
      "relation": "mentions",
      "context": "- \"{{ playbook_dir }}/../../manifests/monitoring/prometheus.yaml\""
    },
    {
      "from": "fa6b94bd",
      "to": "24a2f90a",
      "relation": "references",
      "context": "- \"{{ playbook_dir }}/../../manifests/monitoring/grafana.yaml\""
    },
    {
      "from": "fa6b94bd",
      "to": "24a2f90a",
      "relation": "mentions",
      "context": "- \"{{ playbook_dir }}/../../manifests/monitoring/grafana.yaml\""
    },
    {
      "from": "fa6b94bd",
      "to": "269791b0",
      "relation": "references",
      "context": "src: \"{{ playbook_dir }}/../../manifests/jellyfin/jellyfin.yaml\""
    },
    {
      "from": "fa6b94bd",
      "to": "269791b0",
      "relation": "mentions",
      "context": "src: \"{{ playbook_dir }}/../../manifests/jellyfin/jellyfin.yaml\""
    },
    {
      "from": "fa6b94bd",
      "to": "91932608",
      "relation": "references",
      "context": "- Run verification: ansible-playbook verify-cluster.yml"
    },
    {
      "from": "fa6b94bd",
      "to": "91932608",
      "relation": "mentions",
      "context": "- Run verification: ansible-playbook verify-cluster.yml"
    },
    {
      "from": "2e9ce2c1",
      "to": "f0bc548f",
      "relation": "references",
      "context": "# Builds upon existing setup-cluster.yaml with kubeadm-specific enhancements"
    },
    {
      "from": "2e9ce2c1",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "# Builds upon existing setup-cluster.yaml with kubeadm-specific enhancements"
    },
    {
      "from": "2e9ce2c1",
      "to": "f0bc548f",
      "relation": "references",
      "context": "import_playbook: ../plays/setup-cluster.yaml"
    },
    {
      "from": "2e9ce2c1",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "import_playbook: ../plays/setup-cluster.yaml"
    },
    {
      "from": "2e9ce2c1",
      "to": "0a813314",
      "relation": "references",
      "context": "src: \"{{ playbook_dir }}/../../manifests/cni/flannel.yaml\""
    },
    {
      "from": "2e9ce2c1",
      "to": "0a813314",
      "relation": "mentions",
      "context": "src: \"{{ playbook_dir }}/../../manifests/cni/flannel.yaml\""
    },
    {
      "from": "fd7c8bd5",
      "to": "3d26bfd6",
      "relation": "references",
      "context": "flannel_manifest: \"{{ playbook_dir }}/../../manifests/cni/flannel-minimal.yaml\""
    },
    {
      "from": "fd7c8bd5",
      "to": "3d26bfd6",
      "relation": "mentions",
      "context": "flannel_manifest: \"{{ playbook_dir }}/../../manifests/cni/flannel-minimal.yaml\""
    },
    {
      "from": "fd7c8bd5",
      "to": "0d91b1c9",
      "relation": "references",
      "context": "coredns_manifest: \"{{ playbook_dir }}/../../manifests/network/coredns-minimal.yaml\""
    },
    {
      "from": "fd7c8bd5",
      "to": "0d91b1c9",
      "relation": "mentions",
      "context": "coredns_manifest: \"{{ playbook_dir }}/../../manifests/network/coredns-minimal.yaml\""
    },
    {
      "from": "fd7c8bd5",
      "to": "0aec39aa",
      "relation": "references",
      "context": "kube_proxy_manifest: \"{{ playbook_dir }}/../../manifests/network/kube-proxy-minimal.yaml\""
    },
    {
      "from": "fd7c8bd5",
      "to": "0aec39aa",
      "relation": "mentions",
      "context": "kube_proxy_manifest: \"{{ playbook_dir }}/../../manifests/network/kube-proxy-minimal.yaml\""
    },
    {
      "from": "fd7c8bd5",
      "to": "4ede3e1b",
      "relation": "references",
      "context": "jellyfin_manifest: \"{{ playbook_dir }}/../../manifests/jellyfin/jellyfin-minimal.yaml\""
    },
    {
      "from": "fd7c8bd5",
      "to": "4ede3e1b",
      "relation": "mentions",
      "context": "jellyfin_manifest: \"{{ playbook_dir }}/../../manifests/jellyfin/jellyfin-minimal.yaml\""
    },
    {
      "from": "91932608",
      "to": "1d93e806",
      "relation": "references",
      "context": "- \"{{ playbook_dir }}/../../manifests/monitoring/prometheus.yaml\""
    },
    {
      "from": "91932608",
      "to": "1d93e806",
      "relation": "mentions",
      "context": "- \"{{ playbook_dir }}/../../manifests/monitoring/prometheus.yaml\""
    },
    {
      "from": "91932608",
      "to": "24a2f90a",
      "relation": "references",
      "context": "- \"{{ playbook_dir }}/../../manifests/monitoring/grafana.yaml\""
    },
    {
      "from": "91932608",
      "to": "24a2f90a",
      "relation": "mentions",
      "context": "- \"{{ playbook_dir }}/../../manifests/monitoring/grafana.yaml\""
    },
    {
      "from": "91932608",
      "to": "269791b0",
      "relation": "references",
      "context": "src: \"{{ playbook_dir }}/../../manifests/jellyfin/jellyfin.yaml\""
    },
    {
      "from": "91932608",
      "to": "269791b0",
      "relation": "mentions",
      "context": "src: \"{{ playbook_dir }}/../../manifests/jellyfin/jellyfin.yaml\""
    },
    {
      "from": "91932608",
      "to": "1970801b",
      "relation": "calls",
      "context": "if [ -f \"/home/runner/work/VMStation/VMStation/scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "91932608",
      "to": "1970801b",
      "relation": "mentions",
      "context": "if [ -f \"/home/runner/work/VMStation/VMStation/scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "91932608",
      "to": "1970801b",
      "relation": "calls",
      "context": "chmod +x /home/runner/work/VMStation/VMStation/scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "91932608",
      "to": "1970801b",
      "relation": "mentions",
      "context": "chmod +x /home/runner/work/VMStation/VMStation/scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "91932608",
      "to": "1970801b",
      "relation": "calls",
      "context": "/home/runner/work/VMStation/VMStation/scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "91932608",
      "to": "1970801b",
      "relation": "mentions",
      "context": "/home/runner/work/VMStation/VMStation/scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "bcfb5f43",
      "to": "a249d706",
      "relation": "calls",
      "context": "msg: \"CoreDNS is not running - cluster networking is unstable. Run: ./scripts/fix_homelab_node_issues.sh\""
    },
    {
      "from": "bcfb5f43",
      "to": "a249d706",
      "relation": "mentions",
      "context": "msg: \"CoreDNS is not running - cluster networking is unstable. Run: ./scripts/fix_homelab_node_issues.sh\""
    },
    {
      "from": "2ac1c05f",
      "to": "377c4256",
      "relation": "references",
      "context": "jellyfin_manifest_path: \"kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "2ac1c05f",
      "to": "377c4256",
      "relation": "mentions",
      "context": "jellyfin_manifest_path: \"kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "ca8597d0",
      "to": "607e6e59",
      "relation": "references",
      "context": "# group_vars/all.yml or via --extra-vars when running the playbook."
    },
    {
      "from": "ca8597d0",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# group_vars/all.yml or via --extra-vars when running the playbook."
    },
    {
      "from": "ca8597d0",
      "to": "1f7744bc",
      "relation": "mentions",
      "context": "prometheus-dashboard.json: \"{{ lookup('file', playbook_dir + '/../../files/grafana_dashboards/prometheus-dashboard.json') | string }}\""
    },
    {
      "from": "ca8597d0",
      "to": "1f7744bc",
      "relation": "mentions",
      "context": "prometheus-dashboard.json: \"{{ lookup('file', playbook_dir + '/../../files/grafana_dashboards/prometheus-dashboard.json') | string }}\""
    },
    {
      "from": "ca8597d0",
      "to": "6f2e75b0",
      "relation": "mentions",
      "context": "loki-dashboard.json: \"{{ lookup('file', playbook_dir + '/../../files/grafana_dashboards/loki-dashboard.json') | string }}\""
    },
    {
      "from": "ca8597d0",
      "to": "6f2e75b0",
      "relation": "mentions",
      "context": "loki-dashboard.json: \"{{ lookup('file', playbook_dir + '/../../files/grafana_dashboards/loki-dashboard.json') | string }}\""
    },
    {
      "from": "ca8597d0",
      "to": "b1bc04a3",
      "relation": "mentions",
      "context": "node-dashboard.json: \"{{ lookup('file', playbook_dir + '/../../files/grafana_dashboards/node-dashboard.json') | string }}\""
    },
    {
      "from": "ca8597d0",
      "to": "b1bc04a3",
      "relation": "mentions",
      "context": "node-dashboard.json: \"{{ lookup('file', playbook_dir + '/../../files/grafana_dashboards/node-dashboard.json') | string }}\""
    },
    {
      "from": "7ef785f1",
      "to": "1f7744bc",
      "relation": "mentions",
      "context": "- \"'prometheus-dashboard.json' in grafana_dashboards_cm.resources[0].data\""
    },
    {
      "from": "7ef785f1",
      "to": "6f2e75b0",
      "relation": "mentions",
      "context": "- \"'loki-dashboard.json' in grafana_dashboards_cm.resources[0].data\""
    },
    {
      "from": "7ef785f1",
      "to": "b1bc04a3",
      "relation": "mentions",
      "context": "- \"'node-dashboard.json' in grafana_dashboards_cm.resources[0].data\""
    },
    {
      "from": "b9618ca5",
      "to": "7b8171b6",
      "relation": "mentions",
      "context": "# Based on the expert diagnosis from copilot-instructions.md"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "calls",
      "context": "if [ -x /root/VMStation/scripts/fix_cni_bridge_conflict.sh ]; then"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "mentions",
      "context": "if [ -x /root/VMStation/scripts/fix_cni_bridge_conflict.sh ]; then"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "calls",
      "context": "/root/VMStation/scripts/fix_cni_bridge_conflict.sh || true"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "mentions",
      "context": "/root/VMStation/scripts/fix_cni_bridge_conflict.sh || true"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "calls",
      "context": "elif [ -x /srv/monitoring_data/VMStation/scripts/fix_cni_bridge_conflict.sh ]; then"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "mentions",
      "context": "elif [ -x /srv/monitoring_data/VMStation/scripts/fix_cni_bridge_conflict.sh ]; then"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "calls",
      "context": "/srv/monitoring_data/VMStation/scripts/fix_cni_bridge_conflict.sh || true"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "mentions",
      "context": "/srv/monitoring_data/VMStation/scripts/fix_cni_bridge_conflict.sh || true"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "mentions",
      "context": "echo \"The deployment will complete and run fix_cni_bridge_conflict.sh automatically.\""
    },
    {
      "from": "f0bc548f",
      "to": "7071b363",
      "relation": "calls",
      "context": "After cluster setup completes, run: ./scripts/fix_coredns_unknown_status.sh"
    },
    {
      "from": "f0bc548f",
      "to": "7071b363",
      "relation": "mentions",
      "context": "After cluster setup completes, run: ./scripts/fix_coredns_unknown_status.sh"
    },
    {
      "from": "f0bc548f",
      "to": "1970801b",
      "relation": "mentions",
      "context": "The deployment will automatically run fix_cni_bridge_conflict.sh after cluster setup."
    },
    {
      "from": "f0bc548f",
      "to": "b428256d",
      "relation": "calls",
      "context": "- \"../../scripts/validate_join_prerequisites.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "b428256d",
      "relation": "mentions",
      "context": "- \"../../scripts/validate_join_prerequisites.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "- \"../../scripts/enhanced_kubeadm_join.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "- \"../../scripts/enhanced_kubeadm_join.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "- \"../../scripts/validate_systemd_dropins.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "- \"../../scripts/validate_systemd_dropins.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "1ddb9838",
      "relation": "calls",
      "context": "- \"../../scripts/fix_kubelet_systemd_config.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "1ddb9838",
      "relation": "mentions",
      "context": "- \"../../scripts/fix_kubelet_systemd_config.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "if [ -f /tmp/validate_systemd_dropins.sh ]; then"
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "if ! /tmp/validate_systemd_dropins.sh validate kubelet; then"
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "/tmp/validate_systemd_dropins.sh fix kubelet"
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "/tmp/validate_systemd_dropins.sh ensure-join \"{{ control_plane_ip }}\""
    },
    {
      "from": "f0bc548f",
      "to": "1ddb9838",
      "relation": "mentions",
      "context": "if [ -f /tmp/fix_kubelet_systemd_config.sh ]; then"
    },
    {
      "from": "f0bc548f",
      "to": "1ddb9838",
      "relation": "mentions",
      "context": "/tmp/fix_kubelet_systemd_config.sh"
    },
    {
      "from": "f0bc548f",
      "to": "585e2b00",
      "relation": "calls",
      "context": "src: \"../../scripts/ansible_pre_join_validation.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "585e2b00",
      "relation": "mentions",
      "context": "src: \"../../scripts/ansible_pre_join_validation.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "585e2b00",
      "relation": "mentions",
      "context": "dest: \"/tmp/ansible_pre_join_validation.sh\""
    },
    {
      "from": "f0bc548f",
      "to": "585e2b00",
      "relation": "mentions",
      "context": "shell: \"/tmp/ansible_pre_join_validation.sh {{ control_plane_ip }} {{ worker_was_wiped }}\""
    },
    {
      "from": "f0bc548f",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "/tmp/enhanced_kubeadm_join.sh \"$JOIN_COMMAND\"'"
    },
    {
      "from": "f0bc548f",
      "to": "21313a3e",
      "relation": "calls",
      "context": "sudo ./scripts/manual_containerd_filesystem_fix.sh"
    },
    {
      "from": "f0bc548f",
      "to": "21313a3e",
      "relation": "mentions",
      "context": "sudo ./scripts/manual_containerd_filesystem_fix.sh"
    },
    {
      "from": "f0bc548f",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh"
    },
    {
      "from": "f0bc548f",
      "to": "b3dc3aae",
      "relation": "mentions",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh"
    },
    {
      "from": "f0bc548f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "Run enhanced join: sudo ./scripts/enhanced_kubeadm_join.sh \"<join-command>\""
    },
    {
      "from": "f0bc548f",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "Run enhanced join: sudo ./scripts/enhanced_kubeadm_join.sh \"<join-command>\""
    },
    {
      "from": "f0bc548f",
      "to": "b428256d",
      "relation": "mentions",
      "context": "- /tmp/validate_join_prerequisites.sh"
    },
    {
      "from": "f0bc548f",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "- /tmp/enhanced_kubeadm_join.sh"
    },
    {
      "from": "f0bc548f",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "- /tmp/validate_systemd_dropins.sh"
    },
    {
      "from": "f0bc548f",
      "to": "1ddb9838",
      "relation": "mentions",
      "context": "- /tmp/fix_kubelet_systemd_config.sh"
    },
    {
      "from": "b0430b0d",
      "to": "377c4256",
      "relation": "references",
      "context": "#   kubectl apply -f jellyfin-minimal.yml"
    },
    {
      "from": "b0430b0d",
      "to": "377c4256",
      "relation": "mentions",
      "context": "#   kubectl apply -f jellyfin-minimal.yml"
    },
    {
      "from": "c782d582",
      "to": "377c4256",
      "relation": "references",
      "context": "jellyfin_manifest_path_var: \"{{ jellyfin_manifest_path | default('kubernetes/jellyfin-minimal.yml') }}\""
    },
    {
      "from": "c782d582",
      "to": "377c4256",
      "relation": "mentions",
      "context": "jellyfin_manifest_path_var: \"{{ jellyfin_manifest_path | default('kubernetes/jellyfin-minimal.yml') }}\""
    },
    {
      "from": "c782d582",
      "to": "377c4256",
      "relation": "references",
      "context": "jellyfin_manifest_path: kubernetes/jellyfin-minimal.yml"
    },
    {
      "from": "c782d582",
      "to": "377c4256",
      "relation": "mentions",
      "context": "jellyfin_manifest_path: kubernetes/jellyfin-minimal.yml"
    },
    {
      "from": "c782d582",
      "to": "377c4256",
      "relation": "references",
      "context": "jellyfin_manifest_path: \"{{ '{{' }} playbook_dir {{ '}}' }}/../roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "c782d582",
      "to": "377c4256",
      "relation": "mentions",
      "context": "jellyfin_manifest_path: \"{{ '{{' }} playbook_dir {{ '}}' }}/../roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "c782d582",
      "to": "377c4256",
      "relation": "references",
      "context": "jellyfin_manifest_path: \"{{ playbook_dir | default('.') }}/roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "c782d582",
      "to": "377c4256",
      "relation": "mentions",
      "context": "jellyfin_manifest_path: \"{{ playbook_dir | default('.') }}/roles/jellyfin/files/ansible/plays/kubernetes/jellyfin-minimal.yml\""
    },
    {
      "from": "dd358be7",
      "to": "f0bc548f",
      "relation": "references",
      "context": "- import_playbook: plays/setup-cluster.yaml"
    },
    {
      "from": "dd358be7",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "- import_playbook: plays/setup-cluster.yaml"
    },
    {
      "from": "dd358be7",
      "to": "bcfb5f43",
      "relation": "references",
      "context": "- import_playbook: plays/deploy-apps.yaml"
    },
    {
      "from": "dd358be7",
      "to": "bcfb5f43",
      "relation": "mentions",
      "context": "- import_playbook: plays/deploy-apps.yaml"
    },
    {
      "from": "dd358be7",
      "to": "2ac1c05f",
      "relation": "references",
      "context": "- import_playbook: plays/jellyfin.yml"
    },
    {
      "from": "dd358be7",
      "to": "2ac1c05f",
      "relation": "mentions",
      "context": "- import_playbook: plays/jellyfin.yml"
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "- `ansible/group_vars/` : global variables. `all.yml` is the primary file Ansible auto-loads."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "references",
      "context": "1. Read this file and `ansible/group_vars/all.yml` before proposing changes that affect deployment."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "1. Read this file and `ansible/group_vars/all.yml` before proposing changes that affect deployment."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "references",
      "context": "- `ansible/group_vars/all.yml` is authoritative for defaults; secrets must live in `ansible/group_vars/*.yml` encrypted by `ansible-vault` (e.g. `secrets.yml`)."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "- `ansible/group_vars/all.yml` is authoritative for defaults; secrets must live in `ansible/group_vars/*.yml` encrypted by `ansible-vault` (e.g. `secrets.yml`)."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "references",
      "context": "- Do not modify `ansible/group_vars/all.yml` to add plaintext passwords; instead propose vault edits."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "- Do not modify `ansible/group_vars/all.yml` to add plaintext passwords; instead propose vault edits."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "references",
      "context": "Hidden `ansible/group_vars/all.yml` entries (do NOT add secrets here)"
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "Hidden `ansible/group_vars/all.yml` entries (do NOT add secrets here)"
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "references",
      "context": "The user keeps several variables in `ansible/group_vars/all.yml` but hides secret values for safety. An agent should treat these keys as sensitive and never print their values in plaintext. Common keys present in the file include:"
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "The user keeps several variables in `ansible/group_vars/all.yml` but hides secret values for safety. An agent should treat these keys as sensitive and never print their values in plaintext. Common keys present in the file include:"
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "references",
      "context": "- Task: propose or create `ansible/group_vars/all.yml` defaults (allowed): modify only non-secret values and add comments."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "- Task: propose or create `ansible/group_vars/all.yml` defaults (allowed): modify only non-secret values and add comments."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "- Copy local `all.yml` template to control repo (if missing):"
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "Copy-Item .\\ansible\\group_vars\\all.yml.template .\\ansible\\group_vars\\all.yml -Force"
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "Copy-Item .\\ansible\\group_vars\\all.yml.template .\\ansible\\group_vars\\all.yml -Force"
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "notepad .\\ansible\\group_vars\\all.yml"
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "references",
      "context": "- Validate no duplicate/conflicting settings exist in `ansible/group_vars/all.yml` (e.g., multiple `enable_quay_metrics` values)."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "- Validate no duplicate/conflicting settings exist in `ansible/group_vars/all.yml` (e.g., multiple `enable_quay_metrics` values)."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "references",
      "context": "1. Check `ansible/group_vars/all.yml` for missing/contradictory vars."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "1. Check `ansible/group_vars/all.yml` for missing/contradictory vars."
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "references",
      "context": "- `ansible/group_vars/all.yml`"
    },
    {
      "from": "7b8171b6",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "- `ansible/group_vars/all.yml`"
    },
    {
      "from": "72c7835a",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "INVENTORY_FILE=\"ansible/inventory/hosts.yml\""
    },
    {
      "from": "72c7835a",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "INVENTORY_FILE=\"ansible/inventory/hosts.yml\""
    },
    {
      "from": "72c7835a",
      "to": "91932608",
      "relation": "references",
      "context": "VERIFICATION_PLAYBOOK=\"$PLAYBOOK_DIR/verify-cluster.yml\""
    },
    {
      "from": "72c7835a",
      "to": "91932608",
      "relation": "mentions",
      "context": "VERIFICATION_PLAYBOOK=\"$PLAYBOOK_DIR/verify-cluster.yml\""
    },
    {
      "from": "72c7835a",
      "to": "2e9ce2c1",
      "relation": "references",
      "context": "MAIN_BOOTSTRAP=\"$PLAYBOOK_DIR/cluster-bootstrap.yml\""
    },
    {
      "from": "72c7835a",
      "to": "2e9ce2c1",
      "relation": "mentions",
      "context": "MAIN_BOOTSTRAP=\"$PLAYBOOK_DIR/cluster-bootstrap.yml\""
    },
    {
      "from": "72c7835a",
      "to": "fa6b94bd",
      "relation": "references",
      "context": "SIMPLE_BOOTSTRAP=\"$PLAYBOOK_DIR/cluster-bootstrap/simple-bootstrap.yml\""
    },
    {
      "from": "72c7835a",
      "to": "fa6b94bd",
      "relation": "mentions",
      "context": "SIMPLE_BOOTSTRAP=\"$PLAYBOOK_DIR/cluster-bootstrap/simple-bootstrap.yml\""
    },
    {
      "from": "72c7835a",
      "to": "f0bc548f",
      "relation": "references",
      "context": "--simple            Use simple bootstrap (without existing setup-cluster.yaml)"
    },
    {
      "from": "72c7835a",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "--simple            Use simple bootstrap (without existing setup-cluster.yaml)"
    },
    {
      "from": "72c7835a",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "if [ -f \"scripts/validate_network_prerequisites.sh\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "ca8ac5f9",
      "relation": "mentions",
      "context": "if [ -f \"scripts/validate_network_prerequisites.sh\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "chmod +x scripts/validate_network_prerequisites.sh"
    },
    {
      "from": "72c7835a",
      "to": "ca8ac5f9",
      "relation": "mentions",
      "context": "chmod +x scripts/validate_network_prerequisites.sh"
    },
    {
      "from": "72c7835a",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "if scripts/validate_network_prerequisites.sh; then"
    },
    {
      "from": "72c7835a",
      "to": "ca8ac5f9",
      "relation": "mentions",
      "context": "if scripts/validate_network_prerequisites.sh; then"
    },
    {
      "from": "72c7835a",
      "to": "1970801b",
      "relation": "calls",
      "context": "\"scripts/fix_cni_bridge_conflict.sh\"          # Fix CNI bridge conflicts first"
    },
    {
      "from": "72c7835a",
      "to": "1970801b",
      "relation": "mentions",
      "context": "\"scripts/fix_cni_bridge_conflict.sh\"          # Fix CNI bridge conflicts first"
    },
    {
      "from": "72c7835a",
      "to": "a249d706",
      "relation": "calls",
      "context": "\"scripts/fix_homelab_node_issues.sh\"          # Fix homelab node networking"
    },
    {
      "from": "72c7835a",
      "to": "a249d706",
      "relation": "mentions",
      "context": "\"scripts/fix_homelab_node_issues.sh\"          # Fix homelab node networking"
    },
    {
      "from": "72c7835a",
      "to": "4f42b816",
      "relation": "calls",
      "context": "\"scripts/fix_cluster_dns_configuration.sh\"    # Fix cluster DNS"
    },
    {
      "from": "72c7835a",
      "to": "4f42b816",
      "relation": "mentions",
      "context": "\"scripts/fix_cluster_dns_configuration.sh\"    # Fix cluster DNS"
    },
    {
      "from": "72c7835a",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "\"scripts/setup_static_ips_and_dns.sh\"         # Setup static IPs"
    },
    {
      "from": "72c7835a",
      "to": "4f2a3f41",
      "relation": "mentions",
      "context": "\"scripts/setup_static_ips_and_dns.sh\"         # Setup static IPs"
    },
    {
      "from": "72c7835a",
      "to": "090d10b0",
      "relation": "calls",
      "context": "\"scripts/fix_remaining_pod_issues.sh\"         # Fix remaining issues"
    },
    {
      "from": "72c7835a",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "\"scripts/fix_remaining_pod_issues.sh\"         # Fix remaining issues"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "calls",
      "context": "if [ ! -f \"scripts/smoke-test.sh\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "mentions",
      "context": "if [ ! -f \"scripts/smoke-test.sh\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "calls",
      "context": "error \"Smoke test script not found: scripts/smoke-test.sh\""
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "mentions",
      "context": "error \"Smoke test script not found: scripts/smoke-test.sh\""
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "calls",
      "context": "if chmod +x scripts/smoke-test.sh && scripts/smoke-test.sh; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "calls",
      "context": "if chmod +x scripts/smoke-test.sh && scripts/smoke-test.sh; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "mentions",
      "context": "if chmod +x scripts/smoke-test.sh && scripts/smoke-test.sh; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "mentions",
      "context": "if chmod +x scripts/smoke-test.sh && scripts/smoke-test.sh; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "calls",
      "context": "if scp scripts/smoke-test.sh ${control_plane_user}@$control_plane_ip:/tmp/; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "mentions",
      "context": "if scp scripts/smoke-test.sh ${control_plane_user}@$control_plane_ip:/tmp/; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "mentions",
      "context": "if ssh ${control_plane_user}@$control_plane_ip \"chmod +x /tmp/smoke-test.sh && /tmp/smoke-test.sh\"; then"
    },
    {
      "from": "72c7835a",
      "to": "0f552648",
      "relation": "mentions",
      "context": "if ssh ${control_plane_user}@$control_plane_ip \"chmod +x /tmp/smoke-test.sh && /tmp/smoke-test.sh\"; then"
    },
    {
      "from": "72c7835a",
      "to": "80025501",
      "relation": "references",
      "context": "kubectl get daemonset kube-proxy -n kube-system -o yaml > \"$backup_dir/kube-proxy-daemonset.yaml\" 2>/dev/null || warn \"Failed to backup kube-proxy daemonset\""
    },
    {
      "from": "72c7835a",
      "to": "80025501",
      "relation": "mentions",
      "context": "kubectl get daemonset kube-proxy -n kube-system -o yaml > \"$backup_dir/kube-proxy-daemonset.yaml\" 2>/dev/null || warn \"Failed to backup kube-proxy daemonset\""
    },
    {
      "from": "72c7835a",
      "to": "2883003d",
      "relation": "references",
      "context": "kubectl get cm kube-proxy -n kube-system -o yaml > \"$backup_dir/kube-proxy-configmap.yaml\" 2>/dev/null || warn \"Failed to backup kube-proxy configmap\""
    },
    {
      "from": "72c7835a",
      "to": "2883003d",
      "relation": "mentions",
      "context": "kubectl get cm kube-proxy -n kube-system -o yaml > \"$backup_dir/kube-proxy-configmap.yaml\" 2>/dev/null || warn \"Failed to backup kube-proxy configmap\""
    },
    {
      "from": "72c7835a",
      "to": "81cd44f2",
      "relation": "references",
      "context": "kubectl get deployment coredns -n kube-system -o yaml > \"$backup_dir/coredns-deployment.yaml\" 2>/dev/null || warn \"Failed to backup coredns deployment\""
    },
    {
      "from": "72c7835a",
      "to": "81cd44f2",
      "relation": "mentions",
      "context": "kubectl get deployment coredns -n kube-system -o yaml > \"$backup_dir/coredns-deployment.yaml\" 2>/dev/null || warn \"Failed to backup coredns deployment\""
    },
    {
      "from": "72c7835a",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "kubectl get configmap coredns -n kube-system -o yaml > \"$backup_dir/coredns-configmap.yaml\" 2>/dev/null || warn \"Failed to backup coredns configmap\""
    },
    {
      "from": "72c7835a",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "kubectl get configmap coredns -n kube-system -o yaml > \"$backup_dir/coredns-configmap.yaml\" 2>/dev/null || warn \"Failed to backup coredns configmap\""
    },
    {
      "from": "72c7835a",
      "to": "ad0e321a",
      "relation": "references",
      "context": "kubectl get service kube-dns -n kube-system -o yaml > \"$backup_dir/coredns-service.yaml\" 2>/dev/null || warn \"Failed to backup coredns service\""
    },
    {
      "from": "72c7835a",
      "to": "ad0e321a",
      "relation": "mentions",
      "context": "kubectl get service kube-dns -n kube-system -o yaml > \"$backup_dir/coredns-service.yaml\" 2>/dev/null || warn \"Failed to backup coredns service\""
    },
    {
      "from": "72c7835a",
      "to": "2883003d",
      "relation": "references",
      "context": "\"manifests/network/kube-proxy-configmap.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "2883003d",
      "relation": "mentions",
      "context": "\"manifests/network/kube-proxy-configmap.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "80025501",
      "relation": "references",
      "context": "\"manifests/network/kube-proxy-daemonset.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "80025501",
      "relation": "mentions",
      "context": "\"manifests/network/kube-proxy-daemonset.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "\"manifests/network/coredns-configmap.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "\"manifests/network/coredns-configmap.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "ad0e321a",
      "relation": "references",
      "context": "\"manifests/network/coredns-service.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "ad0e321a",
      "relation": "mentions",
      "context": "\"manifests/network/coredns-service.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "81cd44f2",
      "relation": "references",
      "context": "\"manifests/network/coredns-deployment.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "81cd44f2",
      "relation": "mentions",
      "context": "\"manifests/network/coredns-deployment.yaml\""
    },
    {
      "from": "72c7835a",
      "to": "2883003d",
      "relation": "references",
      "context": "if [ -f \"$backup_dir/kube-proxy-configmap.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "2883003d",
      "relation": "mentions",
      "context": "if [ -f \"$backup_dir/kube-proxy-configmap.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "2883003d",
      "relation": "references",
      "context": "kubectl apply -f \"$backup_dir/kube-proxy-configmap.yaml\" || warn \"Failed to restore kube-proxy configmap\""
    },
    {
      "from": "72c7835a",
      "to": "2883003d",
      "relation": "mentions",
      "context": "kubectl apply -f \"$backup_dir/kube-proxy-configmap.yaml\" || warn \"Failed to restore kube-proxy configmap\""
    },
    {
      "from": "72c7835a",
      "to": "80025501",
      "relation": "references",
      "context": "if [ -f \"$backup_dir/kube-proxy-daemonset.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "80025501",
      "relation": "mentions",
      "context": "if [ -f \"$backup_dir/kube-proxy-daemonset.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "80025501",
      "relation": "references",
      "context": "kubectl apply -f \"$backup_dir/kube-proxy-daemonset.yaml\" || warn \"Failed to restore kube-proxy daemonset\""
    },
    {
      "from": "72c7835a",
      "to": "80025501",
      "relation": "mentions",
      "context": "kubectl apply -f \"$backup_dir/kube-proxy-daemonset.yaml\" || warn \"Failed to restore kube-proxy daemonset\""
    },
    {
      "from": "72c7835a",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "if [ -f \"$backup_dir/coredns-configmap.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "if [ -f \"$backup_dir/coredns-configmap.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "kubectl apply -f \"$backup_dir/coredns-configmap.yaml\" || warn \"Failed to restore coredns configmap\""
    },
    {
      "from": "72c7835a",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "kubectl apply -f \"$backup_dir/coredns-configmap.yaml\" || warn \"Failed to restore coredns configmap\""
    },
    {
      "from": "72c7835a",
      "to": "ad0e321a",
      "relation": "references",
      "context": "if [ -f \"$backup_dir/coredns-service.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "ad0e321a",
      "relation": "mentions",
      "context": "if [ -f \"$backup_dir/coredns-service.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "ad0e321a",
      "relation": "references",
      "context": "kubectl apply -f \"$backup_dir/coredns-service.yaml\" || warn \"Failed to restore coredns service\""
    },
    {
      "from": "72c7835a",
      "to": "ad0e321a",
      "relation": "mentions",
      "context": "kubectl apply -f \"$backup_dir/coredns-service.yaml\" || warn \"Failed to restore coredns service\""
    },
    {
      "from": "72c7835a",
      "to": "81cd44f2",
      "relation": "references",
      "context": "if [ -f \"$backup_dir/coredns-deployment.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "81cd44f2",
      "relation": "mentions",
      "context": "if [ -f \"$backup_dir/coredns-deployment.yaml\" ]; then"
    },
    {
      "from": "72c7835a",
      "to": "81cd44f2",
      "relation": "references",
      "context": "kubectl apply -f \"$backup_dir/coredns-deployment.yaml\" || warn \"Failed to restore coredns deployment\""
    },
    {
      "from": "72c7835a",
      "to": "81cd44f2",
      "relation": "mentions",
      "context": "kubectl apply -f \"$backup_dir/coredns-deployment.yaml\" || warn \"Failed to restore coredns deployment\""
    },
    {
      "from": "d7ce2015",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "INVENTORY_FILE=\"$ANSIBLE_DIR/inventory/hosts.yml\""
    },
    {
      "from": "d7ce2015",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "INVENTORY_FILE=\"$ANSIBLE_DIR/inventory/hosts.yml\""
    },
    {
      "from": "d7ce2015",
      "to": "ad0a12c7",
      "relation": "calls",
      "context": "local reset_script=\"$PROJECT_ROOT/scripts/reset_cni_bridge_minimal.sh\""
    },
    {
      "from": "d7ce2015",
      "to": "ad0a12c7",
      "relation": "mentions",
      "context": "local reset_script=\"$PROJECT_ROOT/scripts/reset_cni_bridge_minimal.sh\""
    },
    {
      "from": "d7ce2015",
      "to": "0a813314",
      "relation": "references",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/cni/flannel.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "0a813314",
      "relation": "mentions",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/cni/flannel.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "81cd44f2",
      "relation": "references",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/network/coredns-deployment.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "81cd44f2",
      "relation": "mentions",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/network/coredns-deployment.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/network/coredns-configmap.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/network/coredns-configmap.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "ad0e321a",
      "relation": "references",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/network/coredns-service.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "ad0e321a",
      "relation": "mentions",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/network/coredns-service.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "80025501",
      "relation": "references",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/network/kube-proxy-daemonset.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "80025501",
      "relation": "mentions",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/network/kube-proxy-daemonset.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "2883003d",
      "relation": "references",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/network/kube-proxy-configmap.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "2883003d",
      "relation": "mentions",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/network/kube-proxy-configmap.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "3d26bfd6",
      "relation": "references",
      "context": "\"$MANIFESTS_DIR/cni/flannel-minimal.yaml\""
    },
    {
      "from": "d7ce2015",
      "to": "3d26bfd6",
      "relation": "mentions",
      "context": "\"$MANIFESTS_DIR/cni/flannel-minimal.yaml\""
    },
    {
      "from": "d7ce2015",
      "to": "0aec39aa",
      "relation": "references",
      "context": "\"$MANIFESTS_DIR/network/kube-proxy-minimal.yaml\""
    },
    {
      "from": "d7ce2015",
      "to": "0aec39aa",
      "relation": "mentions",
      "context": "\"$MANIFESTS_DIR/network/kube-proxy-minimal.yaml\""
    },
    {
      "from": "d7ce2015",
      "to": "0d91b1c9",
      "relation": "references",
      "context": "\"$MANIFESTS_DIR/network/coredns-minimal.yaml\""
    },
    {
      "from": "d7ce2015",
      "to": "0d91b1c9",
      "relation": "mentions",
      "context": "\"$MANIFESTS_DIR/network/coredns-minimal.yaml\""
    },
    {
      "from": "d7ce2015",
      "to": "bcfb5f43",
      "relation": "references",
      "context": "if [ -f \"$ANSIBLE_DIR/plays/deploy-apps.yaml\" ]; then"
    },
    {
      "from": "d7ce2015",
      "to": "bcfb5f43",
      "relation": "mentions",
      "context": "if [ -f \"$ANSIBLE_DIR/plays/deploy-apps.yaml\" ]; then"
    },
    {
      "from": "d7ce2015",
      "to": "bcfb5f43",
      "relation": "references",
      "context": "ansible-playbook -i \"$INVENTORY_FILE\" \"$ANSIBLE_DIR/plays/deploy-apps.yaml\" || warn \"Monitoring deployment had issues\""
    },
    {
      "from": "d7ce2015",
      "to": "bcfb5f43",
      "relation": "mentions",
      "context": "ansible-playbook -i \"$INVENTORY_FILE\" \"$ANSIBLE_DIR/plays/deploy-apps.yaml\" || warn \"Monitoring deployment had issues\""
    },
    {
      "from": "d7ce2015",
      "to": "269791b0",
      "relation": "references",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/jellyfin/jellyfin.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "269791b0",
      "relation": "mentions",
      "context": "kubectl delete -f \"$MANIFESTS_DIR/jellyfin/jellyfin.yaml\" --ignore-not-found=true || true"
    },
    {
      "from": "d7ce2015",
      "to": "4ede3e1b",
      "relation": "references",
      "context": "kubectl apply -f \"$MANIFESTS_DIR/jellyfin/jellyfin-minimal.yaml\""
    },
    {
      "from": "d7ce2015",
      "to": "4ede3e1b",
      "relation": "mentions",
      "context": "kubectl apply -f \"$MANIFESTS_DIR/jellyfin/jellyfin-minimal.yaml\""
    },
    {
      "from": "7a6f331b",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "echo \"Fix: Run ./scripts/fix_worker_kubectl_config.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "2efbd35b",
      "relation": "mentions",
      "context": "echo \"Fix: Run ./scripts/fix_worker_kubectl_config.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"Fix: Run ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "echo \"Fix: Run ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "630db1ca",
      "relation": "calls",
      "context": "echo \"Fix: Run ./scripts/fix_iptables_compatibility.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "630db1ca",
      "relation": "mentions",
      "context": "echo \"Fix: Run ./scripts/fix_iptables_compatibility.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "echo \"Fix: Run ./scripts/fix_cluster_communication.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "echo \"Fix: Run ./scripts/fix_cluster_communication.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "1970801b",
      "relation": "calls",
      "context": "echo \"  1. Run the CNI bridge fix: sudo ./scripts/fix_cni_bridge_conflict.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "1970801b",
      "relation": "mentions",
      "context": "echo \"  1. Run the CNI bridge fix: sudo ./scripts/fix_cni_bridge_conflict.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "echo \"1. Complete cluster communication fix: ./scripts/fix_cluster_communication.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "echo \"1. Complete cluster communication fix: ./scripts/fix_cluster_communication.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "echo \"2. kubectl configuration fix: ./scripts/fix_worker_kubectl_config.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "2efbd35b",
      "relation": "mentions",
      "context": "echo \"2. kubectl configuration fix: ./scripts/fix_worker_kubectl_config.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "630db1ca",
      "relation": "calls",
      "context": "echo \"3. iptables compatibility fix: ./scripts/fix_iptables_compatibility.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "630db1ca",
      "relation": "mentions",
      "context": "echo \"3. iptables compatibility fix: ./scripts/fix_iptables_compatibility.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "1970801b",
      "relation": "calls",
      "context": "echo \"4. CNI Bridge Fix: ./scripts/fix_cni_bridge_conflict.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "1970801b",
      "relation": "mentions",
      "context": "echo \"4. CNI Bridge Fix: ./scripts/fix_cni_bridge_conflict.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"5. kube-proxy and pod issues: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "echo \"5. kube-proxy and pod issues: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "7c606adf",
      "relation": "calls",
      "context": "echo \"6. Cluster validation: ./scripts/validate_cluster_communication.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "echo \"6. Cluster validation: ./scripts/validate_cluster_communication.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "0d0c4a60",
      "relation": "calls",
      "context": "echo \"7. Enhanced Jellyfin Fix: ./fix_jellyfin_readiness.sh\""
    },
    {
      "from": "7a6f331b",
      "to": "0d0c4a60",
      "relation": "mentions",
      "context": "echo \"7. Enhanced Jellyfin Fix: ./fix_jellyfin_readiness.sh\""
    },
    {
      "from": "8c19061a",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"kubeadm join 192.168.4.63:6443 --token abc.123 --discovery-token-ca-cert-hash sha256:xyz\""
    },
    {
      "from": "8c19061a",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"kubeadm join 192.168.4.63:6443 --token abc.123 --discovery-token-ca-cert-hash sha256:xyz\""
    },
    {
      "from": "8c19061a",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "- `scripts/enhanced_kubeadm_join.sh` - Added aggressive_node_reset() function and enhanced cleanup_failed_join()"
    },
    {
      "from": "8c19061a",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "- `scripts/enhanced_kubeadm_join.sh` - Added aggressive_node_reset() function and enhanced cleanup_failed_join()"
    },
    {
      "from": "094a0932",
      "to": "7fb7d1a5",
      "relation": "calls",
      "context": "2. **Manual diagnosis**: `./scripts/check_cni_bridge_conflict.sh`"
    },
    {
      "from": "094a0932",
      "to": "7fb7d1a5",
      "relation": "mentions",
      "context": "2. **Manual diagnosis**: `./scripts/check_cni_bridge_conflict.sh`"
    },
    {
      "from": "094a0932",
      "to": "1970801b",
      "relation": "calls",
      "context": "3. **Manual fix**: `./scripts/fix_cni_bridge_conflict.sh`"
    },
    {
      "from": "094a0932",
      "to": "1970801b",
      "relation": "mentions",
      "context": "3. **Manual fix**: `./scripts/fix_cni_bridge_conflict.sh`"
    },
    {
      "from": "094a0932",
      "to": "a249d706",
      "relation": "mentions",
      "context": "- **`fix_homelab_node_issues.sh`**: Now includes CNI bridge conflict detection as Step 0"
    },
    {
      "from": "094a0932",
      "to": "7fb7d1a5",
      "relation": "calls",
      "context": "./scripts/check_cni_bridge_conflict.sh"
    },
    {
      "from": "094a0932",
      "to": "7fb7d1a5",
      "relation": "mentions",
      "context": "./scripts/check_cni_bridge_conflict.sh"
    },
    {
      "from": "094a0932",
      "to": "1970801b",
      "relation": "calls",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "094a0932",
      "to": "1970801b",
      "relation": "mentions",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "094a0932",
      "to": "9553fda2",
      "relation": "calls",
      "context": "./scripts/test_cni_bridge_fix.sh"
    },
    {
      "from": "094a0932",
      "to": "9553fda2",
      "relation": "mentions",
      "context": "./scripts/test_cni_bridge_fix.sh"
    },
    {
      "from": "f0840b04",
      "to": "7a273252",
      "relation": "calls",
      "context": "sudo ./scripts/reset_cni_bridge.sh"
    },
    {
      "from": "f0840b04",
      "to": "7a273252",
      "relation": "mentions",
      "context": "sudo ./scripts/reset_cni_bridge.sh"
    },
    {
      "from": "f0840b04",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "- Run `./scripts/validate_network_prerequisites.sh` first to detect issues"
    },
    {
      "from": "f0840b04",
      "to": "ca8ac5f9",
      "relation": "mentions",
      "context": "- Run `./scripts/validate_network_prerequisites.sh` first to detect issues"
    },
    {
      "from": "8facf1bd",
      "to": "81cd44f2",
      "relation": "references",
      "context": "### 1. `manifests/network/coredns-deployment.yaml`"
    },
    {
      "from": "8facf1bd",
      "to": "81cd44f2",
      "relation": "mentions",
      "context": "### 1. `manifests/network/coredns-deployment.yaml`"
    },
    {
      "from": "8facf1bd",
      "to": "a249d706",
      "relation": "calls",
      "context": "### 2. `scripts/fix_homelab_node_issues.sh`"
    },
    {
      "from": "8facf1bd",
      "to": "a249d706",
      "relation": "mentions",
      "context": "### 2. `scripts/fix_homelab_node_issues.sh`"
    },
    {
      "from": "8facf1bd",
      "to": "f0bc548f",
      "relation": "references",
      "context": "### 3. `ansible/plays/setup-cluster.yaml`"
    },
    {
      "from": "8facf1bd",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "### 3. `ansible/plays/setup-cluster.yaml`"
    },
    {
      "from": "fcb85303",
      "to": "36bd22c9",
      "relation": "calls",
      "context": "./scripts/check_coredns_status.sh"
    },
    {
      "from": "fcb85303",
      "to": "36bd22c9",
      "relation": "mentions",
      "context": "./scripts/check_coredns_status.sh"
    },
    {
      "from": "fcb85303",
      "to": "7071b363",
      "relation": "calls",
      "context": "./scripts/fix_coredns_unknown_status.sh"
    },
    {
      "from": "fcb85303",
      "to": "7071b363",
      "relation": "mentions",
      "context": "./scripts/fix_coredns_unknown_status.sh"
    },
    {
      "from": "fcb85303",
      "to": "7071b363",
      "relation": "calls",
      "context": "- `scripts/fix_coredns_unknown_status.sh` - Comprehensive fix script"
    },
    {
      "from": "fcb85303",
      "to": "7071b363",
      "relation": "mentions",
      "context": "- `scripts/fix_coredns_unknown_status.sh` - Comprehensive fix script"
    },
    {
      "from": "fcb85303",
      "to": "36bd22c9",
      "relation": "calls",
      "context": "- `scripts/check_coredns_status.sh` - Quick status checker"
    },
    {
      "from": "fcb85303",
      "to": "36bd22c9",
      "relation": "mentions",
      "context": "- `scripts/check_coredns_status.sh` - Quick status checker"
    },
    {
      "from": "fcb85303",
      "to": "f0bc548f",
      "relation": "references",
      "context": "- `ansible/plays/setup-cluster.yaml` - Enhanced cluster setup with CoreDNS validation"
    },
    {
      "from": "fcb85303",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "- `ansible/plays/setup-cluster.yaml` - Enhanced cluster setup with CoreDNS validation"
    },
    {
      "from": "0665ba5c",
      "to": "b428256d",
      "relation": "mentions",
      "context": "1. **`validate_join_prerequisites.sh`** - Comprehensive system validation"
    },
    {
      "from": "0665ba5c",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "2. **`enhanced_kubeadm_join.sh`** - Robust join process with monitoring"
    },
    {
      "from": "0665ba5c",
      "to": "b428256d",
      "relation": "calls",
      "context": "sudo ./scripts/validate_join_prerequisites.sh 192.168.4.63"
    },
    {
      "from": "0665ba5c",
      "to": "b428256d",
      "relation": "mentions",
      "context": "sudo ./scripts/validate_join_prerequisites.sh 192.168.4.63"
    },
    {
      "from": "0665ba5c",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh kubeadm join 192.168.4.63:6443 --token <token> --discovery-token-ca-cert-hash <hash>"
    },
    {
      "from": "0665ba5c",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh kubeadm join 192.168.4.63:6443 --token <token> --discovery-token-ca-cert-hash <hash>"
    },
    {
      "from": "0665ba5c",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"kubeadm join <master-ip>:6443 --token <token> --discovery-token-ca-cert-hash <hash>\""
    },
    {
      "from": "0665ba5c",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"kubeadm join <master-ip>:6443 --token <token> --discovery-token-ca-cert-hash <hash>\""
    },
    {
      "from": "20493e93",
      "to": "a249d706",
      "relation": "calls",
      "context": "**Problem**: After deployment, always had to manually run `./scripts/fix_homelab_node_issues.sh` because kube-proxy and kube-flannel pods were in CrashLoopBackOff."
    },
    {
      "from": "20493e93",
      "to": "a249d706",
      "relation": "mentions",
      "context": "**Problem**: After deployment, always had to manually run `./scripts/fix_homelab_node_issues.sh` because kube-proxy and kube-flannel pods were in CrashLoopBackOff."
    },
    {
      "from": "20493e93",
      "to": "72c7835a",
      "relation": "calls",
      "context": "**Result**: No more manual intervention needed after running `./deploy-cluster.sh deploy`"
    },
    {
      "from": "20493e93",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "**Result**: No more manual intervention needed after running `./deploy-cluster.sh deploy`"
    },
    {
      "from": "20493e93",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "20493e93",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "20493e93",
      "to": "1cd4e1f5",
      "relation": "calls",
      "context": "./scripts/validate_deployment_fixes.sh"
    },
    {
      "from": "20493e93",
      "to": "1cd4e1f5",
      "relation": "mentions",
      "context": "./scripts/validate_deployment_fixes.sh"
    },
    {
      "from": "20493e93",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "./scripts/validate_static_ips_and_dns.sh"
    },
    {
      "from": "20493e93",
      "to": "5e0bdb07",
      "relation": "mentions",
      "context": "./scripts/validate_static_ips_and_dns.sh"
    },
    {
      "from": "20493e93",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "20493e93",
      "to": "a249d706",
      "relation": "mentions",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "20493e93",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "1. `scripts/setup_static_ips_and_dns.sh` - Simplified DNS setup"
    },
    {
      "from": "20493e93",
      "to": "4f2a3f41",
      "relation": "mentions",
      "context": "1. `scripts/setup_static_ips_and_dns.sh` - Simplified DNS setup"
    },
    {
      "from": "20493e93",
      "to": "a249d706",
      "relation": "calls",
      "context": "2. `scripts/fix_homelab_node_issues.sh` - Added preventive checks"
    },
    {
      "from": "20493e93",
      "to": "a249d706",
      "relation": "mentions",
      "context": "2. `scripts/fix_homelab_node_issues.sh` - Added preventive checks"
    },
    {
      "from": "20493e93",
      "to": "24a2f90a",
      "relation": "references",
      "context": "3. `manifests/monitoring/grafana.yaml` - Fixed nodeSelector"
    },
    {
      "from": "20493e93",
      "to": "24a2f90a",
      "relation": "mentions",
      "context": "3. `manifests/monitoring/grafana.yaml` - Fixed nodeSelector"
    },
    {
      "from": "20493e93",
      "to": "1d93e806",
      "relation": "references",
      "context": "4. `manifests/monitoring/prometheus.yaml` - Fixed nodeSelector"
    },
    {
      "from": "20493e93",
      "to": "1d93e806",
      "relation": "mentions",
      "context": "4. `manifests/monitoring/prometheus.yaml` - Fixed nodeSelector"
    },
    {
      "from": "20493e93",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "5. `deploy-cluster.sh` - Reordered fix execution"
    },
    {
      "from": "20493e93",
      "to": "1cd4e1f5",
      "relation": "calls",
      "context": "6. `scripts/validate_deployment_fixes.sh` - New validation script"
    },
    {
      "from": "20493e93",
      "to": "1cd4e1f5",
      "relation": "mentions",
      "context": "6. `scripts/validate_deployment_fixes.sh` - New validation script"
    },
    {
      "from": "e8e06adf",
      "to": "a249d706",
      "relation": "calls",
      "context": "### 1. Enhanced Homelab Node Issue Fix (`scripts/fix_homelab_node_issues.sh`)"
    },
    {
      "from": "e8e06adf",
      "to": "a249d706",
      "relation": "mentions",
      "context": "### 1. Enhanced Homelab Node Issue Fix (`scripts/fix_homelab_node_issues.sh`)"
    },
    {
      "from": "e8e06adf",
      "to": "f0bc548f",
      "relation": "references",
      "context": "Updated `ansible/plays/setup-cluster.yaml` to:"
    },
    {
      "from": "e8e06adf",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "Updated `ansible/plays/setup-cluster.yaml` to:"
    },
    {
      "from": "e8e06adf",
      "to": "bcfb5f43",
      "relation": "references",
      "context": "Enhanced `ansible/plays/deploy-apps.yaml` to:"
    },
    {
      "from": "e8e06adf",
      "to": "bcfb5f43",
      "relation": "mentions",
      "context": "Enhanced `ansible/plays/deploy-apps.yaml` to:"
    },
    {
      "from": "e8e06adf",
      "to": "2ac1c05f",
      "relation": "references",
      "context": "Enhanced `ansible/plays/jellyfin.yml` to:"
    },
    {
      "from": "e8e06adf",
      "to": "2ac1c05f",
      "relation": "mentions",
      "context": "Enhanced `ansible/plays/jellyfin.yml` to:"
    },
    {
      "from": "e8e06adf",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "e8e06adf",
      "to": "a249d706",
      "relation": "mentions",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "e8e06adf",
      "to": "a249d706",
      "relation": "calls",
      "context": "- `scripts/fix_homelab_node_issues.sh` - Main homelab node issue remediation"
    },
    {
      "from": "e8e06adf",
      "to": "a249d706",
      "relation": "mentions",
      "context": "- `scripts/fix_homelab_node_issues.sh` - Main homelab node issue remediation"
    },
    {
      "from": "e8e06adf",
      "to": "7071b363",
      "relation": "calls",
      "context": "- `scripts/fix_coredns_unknown_status.sh` - CoreDNS-specific fixes"
    },
    {
      "from": "e8e06adf",
      "to": "7071b363",
      "relation": "mentions",
      "context": "- `scripts/fix_coredns_unknown_status.sh` - CoreDNS-specific fixes"
    },
    {
      "from": "e8e06adf",
      "to": "f0bc548f",
      "relation": "references",
      "context": "- `ansible/plays/setup-cluster.yaml` - Enhanced cluster setup with CoreDNS scheduling"
    },
    {
      "from": "e8e06adf",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "- `ansible/plays/setup-cluster.yaml` - Enhanced cluster setup with CoreDNS scheduling"
    },
    {
      "from": "e8e06adf",
      "to": "bcfb5f43",
      "relation": "references",
      "context": "- `ansible/plays/deploy-apps.yaml` - Improved application deployment"
    },
    {
      "from": "e8e06adf",
      "to": "bcfb5f43",
      "relation": "mentions",
      "context": "- `ansible/plays/deploy-apps.yaml` - Improved application deployment"
    },
    {
      "from": "e8e06adf",
      "to": "2ac1c05f",
      "relation": "references",
      "context": "- `ansible/plays/jellyfin.yml` - Enhanced Jellyfin deployment"
    },
    {
      "from": "e8e06adf",
      "to": "2ac1c05f",
      "relation": "mentions",
      "context": "- `ansible/plays/jellyfin.yml` - Enhanced Jellyfin deployment"
    },
    {
      "from": "13b742f3",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "./scripts/validate_network_prerequisites.sh"
    },
    {
      "from": "13b742f3",
      "to": "ca8ac5f9",
      "relation": "mentions",
      "context": "./scripts/validate_network_prerequisites.sh"
    },
    {
      "from": "13b742f3",
      "to": "1970801b",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "13b742f3",
      "to": "1970801b",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "13b742f3",
      "to": "a249d706",
      "relation": "calls",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "13b742f3",
      "to": "a249d706",
      "relation": "mentions",
      "context": "./scripts/fix_homelab_node_issues.sh"
    },
    {
      "from": "13b742f3",
      "to": "090d10b0",
      "relation": "calls",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "13b742f3",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "13b742f3",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "./scripts/validate_network_prerequisites.sh"
    },
    {
      "from": "13b742f3",
      "to": "ca8ac5f9",
      "relation": "mentions",
      "context": "./scripts/validate_network_prerequisites.sh"
    },
    {
      "from": "13b742f3",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh net-reset --confirm"
    },
    {
      "from": "13b742f3",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh net-reset --confirm"
    },
    {
      "from": "13b742f3",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh reset --force"
    },
    {
      "from": "13b742f3",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh reset --force"
    },
    {
      "from": "13b742f3",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "13b742f3",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "13b742f3",
      "to": "ca8ac5f9",
      "relation": "calls",
      "context": "- `scripts/validate_network_prerequisites.sh` - Pre-deployment validation"
    },
    {
      "from": "13b742f3",
      "to": "ca8ac5f9",
      "relation": "mentions",
      "context": "- `scripts/validate_network_prerequisites.sh` - Pre-deployment validation"
    },
    {
      "from": "13b742f3",
      "to": "1970801b",
      "relation": "calls",
      "context": "- `scripts/fix_cni_bridge_conflict.sh` - CNI bridge conflict resolution"
    },
    {
      "from": "13b742f3",
      "to": "1970801b",
      "relation": "mentions",
      "context": "- `scripts/fix_cni_bridge_conflict.sh` - CNI bridge conflict resolution"
    },
    {
      "from": "13b742f3",
      "to": "a249d706",
      "relation": "calls",
      "context": "- `scripts/fix_homelab_node_issues.sh` - Mixed OS compatibility fixes"
    },
    {
      "from": "13b742f3",
      "to": "a249d706",
      "relation": "mentions",
      "context": "- `scripts/fix_homelab_node_issues.sh` - Mixed OS compatibility fixes"
    },
    {
      "from": "2267ad9a",
      "to": "1ddb9838",
      "relation": "mentions",
      "context": "#### 1. `fix_kubelet_systemd_config.sh`"
    },
    {
      "from": "2267ad9a",
      "to": "1ddb9838",
      "relation": "calls",
      "context": "sudo ./scripts/fix_kubelet_systemd_config.sh"
    },
    {
      "from": "2267ad9a",
      "to": "1ddb9838",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_kubelet_systemd_config.sh"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "#### 2. `validate_systemd_dropins.sh`"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "./scripts/validate_systemd_dropins.sh validate kubelet"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "./scripts/validate_systemd_dropins.sh validate kubelet"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "./scripts/validate_systemd_dropins.sh fix kubelet"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "./scripts/validate_systemd_dropins.sh fix kubelet"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "./scripts/validate_systemd_dropins.sh ensure-join 192.168.4.63"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "./scripts/validate_systemd_dropins.sh ensure-join 192.168.4.63"
    },
    {
      "from": "2267ad9a",
      "to": "1ddb9838",
      "relation": "calls",
      "context": "sudo ./scripts/fix_kubelet_systemd_config.sh"
    },
    {
      "from": "2267ad9a",
      "to": "1ddb9838",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_kubelet_systemd_config.sh"
    },
    {
      "from": "2267ad9a",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<join-command>\""
    },
    {
      "from": "2267ad9a",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<join-command>\""
    },
    {
      "from": "2267ad9a",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "- `scripts/enhanced_kubeadm_join.sh` - Added systemd validation"
    },
    {
      "from": "2267ad9a",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "- `scripts/enhanced_kubeadm_join.sh` - Added systemd validation"
    },
    {
      "from": "2267ad9a",
      "to": "f0bc548f",
      "relation": "references",
      "context": "- `ansible/plays/setup-cluster.yaml` - Added pre-join systemd fixes"
    },
    {
      "from": "2267ad9a",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "- `ansible/plays/setup-cluster.yaml` - Added pre-join systemd fixes"
    },
    {
      "from": "2267ad9a",
      "to": "1ddb9838",
      "relation": "calls",
      "context": "- `scripts/fix_kubelet_systemd_config.sh` - New comprehensive fix script"
    },
    {
      "from": "2267ad9a",
      "to": "1ddb9838",
      "relation": "mentions",
      "context": "- `scripts/fix_kubelet_systemd_config.sh` - New comprehensive fix script"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "calls",
      "context": "- `scripts/validate_systemd_dropins.sh` - New validation script"
    },
    {
      "from": "2267ad9a",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "- `scripts/validate_systemd_dropins.sh` - New validation script"
    },
    {
      "from": "121dca56",
      "to": "1b255a42",
      "relation": "calls",
      "context": "sudo ./scripts/quick_join_diagnostics.sh"
    },
    {
      "from": "121dca56",
      "to": "1b255a42",
      "relation": "mentions",
      "context": "sudo ./scripts/quick_join_diagnostics.sh"
    },
    {
      "from": "121dca56",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh"
    },
    {
      "from": "121dca56",
      "to": "b3dc3aae",
      "relation": "mentions",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh"
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<your-original-join-command>\""
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<your-original-join-command>\""
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<new-join-command>\""
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<new-join-command>\""
    },
    {
      "from": "121dca56",
      "to": "21313a3e",
      "relation": "calls",
      "context": "sudo ./scripts/manual_containerd_filesystem_fix.sh"
    },
    {
      "from": "121dca56",
      "to": "21313a3e",
      "relation": "mentions",
      "context": "sudo ./scripts/manual_containerd_filesystem_fix.sh"
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<new-join-command>\""
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<new-join-command>\""
    },
    {
      "from": "121dca56",
      "to": "1b255a42",
      "relation": "calls",
      "context": "sudo ./scripts/quick_join_diagnostics.sh [CONTROL_PLANE_IP]"
    },
    {
      "from": "121dca56",
      "to": "1b255a42",
      "relation": "mentions",
      "context": "sudo ./scripts/quick_join_diagnostics.sh [CONTROL_PLANE_IP]"
    },
    {
      "from": "121dca56",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh [CONTROL_PLANE_IP] [WORKER_IPS] [OUTPUT_DIR]"
    },
    {
      "from": "121dca56",
      "to": "b3dc3aae",
      "relation": "mentions",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh [CONTROL_PLANE_IP] [WORKER_IPS] [OUTPUT_DIR]"
    },
    {
      "from": "121dca56",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh 192.168.4.63 \"192.168.4.61,192.168.4.62\""
    },
    {
      "from": "121dca56",
      "to": "b3dc3aae",
      "relation": "mentions",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh 192.168.4.63 \"192.168.4.61,192.168.4.62\""
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<join-command>\""
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<join-command>\""
    },
    {
      "from": "121dca56",
      "to": "1b255a42",
      "relation": "calls",
      "context": "sudo ./scripts/quick_join_diagnostics.sh"
    },
    {
      "from": "121dca56",
      "to": "1b255a42",
      "relation": "mentions",
      "context": "sudo ./scripts/quick_join_diagnostics.sh"
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<join-command>\""
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"<join-command>\""
    },
    {
      "from": "121dca56",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh"
    },
    {
      "from": "121dca56",
      "to": "b3dc3aae",
      "relation": "mentions",
      "context": "sudo ./scripts/gather_worker_diagnostics.sh"
    },
    {
      "from": "121dca56",
      "to": "0665ba5c",
      "relation": "references",
      "context": "- [Enhanced Join Process Details](./ENHANCED_JOIN_PROCESS.md)"
    },
    {
      "from": "121dca56",
      "to": "0665ba5c",
      "relation": "mentions",
      "context": "- [Enhanced Join Process Details](./ENHANCED_JOIN_PROCESS.md)"
    },
    {
      "from": "121dca56",
      "to": "044d8702",
      "relation": "references",
      "context": "- [Post-Wipe Worker Join Process](./POST_WIPE_WORKER_JOIN.md)"
    },
    {
      "from": "121dca56",
      "to": "044d8702",
      "relation": "mentions",
      "context": "- [Post-Wipe Worker Join Process](./POST_WIPE_WORKER_JOIN.md)"
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh <join-command>"
    },
    {
      "from": "121dca56",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh <join-command>"
    },
    {
      "from": "121dca56",
      "to": "0665ba5c",
      "relation": "references",
      "context": "**For complete resolution, see:** [Enhanced Join Process Documentation](./ENHANCED_JOIN_PROCESS.md)"
    },
    {
      "from": "121dca56",
      "to": "0665ba5c",
      "relation": "mentions",
      "context": "**For complete resolution, see:** [Enhanced Join Process Documentation](./ENHANCED_JOIN_PROCESS.md)"
    },
    {
      "from": "121dca56",
      "to": "3e468245",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "121dca56",
      "to": "3e468245",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "121dca56",
      "to": "53177eca",
      "relation": "references",
      "context": "1. Check the [RHEL 10 Troubleshooting Guide](RHEL10_TROUBLESHOOTING.md) for OS-specific issues"
    },
    {
      "from": "121dca56",
      "to": "53177eca",
      "relation": "mentions",
      "context": "1. Check the [RHEL 10 Troubleshooting Guide](RHEL10_TROUBLESHOOTING.md) for OS-specific issues"
    },
    {
      "from": "044d8702",
      "to": "0ecf3a99",
      "relation": "calls",
      "context": "./generate_join_command.sh"
    },
    {
      "from": "044d8702",
      "to": "0ecf3a99",
      "relation": "mentions",
      "context": "./generate_join_command.sh"
    },
    {
      "from": "044d8702",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"kubeadm join 192.168.4.63:6443 --token <token> --discovery-token-ca-cert-hash <hash>\""
    },
    {
      "from": "044d8702",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"kubeadm join 192.168.4.63:6443 --token <token> --discovery-token-ca-cert-hash <hash>\""
    },
    {
      "from": "044d8702",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"kubeadm join ...\""
    },
    {
      "from": "044d8702",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "sudo ./scripts/enhanced_kubeadm_join.sh \"kubeadm join ...\""
    },
    {
      "from": "e7010005",
      "to": "28982728",
      "relation": "references",
      "context": "- [T3500 NAS Server](./devices/T3500_NAS.md)"
    },
    {
      "from": "e7010005",
      "to": "28982728",
      "relation": "mentions",
      "context": "- [T3500 NAS Server](./devices/T3500_NAS.md)"
    },
    {
      "from": "e7010005",
      "to": "37f609a0",
      "relation": "references",
      "context": "- [R430 Compute Engine](./devices/R430_Compute.md)"
    },
    {
      "from": "e7010005",
      "to": "37f609a0",
      "relation": "mentions",
      "context": "- [R430 Compute Engine](./devices/R430_Compute.md)"
    },
    {
      "from": "e7010005",
      "to": "378a3809",
      "relation": "references",
      "context": "- [Catalyst 3650V02 Managed Switch](./devices/Catalyst3650V02_Switch.md)"
    },
    {
      "from": "e7010005",
      "to": "378a3809",
      "relation": "mentions",
      "context": "- [Catalyst 3650V02 Managed Switch](./devices/Catalyst3650V02_Switch.md)"
    },
    {
      "from": "e7010005",
      "to": "76046802",
      "relation": "references",
      "context": "- [Security & Firewall](../security/firewall.md)"
    },
    {
      "from": "e7010005",
      "to": "76046802",
      "relation": "mentions",
      "context": "- [Security & Firewall](../security/firewall.md)"
    },
    {
      "from": "53177eca",
      "to": "3e468245",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "53177eca",
      "to": "3e468245",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "53177eca",
      "to": "3e468245",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml -v"
    },
    {
      "from": "53177eca",
      "to": "3e468245",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml -v"
    },
    {
      "from": "53177eca",
      "to": "3e468245",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "53177eca",
      "to": "3e468245",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/kubernetes/rhel10_setup_fixes.yaml"
    },
    {
      "from": "88482a18",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "**Solution**: Run `./scripts/fix_worker_kubectl_config.sh`"
    },
    {
      "from": "88482a18",
      "to": "2efbd35b",
      "relation": "mentions",
      "context": "**Solution**: Run `./scripts/fix_worker_kubectl_config.sh`"
    },
    {
      "from": "88482a18",
      "to": "090d10b0",
      "relation": "calls",
      "context": "**Solution**: Enhanced `./scripts/fix_remaining_pod_issues.sh` with iptables compatibility fixes"
    },
    {
      "from": "88482a18",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "**Solution**: Enhanced `./scripts/fix_remaining_pod_issues.sh` with iptables compatibility fixes"
    },
    {
      "from": "88482a18",
      "to": "630db1ca",
      "relation": "calls",
      "context": "**Solution**: Run `./scripts/fix_iptables_compatibility.sh`"
    },
    {
      "from": "88482a18",
      "to": "630db1ca",
      "relation": "mentions",
      "context": "**Solution**: Run `./scripts/fix_iptables_compatibility.sh`"
    },
    {
      "from": "88482a18",
      "to": "1970801b",
      "relation": "calls",
      "context": "**Solution**: Existing `./scripts/fix_cni_bridge_conflict.sh` (enhanced)"
    },
    {
      "from": "88482a18",
      "to": "1970801b",
      "relation": "mentions",
      "context": "**Solution**: Existing `./scripts/fix_cni_bridge_conflict.sh` (enhanced)"
    },
    {
      "from": "88482a18",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "- **File**: `./scripts/fix_cluster_communication.sh`"
    },
    {
      "from": "88482a18",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "- **File**: `./scripts/fix_cluster_communication.sh`"
    },
    {
      "from": "88482a18",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "- **File**: `./scripts/fix_worker_kubectl_config.sh`"
    },
    {
      "from": "88482a18",
      "to": "2efbd35b",
      "relation": "mentions",
      "context": "- **File**: `./scripts/fix_worker_kubectl_config.sh`"
    },
    {
      "from": "88482a18",
      "to": "630db1ca",
      "relation": "calls",
      "context": "- **File**: `./scripts/fix_iptables_compatibility.sh`"
    },
    {
      "from": "88482a18",
      "to": "630db1ca",
      "relation": "mentions",
      "context": "- **File**: `./scripts/fix_iptables_compatibility.sh`"
    },
    {
      "from": "88482a18",
      "to": "7c606adf",
      "relation": "calls",
      "context": "- **File**: `./scripts/validate_cluster_communication.sh`"
    },
    {
      "from": "88482a18",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "- **File**: `./scripts/validate_cluster_communication.sh`"
    },
    {
      "from": "88482a18",
      "to": "090d10b0",
      "relation": "calls",
      "context": "- **File**: `./scripts/fix_remaining_pod_issues.sh` (enhanced)"
    },
    {
      "from": "88482a18",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "- **File**: `./scripts/fix_remaining_pod_issues.sh` (enhanced)"
    },
    {
      "from": "88482a18",
      "to": "7a6f331b",
      "relation": "calls",
      "context": "- **File**: `./diagnose_jellyfin_network.sh` (enhanced)"
    },
    {
      "from": "88482a18",
      "to": "7a6f331b",
      "relation": "mentions",
      "context": "- **File**: `./diagnose_jellyfin_network.sh` (enhanced)"
    },
    {
      "from": "88482a18",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "88482a18",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "88482a18",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "sudo ./scripts/fix_worker_kubectl_config.sh"
    },
    {
      "from": "88482a18",
      "to": "2efbd35b",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_worker_kubectl_config.sh"
    },
    {
      "from": "88482a18",
      "to": "630db1ca",
      "relation": "calls",
      "context": "sudo ./scripts/fix_iptables_compatibility.sh"
    },
    {
      "from": "88482a18",
      "to": "630db1ca",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_iptables_compatibility.sh"
    },
    {
      "from": "88482a18",
      "to": "090d10b0",
      "relation": "calls",
      "context": "sudo ./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "88482a18",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "88482a18",
      "to": "7c606adf",
      "relation": "calls",
      "context": "./scripts/validate_cluster_communication.sh"
    },
    {
      "from": "88482a18",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "./scripts/validate_cluster_communication.sh"
    },
    {
      "from": "88482a18",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "- `scripts/fix_worker_kubectl_config.sh` - kubectl configuration fix"
    },
    {
      "from": "88482a18",
      "to": "2efbd35b",
      "relation": "mentions",
      "context": "- `scripts/fix_worker_kubectl_config.sh` - kubectl configuration fix"
    },
    {
      "from": "88482a18",
      "to": "630db1ca",
      "relation": "calls",
      "context": "- `scripts/fix_iptables_compatibility.sh` - iptables compatibility fix"
    },
    {
      "from": "88482a18",
      "to": "630db1ca",
      "relation": "mentions",
      "context": "- `scripts/fix_iptables_compatibility.sh` - iptables compatibility fix"
    },
    {
      "from": "88482a18",
      "to": "7c606adf",
      "relation": "calls",
      "context": "- `scripts/validate_cluster_communication.sh` - comprehensive validation"
    },
    {
      "from": "88482a18",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "- `scripts/validate_cluster_communication.sh` - comprehensive validation"
    },
    {
      "from": "88482a18",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "- `scripts/fix_cluster_communication.sh` - master fix orchestrator"
    },
    {
      "from": "88482a18",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "- `scripts/fix_cluster_communication.sh` - master fix orchestrator"
    },
    {
      "from": "88482a18",
      "to": "090d10b0",
      "relation": "calls",
      "context": "- `scripts/fix_remaining_pod_issues.sh` - added iptables compatibility"
    },
    {
      "from": "88482a18",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "- `scripts/fix_remaining_pod_issues.sh` - added iptables compatibility"
    },
    {
      "from": "88482a18",
      "to": "7a6f331b",
      "relation": "mentions",
      "context": "- `diagnose_jellyfin_network.sh` - added specific issue checks and new fix references"
    },
    {
      "from": "797dca0d",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "### 1. Worker Node CNI Fix (`scripts/fix_worker_node_cni.sh`)"
    },
    {
      "from": "797dca0d",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "### 1. Worker Node CNI Fix (`scripts/fix_worker_node_cni.sh`)"
    },
    {
      "from": "797dca0d",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "sudo ./scripts/fix_worker_node_cni.sh --node storagenodet3500"
    },
    {
      "from": "797dca0d",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_worker_node_cni.sh --node storagenodet3500"
    },
    {
      "from": "797dca0d",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "### 2. Pod Connectivity Validation (`scripts/validate_pod_connectivity.sh`)"
    },
    {
      "from": "797dca0d",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "### 2. Pod Connectivity Validation (`scripts/validate_pod_connectivity.sh`)"
    },
    {
      "from": "797dca0d",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "./scripts/validate_pod_connectivity.sh"
    },
    {
      "from": "797dca0d",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "./scripts/validate_pod_connectivity.sh"
    },
    {
      "from": "797dca0d",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "### 3. Flannel Mixed-OS Fix (`scripts/fix_flannel_mixed_os.sh`)"
    },
    {
      "from": "797dca0d",
      "to": "263c2cb9",
      "relation": "mentions",
      "context": "### 3. Flannel Mixed-OS Fix (`scripts/fix_flannel_mixed_os.sh`)"
    },
    {
      "from": "797dca0d",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "./scripts/fix_flannel_mixed_os.sh"
    },
    {
      "from": "797dca0d",
      "to": "263c2cb9",
      "relation": "mentions",
      "context": "./scripts/fix_flannel_mixed_os.sh"
    },
    {
      "from": "797dca0d",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "The main `scripts/fix_cluster_communication.sh` now includes:"
    },
    {
      "from": "797dca0d",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "The main `scripts/fix_cluster_communication.sh` now includes:"
    },
    {
      "from": "797dca0d",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "797dca0d",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "797dca0d",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "sudo ./scripts/fix_worker_node_cni.sh --node storagenodet3500"
    },
    {
      "from": "797dca0d",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_worker_node_cni.sh --node storagenodet3500"
    },
    {
      "from": "797dca0d",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "./scripts/fix_flannel_mixed_os.sh"
    },
    {
      "from": "797dca0d",
      "to": "263c2cb9",
      "relation": "mentions",
      "context": "./scripts/fix_flannel_mixed_os.sh"
    },
    {
      "from": "797dca0d",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "./scripts/validate_pod_connectivity.sh"
    },
    {
      "from": "797dca0d",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "./scripts/validate_pod_connectivity.sh"
    },
    {
      "from": "797dca0d",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "- `scripts/fix_worker_node_cni.sh` - Worker node specific CNI fixes"
    },
    {
      "from": "797dca0d",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "- `scripts/fix_worker_node_cni.sh` - Worker node specific CNI fixes"
    },
    {
      "from": "797dca0d",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "- `scripts/validate_pod_connectivity.sh` - Problem statement scenario validation"
    },
    {
      "from": "797dca0d",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "- `scripts/validate_pod_connectivity.sh` - Problem statement scenario validation"
    },
    {
      "from": "797dca0d",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "- `scripts/fix_flannel_mixed_os.sh` - Flannel configuration optimization"
    },
    {
      "from": "797dca0d",
      "to": "263c2cb9",
      "relation": "mentions",
      "context": "- `scripts/fix_flannel_mixed_os.sh` - Flannel configuration optimization"
    },
    {
      "from": "797dca0d",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "- `scripts/fix_cluster_communication.sh` - Enhanced with new fixes"
    },
    {
      "from": "797dca0d",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "- `scripts/fix_cluster_communication.sh` - Enhanced with new fixes"
    },
    {
      "from": "f087a852",
      "to": "4f42b816",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cluster_dns_configuration.sh"
    },
    {
      "from": "f087a852",
      "to": "4f42b816",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_cluster_dns_configuration.sh"
    },
    {
      "from": "f087a852",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "f087a852",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "f087a852",
      "to": "4f42b816",
      "relation": "mentions",
      "context": "The script `fix_cluster_dns_configuration.sh` is now run as part of post-deployment fixes."
    },
    {
      "from": "f087a852",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "- Included in `deploy-cluster.sh` as part of post-deployment fixes"
    },
    {
      "from": "e00aacd0",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "- `deploy-cluster.sh`: Added pod health checks before jellyfin deletion"
    },
    {
      "from": "e00aacd0",
      "to": "1970801b",
      "relation": "calls",
      "context": "- `scripts/fix_cni_bridge_conflict.sh`: Replaced aggressive flannel deletion with intelligent restart logic"
    },
    {
      "from": "e00aacd0",
      "to": "1970801b",
      "relation": "mentions",
      "context": "- `scripts/fix_cni_bridge_conflict.sh`: Replaced aggressive flannel deletion with intelligent restart logic"
    },
    {
      "from": "e00aacd0",
      "to": "090d10b0",
      "relation": "calls",
      "context": "- `scripts/fix_remaining_pod_issues.sh`: Added conditional jellyfin pod handling"
    },
    {
      "from": "e00aacd0",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "- `scripts/fix_remaining_pod_issues.sh`: Added conditional jellyfin pod handling"
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "## fix_cluster_communication.sh  Enhanced Cluster communication fix orchestration"
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "Path: `scripts/fix_cluster_communication.sh`"
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "Path: `scripts/fix_cluster_communication.sh`"
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "This document describes what `fix_cluster_communication.sh` does, its prerequisites, safe usage, the individual fix steps it orchestrates, how to verify the cluster after running it, and common troubleshooting steps. **This script has been enhanced to specifically address the networking issues described in GitHub issues, including pod-to-pod communication failures, DNS resolution problems, and NodePort accessibility issues.**"
    },
    {
      "from": "0eb8fc77",
      "to": "7c606adf",
      "relation": "calls",
      "context": "- `./scripts/validate_cluster_communication.sh`  comprehensive cluster validation"
    },
    {
      "from": "0eb8fc77",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "- `./scripts/validate_cluster_communication.sh`  comprehensive cluster validation"
    },
    {
      "from": "0eb8fc77",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "- `./scripts/validate_pod_connectivity.sh`  detailed pod networking tests"
    },
    {
      "from": "0eb8fc77",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "- `./scripts/validate_pod_connectivity.sh`  detailed pod networking tests"
    },
    {
      "from": "0eb8fc77",
      "to": "630db1ca",
      "relation": "mentions",
      "context": "2. `fix_iptables_compatibility.sh`  attempts to resolve iptables / nftables compatibility issues that prevent correct NAT rules, with enhanced backend switching logic"
    },
    {
      "from": "0eb8fc77",
      "to": "1970801b",
      "relation": "mentions",
      "context": "3. `fix_cni_bridge_conflict.sh`  resolves common CNI bridge conflicts (cni0 vs flannel devices, stale bridge interfaces, etc.), with enhanced IP validation and forced recreation"
    },
    {
      "from": "0eb8fc77",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "4. `fix_worker_node_cni.sh`  specifically targets worker node CNI communication issues"
    },
    {
      "from": "0eb8fc77",
      "to": "263c2cb9",
      "relation": "mentions",
      "context": "5. `fix_flannel_mixed_os.sh`  handles Flannel configuration for mixed OS environments"
    },
    {
      "from": "0eb8fc77",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "6. `fix_remaining_pod_issues.sh`  targets kube-proxy and pods that are CrashLoopBackOff or failing readiness/startup probes, with enhanced pod recreation logic"
    },
    {
      "from": "0eb8fc77",
      "to": "2efbd35b",
      "relation": "mentions",
      "context": "7. `fix_worker_kubectl_config.sh`  repairs kubeconfig/kubectl issues on worker nodes (if present)"
    },
    {
      "from": "0eb8fc77",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "9. `validate_cluster_communication.sh`  runs a final validation pass (checks nodes, key pods, and NodePort reachability)"
    },
    {
      "from": "0eb8fc77",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "10. `validate_pod_connectivity.sh`  detailed pod connectivity validation"
    },
    {
      "from": "0eb8fc77",
      "to": "09f42a5d",
      "relation": "mentions",
      "context": "11. `fix_nodeport_external_access.sh`  specifically addresses NodePort accessibility issues including Jellyfin port 30096"
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "sudo bash scripts/fix_cluster_communication.sh"
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "sudo bash scripts/fix_cluster_communication.sh"
    },
    {
      "from": "0eb8fc77",
      "to": "630db1ca",
      "relation": "mentions",
      "context": "- Review the helper scripts in `scripts/` (`fix_iptables_compatibility.sh`, `fix_cni_bridge_conflict.sh`, `fix_remaining_pod_issues.sh`, `fix_worker_kubectl_config.sh`, `validate_cluster_communication.sh`) before running the orchestration."
    },
    {
      "from": "0eb8fc77",
      "to": "1970801b",
      "relation": "mentions",
      "context": "- Review the helper scripts in `scripts/` (`fix_iptables_compatibility.sh`, `fix_cni_bridge_conflict.sh`, `fix_remaining_pod_issues.sh`, `fix_worker_kubectl_config.sh`, `validate_cluster_communication.sh`) before running the orchestration."
    },
    {
      "from": "0eb8fc77",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "- Review the helper scripts in `scripts/` (`fix_iptables_compatibility.sh`, `fix_cni_bridge_conflict.sh`, `fix_remaining_pod_issues.sh`, `fix_worker_kubectl_config.sh`, `validate_cluster_communication.sh`) before running the orchestration."
    },
    {
      "from": "0eb8fc77",
      "to": "2efbd35b",
      "relation": "mentions",
      "context": "- Review the helper scripts in `scripts/` (`fix_iptables_compatibility.sh`, `fix_cni_bridge_conflict.sh`, `fix_remaining_pod_issues.sh`, `fix_worker_kubectl_config.sh`, `validate_cluster_communication.sh`) before running the orchestration."
    },
    {
      "from": "0eb8fc77",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "- Review the helper scripts in `scripts/` (`fix_iptables_compatibility.sh`, `fix_cni_bridge_conflict.sh`, `fix_remaining_pod_issues.sh`, `fix_worker_kubectl_config.sh`, `validate_cluster_communication.sh`) before running the orchestration."
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "- 2025-09-12: Created documentation for `scripts/fix_cluster_communication.sh`."
    },
    {
      "from": "0eb8fc77",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "- 2025-09-12: Created documentation for `scripts/fix_cluster_communication.sh`."
    },
    {
      "from": "207b4d0f",
      "to": "ca8597d0",
      "relation": "references",
      "context": "**File**: `ansible/plays/kubernetes/deploy_monitoring.yaml`"
    },
    {
      "from": "207b4d0f",
      "to": "ca8597d0",
      "relation": "mentions",
      "context": "**File**: `ansible/plays/kubernetes/deploy_monitoring.yaml`"
    },
    {
      "from": "207b4d0f",
      "to": "ca8597d0",
      "relation": "references",
      "context": "**File**: `ansible/plays/kubernetes/deploy_monitoring.yaml`"
    },
    {
      "from": "207b4d0f",
      "to": "ca8597d0",
      "relation": "mentions",
      "context": "**File**: `ansible/plays/kubernetes/deploy_monitoring.yaml`"
    },
    {
      "from": "207b4d0f",
      "to": "8036f074",
      "relation": "references",
      "context": "prometheus-datasource.yaml: \"{{ lookup('file', playbook_dir + '/../../files/grafana_datasources/prometheus-datasource.yaml') }}\""
    },
    {
      "from": "207b4d0f",
      "to": "8036f074",
      "relation": "references",
      "context": "prometheus-datasource.yaml: \"{{ lookup('file', playbook_dir + '/../../files/grafana_datasources/prometheus-datasource.yaml') }}\""
    },
    {
      "from": "207b4d0f",
      "to": "8036f074",
      "relation": "mentions",
      "context": "prometheus-datasource.yaml: \"{{ lookup('file', playbook_dir + '/../../files/grafana_datasources/prometheus-datasource.yaml') }}\""
    },
    {
      "from": "207b4d0f",
      "to": "8036f074",
      "relation": "mentions",
      "context": "prometheus-datasource.yaml: \"{{ lookup('file', playbook_dir + '/../../files/grafana_datasources/prometheus-datasource.yaml') }}\""
    },
    {
      "from": "207b4d0f",
      "to": "7ef785f1",
      "relation": "references",
      "context": "**File**: `ansible/plays/kubernetes/monitoring_validation.yaml`"
    },
    {
      "from": "207b4d0f",
      "to": "7ef785f1",
      "relation": "mentions",
      "context": "**File**: `ansible/plays/kubernetes/monitoring_validation.yaml`"
    },
    {
      "from": "207b4d0f",
      "to": "ca8597d0",
      "relation": "references",
      "context": "ansible-playbook -i inventory.txt ansible/plays/kubernetes/deploy_monitoring.yaml"
    },
    {
      "from": "207b4d0f",
      "to": "ca8597d0",
      "relation": "mentions",
      "context": "ansible-playbook -i inventory.txt ansible/plays/kubernetes/deploy_monitoring.yaml"
    },
    {
      "from": "207b4d0f",
      "to": "ca8597d0",
      "relation": "references",
      "context": "- `ansible/plays/kubernetes/deploy_monitoring.yaml` - Main fix"
    },
    {
      "from": "207b4d0f",
      "to": "ca8597d0",
      "relation": "mentions",
      "context": "- `ansible/plays/kubernetes/deploy_monitoring.yaml` - Main fix"
    },
    {
      "from": "207b4d0f",
      "to": "7ef785f1",
      "relation": "references",
      "context": "- `ansible/plays/kubernetes/monitoring_validation.yaml` - Updated validation"
    },
    {
      "from": "207b4d0f",
      "to": "7ef785f1",
      "relation": "mentions",
      "context": "- `ansible/plays/kubernetes/monitoring_validation.yaml` - Updated validation"
    },
    {
      "from": "207b4d0f",
      "to": "8036f074",
      "relation": "references",
      "context": "- `ansible/files/grafana_datasources/prometheus-datasource.yaml` - Added deprecation notice"
    },
    {
      "from": "207b4d0f",
      "to": "8036f074",
      "relation": "mentions",
      "context": "- `ansible/files/grafana_datasources/prometheus-datasource.yaml` - Added deprecation notice"
    },
    {
      "from": "da894f5f",
      "to": "607e6e59",
      "relation": "references",
      "context": "- Configurable via `jellyfin_media_path` in `ansible/group_vars/all.yml`"
    },
    {
      "from": "da894f5f",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "- Configurable via `jellyfin_media_path` in `ansible/group_vars/all.yml`"
    },
    {
      "from": "da894f5f",
      "to": "607e6e59",
      "relation": "references",
      "context": "To customize the media directory path, edit `ansible/group_vars/all.yml`:"
    },
    {
      "from": "da894f5f",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "To customize the media directory path, edit `ansible/group_vars/all.yml`:"
    },
    {
      "from": "8a5efc5c",
      "to": "da894f5f",
      "relation": "mentions",
      "context": "- **Documentation**: `docs/jellyfin/JELLYFIN_HA_DEPLOYMENT.md`"
    },
    {
      "from": "0c29bbcc",
      "to": "ebb30773",
      "relation": "calls",
      "context": "sudo ./fix_jellyfin_cni_bridge_conflict.sh"
    },
    {
      "from": "0c29bbcc",
      "to": "ebb30773",
      "relation": "mentions",
      "context": "sudo ./fix_jellyfin_cni_bridge_conflict.sh"
    },
    {
      "from": "0c29bbcc",
      "to": "269791b0",
      "relation": "references",
      "context": "7. Recreate Jellyfin: `kubectl delete pod -n jellyfin jellyfin && kubectl apply -f manifests/jellyfin/jellyfin.yaml`"
    },
    {
      "from": "0c29bbcc",
      "to": "269791b0",
      "relation": "mentions",
      "context": "7. Recreate Jellyfin: `kubectl delete pod -n jellyfin jellyfin && kubectl apply -f manifests/jellyfin/jellyfin.yaml`"
    },
    {
      "from": "0c29bbcc",
      "to": "4d7840b3",
      "relation": "references",
      "context": "- [General CNI Fix Guide](README-CNI-FIX.md)"
    },
    {
      "from": "0c29bbcc",
      "to": "4d7840b3",
      "relation": "mentions",
      "context": "- [General CNI Fix Guide](README-CNI-FIX.md)"
    },
    {
      "from": "0c29bbcc",
      "to": "f0859b7a",
      "relation": "references",
      "context": "- [Network Diagnosis Quickstart](NETWORK-DIAGNOSIS-QUICKSTART.md)"
    },
    {
      "from": "0c29bbcc",
      "to": "f0859b7a",
      "relation": "mentions",
      "context": "- [Network Diagnosis Quickstart](NETWORK-DIAGNOSIS-QUICKSTART.md)"
    },
    {
      "from": "0c29bbcc",
      "to": "7036e452",
      "relation": "references",
      "context": "- [Cluster Network Reset Runbook](NETWORK_RESET_RUNBOOK.md)"
    },
    {
      "from": "0c29bbcc",
      "to": "7036e452",
      "relation": "mentions",
      "context": "- [Cluster Network Reset Runbook](NETWORK_RESET_RUNBOOK.md)"
    },
    {
      "from": "1e068979",
      "to": "e3453d63",
      "relation": "calls",
      "context": "./fix_jellyfin_network_issue.sh"
    },
    {
      "from": "1e068979",
      "to": "e3453d63",
      "relation": "mentions",
      "context": "./fix_jellyfin_network_issue.sh"
    },
    {
      "from": "1e068979",
      "to": "1970801b",
      "relation": "calls",
      "context": "2. Apply the CNI bridge fix if needed (using `scripts/fix_cni_bridge_conflict.sh`)"
    },
    {
      "from": "1e068979",
      "to": "1970801b",
      "relation": "mentions",
      "context": "2. Apply the CNI bridge fix if needed (using `scripts/fix_cni_bridge_conflict.sh`)"
    },
    {
      "from": "1e068979",
      "to": "1970801b",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "1e068979",
      "to": "1970801b",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "1e068979",
      "to": "269791b0",
      "relation": "references",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml"
    },
    {
      "from": "1e068979",
      "to": "269791b0",
      "relation": "mentions",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml"
    },
    {
      "from": "1e57853e",
      "to": "269791b0",
      "relation": "references",
      "context": "**File**: `manifests/jellyfin/jellyfin.yaml`"
    },
    {
      "from": "1e57853e",
      "to": "269791b0",
      "relation": "mentions",
      "context": "**File**: `manifests/jellyfin/jellyfin.yaml`"
    },
    {
      "from": "1e57853e",
      "to": "269791b0",
      "relation": "references",
      "context": "**Files**: `manifests/jellyfin/jellyfin.yaml`, `ansible/playbooks/verify-cluster.yml`"
    },
    {
      "from": "1e57853e",
      "to": "91932608",
      "relation": "references",
      "context": "**Files**: `manifests/jellyfin/jellyfin.yaml`, `ansible/playbooks/verify-cluster.yml`"
    },
    {
      "from": "1e57853e",
      "to": "269791b0",
      "relation": "mentions",
      "context": "**Files**: `manifests/jellyfin/jellyfin.yaml`, `ansible/playbooks/verify-cluster.yml`"
    },
    {
      "from": "1e57853e",
      "to": "91932608",
      "relation": "mentions",
      "context": "**Files**: `manifests/jellyfin/jellyfin.yaml`, `ansible/playbooks/verify-cluster.yml`"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "**File**: `deploy-cluster.sh`"
    },
    {
      "from": "1e57853e",
      "to": "a249d706",
      "relation": "calls",
      "context": "- `scripts/fix_homelab_node_issues.sh` (handles kube-proxy CrashLoopBackOff)"
    },
    {
      "from": "1e57853e",
      "to": "a249d706",
      "relation": "mentions",
      "context": "- `scripts/fix_homelab_node_issues.sh` (handles kube-proxy CrashLoopBackOff)"
    },
    {
      "from": "1e57853e",
      "to": "090d10b0",
      "relation": "calls",
      "context": "- `scripts/fix_remaining_pod_issues.sh` (handles various pod issues)"
    },
    {
      "from": "1e57853e",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "- `scripts/fix_remaining_pod_issues.sh` (handles various pod issues)"
    },
    {
      "from": "1e57853e",
      "to": "269791b0",
      "relation": "references",
      "context": "**File**: `manifests/jellyfin/jellyfin.yaml`"
    },
    {
      "from": "1e57853e",
      "to": "269791b0",
      "relation": "mentions",
      "context": "**File**: `manifests/jellyfin/jellyfin.yaml`"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "The deploy-cluster.sh script now:"
    },
    {
      "from": "1e57853e",
      "to": "269791b0",
      "relation": "references",
      "context": "1. **`manifests/jellyfin/jellyfin.yaml`** - Complete rewrite from PVC to hostPath"
    },
    {
      "from": "1e57853e",
      "to": "269791b0",
      "relation": "mentions",
      "context": "1. **`manifests/jellyfin/jellyfin.yaml`** - Complete rewrite from PVC to hostPath"
    },
    {
      "from": "1e57853e",
      "to": "91932608",
      "relation": "references",
      "context": "2. **`ansible/playbooks/verify-cluster.yml`** - Fixed health check endpoint"
    },
    {
      "from": "1e57853e",
      "to": "91932608",
      "relation": "mentions",
      "context": "2. **`ansible/playbooks/verify-cluster.yml`** - Fixed health check endpoint"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "3. **`deploy-cluster.sh`** - Added post-deployment fixes integration"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh --simple deploy"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh --simple deploy"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh --dry-run deploy"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh --dry-run deploy"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh verify"
    },
    {
      "from": "1e57853e",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh verify"
    },
    {
      "from": "98b95b67",
      "to": "269791b0",
      "relation": "references",
      "context": "In `manifests/jellyfin/jellyfin.yaml`, line 170 had:"
    },
    {
      "from": "98b95b67",
      "to": "269791b0",
      "relation": "mentions",
      "context": "In `manifests/jellyfin/jellyfin.yaml`, line 170 had:"
    },
    {
      "from": "42b5112f",
      "to": "607e6e59",
      "relation": "references",
      "context": "Add these variables to your `ansible/group_vars/all.yml`:"
    },
    {
      "from": "42b5112f",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "Add these variables to your `ansible/group_vars/all.yml`:"
    },
    {
      "from": "d6e5693f",
      "to": "311b37fb",
      "relation": "mentions",
      "context": "The `fix-cluster.sh` script was failing with the error:"
    },
    {
      "from": "d6e5693f",
      "to": "fd7c8bd5",
      "relation": "references",
      "context": "- `ansible/playbooks/minimal-network-fix.yml` - Enhanced namespace cleanup logic"
    },
    {
      "from": "d6e5693f",
      "to": "fd7c8bd5",
      "relation": "mentions",
      "context": "- `ansible/playbooks/minimal-network-fix.yml` - Enhanced namespace cleanup logic"
    },
    {
      "from": "d6e5693f",
      "to": "311b37fb",
      "relation": "mentions",
      "context": "After this change, `fix-cluster.sh` should:"
    },
    {
      "from": "19863979",
      "to": "09b572dc",
      "relation": "calls",
      "context": "./scripts/run_network_diagnosis.sh"
    },
    {
      "from": "19863979",
      "to": "09b572dc",
      "relation": "mentions",
      "context": "./scripts/run_network_diagnosis.sh"
    },
    {
      "from": "19863979",
      "to": "09b572dc",
      "relation": "calls",
      "context": "./scripts/run_network_diagnosis.sh --verbose"
    },
    {
      "from": "19863979",
      "to": "09b572dc",
      "relation": "mentions",
      "context": "./scripts/run_network_diagnosis.sh --verbose"
    },
    {
      "from": "19863979",
      "to": "09b572dc",
      "relation": "calls",
      "context": "./scripts/run_network_diagnosis.sh --check"
    },
    {
      "from": "19863979",
      "to": "09b572dc",
      "relation": "mentions",
      "context": "./scripts/run_network_diagnosis.sh --check"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml -vv"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml -vv"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook ansible/plays/network-diagnosis.yaml --syntax-check"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "mentions",
      "context": "ansible-playbook ansible/plays/network-diagnosis.yaml --syntax-check"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml --check"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml --check"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "references",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml -vv"
    },
    {
      "from": "19863979",
      "to": "b9618ca5",
      "relation": "mentions",
      "context": "ansible-playbook -i ansible/inventory.txt ansible/plays/network-diagnosis.yaml -vv"
    },
    {
      "from": "19863979",
      "to": "0eb8fc77",
      "relation": "references",
      "context": "- [Inter-Pod Communication Troubleshooting](docs/fix_cluster_communication.md)"
    },
    {
      "from": "19863979",
      "to": "0eb8fc77",
      "relation": "mentions",
      "context": "- [Inter-Pod Communication Troubleshooting](docs/fix_cluster_communication.md)"
    },
    {
      "from": "19863979",
      "to": "f087a852",
      "relation": "references",
      "context": "- [DNS Fix Guide](docs/dns-fix-guide.md)"
    },
    {
      "from": "19863979",
      "to": "f087a852",
      "relation": "mentions",
      "context": "- [DNS Fix Guide](docs/dns-fix-guide.md)"
    },
    {
      "from": "19863979",
      "to": "094a0932",
      "relation": "references",
      "context": "- [CNI Bridge Fix](docs/CNI_BRIDGE_FIX.md)"
    },
    {
      "from": "19863979",
      "to": "094a0932",
      "relation": "mentions",
      "context": "- [CNI Bridge Fix](docs/CNI_BRIDGE_FIX.md)"
    },
    {
      "from": "19863979",
      "to": "fcb85303",
      "relation": "references",
      "context": "- [CoreDNS Unknown Status Fix](docs/COREDNS_UNKNOWN_STATUS_FIX.md)"
    },
    {
      "from": "19863979",
      "to": "fcb85303",
      "relation": "mentions",
      "context": "- [CoreDNS Unknown Status Fix](docs/COREDNS_UNKNOWN_STATUS_FIX.md)"
    },
    {
      "from": "c914457e",
      "to": "09f42a5d",
      "relation": "calls",
      "context": "#### 1. `scripts/fix_nodeport_external_access.sh`"
    },
    {
      "from": "c914457e",
      "to": "09f42a5d",
      "relation": "mentions",
      "context": "#### 1. `scripts/fix_nodeport_external_access.sh`"
    },
    {
      "from": "c914457e",
      "to": "09f42a5d",
      "relation": "calls",
      "context": "sudo ./scripts/fix_nodeport_external_access.sh"
    },
    {
      "from": "c914457e",
      "to": "09f42a5d",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_nodeport_external_access.sh"
    },
    {
      "from": "c914457e",
      "to": "27edfbfd",
      "relation": "calls",
      "context": "#### 2. `scripts/validate_nodeport_external_access.sh`"
    },
    {
      "from": "c914457e",
      "to": "27edfbfd",
      "relation": "mentions",
      "context": "#### 2. `scripts/validate_nodeport_external_access.sh`"
    },
    {
      "from": "c914457e",
      "to": "27edfbfd",
      "relation": "calls",
      "context": "./scripts/validate_nodeport_external_access.sh"
    },
    {
      "from": "c914457e",
      "to": "27edfbfd",
      "relation": "mentions",
      "context": "./scripts/validate_nodeport_external_access.sh"
    },
    {
      "from": "c914457e",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "#### Updated: `scripts/fix_cluster_communication.sh`"
    },
    {
      "from": "c914457e",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "#### Updated: `scripts/fix_cluster_communication.sh`"
    },
    {
      "from": "c914457e",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "sudo ./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "c914457e",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "c914457e",
      "to": "09f42a5d",
      "relation": "calls",
      "context": "sudo ./scripts/fix_nodeport_external_access.sh"
    },
    {
      "from": "c914457e",
      "to": "09f42a5d",
      "relation": "mentions",
      "context": "sudo ./scripts/fix_nodeport_external_access.sh"
    },
    {
      "from": "c914457e",
      "to": "27edfbfd",
      "relation": "calls",
      "context": "./scripts/validate_nodeport_external_access.sh"
    },
    {
      "from": "c914457e",
      "to": "27edfbfd",
      "relation": "mentions",
      "context": "./scripts/validate_nodeport_external_access.sh"
    },
    {
      "from": "c914457e",
      "to": "09f42a5d",
      "relation": "calls",
      "context": "- `scripts/fix_nodeport_external_access.sh` (new)"
    },
    {
      "from": "c914457e",
      "to": "09f42a5d",
      "relation": "mentions",
      "context": "- `scripts/fix_nodeport_external_access.sh` (new)"
    },
    {
      "from": "c914457e",
      "to": "27edfbfd",
      "relation": "calls",
      "context": "- `scripts/validate_nodeport_external_access.sh` (new)"
    },
    {
      "from": "c914457e",
      "to": "27edfbfd",
      "relation": "mentions",
      "context": "- `scripts/validate_nodeport_external_access.sh` (new)"
    },
    {
      "from": "c914457e",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "- `scripts/fix_cluster_communication.sh` (updated)"
    },
    {
      "from": "c914457e",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "- `scripts/fix_cluster_communication.sh` (updated)"
    },
    {
      "from": "84b873a0",
      "to": "207b4d0f",
      "relation": "mentions",
      "context": "# Or apply manual fix (see docs/grafana_datasource_conflict_fix.md for details)"
    },
    {
      "from": "84b873a0",
      "to": "207b4d0f",
      "relation": "mentions",
      "context": "For complete fix details, see: `docs/grafana_datasource_conflict_fix.md`"
    },
    {
      "from": "84b873a0",
      "to": "607e6e59",
      "relation": "references",
      "context": "# Option 2: Change monitoring scheduling mode to 'unrestricted' in ansible/group_vars/all.yml"
    },
    {
      "from": "84b873a0",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "# Option 2: Change monitoring scheduling mode to 'unrestricted' in ansible/group_vars/all.yml"
    },
    {
      "from": "84b873a0",
      "to": "ca8597d0",
      "relation": "references",
      "context": "# Then redeploy: ansible-playbook -i inventory.txt plays/kubernetes/deploy_monitoring.yaml"
    },
    {
      "from": "84b873a0",
      "to": "ca8597d0",
      "relation": "mentions",
      "context": "# Then redeploy: ansible-playbook -i inventory.txt plays/kubernetes/deploy_monitoring.yaml"
    },
    {
      "from": "84b873a0",
      "to": "607e6e59",
      "relation": "references",
      "context": "For permanent fix, configure `monitoring_scheduling_mode` in `ansible/group_vars/all.yml`:"
    },
    {
      "from": "84b873a0",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "For permanent fix, configure `monitoring_scheduling_mode` in `ansible/group_vars/all.yml`:"
    },
    {
      "from": "84b873a0",
      "to": "607e6e59",
      "relation": "references",
      "context": "**Grafana Pending**: pods stuck in Pending state cannot be scheduled due to strict node selectors, taints, or resource constraints  fix by setting `monitoring_scheduling_mode: unrestricted` in `ansible/group_vars/all.yml` or removing node taints."
    },
    {
      "from": "84b873a0",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "**Grafana Pending**: pods stuck in Pending state cannot be scheduled due to strict node selectors, taints, or resource constraints  fix by setting `monitoring_scheduling_mode: unrestricted` in `ansible/group_vars/all.yml` or removing node taints."
    },
    {
      "from": "4096f24e",
      "to": "ad0e321a",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/network/coredns-service.yaml`"
    },
    {
      "from": "4096f24e",
      "to": "ad0e321a",
      "relation": "mentions",
      "context": "- **Configuration**: `/manifests/network/coredns-service.yaml`"
    },
    {
      "from": "4096f24e",
      "to": "80025501",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/network/kube-proxy-daemonset.yaml`"
    },
    {
      "from": "4096f24e",
      "to": "80025501",
      "relation": "mentions",
      "context": "- **Configuration**: `/manifests/network/kube-proxy-daemonset.yaml`"
    },
    {
      "from": "4096f24e",
      "to": "0a813314",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/cni/flannel.yaml`"
    },
    {
      "from": "4096f24e",
      "to": "0a813314",
      "relation": "mentions",
      "context": "- **Configuration**: `/manifests/cni/flannel.yaml`"
    },
    {
      "from": "4096f24e",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "- **File**: `/manifests/network/coredns-configmap.yaml`"
    },
    {
      "from": "4096f24e",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "- **File**: `/manifests/network/coredns-configmap.yaml`"
    },
    {
      "from": "4096f24e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "4096f24e",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh deploy"
    },
    {
      "from": "4096f24e",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "sudo ./scripts/setup_static_ips_and_dns.sh"
    },
    {
      "from": "4096f24e",
      "to": "4f2a3f41",
      "relation": "mentions",
      "context": "sudo ./scripts/setup_static_ips_and_dns.sh"
    },
    {
      "from": "4096f24e",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "./scripts/validate_static_ips_and_dns.sh"
    },
    {
      "from": "4096f24e",
      "to": "5e0bdb07",
      "relation": "mentions",
      "context": "./scripts/validate_static_ips_and_dns.sh"
    },
    {
      "from": "4096f24e",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "./scripts/validate_static_ips_and_dns.sh static-ips"
    },
    {
      "from": "4096f24e",
      "to": "5e0bdb07",
      "relation": "mentions",
      "context": "./scripts/validate_static_ips_and_dns.sh static-ips"
    },
    {
      "from": "4096f24e",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "./scripts/validate_static_ips_and_dns.sh dns"
    },
    {
      "from": "4096f24e",
      "to": "5e0bdb07",
      "relation": "mentions",
      "context": "./scripts/validate_static_ips_and_dns.sh dns"
    },
    {
      "from": "4096f24e",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "sudo ./scripts/setup_static_ips_and_dns.sh --verify"
    },
    {
      "from": "4096f24e",
      "to": "4f2a3f41",
      "relation": "mentions",
      "context": "sudo ./scripts/setup_static_ips_and_dns.sh --verify"
    },
    {
      "from": "4096f24e",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "- **Deployment Scripts**: Automatically applied during `deploy-cluster.sh`"
    },
    {
      "from": "311b37fb",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "INVENTORY_FILE=\"$ANSIBLE_DIR/inventory/hosts.yml\""
    },
    {
      "from": "311b37fb",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "INVENTORY_FILE=\"$ANSIBLE_DIR/inventory/hosts.yml\""
    },
    {
      "from": "311b37fb",
      "to": "fd7c8bd5",
      "relation": "references",
      "context": "PLAYBOOK=\"$ANSIBLE_DIR/playbooks/minimal-network-fix.yml\""
    },
    {
      "from": "311b37fb",
      "to": "fd7c8bd5",
      "relation": "mentions",
      "context": "PLAYBOOK=\"$ANSIBLE_DIR/playbooks/minimal-network-fix.yml\""
    },
    {
      "from": "311b37fb",
      "to": "bcfb5f43",
      "relation": "references",
      "context": "echo \"3. If monitoring is needed, run: ansible-playbook -i $INVENTORY_FILE $ANSIBLE_DIR/plays/deploy-apps.yaml\""
    },
    {
      "from": "311b37fb",
      "to": "bcfb5f43",
      "relation": "mentions",
      "context": "echo \"3. If monitoring is needed, run: ansible-playbook -i $INVENTORY_FILE $ANSIBLE_DIR/plays/deploy-apps.yaml\""
    },
    {
      "from": "ebb30773",
      "to": "269791b0",
      "relation": "references",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml"
    },
    {
      "from": "ebb30773",
      "to": "269791b0",
      "relation": "mentions",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml"
    },
    {
      "from": "ebb30773",
      "to": "090d10b0",
      "relation": "calls",
      "context": "if [ -f \"./scripts/fix_remaining_pod_issues.sh\" ]; then"
    },
    {
      "from": "ebb30773",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "if [ -f \"./scripts/fix_remaining_pod_issues.sh\" ]; then"
    },
    {
      "from": "ebb30773",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "info \"Calling fix_remaining_pod_issues.sh to handle kube-proxy CrashLoopBackOff\""
    },
    {
      "from": "ebb30773",
      "to": "090d10b0",
      "relation": "calls",
      "context": "./scripts/fix_remaining_pod_issues.sh || warn \"kube-proxy fix script encountered issues\""
    },
    {
      "from": "ebb30773",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "./scripts/fix_remaining_pod_issues.sh || warn \"kube-proxy fix script encountered issues\""
    },
    {
      "from": "ebb30773",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "warn \"fix_remaining_pod_issues.sh not found - applying basic kube-proxy fix\""
    },
    {
      "from": "9088164c",
      "to": "269791b0",
      "relation": "references",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml || {"
    },
    {
      "from": "9088164c",
      "to": "269791b0",
      "relation": "mentions",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml || {"
    },
    {
      "from": "9088164c",
      "to": "4ede3e1b",
      "relation": "references",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin-minimal.yaml"
    },
    {
      "from": "9088164c",
      "to": "4ede3e1b",
      "relation": "mentions",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin-minimal.yaml"
    },
    {
      "from": "e3453d63",
      "to": "1970801b",
      "relation": "calls",
      "context": "if [ ! -f \"scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "e3453d63",
      "to": "1970801b",
      "relation": "mentions",
      "context": "if [ ! -f \"scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "e3453d63",
      "to": "1970801b",
      "relation": "calls",
      "context": "if sudo ./scripts/fix_cni_bridge_conflict.sh; then"
    },
    {
      "from": "e3453d63",
      "to": "1970801b",
      "relation": "mentions",
      "context": "if sudo ./scripts/fix_cni_bridge_conflict.sh; then"
    },
    {
      "from": "e3453d63",
      "to": "269791b0",
      "relation": "references",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml"
    },
    {
      "from": "e3453d63",
      "to": "269791b0",
      "relation": "mentions",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml"
    },
    {
      "from": "e3453d63",
      "to": "269791b0",
      "relation": "references",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml"
    },
    {
      "from": "e3453d63",
      "to": "269791b0",
      "relation": "mentions",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml"
    },
    {
      "from": "0d0c4a60",
      "to": "269791b0",
      "relation": "references",
      "context": "if [ ! -f \"manifests/jellyfin/jellyfin.yaml\" ]; then"
    },
    {
      "from": "0d0c4a60",
      "to": "269791b0",
      "relation": "mentions",
      "context": "if [ ! -f \"manifests/jellyfin/jellyfin.yaml\" ]; then"
    },
    {
      "from": "0d0c4a60",
      "to": "1970801b",
      "relation": "calls",
      "context": "if [ -f \"scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "0d0c4a60",
      "to": "1970801b",
      "relation": "mentions",
      "context": "if [ -f \"scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "0d0c4a60",
      "to": "1970801b",
      "relation": "calls",
      "context": "if sudo ./scripts/fix_cni_bridge_conflict.sh; then"
    },
    {
      "from": "0d0c4a60",
      "to": "1970801b",
      "relation": "mentions",
      "context": "if sudo ./scripts/fix_cni_bridge_conflict.sh; then"
    },
    {
      "from": "0d0c4a60",
      "to": "269791b0",
      "relation": "references",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml"
    },
    {
      "from": "0d0c4a60",
      "to": "269791b0",
      "relation": "mentions",
      "context": "kubectl apply -f manifests/jellyfin/jellyfin.yaml"
    },
    {
      "from": "24a2f90a",
      "to": "1d93e806",
      "relation": "references",
      "context": "prometheus.yaml: |"
    },
    {
      "from": "24a2f90a",
      "to": "1d93e806",
      "relation": "mentions",
      "context": "prometheus.yaml: |"
    },
    {
      "from": "f157568d",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "if [ -f \"./scripts/validate_pod_connectivity.sh\" ]; then"
    },
    {
      "from": "f157568d",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "if [ -f \"./scripts/validate_pod_connectivity.sh\" ]; then"
    },
    {
      "from": "f157568d",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "if ./scripts/validate_pod_connectivity.sh; then"
    },
    {
      "from": "f157568d",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "if ./scripts/validate_pod_connectivity.sh; then"
    },
    {
      "from": "f157568d",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "if [ -f \"./scripts/fix_cluster_communication.sh\" ]; then"
    },
    {
      "from": "f157568d",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "if [ -f \"./scripts/fix_cluster_communication.sh\" ]; then"
    },
    {
      "from": "f157568d",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "./scripts/fix_cluster_communication.sh --non-interactive"
    },
    {
      "from": "f157568d",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "./scripts/fix_cluster_communication.sh --non-interactive"
    },
    {
      "from": "f157568d",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "if [ -f \"./scripts/fix_worker_node_cni.sh\" ]; then"
    },
    {
      "from": "f157568d",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "if [ -f \"./scripts/fix_worker_node_cni.sh\" ]; then"
    },
    {
      "from": "f157568d",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "./scripts/fix_worker_node_cni.sh --node storagenodet3500 --non-interactive"
    },
    {
      "from": "f157568d",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "./scripts/fix_worker_node_cni.sh --node storagenodet3500 --non-interactive"
    },
    {
      "from": "f157568d",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "if [ -f \"./scripts/fix_flannel_mixed_os.sh\" ]; then"
    },
    {
      "from": "f157568d",
      "to": "263c2cb9",
      "relation": "mentions",
      "context": "if [ -f \"./scripts/fix_flannel_mixed_os.sh\" ]; then"
    },
    {
      "from": "f157568d",
      "to": "263c2cb9",
      "relation": "calls",
      "context": "./scripts/fix_flannel_mixed_os.sh"
    },
    {
      "from": "f157568d",
      "to": "263c2cb9",
      "relation": "mentions",
      "context": "./scripts/fix_flannel_mixed_os.sh"
    },
    {
      "from": "f157568d",
      "to": "1970801b",
      "relation": "calls",
      "context": "if [ -f \"./scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "f157568d",
      "to": "1970801b",
      "relation": "mentions",
      "context": "if [ -f \"./scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "f157568d",
      "to": "1970801b",
      "relation": "calls",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "f157568d",
      "to": "1970801b",
      "relation": "mentions",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "f157568d",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "if [ -f \"./scripts/validate_pod_connectivity.sh\" ]; then"
    },
    {
      "from": "f157568d",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "if [ -f \"./scripts/validate_pod_connectivity.sh\" ]; then"
    },
    {
      "from": "f157568d",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "if ./scripts/validate_pod_connectivity.sh; then"
    },
    {
      "from": "f157568d",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "if ./scripts/validate_pod_connectivity.sh; then"
    },
    {
      "from": "f157568d",
      "to": "797dca0d",
      "relation": "mentions",
      "context": "echo \"  docs/cni-pod-communication-fix.md\""
    },
    {
      "from": "efc0b58e",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "- **`enhanced_kubeadm_join.sh`** - Enhanced Kubernetes join process with comprehensive validation"
    },
    {
      "from": "efc0b58e",
      "to": "37f70066",
      "relation": "mentions",
      "context": "- **`comprehensive_worker_setup.sh`** - Complete worker node setup and configuration"
    },
    {
      "from": "efc0b58e",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "- **`fix_cluster_communication.sh`** - Fixes cluster communication and networking issues"
    },
    {
      "from": "efc0b58e",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "- **`fix_remaining_pod_issues.sh`** - Fixes common pod issues including Jellyfin readiness and kube-proxy"
    },
    {
      "from": "efc0b58e",
      "to": "62721f19",
      "relation": "mentions",
      "context": "- **`validate_pod_health.sh`** - Validates pod health and cluster status"
    },
    {
      "from": "efc0b58e",
      "to": "563e9b03",
      "relation": "mentions",
      "context": "- **`vmstation_status.sh`** - Comprehensive cluster status and diagnostic information"
    },
    {
      "from": "efc0b58e",
      "to": "1970801b",
      "relation": "mentions",
      "context": "- **`fix_cni_bridge_conflict.sh`** - Fixes CNI bridge IP conflicts causing ContainerCreating errors"
    },
    {
      "from": "efc0b58e",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "- **`fix_worker_node_cni.sh`** - Fixes worker node CNI communication issues"
    },
    {
      "from": "efc0b58e",
      "to": "263c2cb9",
      "relation": "mentions",
      "context": "- **`fix_flannel_mixed_os.sh`** - Fixes Flannel issues in mixed OS environments"
    },
    {
      "from": "efc0b58e",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "- **`validate_cluster_communication.sh`** - Validates cluster communication and NodePort services"
    },
    {
      "from": "efc0b58e",
      "to": "3128233f",
      "relation": "mentions",
      "context": "- **`diagnose_remaining_pod_issues.sh`** - Analyzes specific pod failures and issues"
    },
    {
      "from": "efc0b58e",
      "to": "b3dc3aae",
      "relation": "mentions",
      "context": "- **`gather_worker_diagnostics.sh`** - Comprehensive worker node diagnostic collection"
    },
    {
      "from": "efc0b58e",
      "to": "09b572dc",
      "relation": "mentions",
      "context": "- **`run_network_diagnosis.sh`** - Network diagnosis and connectivity testing"
    },
    {
      "from": "efc0b58e",
      "to": "36bd22c9",
      "relation": "mentions",
      "context": "- **`check_coredns_status.sh`** - CoreDNS status and configuration validation"
    },
    {
      "from": "efc0b58e",
      "to": "b428256d",
      "relation": "mentions",
      "context": "- **`validate_join_prerequisites.sh`** - Comprehensive validation before kubeadm join"
    },
    {
      "from": "efc0b58e",
      "to": "6ea2bc0f",
      "relation": "mentions",
      "context": "- **`validate_post_wipe_functionality.sh`** - Validates post-wipe worker join functionality"
    },
    {
      "from": "efc0b58e",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "- **`validate_pod_connectivity.sh`** - Tests pod-to-pod CNI communication"
    },
    {
      "from": "efc0b58e",
      "to": "27edfbfd",
      "relation": "mentions",
      "context": "- **`validate_nodeport_external_access.sh`** - Validates external access to NodePort services"
    },
    {
      "from": "efc0b58e",
      "to": "72c7835a",
      "relation": "calls",
      "context": "./deploy-cluster.sh"
    },
    {
      "from": "efc0b58e",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "./deploy-cluster.sh"
    },
    {
      "from": "efc0b58e",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "./scripts/enhanced_kubeadm_join.sh"
    },
    {
      "from": "efc0b58e",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "./scripts/enhanced_kubeadm_join.sh"
    },
    {
      "from": "efc0b58e",
      "to": "37f70066",
      "relation": "calls",
      "context": "./scripts/comprehensive_worker_setup.sh"
    },
    {
      "from": "efc0b58e",
      "to": "37f70066",
      "relation": "mentions",
      "context": "./scripts/comprehensive_worker_setup.sh"
    },
    {
      "from": "efc0b58e",
      "to": "563e9b03",
      "relation": "calls",
      "context": "./scripts/vmstation_status.sh"
    },
    {
      "from": "efc0b58e",
      "to": "563e9b03",
      "relation": "mentions",
      "context": "./scripts/vmstation_status.sh"
    },
    {
      "from": "efc0b58e",
      "to": "1e7cdd43",
      "relation": "calls",
      "context": "./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "efc0b58e",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "./scripts/fix_cluster_communication.sh"
    },
    {
      "from": "efc0b58e",
      "to": "090d10b0",
      "relation": "calls",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "efc0b58e",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "efc0b58e",
      "to": "1970801b",
      "relation": "calls",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "efc0b58e",
      "to": "1970801b",
      "relation": "mentions",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "efc0b58e",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "./scripts/fix_worker_node_cni.sh"
    },
    {
      "from": "efc0b58e",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "./scripts/fix_worker_node_cni.sh"
    },
    {
      "from": "efc0b58e",
      "to": "62721f19",
      "relation": "calls",
      "context": "./scripts/validate_pod_health.sh"
    },
    {
      "from": "efc0b58e",
      "to": "62721f19",
      "relation": "mentions",
      "context": "./scripts/validate_pod_health.sh"
    },
    {
      "from": "efc0b58e",
      "to": "7c606adf",
      "relation": "calls",
      "context": "./scripts/validate_cluster_communication.sh"
    },
    {
      "from": "efc0b58e",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "./scripts/validate_cluster_communication.sh"
    },
    {
      "from": "efc0b58e",
      "to": "3128233f",
      "relation": "calls",
      "context": "./scripts/diagnose_remaining_pod_issues.sh"
    },
    {
      "from": "efc0b58e",
      "to": "3128233f",
      "relation": "mentions",
      "context": "./scripts/diagnose_remaining_pod_issues.sh"
    },
    {
      "from": "efc0b58e",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "./scripts/gather_worker_diagnostics.sh"
    },
    {
      "from": "efc0b58e",
      "to": "b3dc3aae",
      "relation": "mentions",
      "context": "./scripts/gather_worker_diagnostics.sh"
    },
    {
      "from": "efc0b58e",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "| `enhanced_kubeadm_join.sh` | Enhanced Kubernetes join process | 1388 |"
    },
    {
      "from": "efc0b58e",
      "to": "37f70066",
      "relation": "mentions",
      "context": "| `comprehensive_worker_setup.sh` | Complete worker node setup | 481 |"
    },
    {
      "from": "efc0b58e",
      "to": "1e7cdd43",
      "relation": "mentions",
      "context": "| `fix_cluster_communication.sh` | Cluster communication fixes | 705 |"
    },
    {
      "from": "efc0b58e",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "| `fix_remaining_pod_issues.sh` | Pod issue remediation | 673 |"
    },
    {
      "from": "efc0b58e",
      "to": "1970801b",
      "relation": "mentions",
      "context": "| `fix_cni_bridge_conflict.sh` | CNI bridge IP conflict fixes | 378 |"
    },
    {
      "from": "efc0b58e",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "| `fix_worker_node_cni.sh` | Worker CNI communication fixes | 423 |"
    },
    {
      "from": "efc0b58e",
      "to": "263c2cb9",
      "relation": "mentions",
      "context": "| `fix_flannel_mixed_os.sh` | Mixed OS Flannel fixes | 403 |"
    },
    {
      "from": "efc0b58e",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "| `validate_cluster_communication.sh` | Cluster communication validation | 339 |"
    },
    {
      "from": "efc0b58e",
      "to": "b3dc3aae",
      "relation": "mentions",
      "context": "| `gather_worker_diagnostics.sh` | Worker diagnostic collection | 453 |"
    },
    {
      "from": "efc0b58e",
      "to": "b428256d",
      "relation": "mentions",
      "context": "| `validate_join_prerequisites.sh` | Pre-join validation | 398 |"
    },
    {
      "from": "efc0b58e",
      "to": "3128233f",
      "relation": "mentions",
      "context": "| `diagnose_remaining_pod_issues.sh` | Pod issue diagnosis | 195 |"
    },
    {
      "from": "efc0b58e",
      "to": "62721f19",
      "relation": "mentions",
      "context": "| `validate_pod_health.sh` | Pod health validation | 109 |"
    },
    {
      "from": "efc0b58e",
      "to": "607e6e59",
      "relation": "references",
      "context": "- **Configuration**: Falls back to `ansible/group_vars/all.yml`"
    },
    {
      "from": "efc0b58e",
      "to": "607e6e59",
      "relation": "mentions",
      "context": "- **Configuration**: Falls back to `ansible/group_vars/all.yml`"
    },
    {
      "from": "36bd22c9",
      "to": "7071b363",
      "relation": "calls",
      "context": "echo \"  ./scripts/fix_coredns_unknown_status.sh\""
    },
    {
      "from": "36bd22c9",
      "to": "7071b363",
      "relation": "mentions",
      "context": "echo \"  ./scripts/fix_coredns_unknown_status.sh\""
    },
    {
      "from": "36bd22c9",
      "to": "72c7835a",
      "relation": "calls",
      "context": "echo \"  ./deploy-cluster.sh full && ./scripts/fix_coredns_unknown_status.sh\""
    },
    {
      "from": "36bd22c9",
      "to": "7071b363",
      "relation": "calls",
      "context": "echo \"  ./deploy-cluster.sh full && ./scripts/fix_coredns_unknown_status.sh\""
    },
    {
      "from": "36bd22c9",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "echo \"  ./deploy-cluster.sh full && ./scripts/fix_coredns_unknown_status.sh\""
    },
    {
      "from": "36bd22c9",
      "to": "7071b363",
      "relation": "mentions",
      "context": "echo \"  ./deploy-cluster.sh full && ./scripts/fix_coredns_unknown_status.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "b428256d",
      "relation": "mentions",
      "context": "local validator=\"$script_dir/validate_join_prerequisites.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "21313a3e",
      "relation": "mentions",
      "context": "local manual_fix_script=\"$script_dir/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "21313a3e",
      "relation": "calls",
      "context": "error \"   sudo ./scripts/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "21313a3e",
      "relation": "mentions",
      "context": "error \"   sudo ./scripts/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "59ecfc80",
      "relation": "mentions",
      "context": "local validator=\"$script_dir/validate_systemd_dropins.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "21313a3e",
      "relation": "mentions",
      "context": "local manual_fix_script=\"$script_dir/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "21313a3e",
      "relation": "calls",
      "context": "error \"   sudo ./scripts/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "21313a3e",
      "relation": "mentions",
      "context": "error \"   sudo ./scripts/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "6564808c",
      "relation": "calls",
      "context": "error \"   sudo ./worker_node_join_remediation.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "6564808c",
      "relation": "mentions",
      "context": "error \"   sudo ./worker_node_join_remediation.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "1b255a42",
      "relation": "calls",
      "context": "error \"   sudo ./scripts/quick_join_diagnostics.sh\""
    },
    {
      "from": "d3a169d7",
      "to": "1b255a42",
      "relation": "mentions",
      "context": "error \"   sudo ./scripts/quick_join_diagnostics.sh\""
    },
    {
      "from": "1e7cdd43",
      "to": "630db1ca",
      "relation": "mentions",
      "context": "if ! run_fix_script \"fix_iptables_compatibility.sh\" \"iptables/nftables compatibility fix\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "1970801b",
      "relation": "mentions",
      "context": "if ! run_fix_script \"fix_cni_bridge_conflict.sh\" \"CNI bridge conflict fix\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "if ! run_fix_script \"fix_worker_node_cni.sh\" \"Worker node CNI communication fix\" \"--node\" \"storagenodet3500\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "263c2cb9",
      "relation": "mentions",
      "context": "if ! run_fix_script \"fix_flannel_mixed_os.sh\" \"Flannel mixed-OS configuration fix\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "if ! run_fix_script \"fix_remaining_pod_issues.sh\" \"kube-proxy and pod issues fix\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "2efbd35b",
      "relation": "mentions",
      "context": "if ! run_fix_script \"fix_worker_kubectl_config.sh\" \"kubectl worker node configuration\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "if ! run_fix_script \"validate_cluster_communication.sh\" \"cluster communication validation\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "if ! run_fix_script \"validate_pod_connectivity.sh\" \"detailed pod-to-pod connectivity validation\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "09f42a5d",
      "relation": "mentions",
      "context": "if ! run_fix_script \"fix_nodeport_external_access.sh\" \"NodePort external access fix\"; then"
    },
    {
      "from": "1e7cdd43",
      "to": "7c606adf",
      "relation": "calls",
      "context": "echo \"  ./scripts/validate_cluster_communication.sh\""
    },
    {
      "from": "1e7cdd43",
      "to": "7c606adf",
      "relation": "mentions",
      "context": "echo \"  ./scripts/validate_cluster_communication.sh\""
    },
    {
      "from": "1e7cdd43",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "echo \"  ./scripts/validate_pod_connectivity.sh\""
    },
    {
      "from": "1e7cdd43",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "echo \"  ./scripts/validate_pod_connectivity.sh\""
    },
    {
      "from": "7071b363",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "# after flannel pods have been regenerated by deploy-cluster.sh full"
    },
    {
      "from": "263c2cb9",
      "to": "52dfbbac",
      "relation": "calls",
      "context": "echo \"2. Test pod-to-pod connectivity: ./scripts/validate_pod_connectivity.sh\""
    },
    {
      "from": "263c2cb9",
      "to": "52dfbbac",
      "relation": "mentions",
      "context": "echo \"2. Test pod-to-pod connectivity: ./scripts/validate_pod_connectivity.sh\""
    },
    {
      "from": "a249d706",
      "to": "1970801b",
      "relation": "calls",
      "context": "if [ -f \"./scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "a249d706",
      "to": "1970801b",
      "relation": "mentions",
      "context": "if [ -f \"./scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "a249d706",
      "to": "1970801b",
      "relation": "calls",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "a249d706",
      "to": "1970801b",
      "relation": "mentions",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "a249d706",
      "to": "090d10b0",
      "relation": "calls",
      "context": "if [ -f \"./scripts/fix_remaining_pod_issues.sh\" ]; then"
    },
    {
      "from": "a249d706",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "if [ -f \"./scripts/fix_remaining_pod_issues.sh\" ]; then"
    },
    {
      "from": "a249d706",
      "to": "090d10b0",
      "relation": "calls",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "a249d706",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "./scripts/fix_remaining_pod_issues.sh"
    },
    {
      "from": "a249d706",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"4. If Jellyfin shows 0/1 ready, run: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "a249d706",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "echo \"4. If Jellyfin shows 0/1 ready, run: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "090d10b0",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "# Addresses specific problems after deploy-cluster.sh and fix_homelab_node_issues.sh"
    },
    {
      "from": "090d10b0",
      "to": "a249d706",
      "relation": "mentions",
      "context": "# Addresses specific problems after deploy-cluster.sh and fix_homelab_node_issues.sh"
    },
    {
      "from": "090d10b0",
      "to": "1970801b",
      "relation": "calls",
      "context": "if [ -f \"./scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "090d10b0",
      "to": "1970801b",
      "relation": "mentions",
      "context": "if [ -f \"./scripts/fix_cni_bridge_conflict.sh\" ]; then"
    },
    {
      "from": "090d10b0",
      "to": "1970801b",
      "relation": "calls",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "090d10b0",
      "to": "1970801b",
      "relation": "mentions",
      "context": "./scripts/fix_cni_bridge_conflict.sh"
    },
    {
      "from": "1b255a42",
      "to": "21313a3e",
      "relation": "calls",
      "context": "error \"   sudo ./scripts/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "1b255a42",
      "to": "21313a3e",
      "relation": "mentions",
      "context": "error \"   sudo ./scripts/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "1b255a42",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "info \"   sudo ./scripts/enhanced_kubeadm_join.sh \\\"<your-join-command>\\\"\""
    },
    {
      "from": "1b255a42",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "info \"   sudo ./scripts/enhanced_kubeadm_join.sh \\\"<your-join-command>\\\"\""
    },
    {
      "from": "09b572dc",
      "to": "b9618ca5",
      "relation": "references",
      "context": "PLAYBOOK_PATH=\"$REPO_ROOT/ansible/plays/network-diagnosis.yaml\""
    },
    {
      "from": "09b572dc",
      "to": "b9618ca5",
      "relation": "mentions",
      "context": "PLAYBOOK_PATH=\"$REPO_ROOT/ansible/plays/network-diagnosis.yaml\""
    },
    {
      "from": "4f2a3f41",
      "to": "ad0e321a",
      "relation": "references",
      "context": "local coredns_manifest=\"/home/runner/work/VMStation/VMStation/manifests/network/coredns-service.yaml\""
    },
    {
      "from": "4f2a3f41",
      "to": "ad0e321a",
      "relation": "mentions",
      "context": "local coredns_manifest=\"/home/runner/work/VMStation/VMStation/manifests/network/coredns-service.yaml\""
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "if [ -f \"manifests/network/coredns-configmap.yaml\" ]; then"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "if [ -f \"manifests/network/coredns-configmap.yaml\" ]; then"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "kubectl apply -f manifests/network/coredns-configmap.yaml"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "kubectl apply -f manifests/network/coredns-configmap.yaml"
    },
    {
      "from": "4f2a3f41",
      "to": "4096f24e",
      "relation": "mentions",
      "context": "local doc_file=\"/home/runner/work/VMStation/VMStation/docs/static-ips-and-dns.md\""
    },
    {
      "from": "4f2a3f41",
      "to": "ad0e321a",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/network/coredns-service.yaml`"
    },
    {
      "from": "4f2a3f41",
      "to": "ad0e321a",
      "relation": "mentions",
      "context": "- **Configuration**: `/manifests/network/coredns-service.yaml`"
    },
    {
      "from": "4f2a3f41",
      "to": "80025501",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/network/kube-proxy-daemonset.yaml`"
    },
    {
      "from": "4f2a3f41",
      "to": "80025501",
      "relation": "mentions",
      "context": "- **Configuration**: `/manifests/network/kube-proxy-daemonset.yaml`"
    },
    {
      "from": "4f2a3f41",
      "to": "0a813314",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/cni/flannel.yaml`"
    },
    {
      "from": "4f2a3f41",
      "to": "0a813314",
      "relation": "mentions",
      "context": "- **Configuration**: `/manifests/cni/flannel.yaml`"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "- **Configuration**: `/manifests/network/coredns-configmap.yaml`"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "- **Configuration**: `/manifests/network/coredns-configmap.yaml`"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "kubectl apply -f manifests/network/coredns-configmap.yaml"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "kubectl apply -f manifests/network/coredns-configmap.yaml"
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "echo \"   CoreDNS config: manifests/network/coredns-configmap.yaml\""
    },
    {
      "from": "4f2a3f41",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "echo \"   CoreDNS config: manifests/network/coredns-configmap.yaml\""
    },
    {
      "from": "4f2a3f41",
      "to": "4096f24e",
      "relation": "mentions",
      "context": "echo \"   Documentation: docs/static-ips-and-dns.md\""
    },
    {
      "from": "0f552648",
      "to": "1e67c2e0",
      "relation": "references",
      "context": "INVENTORY_FILE=\"ansible/inventory/hosts.yml\""
    },
    {
      "from": "0f552648",
      "to": "1e67c2e0",
      "relation": "mentions",
      "context": "INVENTORY_FILE=\"ansible/inventory/hosts.yml\""
    },
    {
      "from": "0f552648",
      "to": "91932608",
      "relation": "references",
      "context": "echo \"   - Run full verification: ansible-playbook ansible/playbooks/verify-cluster.yml\""
    },
    {
      "from": "0f552648",
      "to": "91932608",
      "relation": "mentions",
      "context": "echo \"   - Run full verification: ansible-playbook ansible/playbooks/verify-cluster.yml\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script exists\" \"test -x '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "run_test \"Enhanced join script exists\" \"test -x '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "run_test \"Gather diagnostics script exists\" \"test -x '$PROJECT_ROOT/scripts/gather_worker_diagnostics.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "b3dc3aae",
      "relation": "mentions",
      "context": "run_test \"Gather diagnostics script exists\" \"test -x '$PROJECT_ROOT/scripts/gather_worker_diagnostics.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "1b255a42",
      "relation": "calls",
      "context": "run_test \"Quick diagnostics script exists\" \"test -x '$PROJECT_ROOT/scripts/quick_join_diagnostics.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "1b255a42",
      "relation": "mentions",
      "context": "run_test \"Quick diagnostics script exists\" \"test -x '$PROJECT_ROOT/scripts/quick_join_diagnostics.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "f0bc548f",
      "relation": "references",
      "context": "run_test \"Ansible playbook syntax check\" \"ansible-playbook --syntax-check '$PROJECT_ROOT/ansible/plays/setup-cluster.yaml'\""
    },
    {
      "from": "b804c80b",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "run_test \"Ansible playbook syntax check\" \"ansible-playbook --syntax-check '$PROJECT_ROOT/ansible/plays/setup-cluster.yaml'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script invalid args handling\" \"'$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh' 2>&1 | grep -q 'Usage\\|kubeadm.*join\\|Error'\" 1"
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "run_test \"Enhanced join script invalid args handling\" \"'$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh' 2>&1 | grep -q 'Usage\\|kubeadm.*join\\|Error'\" 1"
    },
    {
      "from": "b804c80b",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "run_test \"Gather diagnostics script help\" \"'$PROJECT_ROOT/scripts/gather_worker_diagnostics.sh' --help\""
    },
    {
      "from": "b804c80b",
      "to": "b3dc3aae",
      "relation": "mentions",
      "context": "run_test \"Gather diagnostics script help\" \"'$PROJECT_ROOT/scripts/gather_worker_diagnostics.sh' --help\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script contains crictl fixes\" \"grep -q 'crictl.*config\\|socket.*perm' '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "run_test \"Enhanced join script contains crictl fixes\" \"grep -q 'crictl.*config\\|socket.*perm' '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script contains token refresh\" \"grep -q 'refresh.*token\\|TOKEN_REFRESH' '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "run_test \"Enhanced join script contains token refresh\" \"grep -q 'refresh.*token\\|TOKEN_REFRESH' '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script contains kubelet validation\" \"grep -q 'validate.*kubelet.*config\\|kubelet.*config.*yaml' '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "run_test \"Enhanced join script contains kubelet validation\" \"grep -q 'validate.*kubelet.*config\\|kubelet.*config.*yaml' '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "f0bc548f",
      "relation": "references",
      "context": "run_test \"Ansible playbook contains enhanced error handling\" \"grep -q 'failure.*diagnostics\\|Handle.*join.*failures' '$PROJECT_ROOT/ansible/plays/setup-cluster.yaml'\""
    },
    {
      "from": "b804c80b",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "run_test \"Ansible playbook contains enhanced error handling\" \"grep -q 'failure.*diagnostics\\|Handle.*join.*failures' '$PROJECT_ROOT/ansible/plays/setup-cluster.yaml'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script has comprehensive logging\" \"grep -q 'LOG_FILE\\|log_both\\|tee.*LOG' '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "run_test \"Enhanced join script has comprehensive logging\" \"grep -q 'LOG_FILE\\|log_both\\|tee.*LOG' '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "121dca56",
      "relation": "mentions",
      "context": "run_test \"Enhanced troubleshooting documentation exists\" \"test -f '$PROJECT_ROOT/docs/MANUAL_CLUSTER_TROUBLESHOOTING.md'\""
    },
    {
      "from": "b804c80b",
      "to": "121dca56",
      "relation": "mentions",
      "context": "run_test \"Documentation contains crictl troubleshooting\" \"grep -q 'crictl.*communication\\|socket.*permission' '$PROJECT_ROOT/docs/MANUAL_CLUSTER_TROUBLESHOOTING.md'\""
    },
    {
      "from": "b804c80b",
      "to": "121dca56",
      "relation": "mentions",
      "context": "run_test \"Documentation mentions containerd filesystem fixes\" \"grep -q 'invalid capacity 0\\|filesystem.*capacity' '$PROJECT_ROOT/docs/MANUAL_CLUSTER_TROUBLESHOOTING.md'\""
    },
    {
      "from": "b804c80b",
      "to": "b3dc3aae",
      "relation": "calls",
      "context": "run_test \"Gather script collects comprehensive diagnostics\" \"grep -q 'kubelet.*log\\|containerd.*log\\|kubeadm.*join.*log' '$PROJECT_ROOT/scripts/gather_worker_diagnostics.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "b3dc3aae",
      "relation": "mentions",
      "context": "run_test \"Gather script collects comprehensive diagnostics\" \"grep -q 'kubelet.*log\\|containerd.*log\\|kubeadm.*join.*log' '$PROJECT_ROOT/scripts/gather_worker_diagnostics.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script crictl test simulation\" \"bash -n '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "run_test \"Enhanced join script crictl test simulation\" \"bash -n '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "run_test \"Enhanced join script has proper error handling\" \"grep -q 'exit 1\\|failed_when.*false\\|error.*exit' '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "run_test \"Enhanced join script has proper error handling\" \"grep -q 'exit 1\\|failed_when.*false\\|error.*exit' '$PROJECT_ROOT/scripts/enhanced_kubeadm_join.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "1b255a42",
      "relation": "calls",
      "context": "run_test \"Quick diagnostics script syntax check\" \"bash -n '$PROJECT_ROOT/scripts/quick_join_diagnostics.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "1b255a42",
      "relation": "mentions",
      "context": "run_test \"Quick diagnostics script syntax check\" \"bash -n '$PROJECT_ROOT/scripts/quick_join_diagnostics.sh'\""
    },
    {
      "from": "b804c80b",
      "to": "f0bc548f",
      "relation": "references",
      "context": "run_test \"Ansible playbook handles socket permissions\" \"grep -q 'containerd.*socket\\|socket.*permission\\|chgrp.*containerd' '$PROJECT_ROOT/ansible/plays/setup-cluster.yaml'\""
    },
    {
      "from": "b804c80b",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "run_test \"Ansible playbook handles socket permissions\" \"grep -q 'containerd.*socket\\|socket.*permission\\|chgrp.*containerd' '$PROJECT_ROOT/ansible/plays/setup-cluster.yaml'\""
    },
    {
      "from": "b804c80b",
      "to": "f0bc548f",
      "relation": "references",
      "context": "run_test \"Ansible playbook has skip join logic\" \"grep -q 'skip.*join\\|already.*joined\\|KUBELET_ALREADY_JOINED' '$PROJECT_ROOT/ansible/plays/setup-cluster.yaml'\""
    },
    {
      "from": "b804c80b",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "run_test \"Ansible playbook has skip join logic\" \"grep -q 'skip.*join\\|already.*joined\\|KUBELET_ALREADY_JOINED' '$PROJECT_ROOT/ansible/plays/setup-cluster.yaml'\""
    },
    {
      "from": "7daa8953",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "\"[ -x 'scripts/setup_static_ips_and_dns.sh' ]\""
    },
    {
      "from": "7daa8953",
      "to": "4f2a3f41",
      "relation": "mentions",
      "context": "\"[ -x 'scripts/setup_static_ips_and_dns.sh' ]\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "\"[ -x 'scripts/validate_static_ips_and_dns.sh' ]\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "mentions",
      "context": "\"[ -x 'scripts/validate_static_ips_and_dns.sh' ]\""
    },
    {
      "from": "7daa8953",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "\"bash -n scripts/setup_static_ips_and_dns.sh\""
    },
    {
      "from": "7daa8953",
      "to": "4f2a3f41",
      "relation": "mentions",
      "context": "\"bash -n scripts/setup_static_ips_and_dns.sh\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "\"bash -n scripts/validate_static_ips_and_dns.sh\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "mentions",
      "context": "\"bash -n scripts/validate_static_ips_and_dns.sh\""
    },
    {
      "from": "7daa8953",
      "to": "4096f24e",
      "relation": "mentions",
      "context": "\"[ -f 'docs/static-ips-and-dns.md' ]\""
    },
    {
      "from": "7daa8953",
      "to": "4096f24e",
      "relation": "mentions",
      "context": "\"grep -q 'Static IP Assignment' docs/static-ips-and-dns.md && grep -q 'homelab.com' docs/static-ips-and-dns.md\""
    },
    {
      "from": "7daa8953",
      "to": "4096f24e",
      "relation": "mentions",
      "context": "\"grep -q 'Static IP Assignment' docs/static-ips-and-dns.md && grep -q 'homelab.com' docs/static-ips-and-dns.md\""
    },
    {
      "from": "7daa8953",
      "to": "ad0e321a",
      "relation": "references",
      "context": "\"grep -q 'clusterIP: 10.96.0.10' manifests/network/coredns-service.yaml\""
    },
    {
      "from": "7daa8953",
      "to": "ad0e321a",
      "relation": "mentions",
      "context": "\"grep -q 'clusterIP: 10.96.0.10' manifests/network/coredns-service.yaml\""
    },
    {
      "from": "7daa8953",
      "to": "71a6fc0c",
      "relation": "references",
      "context": "\"grep -q 'homelab.com:53' manifests/network/coredns-configmap.yaml\""
    },
    {
      "from": "7daa8953",
      "to": "71a6fc0c",
      "relation": "mentions",
      "context": "\"grep -q 'homelab.com:53' manifests/network/coredns-configmap.yaml\""
    },
    {
      "from": "7daa8953",
      "to": "80025501",
      "relation": "references",
      "context": "\"grep -q 'hostNetwork: true' manifests/network/kube-proxy-daemonset.yaml\""
    },
    {
      "from": "7daa8953",
      "to": "80025501",
      "relation": "mentions",
      "context": "\"grep -q 'hostNetwork: true' manifests/network/kube-proxy-daemonset.yaml\""
    },
    {
      "from": "7daa8953",
      "to": "0a813314",
      "relation": "references",
      "context": "\"grep -q 'hostNetwork: true' manifests/cni/flannel.yaml\""
    },
    {
      "from": "7daa8953",
      "to": "0a813314",
      "relation": "mentions",
      "context": "\"grep -q 'hostNetwork: true' manifests/cni/flannel.yaml\""
    },
    {
      "from": "7daa8953",
      "to": "4f2a3f41",
      "relation": "mentions",
      "context": "\"grep -q 'setup_static_ips_and_dns.sh' deploy-cluster.sh\""
    },
    {
      "from": "7daa8953",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "\"grep -q 'setup_static_ips_and_dns.sh' deploy-cluster.sh\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "\"scripts/validate_static_ips_and_dns.sh --help | grep -q 'Usage:'\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "mentions",
      "context": "\"scripts/validate_static_ips_and_dns.sh --help | grep -q 'Usage:'\""
    },
    {
      "from": "7daa8953",
      "to": "4f2a3f41",
      "relation": "calls",
      "context": "\"timeout 10 scripts/setup_static_ips_and_dns.sh verify >/dev/null 2>&1 || true\""
    },
    {
      "from": "7daa8953",
      "to": "4f2a3f41",
      "relation": "mentions",
      "context": "\"timeout 10 scripts/setup_static_ips_and_dns.sh verify >/dev/null 2>&1 || true\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "\"scripts/validate_static_ips_and_dns.sh manifests | grep -q 'All tests passed'\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "mentions",
      "context": "\"scripts/validate_static_ips_and_dns.sh manifests | grep -q 'All tests passed'\""
    },
    {
      "from": "7daa8953",
      "to": "72c7835a",
      "relation": "calls",
      "context": "info \"  1. Deploy a cluster: ./deploy-cluster.sh deploy\""
    },
    {
      "from": "7daa8953",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "info \"  1. Deploy a cluster: ./deploy-cluster.sh deploy\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "calls",
      "context": "info \"  2. Run full validation: ./scripts/validate_static_ips_and_dns.sh\""
    },
    {
      "from": "7daa8953",
      "to": "5e0bdb07",
      "relation": "mentions",
      "context": "info \"  2. Run full validation: ./scripts/validate_static_ips_and_dns.sh\""
    },
    {
      "from": "7c606adf",
      "to": "2efbd35b",
      "relation": "calls",
      "context": "echo \"1. Run kubectl configuration fix: ./scripts/fix_worker_kubectl_config.sh\""
    },
    {
      "from": "7c606adf",
      "to": "2efbd35b",
      "relation": "mentions",
      "context": "echo \"1. Run kubectl configuration fix: ./scripts/fix_worker_kubectl_config.sh\""
    },
    {
      "from": "7c606adf",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"2. Fix kube-proxy issues: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "7c606adf",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "echo \"2. Fix kube-proxy issues: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "7c606adf",
      "to": "1970801b",
      "relation": "calls",
      "context": "echo \"3. Fix CNI bridge conflicts: ./scripts/fix_cni_bridge_conflict.sh\""
    },
    {
      "from": "7c606adf",
      "to": "1970801b",
      "relation": "mentions",
      "context": "echo \"3. Fix CNI bridge conflicts: ./scripts/fix_cni_bridge_conflict.sh\""
    },
    {
      "from": "7c606adf",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"3. Run comprehensive fix: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "7c606adf",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "echo \"3. Run comprehensive fix: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "ac18a0b8",
      "to": "6ec02888",
      "relation": "references",
      "context": "local template_file=\"/home/runner/work/VMStation/VMStation/ansible/plays/kubernetes/templates/kube-flannel-allnodes.yml\""
    },
    {
      "from": "ac18a0b8",
      "to": "6ec02888",
      "relation": "mentions",
      "context": "local template_file=\"/home/runner/work/VMStation/VMStation/ansible/plays/kubernetes/templates/kube-flannel-allnodes.yml\""
    },
    {
      "from": "ac18a0b8",
      "to": "f0bc548f",
      "relation": "references",
      "context": "local playbook_path=\"/home/runner/work/VMStation/VMStation/ansible/plays/setup-cluster.yaml\""
    },
    {
      "from": "ac18a0b8",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "local playbook_path=\"/home/runner/work/VMStation/VMStation/ansible/plays/setup-cluster.yaml\""
    },
    {
      "from": "ac18a0b8",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if cd \"$(dirname \"$playbook_path\")/../..\" && ansible-playbook --syntax-check -i ansible/inventory.txt ansible/plays/setup-cluster.yaml >/dev/null 2>&1; then"
    },
    {
      "from": "ac18a0b8",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "if cd \"$(dirname \"$playbook_path\")/../..\" && ansible-playbook --syntax-check -i ansible/inventory.txt ansible/plays/setup-cluster.yaml >/dev/null 2>&1; then"
    },
    {
      "from": "ca8ac5f9",
      "to": "7a273252",
      "relation": "calls",
      "context": "warn \"SOLUTION: Run 'sudo ./scripts/reset_cni_bridge.sh' to fix this issue\""
    },
    {
      "from": "ca8ac5f9",
      "to": "7a273252",
      "relation": "mentions",
      "context": "warn \"SOLUTION: Run 'sudo ./scripts/reset_cni_bridge.sh' to fix this issue\""
    },
    {
      "from": "ca8ac5f9",
      "to": "7a273252",
      "relation": "calls",
      "context": "warn \"SOLUTION: Run 'sudo ./scripts/reset_cni_bridge.sh' to fix this issue\""
    },
    {
      "from": "ca8ac5f9",
      "to": "7a273252",
      "relation": "mentions",
      "context": "warn \"SOLUTION: Run 'sudo ./scripts/reset_cni_bridge.sh' to fix this issue\""
    },
    {
      "from": "ca8ac5f9",
      "to": "7a273252",
      "relation": "calls",
      "context": "warn \" IMMEDIATE FIX: Run 'sudo ./scripts/reset_cni_bridge.sh'\""
    },
    {
      "from": "ca8ac5f9",
      "to": "7a273252",
      "relation": "mentions",
      "context": "warn \" IMMEDIATE FIX: Run 'sudo ./scripts/reset_cni_bridge.sh'\""
    },
    {
      "from": "ca8ac5f9",
      "to": "7a273252",
      "relation": "calls",
      "context": "echo \"  1. Run sudo ./scripts/reset_cni_bridge.sh if CNI bridge conflicts detected\""
    },
    {
      "from": "ca8ac5f9",
      "to": "7a273252",
      "relation": "mentions",
      "context": "echo \"  1. Run sudo ./scripts/reset_cni_bridge.sh if CNI bridge conflicts detected\""
    },
    {
      "from": "ca8ac5f9",
      "to": "1970801b",
      "relation": "calls",
      "context": "echo \"  2. Run ./scripts/fix_cni_bridge_conflict.sh for comprehensive CNI fixes\""
    },
    {
      "from": "ca8ac5f9",
      "to": "1970801b",
      "relation": "mentions",
      "context": "echo \"  2. Run ./scripts/fix_cni_bridge_conflict.sh for comprehensive CNI fixes\""
    },
    {
      "from": "ca8ac5f9",
      "to": "a249d706",
      "relation": "calls",
      "context": "echo \"  3. Run ./scripts/fix_homelab_node_issues.sh for node-specific problems\""
    },
    {
      "from": "ca8ac5f9",
      "to": "a249d706",
      "relation": "mentions",
      "context": "echo \"  3. Run ./scripts/fix_homelab_node_issues.sh for node-specific problems\""
    },
    {
      "from": "27edfbfd",
      "to": "09f42a5d",
      "relation": "mentions",
      "context": "warn \" Some validations failed. Run fix_nodeport_external_access.sh if needed.\""
    },
    {
      "from": "52dfbbac",
      "to": "52a04a1d",
      "relation": "calls",
      "context": "echo \"  1. sudo ./scripts/fix_worker_node_cni.sh --node storagenodet3500\""
    },
    {
      "from": "52dfbbac",
      "to": "52a04a1d",
      "relation": "mentions",
      "context": "echo \"  1. sudo ./scripts/fix_worker_node_cni.sh --node storagenodet3500\""
    },
    {
      "from": "52dfbbac",
      "to": "1970801b",
      "relation": "calls",
      "context": "echo \"  2. ./scripts/fix_cni_bridge_conflict.sh\""
    },
    {
      "from": "52dfbbac",
      "to": "1970801b",
      "relation": "mentions",
      "context": "echo \"  2. ./scripts/fix_cni_bridge_conflict.sh\""
    },
    {
      "from": "62721f19",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"Run: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "62721f19",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "echo \"Run: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "62721f19",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"Run: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "62721f19",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "echo \"Run: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "62721f19",
      "to": "090d10b0",
      "relation": "calls",
      "context": "echo \"1. Run: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "62721f19",
      "to": "090d10b0",
      "relation": "mentions",
      "context": "echo \"1. Run: ./scripts/fix_remaining_pod_issues.sh\""
    },
    {
      "from": "6ea2bc0f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "if grep -q \"detect_post_wipe_state\" scripts/enhanced_kubeadm_join.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "if grep -q \"detect_post_wipe_state\" scripts/enhanced_kubeadm_join.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "if grep -q \"WORKER_POST_WIPE\" scripts/enhanced_kubeadm_join.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "if grep -q \"WORKER_POST_WIPE\" scripts/enhanced_kubeadm_join.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"Detect post-wipe worker state\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "if grep -q \"Detect post-wipe worker state\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"worker_was_wiped\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "if grep -q \"worker_was_wiped\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"control-plane readiness\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "if grep -q \"control-plane readiness\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"Generate fresh join command for wiped workers\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "if grep -q \"Generate fresh join command for wiped workers\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"ttl=2h\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "if grep -q \"ttl=2h\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"Enhanced kubeadm join with post-wipe worker support\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "if grep -q \"Enhanced kubeadm join with post-wipe worker support\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"Post-join validation and verification for wiped workers\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "if grep -q \"Post-join validation and verification for wiped workers\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"NOT standalone\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "if grep -q \"NOT standalone\" ansible/plays/setup-cluster.yaml; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "044d8702",
      "relation": "mentions",
      "context": "if [ -f docs/POST_WIPE_WORKER_JOIN.md ]; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "3fbfd7f8",
      "relation": "mentions",
      "context": "if grep -q \"Post-Wipe Worker Recovery\" USAGE_INSTRUCTIONS.md; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "# Test 6: Validate deploy-cluster.sh integration"
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "info \"Test 6: Validating deploy-cluster.sh integration...\""
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -q \"cluster\" deploy-cluster.sh && grep -q \"setup-cluster.yaml\" deploy-cluster.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "if grep -q \"cluster\" deploy-cluster.sh && grep -q \"setup-cluster.yaml\" deploy-cluster.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "if grep -q \"cluster\" deploy-cluster.sh && grep -q \"setup-cluster.yaml\" deploy-cluster.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "if grep -q \"cluster\" deploy-cluster.sh && grep -q \"setup-cluster.yaml\" deploy-cluster.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "info \" deploy-cluster.sh integrates with enhanced cluster setup\""
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "error \" deploy-cluster.sh missing cluster setup integration\""
    },
    {
      "from": "6ea2bc0f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "if bash -n scripts/enhanced_kubeadm_join.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "if bash -n scripts/enhanced_kubeadm_join.sh; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if ansible-playbook --syntax-check ansible/plays/setup-cluster.yaml >/dev/null 2>&1; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "if ansible-playbook --syntax-check ansible/plays/setup-cluster.yaml >/dev/null 2>&1; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "if grep -r \"$marker\" ansible/plays/setup-cluster.yaml scripts/enhanced_kubeadm_join.sh >/dev/null 2>&1; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "references",
      "context": "if grep -r \"$marker\" ansible/plays/setup-cluster.yaml scripts/enhanced_kubeadm_join.sh >/dev/null 2>&1; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "f0bc548f",
      "relation": "mentions",
      "context": "if grep -r \"$marker\" ansible/plays/setup-cluster.yaml scripts/enhanced_kubeadm_join.sh >/dev/null 2>&1; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "if grep -r \"$marker\" ansible/plays/setup-cluster.yaml scripts/enhanced_kubeadm_join.sh >/dev/null 2>&1; then"
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "info \" Full integration with deploy-cluster.sh cluster deployment\""
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "calls",
      "context": "info \"  2. From master node: ./deploy-cluster.sh cluster\""
    },
    {
      "from": "6ea2bc0f",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "info \"  2. From master node: ./deploy-cluster.sh cluster\""
    },
    {
      "from": "5e0bdb07",
      "to": "ad0e321a",
      "relation": "references",
      "context": "\"grep -q 'clusterIP: 10.96.0.10' /home/runner/work/VMStation/VMStation/manifests/network/coredns-service.yaml\""
    },
    {
      "from": "5e0bdb07",
      "to": "ad0e321a",
      "relation": "mentions",
      "context": "\"grep -q 'clusterIP: 10.96.0.10' /home/runner/work/VMStation/VMStation/manifests/network/coredns-service.yaml\""
    },
    {
      "from": "5e0bdb07",
      "to": "80025501",
      "relation": "references",
      "context": "\"grep -q 'hostNetwork: true' /home/runner/work/VMStation/VMStation/manifests/network/kube-proxy-daemonset.yaml\""
    },
    {
      "from": "5e0bdb07",
      "to": "80025501",
      "relation": "mentions",
      "context": "\"grep -q 'hostNetwork: true' /home/runner/work/VMStation/VMStation/manifests/network/kube-proxy-daemonset.yaml\""
    },
    {
      "from": "5e0bdb07",
      "to": "0a813314",
      "relation": "references",
      "context": "\"grep -q 'hostNetwork: true' /home/runner/work/VMStation/VMStation/manifests/cni/flannel.yaml\""
    },
    {
      "from": "5e0bdb07",
      "to": "0a813314",
      "relation": "mentions",
      "context": "\"grep -q 'hostNetwork: true' /home/runner/work/VMStation/VMStation/manifests/cni/flannel.yaml\""
    },
    {
      "from": "5e0bdb07",
      "to": "4096f24e",
      "relation": "mentions",
      "context": "\"[ -f '/home/runner/work/VMStation/VMStation/docs/static-ips-and-dns.md' ]\""
    },
    {
      "from": "563e9b03",
      "to": "a249d706",
      "relation": "calls",
      "context": "echo \"   ./scripts/fix_homelab_node_issues.sh\""
    },
    {
      "from": "563e9b03",
      "to": "a249d706",
      "relation": "mentions",
      "context": "echo \"   ./scripts/fix_homelab_node_issues.sh\""
    },
    {
      "from": "563e9b03",
      "to": "36bd22c9",
      "relation": "calls",
      "context": "echo \"   ./scripts/check_coredns_status.sh\""
    },
    {
      "from": "563e9b03",
      "to": "36bd22c9",
      "relation": "mentions",
      "context": "echo \"   ./scripts/check_coredns_status.sh\""
    },
    {
      "from": "563e9b03",
      "to": "7071b363",
      "relation": "calls",
      "context": "echo \"   ./scripts/fix_coredns_unknown_status.sh\""
    },
    {
      "from": "563e9b03",
      "to": "7071b363",
      "relation": "mentions",
      "context": "echo \"   ./scripts/fix_coredns_unknown_status.sh\""
    },
    {
      "from": "563e9b03",
      "to": "72c7835a",
      "relation": "calls",
      "context": "echo \"   ./deploy-cluster.sh apps\""
    },
    {
      "from": "563e9b03",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "echo \"   ./deploy-cluster.sh apps\""
    },
    {
      "from": "563e9b03",
      "to": "a249d706",
      "relation": "calls",
      "context": "echo \"   ./scripts/fix_homelab_node_issues.sh\""
    },
    {
      "from": "563e9b03",
      "to": "a249d706",
      "relation": "mentions",
      "context": "echo \"   ./scripts/fix_homelab_node_issues.sh\""
    },
    {
      "from": "563e9b03",
      "to": "72c7835a",
      "relation": "calls",
      "context": "echo \"   ./deploy-cluster.sh apps\""
    },
    {
      "from": "563e9b03",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "echo \"   ./deploy-cluster.sh apps\""
    },
    {
      "from": "563e9b03",
      "to": "e8e06adf",
      "relation": "mentions",
      "context": "note \"For detailed troubleshooting, see: docs/HOMELAB_NODE_FIXES.md\""
    },
    {
      "from": "6564808c",
      "to": "1b255a42",
      "relation": "mentions",
      "context": "local diag_script=\"$script_dir/quick_join_diagnostics.sh\""
    },
    {
      "from": "6564808c",
      "to": "21313a3e",
      "relation": "mentions",
      "context": "local manual_fix=\"$script_dir/manual_containerd_filesystem_fix.sh\""
    },
    {
      "from": "6564808c",
      "to": "d3a169d7",
      "relation": "calls",
      "context": "info \"   sudo ./scripts/enhanced_kubeadm_join.sh \\\"<join-command-from-step-1>\\\"\""
    },
    {
      "from": "6564808c",
      "to": "d3a169d7",
      "relation": "mentions",
      "context": "info \"   sudo ./scripts/enhanced_kubeadm_join.sh \\\"<join-command-from-step-1>\\\"\""
    },
    {
      "from": "9553fda2",
      "to": "9088164c",
      "relation": "calls",
      "context": "echo \"2. Consider running: sudo ./fix_jellyfin_immediate.sh\""
    },
    {
      "from": "9553fda2",
      "to": "9088164c",
      "relation": "mentions",
      "context": "echo \"2. Consider running: sudo ./fix_jellyfin_immediate.sh\""
    },
    {
      "from": "9553fda2",
      "to": "9088164c",
      "relation": "calls",
      "context": "echo \"1. Run: sudo ./fix_jellyfin_immediate.sh\""
    },
    {
      "from": "9553fda2",
      "to": "9088164c",
      "relation": "mentions",
      "context": "echo \"1. Run: sudo ./fix_jellyfin_immediate.sh\""
    },
    {
      "from": "9553fda2",
      "to": "72c7835a",
      "relation": "calls",
      "context": "echo \"4. Consider cluster reset: ./deploy-cluster.sh reset\""
    },
    {
      "from": "9553fda2",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "echo \"4. Consider cluster reset: ./deploy-cluster.sh reset\""
    },
    {
      "from": "11010dc1",
      "to": "311b37fb",
      "relation": "calls",
      "context": "echo \"3. Re-run the fix: ./fix-cluster.sh\""
    },
    {
      "from": "11010dc1",
      "to": "311b37fb",
      "relation": "mentions",
      "context": "echo \"3. Re-run the fix: ./fix-cluster.sh\""
    },
    {
      "from": "d5c6d418",
      "to": "ebb30773",
      "relation": "calls",
      "context": "echo \"  sudo ./fix_jellyfin_cni_bridge_conflict.sh\""
    },
    {
      "from": "d5c6d418",
      "to": "ebb30773",
      "relation": "mentions",
      "context": "echo \"  sudo ./fix_jellyfin_cni_bridge_conflict.sh\""
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "info \"Test 1: Validating deploy-cluster.sh syntax...\""
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "if bash -n deploy-cluster.sh; then"
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "calls",
      "context": "if ./deploy-cluster.sh --help | grep -q \"net-reset\"; then"
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "if ./deploy-cluster.sh --help | grep -q \"net-reset\"; then"
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "calls",
      "context": "if ./deploy-cluster.sh --dry-run net-reset --confirm | grep -q \"DRY RUN MODE\"; then"
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "if ./deploy-cluster.sh --dry-run net-reset --confirm | grep -q \"DRY RUN MODE\"; then"
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "calls",
      "context": "if ./deploy-cluster.sh net-reset 2>&1 | grep -q \"requires explicit confirmation\"; then"
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "if ./deploy-cluster.sh net-reset 2>&1 | grep -q \"requires explicit confirmation\"; then"
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "if grep -q \"create_backup_directory\" deploy-cluster.sh; then"
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "if grep -q \"rollback_network_reset\" deploy-cluster.sh; then"
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "if grep -q \"verify_network_functionality\" deploy-cluster.sh; then"
    },
    {
      "from": "f07eb5d6",
      "to": "7036e452",
      "relation": "mentions",
      "context": "\"NETWORK_RESET_RUNBOOK.md\""
    },
    {
      "from": "f07eb5d6",
      "to": "4ac32a78",
      "relation": "mentions",
      "context": "\"CHANGELOG.md\""
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "if grep -q \"$feature\" deploy-cluster.sh; then"
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "calls",
      "context": "info \"  ./deploy-cluster.sh --dry-run net-reset --confirm  # Preview operations\""
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "info \"  ./deploy-cluster.sh --dry-run net-reset --confirm  # Preview operations\""
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "calls",
      "context": "info \"  ./deploy-cluster.sh net-reset --confirm            # Execute reset\""
    },
    {
      "from": "f07eb5d6",
      "to": "72c7835a",
      "relation": "mentions",
      "context": "info \"  ./deploy-cluster.sh net-reset --confirm            # Execute reset\""
    }
  ],
  "metadata": {
    "totalFiles": 162,
    "extractedAt": "2025-09-16T22:05:35.087Z",
    "repoPath": "/home/runner/work/VMStation/VMStation",
    "stats": {
      "byType": {
        "doc": 59,
        "config": 43,
        "template": 5,
        "script": 55
      },
      "totalSize": 1644349,
      "averageSize": 10150
    }
  }
}