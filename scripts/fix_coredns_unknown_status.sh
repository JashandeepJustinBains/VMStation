#!/bin/bash

# Fix CoreDNS Unknown Status After Flannel Regeneration
# This script addresses the issue where CoreDNS pods show "Unknown" status with no IP
# after flannel pods have been regenerated by deploy.sh full

set -e

# Color output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

info() { echo -e "${GREEN}[INFO]${NC} $1"; }
warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
error() { echo -e "${RED}[ERROR]${NC} $1"; }

# Check if we're running as root or with proper kubectl access
if ! kubectl get nodes >/dev/null 2>&1; then
    error "Cannot access Kubernetes cluster. Ensure kubectl is configured and you have cluster access."
    exit 1
fi

info "=== CoreDNS Unknown Status Fix ==="
echo "This script will diagnose and fix CoreDNS pods stuck in 'Unknown' status"
echo "after flannel regeneration."
echo

# Step 1: Diagnose the current state
info "Step 1: Diagnosing current cluster state"

echo "Current pod status across all namespaces:"
kubectl get pods -o wide --all-namespaces | grep -E "(coredns|flannel|NAME)"

echo
echo "CoreDNS specific status:"
kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide

echo
echo "Node status:"
kubectl get nodes -o wide

echo
echo "Flannel status:"
kubectl get pods -n kube-flannel -o wide

# Step 2: Check for specific CoreDNS issues
info "Step 2: Analyzing CoreDNS issues"

# Get CoreDNS pod details
COREDNS_PODS=$(kubectl get pods -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[*].metadata.name}')

if [ -z "$COREDNS_PODS" ]; then
    error "No CoreDNS pods found!"
    exit 1
fi

echo "Found CoreDNS pods: $COREDNS_PODS"

for pod in $COREDNS_PODS; do
    echo
    echo "=== Analyzing pod: $pod ==="
    
    # Get pod status
    POD_STATUS=$(kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.phase}')
    POD_IP=$(kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.podIP}')
    NODE_NAME=$(kubectl get pod -n kube-system "$pod" -o jsonpath='{.spec.nodeName}')
    
    echo "Pod Status: $POD_STATUS"
    echo "Pod IP: ${POD_IP:-<none>}"
    echo "Scheduled on node: ${NODE_NAME:-<none>}"
    
    # Check if pod has conditions
    kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.conditions[*]}' | jq -r . 2>/dev/null || echo "No detailed conditions available"
    
    # Check if there are events related to this pod
    echo "Recent events for $pod:"
    kubectl get events -n kube-system --field-selector involvedObject.name="$pod" --sort-by='.lastTimestamp' | tail -5
done

# Step 3: Check node taints and CNI readiness
info "Step 3: Checking node taints and CNI readiness"

echo "Node taints:"
kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints

echo
echo "Flannel DaemonSet status:"
kubectl get daemonset -n kube-flannel

echo
info "Checking and restarting any crashed flannel pods..."

# Check for crashed flannel pods
CRASHED_FLANNEL=$(kubectl get pods -n kube-flannel --no-headers | grep -E "(CrashLoopBackOff|Error)" | awk '{print $1}' || true)

if [ -n "$CRASHED_FLANNEL" ]; then
    warn "Found crashed flannel pods: $CRASHED_FLANNEL"
    
    for pod in $CRASHED_FLANNEL; do
        node_name=$(kubectl get pod -n kube-flannel "$pod" -o jsonpath='{.spec.nodeName}')
        echo "Crashed flannel pod $pod on node: $node_name"
        
        # Show recent logs for troubleshooting
        echo "Recent logs from crashed pod:"
        kubectl logs -n kube-flannel "$pod" --tail=15 || echo "Could not retrieve logs"
        echo
        
        # Delete the crashed pod to force restart
        echo "Restarting crashed flannel pod: $pod"
        kubectl delete pod -n kube-flannel "$pod" --force --grace-period=0
    done
    
    echo "Waiting 30 seconds for flannel pods to restart..."
    sleep 30
    
    # Show updated status
    echo "Updated flannel pod status:"
    kubectl get pods -n kube-flannel -o wide
else
    info "No crashed flannel pods found"
fi

echo
echo "Checking CNI configuration on nodes..."
for node in $(kubectl get nodes -o jsonpath='{.items[*].metadata.name}'); do
    echo "Node: $node"
    # Check if flannel pod is running on this node
    FLANNEL_POD=$(kubectl get pods -n kube-flannel -o wide | grep "$node" | awk '{print $1}' | head -1)
    if [ -n "$FLANNEL_POD" ]; then
        echo "  Flannel pod: $FLANNEL_POD"
        FLANNEL_STATUS=$(kubectl get pod -n kube-flannel "$FLANNEL_POD" -o jsonpath='{.status.phase}')
        echo "  Flannel status: $FLANNEL_STATUS"
    else
        echo "  No flannel pod found on this node"
    fi
done

# Step 4: Apply fixes
info "Step 4: Applying CoreDNS fixes"

# Fix 1: Remove any node taints that might prevent scheduling
echo "Checking for NoSchedule taints on control-plane..."
CONTROL_PLANE_NODES=$(kubectl get nodes -l node-role.kubernetes.io/control-plane --no-headers -o custom-columns=":metadata.name")

for node in $CONTROL_PLANE_NODES; do
    echo "Removing NoSchedule taint from control-plane node: $node"
    kubectl taint node "$node" node-role.kubernetes.io/control-plane:NoSchedule- 2>/dev/null || echo "  Taint not present or already removed"
    kubectl taint node "$node" node-role.kubernetes.io/master:NoSchedule- 2>/dev/null || echo "  Legacy master taint not present"
done

# Fix 1.5: Configure CoreDNS node affinity to prefer control-plane
echo
info "Configuring CoreDNS to prefer control-plane nodes..."
kubectl patch deployment coredns -n kube-system --type='merge' -p='
{
  "spec": {
    "template": {
      "spec": {
        "affinity": {
          "nodeAffinity": {
            "preferredDuringSchedulingIgnoredDuringExecution": [
              {
                "weight": 100,
                "preference": {
                  "matchExpressions": [
                    {
                      "key": "node-role.kubernetes.io/control-plane",
                      "operator": "Exists"
                    }
                  ]
                }
              },
              {
                "weight": 90,
                "preference": {
                  "matchExpressions": [
                    {
                      "key": "node-role.kubernetes.io/master",
                      "operator": "Exists"
                    }
                  ]
                }
              }
            ]
          }
        }
      }
    }
  }
}'
echo "CoreDNS node affinity configured to prefer control-plane nodes"

# Fix 2: Ensure CoreDNS has the right replica count for the cluster size
echo
info "Adjusting CoreDNS replica count..."

NODE_COUNT=$(kubectl get nodes --no-headers | wc -l)
if [ "$NODE_COUNT" -eq 1 ]; then
    DESIRED_REPLICAS=1
elif [ "$NODE_COUNT" -eq 2 ]; then
    DESIRED_REPLICAS=1
else
    DESIRED_REPLICAS=2
fi

echo "Cluster has $NODE_COUNT nodes, setting CoreDNS replicas to $DESIRED_REPLICAS"
kubectl scale deployment coredns -n kube-system --replicas="$DESIRED_REPLICAS"

# Fix 3: Delete stuck CoreDNS pods to force rescheduling
echo
info "Deleting stuck CoreDNS pods to force rescheduling..."

for pod in $COREDNS_PODS; do
    POD_STATUS=$(kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.phase}')
    POD_IP=$(kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.podIP}')
    
    if [ "$POD_STATUS" = "Unknown" ] || [ -z "$POD_IP" ] || [ "$POD_IP" = "null" ]; then
        echo "Deleting stuck CoreDNS pod: $pod (status: $POD_STATUS, IP: ${POD_IP:-none})"
        kubectl delete pod -n kube-system "$pod" --force --grace-period=0
    else
        echo "CoreDNS pod $pod looks healthy (status: $POD_STATUS, IP: $POD_IP)"
    fi
done

# Step 5: Wait for CoreDNS to be rescheduled and get IPs
info "Step 5: Waiting for CoreDNS to be rescheduled with proper IPs..."

echo "Waiting for CoreDNS deployment to be ready..."
kubectl rollout status deployment/coredns -n kube-system --timeout=300s

echo
echo "Waiting for all CoreDNS pods to have IP addresses..."

for i in {1..30}; do
    ALL_HAVE_IPS=true
    CURRENT_COREDNS_PODS=$(kubectl get pods -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[*].metadata.name}')
    
    echo "Attempt $i/30: Checking CoreDNS pod IPs..."
    
    for pod in $CURRENT_COREDNS_PODS; do
        POD_IP=$(kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.podIP}')
        POD_STATUS=$(kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.phase}')
        
        if [ -z "$POD_IP" ] || [ "$POD_IP" = "null" ] || [ "$POD_STATUS" != "Running" ]; then
            echo "  Pod $pod: Status=$POD_STATUS, IP=${POD_IP:-none} (not ready yet)"
            ALL_HAVE_IPS=false
        else
            echo "  Pod $pod: Status=$POD_STATUS, IP=$POD_IP (ready)"
        fi
    done
    
    if [ "$ALL_HAVE_IPS" = "true" ]; then
        info "All CoreDNS pods have IP addresses!"
        break
    fi
    
    echo "  Waiting 10 seconds before next check..."
    sleep 10
done

if [ "$ALL_HAVE_IPS" != "true" ]; then
    warn "Some CoreDNS pods still don't have IP addresses after 5 minutes"
    echo "This may indicate ongoing CNI issues. Check flannel logs:"
    echo "  kubectl logs -n kube-flannel -l app=flannel"
fi

# Step 6: Verify DNS functionality
info "Step 6: Testing DNS functionality"

echo "Testing DNS resolution from a test pod..."

# Create a test pod for DNS testing
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: dns-test
  namespace: kube-system
spec:
  containers:
  - name: dns-test
    image: busybox:1.35
    command: ['sleep', '300']
  restartPolicy: Never
  dnsPolicy: ClusterFirst
EOF

# Wait for test pod to be ready
echo "Waiting for DNS test pod to be ready..."
kubectl wait --for=condition=Ready pod/dns-test -n kube-system --timeout=60s

# Test DNS resolution
echo "Testing DNS resolution..."
if kubectl exec -n kube-system dns-test -- nslookup kubernetes.default.svc.cluster.local >/dev/null 2>&1; then
    info "DNS resolution is working correctly!"
    
    # Test external DNS
    if kubectl exec -n kube-system dns-test -- nslookup google.com >/dev/null 2>&1; then
        info "External DNS resolution also works!"
    else
        warn "External DNS resolution failed, but internal cluster DNS works"
    fi
else
    error "DNS resolution is still not working"
    echo "Manual troubleshooting needed:"
    echo "  kubectl logs -n kube-system -l k8s-app=kube-dns"
    echo "  kubectl exec -n kube-system dns-test -- nslookup kubernetes.default.svc.cluster.local"
fi

# Clean up test pod
kubectl delete pod dns-test -n kube-system --ignore-not-found

# Step 7: Final status check
info "Step 7: Final cluster status check"

echo "Current CoreDNS status:"
kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide

echo
echo "Current problematic pods status:"
kubectl get pods --all-namespaces | grep -E "(ContainerCreating|Pending|Unknown|Error|CrashLoopBackOff)" || echo "No problematic pods found!"

echo
echo "Flannel DaemonSet final status:"
kubectl get daemonset -n kube-flannel -o wide

info "=== CoreDNS Fix Complete ==="
echo
echo "If CoreDNS pods now have IP addresses and DNS resolution works,"
echo "the other pending pods should start to schedule and run properly."
echo
echo "Monitor progress with:"
echo "  kubectl get pods --all-namespaces -w"
echo
echo "If issues persist, check:"
echo "  1. Flannel logs: kubectl logs -n kube-flannel -l app=flannel"
echo "  2. CoreDNS logs: kubectl logs -n kube-system -l k8s-app=kube-dns"
echo "  3. Node conditions: kubectl describe nodes"