# Premium GitHub Copilot Agent Prompt for Kubernetes Monitoring Stack Troubleshooting

## Overview

This document contains a ready-to-use prompt for the premium GitHub Copilot agent to troubleshoot monitoring stack pods stuck in CrashLoopBackOff. The prompt is designed to provide safe, CLI-first triage and remediation plans for Prometheus/Grafana/Loki issues.

## Usage

Copy the prompt below and paste it to the premium GitHub Copilot agent. Then provide your cluster snapshot output when requested.

---

## The Prompt

You are an expert Kubernetes troubleshooting assistant. I have a cluster where the monitoring stack (Prometheus/Grafana/Loki/kube-prometheus-stack) has several pods stuck in CrashLoopBackOff (examples: `kube-prometheus-stack-grafana` Init:CrashLoopBackOff, `loki-stack-0` CrashLoopBackOff). Use the facts below and produce a safe, repeatable triage and remediation plan that an operator (me) will run manually.

### Cluster facts (use these hostnames/IPs exactly when referencing nodes)
- masternode — 192.168.4.63
- storagenodet3500 — 192.168.4.61
- localhost.localdomain — 192.168.4.62

### Constraints (MUST follow)
- Do NOT modify file permissions, create directories, or make changes automatically. Instead, provide the exact shell/cli commands I should run on the cluster or on the host nodes to inspect and fix problems.
- All suggested fix commands must be prefixed with an explanation of purpose and the expected safe outcome; mark any destructive or irreversible commands clearly and require explicit operator confirmation before running.
- When you suggest commands that would change permissions or create directories, present them only as operator-run suggestions (show the command but do not say you ran them).
- When suggesting kubectl or ansible actions, provide the exact command and the expected verification steps and outputs.
- Prefer idempotent checks and commands. Where there are multiple ways, provide the minimally invasive option first.

### What I want you to produce

1. **Quick triage checklist** — prioritized likely causes for CrashLoopBackOff specific to monitoring stacks (init-container chown, hostPath permissions, PVC/PV binding issues, RBAC/serviceaccount failures, missing configmaps/secrets, container image errors, readiness/liveness probe misconfigured). For each cause, include the exact commands to detect it and the expected signs in command output.

2. **Per-failing-pod diagnostic recipe** — for each failing pod you mention (use the pod names I gave as examples), give:
   - kubectl commands to run (describe, logs including init containers, events, manifest) and what to inspect in their output.
   - recommended node-side inspection commands (ssh to node -> list, stat, journalctl/kubelet/container runtime logs) but do NOT change anything. Use the node hostnames above.
   - common fix commands shown as operator-run suggestions (e.g., kubectl patch, kubectl rollout restart, kubectl delete pod, chown/chmod/mkdir examples to run on node) — each with an explanation and verification command.

3. **RBAC & ServiceAccount checks** — exact kubectl queries to verify required ClusterRole/ClusterRoleBinding and secrets mounted for the monitoring stack; commands to generate the minimal patch/role-binding you would apply (but do not apply it).

4. **Storage/PV/PVC checks** — commands to inspect PVCs, PVs, StorageClass, and volume events; safe commands to force rebind or recreate PVCs (described, not executed). Include instructions to verify hostPath vs PVC usages in the manifests.

5. **Config/Manifest issues** — commands to diff live manifests vs repo files (if manifests generated by helm/operator), and commands to inspect helm values and operator ConfigMaps. If hostnames are referenced in manifests, verify they are the correct hostnames (masternode, storagenodet3500, localhost.localdomain).

6. **Init-container specific checks** — many grafana/grafana sidecars run chown ops; provide the exact kubectl logs command to capture init container output and the node stat/ls commands to inspect the mounted path and its owner on the node.

7. **Node runtime logs** — provide node commands for journalctl (kubelet) and container runtime (containerd or dockerd) to capture container start failures; show both containerd and docker variants.

8. **Minimal ansible "check-only" tasks/snippets** — provide 2–3 Ansible tasks in check_mode that detect (but do not fix) common problems: missing dirs, wrong owners, missing SELinux contexts, unbound PVCs.

9. **Verification & smoke tests** — concrete kubectl commands to run after each proposed fix to confirm the pods move to Running and Ready.

10. **A concise prioritized action plan** (3–5 steps) to get the monitoring stack healthy, with expected verification after each step.

### Formatting and style requirements for the agent's response
- Use numbered, short steps and one-liner commands; each command must be copy-paste ready for a Linux shell or `kubectl` context. When node shell commands are required, label them "run on <hostname>".
- For each diagnostic command include: purpose (1 short sentence), command, and 1–2 lines describing expected good vs bad output.
- Highlight any command that would change filesystem state or permissions with a clear "operator-only: modifies node filesystem" warning.
- When suggesting manifest changes, show the minimal patch (kubectl patch or helm value snippet) and the exact verification commands; do not apply them.
- Use the hostnames provided above whenever the node is referenced.

### Example items I expect (sample, not exhaustive)
- kubectl -n monitoring describe pod kube-prometheus-stack-grafana-... -> inspect Init container exit code, message "chown: permission denied" etc.
- kubectl -n monitoring logs pod -c init-container -> capture chown output
- run on storagenodet3500: sudo ls -ld /var/lib/grafana /path/from/volume -> check ownership (show example good vs bad)
- operator-only suggested fix: sudo chown 472:472 /var/lib/grafana && sudo chmod 0755 /var/lib/grafana — mark this as operator-run and do not execute
- kubectl get pvc -n monitoring; kubectl describe pv <pv> -> check largewarning events
- kubectl -n monitoring get events --sort-by='.lastTimestamp' -> check for FailedMount, MountVolume.WaitForAttach
- kubectl -n monitoring get pods -o wide -> verify node assignment; identify hostPath mounts vs PVCs in pod manifest

### Finish with
- A prioritized 3-step action plan I can follow interactively (each step with the exact commands to run and verify).
- A short summary of likely root cause(s) and a one-line recommended change to the cluster/ansible playbook that would prevent recurrence (do not modify files).

### Cluster snapshot (paste when asking)
I will paste current pod output when running this prompt — use it to reference failing pod names. Use the hostnames exactly as given.

---

## Instructions for Use

1. Copy the entire prompt above (from "You are an expert..." to "...exactly as given.")
2. Paste it to the premium GitHub Copilot agent
3. When prompted, provide your cluster diagnostic output such as:
   - `kubectl -n monitoring get pods -o wide`
   - `kubectl -n monitoring describe pod <failing-pod-name>`
   - `kubectl -n monitoring get events --sort-by='.lastTimestamp'`
4. Follow the resulting action plan step-by-step

## Integration with VMStation

This prompt is designed to work alongside the existing VMStation diagnostic tools:

- `scripts/analyze_k8s_monitoring_diagnostics.sh` - For automated analysis of specific Grafana/Loki issues
- `scripts/diagnose_monitoring_permissions.sh` - For permission-specific diagnostics
- `scripts/validate_k8s_monitoring.sh` - For comprehensive monitoring stack validation

## Expected Output Format

The premium Copilot agent will provide:

1. **Immediate triage commands** - Safe read-only diagnostic commands to run first
2. **Detailed per-pod analysis** - Specific remediation steps for each failing component
3. **Storage and permission fixes** - Operator-run commands to fix common issues
4. **Verification steps** - Commands to confirm each fix worked
5. **Prevention recommendations** - Suggested playbook/configuration changes

## Safety Features

- ✅ No automatic execution of commands
- ✅ Clear marking of destructive operations
- ✅ Step-by-step verification after each change
- ✅ Minimal, surgical fixes preferred
- ✅ Proper hostname usage for the VMStation environment
- ✅ Integration with existing diagnostic workflow