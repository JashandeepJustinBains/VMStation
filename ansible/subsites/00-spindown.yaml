---
# Subsite 00: Spin-down / destructive cleanup helper
#
# This playbook helps operators fully remove Kubernetes and Podman from hosts so
# the environment can be tested from a clean state. It's intentionally
# conservative by default and requires an explicit variable to perform
# destructive actions.
#
# Usage (safe dry-run):
#   ansible-playbook -i ansible/inventory.txt ansible/subsites/00-spindown.yaml
#
# Run for real (destructive) - MUST set confirm_spindown=true:
#   ansible-playbook -i ansible/inventory.txt ansible/subsites/00-spindown.yaml -e confirm_spindown=true

- name: 00-spindown - Dry-run checks (non-destructive)
  hosts: all
  gather_facts: true
  become: true
  tasks:
    - name: Show warning and instructions when not confirmed
      ansible.builtin.debug:
        msg: |
          This playbook can remove Kubernetes components, container runtimes and data.
          To actually perform the destructive cleanup set the extra var:
            -e confirm_spindown=true
          Running without that variable will only report what would be changed.
      when: not (confirm_spindown | default(false))

    - name: Report installed services and packages (dry-run info)
      block:
        - name: Check for kubelet service
          ansible.builtin.service_facts:

        - name: Print candidate services (kubelet/containerd/podman/docker)
          ansible.builtin.debug:
            msg: |
              Services present: kubelet={{ ('kubelet' in services) | ternary('yes','no') }}; \
              containerd={{ ('containerd' in services) | ternary('yes','no') }}; \
              docker={{ ('docker' in services) | ternary('yes','no') }}; \
              podman={{ ('podman' in services) | ternary('yes','no') }}

        - name: Show package manager and installed k8s packages (if present)
          ansible.builtin.shell: |
            if command -v kubeadm >/dev/null 2>&1; then echo "kubeadm: installed"; else echo "kubeadm: missing"; fi
            if command -v kubectl >/dev/null 2>&1; then echo "kubectl: installed"; else echo "kubectl: missing"; fi
            if command -v kubelet >/dev/null 2>&1; then echo "kubelet: installed"; else echo "kubelet: missing"; fi
          register: pkg_check
        - name: Show pkg_check
          ansible.builtin.debug:
            var: pkg_check.stdout_lines

      when: not (confirm_spindown | default(false))

- name: 00-spindown - Perform destructive cleanup (requires confirm_spindown=true)
  hosts: all
  gather_facts: true
  become: true
  vars:
    cleanup_dirs:
      - /etc/kubernetes
      - /var/lib/kubelet
      - /var/lib/etcd
      - /var/lib/containerd
      - /var/lib/docker
      - /var/lib/podman
      - /var/lib/cni
      - /etc/cni
      - /var/lib/local-path-provisioner
      - /var/lib/k0s
      - /srv/monitoring_data
    
    # Legacy Podman containers to specifically stop and remove
    podman_containers:
      - prometheus
      - grafana
      - loki
      - promtail_local
      - local_registry
      - node_exporter
      - podman_exporter
      - podman_system_metrics
      - monitoring_pod
    
    # VMStation systemd services to clean up
    vmstation_systemd_services:
      - podman-monitoring
      - podman-prometheus
      - podman-grafana
      - podman-loki
    
    # Additional packages that might be installed
    additional_packages_apt:
      - kubernetes-cni
      - docker-ce
      - docker-ce-cli
      - containerd.io
      - runc
      - helm
    
    additional_packages_yum:
      - kubernetes-cni
      - docker-ce
      - docker-ce-cli
      - containerd.io
      - runc
      - helm
  tasks:
    - name: Safety gate - require explicit confirmation for destructive operations
      ansible.builtin.fail:
        msg: "confirm_spindown not set to true; aborting destructive cleanup. To proceed re-run with -e confirm_spindown=true"
      when: not (confirm_spindown | default(false))

    - name: Detect embedded ansible repo under /srv/monitoring_data to avoid deleting the repo by accident
      ansible.builtin.stat:
        path: /srv/monitoring_data/VMStation/ansible
      register: monitoring_repo_stat
      ignore_errors: true

    - name: Stop kubelet early to prevent static pods from being recreated while we clean manifests
      ansible.builtin.service:
        name: kubelet
        state: stopped
        enabled: false
      ignore_errors: true

    - name: Try to stop kube-apiserver systemd unit (if present) to avoid stray service instances
      ansible.builtin.service:
        name: kube-apiserver
        state: stopped
        enabled: false
      ignore_errors: true

    # === Enhanced Container-Specific Cleanup ===
    - name: Stop specific Podman containers (legacy VMStation)
      ansible.builtin.shell: |
        if command -v podman >/dev/null 2>&1; then
          if podman ps -a --format "{% raw %}{{.Names}}{% endraw %}" | grep -q "^{{ item }}$"; then
            echo "Stopping and removing container: {{ item }}"
            podman stop "{{ item }}" 2>/dev/null || echo "Failed to stop {{ item }}"
            podman rm "{{ item }}" 2>/dev/null || echo "Failed to remove {{ item }}"
          else
            echo "Container {{ item }} not found (already removed)"
          fi
        else
          echo "Podman not installed, skipping container {{ item }}"
        fi
      loop: "{{ podman_containers }}"
      ignore_errors: true

    - name: Remove Podman monitoring pod (if exists)
      ansible.builtin.shell: |
        if command -v podman >/dev/null 2>&1; then
          if podman pod exists monitoring_pod 2>/dev/null; then
            echo "Removing monitoring pod..."
            podman pod rm -f monitoring_pod
          else
            echo "Monitoring pod not found (already removed)"
          fi
        else
          echo "Podman not installed, skipping pod cleanup"
        fi
      ignore_errors: true

    # === Systemd Service Cleanup ===  
    - name: Stop and disable VMStation systemd services
      ansible.builtin.systemd:
        name: "{{ item }}"
        state: stopped
        enabled: false
      loop: "{{ vmstation_systemd_services }}"
      ignore_errors: true

    - name: Remove VMStation systemd service files
      ansible.builtin.file:
        path: "/etc/systemd/system/{{ item }}.service"
        state: absent
      loop: "{{ vmstation_systemd_services }}"
      ignore_errors: true

    - name: Reload systemd daemon after service removal
      ansible.builtin.systemd:
        daemon_reload: true
      ignore_errors: true

    # === Kubernetes API Server and etcd Cleanup ===
    - name: Stop kube-apiserver static pod if present
      ansible.builtin.shell: |
        if [ -f /etc/kubernetes/manifests/kube-apiserver.yaml ]; then
          echo "Removing kube-apiserver static pod manifest..."
          mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/kube-apiserver.yaml.backup || true
        else
          echo "kube-apiserver static pod manifest not found"
        fi
      ignore_errors: true

    - name: Stop kube-controller-manager static pod if present  
      ansible.builtin.shell: |
        if [ -f /etc/kubernetes/manifests/kube-controller-manager.yaml ]; then
          echo "Removing kube-controller-manager static pod manifest..."
          mv /etc/kubernetes/manifests/kube-controller-manager.yaml /tmp/kube-controller-manager.yaml.backup || true
        else
          echo "kube-controller-manager static pod manifest not found"
        fi
      ignore_errors: true

    - name: Stop kube-scheduler static pod if present
      ansible.builtin.shell: |
        if [ -f /etc/kubernetes/manifests/kube-scheduler.yaml ]; then
          echo "Removing kube-scheduler static pod manifest..."
          mv /etc/kubernetes/manifests/kube-scheduler.yaml /tmp/kube-scheduler.yaml.backup || true
        else
          echo "kube-scheduler static pod manifest not found"
        fi
      ignore_errors: true

    - name: Stop etcd static pod if present
      ansible.builtin.shell: |
        if [ -f /etc/kubernetes/manifests/etcd.yaml ]; then
          echo "Removing etcd static pod manifest..."
          mv /etc/kubernetes/manifests/etcd.yaml /tmp/etcd.yaml.backup || true
        else
          echo "etcd static pod manifest not found"  
        fi
      ignore_errors: true

    - name: Wait for static pods to terminate
      ansible.builtin.pause:
        seconds: 10
        prompt: "Waiting for static pods to terminate..."

    - name: Force kill any remaining kube processes
      ansible.builtin.shell: |
        echo "Force killing any remaining kubernetes processes..."
        pkill -f "kube-apiserver" || true
        pkill -f "kube-controller-manager" || true
        pkill -f "kube-scheduler" || true
        pkill -f "etcd" || true
        pkill -f "kubelet" || true
      ignore_errors: true

    - name: Clean up active CNI network interfaces
      ansible.builtin.shell: |
        echo "Cleaning up active CNI network interfaces..."
        # Remove cni0 interface if it exists
        if ip link show cni0 2>/dev/null; then
          echo "Removing cni0 interface"
          ip link set cni0 down || true
          ip link delete cni0 || true
        fi
        
        # Remove cbr0 interface if it exists  
        if ip link show cbr0 2>/dev/null; then
          echo "Removing cbr0 interface"
          ip link set cbr0 down || true
          ip link delete cbr0 || true
        fi
        
        # Remove flannel.1 interface if it exists
        if ip link show flannel.1 2>/dev/null; then
          echo "Removing flannel.1 interface"
          ip link set flannel.1 down || true
          ip link delete flannel.1 || true
        fi
        
        # Remove any vxlan interfaces that might be leftover
        for iface in $(ip link show | grep vxlan | cut -d: -f2 | tr -d ' '); do
          echo "Removing vxlan interface: $iface"
          ip link set $iface down || true
          ip link delete $iface || true
        done
      ignore_errors: true

    - name: Stop and disable kubelet if present
      ansible.builtin.service:
        name: kubelet
        state: stopped
        enabled: false
      ignore_errors: true

    - name: Stop and disable containerd if present
      ansible.builtin.service:
        name: containerd
        state: stopped
        enabled: false
      ignore_errors: true

    - name: Stop and disable docker if present
      ansible.builtin.service:
        name: docker
        state: stopped
        enabled: false
      ignore_errors: true

    - name: Stop and disable podman if present
      ansible.builtin.service:
        name: podman
        state: stopped
        enabled: false
      ignore_errors: true

    - name: Reset kubeadm if kubeadm is installed
      ansible.builtin.shell: kubeadm reset -f
      when: "ansible_facts.packages.kubeadm is defined or (lookup('ansible.builtin.pipe','command -v kubeadm >/dev/null 2>&1 && echo yes || true') == 'yes')"
      ignore_errors: true

    - name: Remove Kubernetes, container runtime packages (apt)
      ansible.builtin.apt:
        name: "{{ base_packages + additional_packages_apt }}"
        state: absent
        purge: yes
      vars:
        base_packages:
          - kubeadm
          - kubelet
          - kubectl
          - containerd
          - docker.io
          - podman
      when: ansible_pkg_mgr == 'apt'
      ignore_errors: true

    - name: Remove Kubernetes, container runtime packages (dnf/yum)
      ansible.builtin.yum:
        name: "{{ base_packages + additional_packages_yum }}"
        state: absent
      vars:
        base_packages:
          - kubeadm
          - kubelet
          - kubectl
          - containerd
          - docker
          - podman
      when: ansible_pkg_mgr in ['dnf','yum']
      ignore_errors: true

    - name: Preserve worker node kubelet.conf before cleanup (allows faster recovery)
      ansible.builtin.shell: |
        if [ -f /etc/kubernetes/kubelet.conf ] && [ "{{ inventory_hostname }}" != "{{ groups['monitoring_nodes'][0] }}" ]; then
          echo "Backing up worker kubelet.conf for faster recovery..."
          mkdir -p /tmp/vmstation-backup
          cp /etc/kubernetes/kubelet.conf /tmp/vmstation-backup/kubelet.conf.backup
          echo "Worker kubelet.conf backed up to /tmp/vmstation-backup/kubelet.conf.backup"
        else
          echo "Skipping kubelet.conf backup (not a worker or file doesn't exist)"
        fi
      ignore_errors: true
      when: inventory_hostname not in groups['monitoring_nodes']

    - name: Remove files from common cluster directories (preserve directory structure and permissions)
      ansible.builtin.shell: |
        if [ -d "{{ item }}" ]; then
          echo "Cleaning files from directory: {{ item }}"
          # For /etc/kubernetes on worker nodes, preserve the backed up kubelet.conf
          if [ "{{ item }}" == "/etc/kubernetes" ] && [ "{{ inventory_hostname }}" != "{{ groups['monitoring_nodes'][0] }}" ] && [ -f /tmp/vmstation-backup/kubelet.conf.backup ]; then
            echo "Preserving worker kubelet.conf during cleanup..."
            find "{{ item }}" -type f ! -name "kubelet.conf" -delete 2>/dev/null || true
          else
            find "{{ item }}" -type f -delete 2>/dev/null || true
          fi
        else
          echo "Directory {{ item }} does not exist, skipping"
        fi
      loop: "{{ cleanup_dirs }}"
      ignore_errors: true

    - name: Restore worker kubelet.conf after cleanup (for easier recovery)
      ansible.builtin.shell: |
        if [ -f /tmp/vmstation-backup/kubelet.conf.backup ] && [ "{{ inventory_hostname }}" != "{{ groups['monitoring_nodes'][0] }}" ]; then
          echo "Restoring worker kubelet.conf for easier recovery..."
          mkdir -p /etc/kubernetes
          cp /tmp/vmstation-backup/kubelet.conf.backup /etc/kubernetes/kubelet.conf
          chown root:root /etc/kubernetes/kubelet.conf
          chmod 600 /etc/kubernetes/kubelet.conf
          echo "Worker kubelet.conf restored from backup"
        else
          echo "No kubelet.conf backup to restore or not a worker node"
        fi
      ignore_errors: true
      when: inventory_hostname not in groups['monitoring_nodes']

    - name: Remove kube config for current user
      ansible.builtin.file:
        path: "{{ lookup('env','HOME') }}/.kube/config"
        state: absent
      ignore_errors: true

    - name: Remove /etc/cni/net.d
      ansible.builtin.file:
        path: /etc/cni/net.d
        state: absent
      ignore_errors: true

    - name: Remove local-path provisioner data directory if present
      ansible.builtin.file:
        path: /var/lib/local-path-provisioner
        state: absent
      ignore_errors: true

    - name: Remove leftover container state directories (safe cleanup)
      ansible.builtin.file:
        path: "{{ item }}"
        state: absent
      loop:
        - /run/containerd
        - /run/docker
        - /run/podman
      ignore_errors: true

    # === Additional Podman/Container Cleanup ===
    - name: Podman system prune (remove unused images and volumes)
      ansible.builtin.shell: |
        if command -v podman >/dev/null 2>&1; then
          echo "Running podman system prune..."
          podman system prune -a -f
        else
          echo "Podman not installed, skipping system prune"
        fi
      ignore_errors: true

    - name: Clean up container image stores and overlayfs mounts
      ansible.builtin.shell: |
        echo "Cleaning up container storage overlays..."
        # Unmount any overlay filesystems
        mount | grep overlay | awk '{print $3}' | xargs -r umount -l 2>/dev/null || true
        
        # Clean up container storage
        rm -rf /var/lib/containers/storage/overlay* 2>/dev/null || true
        rm -rf /var/lib/containers/storage/vfs* 2>/dev/null || true
        
        # Clean up containerd storage
        rm -rf /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/* 2>/dev/null || true
        
        echo "Container storage cleanup completed"
      ignore_errors: true

    # === Registry Configuration Cleanup ===
    - name: Remove Podman registry configuration
      ansible.builtin.file:
        path: /etc/containers/registries.conf.d/local-registry.conf
        state: absent
      ignore_errors: true

    # === Helm Cleanup ===
    - name: Remove Helm releases (if Helm is available)
      ansible.builtin.shell: |
        if command -v helm >/dev/null 2>&1; then
          echo "Removing all Helm releases..."
          helm list --all-namespaces -q | xargs -r helm uninstall --namespace
        else
          echo "Helm not found, skipping Helm cleanup"
        fi
      ignore_errors: true

    # === Certificate and TLS Cleanup ===
    - name: Remove cert-manager and VMStation certificates
      ansible.builtin.shell: |
        if command -v kubectl >/dev/null 2>&1; then
          echo "Removing cert-manager and VMStation certificates..."
          kubectl delete certificates --all --all-namespaces --ignore-not-found || true
          kubectl delete certificaterequests --all --all-namespaces --ignore-not-found || true
          kubectl delete secrets -l cert-manager.io/certificate-name --all-namespaces --ignore-not-found || true
        else
          echo "kubectl not found, skipping certificate cleanup"
        fi
      ignore_errors: true

    - name: Remove certificate directories
      ansible.builtin.file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/ssl/certs/vmstation
        - /etc/pki/tls/certs/vmstation
        - /usr/local/share/ca-certificates/vmstation
      ignore_errors: true

    # === Firewall Rules Cleanup (if ufw is used) ===
    - name: Remove VMStation firewall rules (ufw)
      ansible.builtin.shell: |
        if command -v ufw >/dev/null 2>&1; then
          echo "Removing VMStation firewall rules..."
          # Remove common VMStation ports
          ufw --force delete allow 6443 2>/dev/null || true
          ufw --force delete allow 30300 2>/dev/null || true
          ufw --force delete allow 30090 2>/dev/null || true
          ufw --force delete allow 31100 2>/dev/null || true
          ufw --force delete allow 30903 2>/dev/null || true
          ufw --force delete allow 5000 2>/dev/null || true
          ufw --force delete allow 32000 2>/dev/null || true
          ufw --force delete allow 32001 2>/dev/null || true
          ufw --force delete allow 32002 2>/dev/null || true
        else
          echo "ufw not found, skipping firewall cleanup"
        fi
      ignore_errors: true

    # === Additional Configuration Cleanup ===
    - name: Remove Docker daemon configuration
      ansible.builtin.file:
        path: /etc/docker/daemon.json
        state: absent
      ignore_errors: true

    - name: Remove containerd configuration
      ansible.builtin.file:
        path: /etc/containerd/config.toml
        state: absent
      ignore_errors: true

    # === Network and iptables cleanup ===
    - name: Clean up iptables rules related to Kubernetes and containers
      ansible.builtin.shell: |
        echo "Cleaning up iptables rules..."
        
        # Clean up Kubernetes chains
        iptables -t nat -F KUBE-SERVICES 2>/dev/null || true
        iptables -t nat -X KUBE-SERVICES 2>/dev/null || true
        iptables -t nat -F KUBE-NODEPORTS 2>/dev/null || true
        iptables -t nat -X KUBE-NODEPORTS 2>/dev/null || true
        iptables -t nat -F KUBE-POSTROUTING 2>/dev/null || true
        iptables -t nat -X KUBE-POSTROUTING 2>/dev/null || true
        iptables -t nat -F KUBE-MARK-MASQ 2>/dev/null || true
        iptables -t nat -X KUBE-MARK-MASQ 2>/dev/null || true
        
        # Clean up filter chains
        iptables -t filter -F KUBE-FORWARD 2>/dev/null || true
        iptables -t filter -X KUBE-FORWARD 2>/dev/null || true
        iptables -t filter -F KUBE-FIREWALL 2>/dev/null || true
        iptables -t filter -X KUBE-FIREWALL 2>/dev/null || true
        
        # Clean up CNI related rules
        iptables -t nat -F CNI-* 2>/dev/null || true
        iptables -t nat -X CNI-* 2>/dev/null || true
        iptables -t filter -F CNI-* 2>/dev/null || true
        iptables -t filter -X CNI-* 2>/dev/null || true
        
        # Clean up FLANNEL chains
        iptables -t nat -F FLANNEL 2>/dev/null || true
        iptables -t nat -X FLANNEL 2>/dev/null || true
        iptables -t filter -F FLANNEL 2>/dev/null || true
        iptables -t filter -X FLANNEL 2>/dev/null || true
        
        echo "iptables cleanup completed"
      ignore_errors: true

    - name: Clean up routing tables and IP routes
      ansible.builtin.shell: |
        echo "Cleaning up IP routes..."
        
        # Remove pod network routes (10.244.0.0/16 is common Flannel range)
        ip route del 10.244.0.0/16 2>/dev/null || true
        
        # Remove any routes pointing to CNI interfaces
        ip route show | grep -E "(cni0|cbr0|flannel)" | while read route; do
          echo "Removing route: $route"
          ip route del $route 2>/dev/null || true
        done
        
        # Flush any remaining routes in custom tables
        for table in $(ip route show table all | grep -o 'table [0-9]*' | cut -d' ' -f2 | sort -u); do
          if [ "$table" -gt 255 ]; then  # Custom tables are usually > 255
            echo "Flushing routing table $table"
            ip route flush table "$table" 2>/dev/null || true
          fi
        done
        
        echo "IP route cleanup completed"
      ignore_errors: true

    # === Systemd and service cleanup ===
    - name: Clean up systemd drop-in directories and overrides
      ansible.builtin.shell: |
        echo "Cleaning up systemd drop-ins and overrides..."
        
        # Remove kubelet drop-ins
        rm -rf /etc/systemd/system/kubelet.service.d/ 2>/dev/null || true
        rm -rf /usr/lib/systemd/system/kubelet.service.d/ 2>/dev/null || true
        
        # Remove containerd drop-ins
        rm -rf /etc/systemd/system/containerd.service.d/ 2>/dev/null || true
        
        # Remove docker drop-ins
        rm -rf /etc/systemd/system/docker.service.d/ 2>/dev/null || true
        
        # Remove any VMStation-related timers or services
        find /etc/systemd/system -name "*vmstation*" -o -name "*monitoring*" -o -name "*k8s*" -o -name "*kubernetes*" | xargs rm -f 2>/dev/null || true
        
        echo "systemd cleanup completed"
      ignore_errors: true

    # === User configuration cleanup ===
    - name: Clean up user bash history and profiles related to VMStation
      ansible.builtin.shell: |
        echo "Cleaning up user configurations..."
        
        # Clean up root user configs
        sed -i '/# VMStation/,/# End VMStation/d' /root/.bashrc 2>/dev/null || true
        sed -i '/kubectl\|kubeadm\|docker\|podman/d' /root/.bash_history 2>/dev/null || true
        
        # Clean up other users that might have been used
        for user_home in /home/*; do
          if [ -d "$user_home" ]; then
            user=$(basename "$user_home")
            sed -i '/# VMStation/,/# End VMStation/d' "$user_home/.bashrc" 2>/dev/null || true
            sed -i '/kubectl\|kubeadm\|docker\|podman/d' "$user_home/.bash_history" 2>/dev/null || true
          fi
        done
        
        echo "User configuration cleanup completed"
      ignore_errors: true

    # === Temporary and cache cleanup ===
    - name: Clean up temporary files and caches
      ansible.builtin.shell: |
        echo "Cleaning up temporary files and caches..."
        
        # Clean up VMStation temp files
        rm -rf /tmp/vmstation-* 2>/dev/null || true
        rm -rf /tmp/k8s-* 2>/dev/null || true
        rm -rf /tmp/kube-* 2>/dev/null || true
        rm -rf /tmp/flannel-* 2>/dev/null || true
        
        # Clean up package manager caches
        if command -v apt >/dev/null 2>&1; then
          apt clean 2>/dev/null || true
        fi
        
        if command -v yum >/dev/null 2>&1; then
          yum clean all 2>/dev/null || true
        fi
        
        # Clean up container-related caches
        rm -rf /var/cache/containers/ 2>/dev/null || true
        rm -rf /var/cache/docker/ 2>/dev/null || true
        
        echo "Cache cleanup completed"
      ignore_errors: true

    - name: Inform operator - host cleanup completed
      ansible.builtin.debug:
        msg: "Host-level spin-down tasks completed on {{ inventory_hostname }}"

    # === Final validation and cleanup report ===
    - name: Validate cleanup completeness
      ansible.builtin.shell: |
        echo "=== Cleanup Validation Report for {{ inventory_hostname }} ==="
        
        # Check for remaining processes
        echo "Checking for remaining Kubernetes/container processes..."
        remaining_procs=$(ps aux | grep -E "(kube|docker|podman|containerd|etcd)" | grep -v grep || echo "None found")
        if [ "$remaining_procs" = "None found" ]; then
          echo "✓ No Kubernetes/container processes running"
        else
          echo "⚠ Remaining processes found:"
          echo "$remaining_procs"
        fi
        
        # Check for remaining network interfaces
        echo ""
        echo "Checking for remaining CNI interfaces..."
        remaining_ifaces=$(ip link show | grep -E "(cni0|cbr0|flannel)" || echo "None found")
        if [ "$remaining_ifaces" = "None found" ]; then
          echo "✓ No CNI interfaces remaining"
        else
          echo "⚠ Remaining CNI interfaces:"
          echo "$remaining_ifaces"
        fi
        
        # Check for remaining mounts
        echo ""
        echo "Checking for remaining container/k8s mounts..."
        remaining_mounts=$(mount | grep -E "(overlay|kubelet|docker|podman|containerd)" || echo "None found")
        if [ "$remaining_mounts" = "None found" ]; then
          echo "✓ No container/k8s mounts remaining"
        else
          echo "⚠ Remaining mounts:"
          echo "$remaining_mounts"
        fi
        
        # Check directory cleanup
        echo ""
        echo "Checking directory cleanup..."
        for dir in /etc/kubernetes /var/lib/kubelet /var/lib/etcd /var/lib/containerd /etc/cni; do
          if [ -d "$dir" ] && [ "$(ls -A "$dir" 2>/dev/null)" ]; then
            echo "⚠ Directory $dir still contains files"
          else
            echo "✓ Directory $dir is clean or removed"
          fi
        done
        
        echo ""
        echo "=== Cleanup validation completed ==="
      register: cleanup_validation
      ignore_errors: true
    
    - name: Display cleanup validation results
      ansible.builtin.debug:
        var: cleanup_validation.stdout_lines

- name: 00-spindown - cluster control cleanup (localhost)
  hosts: all
  gather_facts: false
  connection: local
  become: false
  tasks:
    - name: Attempt to delete all namespaces and CRDs (best-effort)
      ansible.builtin.shell: |
        if command -v kubectl >/dev/null 2>&1; then
          kubectl delete namespaces --all --ignore-not-found || true
          kubectl delete crd --all --ignore-not-found || true
        else
          echo "kubectl not present"
        fi
      ignore_errors: true

    - name: Remove all Helm releases from localhost (if available)
      ansible.builtin.shell: |
        if command -v helm >/dev/null 2>&1; then
          echo "Removing all Helm releases..."
          helm list --all-namespaces -q | xargs -r -I {} helm uninstall {} || true
        else
          echo "Helm not found on localhost"
        fi
      ignore_errors: true

    - name: Clean up kubectl contexts and clusters
      ansible.builtin.shell: |
        if command -v kubectl >/dev/null 2>&1; then
          echo "Cleaning up kubectl contexts..."
          kubectl config delete-context --all 2>/dev/null || true
          kubectl config delete-cluster --all 2>/dev/null || true
          kubectl config unset current-context 2>/dev/null || true
        else
          echo "kubectl not present on localhost"
        fi
      ignore_errors: true

    - name: Remove kube config from control user
      ansible.builtin.file:
        path: "{{ lookup('env','HOME') }}/.kube/config"
        state: absent
      ignore_errors: true

    - name: Remove entire .kube directory
      ansible.builtin.file:
        path: "{{ lookup('env','HOME') }}/.kube"
        state: absent
      ignore_errors: true

    - name: Remove Helm configuration
      ansible.builtin.file:
        path: "{{ lookup('env','HOME') }}/.config/helm"
        state: absent
      ignore_errors: true

    - name: Final message
      ansible.builtin.debug:
        msg: |
          Spin-down complete (best-effort). If you used a kubeadm or other custom
          installer, additional manual cleanup may be required (for example removing
          cloud-provider resources, leftover mounts, or PV hostPaths on nodes).
          
          Enhanced cleanup performed:
          - Stopped and removed legacy Podman containers
          - Cleaned up VMStation systemd services
          - Removed registry configurations
          - Cleaned up certificates and TLS configurations
          - Removed firewall rules (ufw)
          - Cleaned up Helm releases and configuration
          - Removed kubectl contexts and clusters
