---
# VMStation Cluster Verification and Smoke Tests
# Comprehensive validation of the Kubernetes cluster deployment

- name: "Cluster Health Verification"
  hosts: monitoring_nodes
  become: true
  vars:
    kubeconfig_path: "/etc/kubernetes/admin.conf"
    verification_timeout: 300
  tasks:
    - name: "Debug show kubeconfig_path and check file"
      block:
        - name: "Show kubeconfig_path variable"
          ansible.builtin.debug:
            msg: "kubeconfig_path={{ kubeconfig_path | default('UNDEFINED') }}"

        - name: "Stat kubeconfig file"
          ansible.builtin.stat:
            path: "{{ kubeconfig_path }}"
          register: kubeconfig_file
          failed_when: false

        - name: "Debug kubeconfig file stat result"
          ansible.builtin.debug:
            var: kubeconfig_file
      when: kubeconfig_path is defined

    - name: "Verify kubectl is working"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        kubeconfig: "{{ kubeconfig_path }}"
      register: cluster_nodes
      failed_when: cluster_nodes.resources | length == 0

    - name: "Check all nodes are Ready"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        kubeconfig: "{{ kubeconfig_path }}"
      register: node_status
      failed_when: >
        node_status.resources | map(attribute='status.conditions') |
        map('selectattr', 'type', 'equalto', 'Ready') |
        map('map', attribute='status') | map('first') | list |
        select('equalto', 'False') | list | length > 0

    - name: "Display node status"
      debug:
        msg: "Node {{ item.metadata.name }} ({{ item.status.addresses | selectattr('type', 'equalto', 'InternalIP') | map(attribute='address') | first }}) is {{ item.status.conditions | selectattr('type', 'equalto', 'Ready') | map(attribute='status') | first }}"
      loop: "{{ node_status.resources }}"

    - name: "Verify correct number of nodes"
      assert:
        that:
          - node_status.resources | length == 3
        fail_msg: "Expected 3 nodes, found {{ node_status.resources | length }}"
        success_msg: "‚úÖ All 3 nodes are present in the cluster"

    - name: "Verify control plane node"
      assert:
        that:
          - node_status.resources | selectattr('metadata.name', 'search', 'masternode|monitoring') | list | length == 1
        fail_msg: "Control plane node not found or multiple control planes detected"
        success_msg: "‚úÖ Control plane node verified"

    - name: "Verify worker nodes"
      assert:
        that:
          - node_status.resources | selectattr('metadata.name', 'search', 'storagenodet3500|storage') | list | length == 1
          - node_status.resources | selectattr('metadata.name', 'search', 'homelab|compute') | list | length == 1
        fail_msg: "Expected worker nodes not found"
        success_msg: "‚úÖ Both worker nodes verified"

    - name: "Check CoreDNS pods are Running"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: kube-system
        label_selectors:
          - k8s-app=kube-dns
        kubeconfig: "{{ kubeconfig_path }}"
      register: coredns_pods
      failed_when: >
        coredns_pods.resources | length == 0 or
        coredns_pods.resources | map(attribute='status.phase') | list |
        select('equalto', 'Running') | list | length != coredns_pods.resources | length

    - name: "Display CoreDNS status"
      debug:
        msg: "CoreDNS pod {{ item.metadata.name }} is {{ item.status.phase }}"
      loop: "{{ coredns_pods.resources }}"

    - name: "Verify CNI (Flannel) pods are Running"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: kube-flannel
        label_selectors:
          - app=flannel
        kubeconfig: "{{ kubeconfig_path }}"
      register: flannel_pods
      failed_when: >
        flannel_pods.resources | length == 0 or
        flannel_pods.resources | map(attribute='status.phase') | list |
        select('equalto', 'Running') | list | length != flannel_pods.resources | length

    - name: "Display Flannel status"
      debug:
        msg: "Flannel pod {{ item.metadata.name }} on node {{ item.spec.nodeName }} is {{ item.status.phase }}"
      loop: "{{ flannel_pods.resources }}"

- name: "Deploy and Verify Monitoring Stack"
  hosts: monitoring_nodes
  become: true
  vars:
    kubeconfig_path: "/etc/kubernetes/admin.conf"
  tasks:
    - name: "Ensure node labels are applied for monitoring deployment"
      shell: |
        {% for label in hostvars[item]['node_labels'] | default([]) %}
        kubectl label node {{ item }} {{ label }} --overwrite || true
        {% endfor %}
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      loop: "{{ groups['all'] }}"
      when: hostvars[item]['node_labels'] is defined
      ignore_errors: yes

    - name: "Check existing Prometheus Deployment selector"
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: Deployment
        name: prometheus
        namespace: monitoring
        kubeconfig: "{{ kubeconfig_path }}"
      register: existing_prometheus
      failed_when: false

    - name: "Delete Prometheus Deployment if selector doesn't match manifest expectation"
      when: existing_prometheus.resources | length > 0 and (existing_prometheus.resources[0].spec.selector.matchLabels.get('app.kubernetes.io/name','') != 'prometheus')
      kubernetes.core.k8s:
        state: absent
        kind: Deployment
        api_version: apps/v1
        name: prometheus
        namespace: monitoring
        kubeconfig: "{{ kubeconfig_path }}"

    - name: "Check existing Grafana Deployment selector"
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: Deployment
        name: grafana
        namespace: monitoring
        kubeconfig: "{{ kubeconfig_path }}"
      register: existing_grafana
      failed_when: false

    - name: "Delete Grafana Deployment if selector doesn't match manifest expectation"
      when: existing_grafana.resources | length > 0 and (existing_grafana.resources[0].spec.selector.matchLabels.get('app.kubernetes.io/name','') != 'grafana')
      kubernetes.core.k8s:
        state: absent
        kind: Deployment
        api_version: apps/v1
        name: grafana
        namespace: monitoring
        kubeconfig: "{{ kubeconfig_path }}"

    - name: "Deploy monitoring namespace and components"
      kubernetes.core.k8s:
        state: present
        src: "{{ item }}"
        kubeconfig: "{{ kubeconfig_path }}"
      loop:
        - "{{ playbook_dir }}/../../manifests/monitoring/prometheus.yaml"
        - "{{ playbook_dir }}/../../manifests/monitoring/grafana.yaml"
      register: monitoring_deploy

    - name: "Wait for Prometheus pod to be Running"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: monitoring
        label_selectors:
          - app=prometheus
        kubeconfig: "{{ kubeconfig_path }}"
      register: prometheus_pods
      until: >
        prometheus_pods.resources | length > 0 and
        prometheus_pods.resources | map(attribute='status.phase') | list | unique == ['Running']
      retries: 20
      delay: 15
      failed_when: >
        prometheus_pods.resources | length == 0 or
        'Running' not in (prometheus_pods.resources | map(attribute='status.phase') | list)

    - name: "Wait for Grafana pod to be Running"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: monitoring
        label_selectors:
          - app=grafana
        kubeconfig: "{{ kubeconfig_path }}"
      register: grafana_pods
      until: >
        grafana_pods.resources | length > 0 and
        grafana_pods.resources | map(attribute='status.phase') | list | unique == ['Running']
      retries: 20
      delay: 15
      failed_when: >
        grafana_pods.resources | length == 0 or
        'Running' not in (grafana_pods.resources | map(attribute='status.phase') | list)

    - name: "Verify monitoring pods are scheduled on control plane"
      assert:
        that:
          - prometheus_pods.resources | map(attribute='spec.nodeName') | list | intersect(['masternode', 'monitoring_nodes']) | length > 0
          - grafana_pods.resources | map(attribute='spec.nodeName') | list | intersect(['masternode', 'monitoring_nodes']) | length > 0
        fail_msg: "Monitoring pods are not scheduled on the control plane node"
        success_msg: "‚úÖ Monitoring pods correctly scheduled on control plane"

    - name: "Test Prometheus metrics endpoint"
      uri:
        url: "http://192.168.4.63:30090/api/v1/query?query=up"
        method: GET
        return_content: yes
      register: prometheus_test
      retries: 5
      delay: 10
      until: prometheus_test.status == 200

    - name: "Test Grafana web interface"
      uri:
        url: "http://192.168.4.63:30300/api/health"
        method: GET
        return_content: yes
      register: grafana_test
      retries: 5
      delay: 10
      until: grafana_test.status == 200

- name: "Deploy and Verify Jellyfin"
  hosts: monitoring_nodes
  become: true
  vars:
    kubeconfig_path: "/etc/kubernetes/admin.conf"
  tasks:
    - name: "Clean up any existing Jellyfin resources to prevent conflicts"
      kubernetes.core.k8s:
        state: absent
        kubeconfig: "{{ kubeconfig_path }}"
        api_version: "{{ item.api_version }}"
        kind: "{{ item.kind }}"
        name: "{{ item.name }}"
        namespace: "{{ item.namespace | default(omit) }}"
      loop:
        - { api_version: "v1", kind: "Service", name: "jellyfin-service", namespace: "jellyfin" }
        - { api_version: "v1", kind: "Service", name: "jellyfin", namespace: "jellyfin" }
        - { api_version: "apps/v1", kind: "Deployment", name: "jellyfin", namespace: "jellyfin" }
        - { api_version: "v1", kind: "Pod", name: "jellyfin", namespace: "jellyfin" }
        - { api_version: "v1", kind: "PersistentVolumeClaim", name: "jellyfin-config-pvc", namespace: "jellyfin" }
        - { api_version: "v1", kind: "PersistentVolumeClaim", name: "jellyfin-media-pvc", namespace: "jellyfin" }
        - { api_version: "v1", kind: "PersistentVolume", name: "jellyfin-config-pv" }
        - { api_version: "v1", kind: "PersistentVolume", name: "jellyfin-media-pv" }
      ignore_errors: true

    - name: "Deploy Jellyfin"
      kubernetes.core.k8s:
        state: present
        src: "{{ playbook_dir }}/../../manifests/jellyfin/jellyfin.yaml"
        kubeconfig: "{{ kubeconfig_path }}"
      register: jellyfin_deploy

    - name: "Wait for Jellyfin pod to be Running"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: jellyfin
        label_selectors:
          - app=jellyfin
        kubeconfig: "{{ kubeconfig_path }}"
      register: jellyfin_pods
      until: >
        jellyfin_pods.resources | length > 0 and
        jellyfin_pods.resources | map(attribute='status.phase') | list | unique == ['Running']
      retries: 15  # Increased from 30 to allow for network fixes
      delay: 15    # Increased delay for network stabilization
      failed_when: false  # Don't fail immediately, allow for diagnosis

    - name: "Diagnose Jellyfin networking issues if pod is not ready"
      block:
        - name: "Check Jellyfin pod detailed status"
          kubernetes.core.k8s_info:
            api_version: v1
            kind: Pod
            namespace: jellyfin
            label_selectors:
              - app=jellyfin
            kubeconfig: "{{ kubeconfig_path }}"
          register: jellyfin_status

        - name: "Display Jellyfin pod networking diagnostics"
          debug:
            msg: |
              Jellyfin Pod Networking Diagnostics:
              Pod Phase: {{ jellyfin_status.resources[0].status.phase if jellyfin_status.resources else 'Not Found' }}
              Pod IP: {{ jellyfin_status.resources[0].status.podIP | default('No IP assigned') if jellyfin_status.resources else 'N/A' }}
              Host IP: {{ jellyfin_status.resources[0].status.hostIP | default('Unknown') if jellyfin_status.resources else 'N/A' }}
              Node: {{ jellyfin_status.resources[0].spec.nodeName | default('Not scheduled') if jellyfin_status.resources else 'N/A' }}
              {% if jellyfin_status.resources and jellyfin_status.resources[0].status.containerStatuses %}
              Container Ready: {{ jellyfin_status.resources[0].status.containerStatuses[0].ready | default('Unknown') }}
              Container State: {{ jellyfin_status.resources[0].status.containerStatuses[0].state.keys() | first if jellyfin_status.resources[0].status.containerStatuses[0].state else 'Unknown' }}
              {% endif %}

        - name: "Check for CNI bridge networking events"
          shell: kubectl get events --all-namespaces --sort-by='.lastTimestamp' | grep -i "failed to create pod sandbox\|network plugin is not ready\|cni0\|failed to set bridge addr" | tail -5
          register: cni_events
          failed_when: false

        - name: "Display CNI networking events"
          debug:
            msg: |
              Recent CNI/Networking Events:
              {{ cni_events.stdout if cni_events.stdout else 'No recent CNI errors found' }}

        - name: "Check flannel pod status for networking issues"
          kubernetes.core.k8s_info:
            api_version: v1
            kind: Pod
            namespace: kube-flannel
            label_selectors:
              - app=flannel
            kubeconfig: "{{ kubeconfig_path }}"
          register: flannel_status

        - name: "Display flannel pod status"
          debug:
            msg: |
              Flannel Pod Status:
              {% for pod in flannel_status.resources %}
              {{ pod.metadata.name }} on {{ pod.spec.nodeName }}: {{ pod.status.phase }}
              {% endfor %}

        - name: "Apply automatic CNI bridge fix if needed"
          shell: |
            if [ -f "/home/runner/work/VMStation/VMStation/scripts/fix_cni_bridge_conflict.sh" ]; then
              echo "Running CNI bridge conflict fix..."
              chmod +x /home/runner/work/VMStation/VMStation/scripts/fix_cni_bridge_conflict.sh
              /home/runner/work/VMStation/VMStation/scripts/fix_cni_bridge_conflict.sh
            else
              echo "CNI bridge fix script not found"
            fi
          register: cni_fix_result
          failed_when: false
          when: jellyfin_status.resources and jellyfin_status.resources[0].status.phase == 'Pending'

        - name: "Display CNI fix results"
          debug:
            msg: "{{ cni_fix_result.stdout }}"
          when: cni_fix_result is defined and cni_fix_result.stdout

        - name: "Wait additional time for Jellyfin after CNI fix"
          kubernetes.core.k8s_info:
            api_version: v1
            kind: Pod
            namespace: jellyfin
            label_selectors:
              - app=jellyfin
            kubeconfig: "{{ kubeconfig_path }}"
          register: jellyfin_retry
          until: >
            jellyfin_retry.resources | length > 0 and
            jellyfin_retry.resources | map(attribute='status.phase') | list | unique == ['Running']
          retries: 20
          delay: 15
          failed_when: false
          when: cni_fix_result is defined

      when: >
        jellyfin_pods.resources | length == 0 or
        'Running' not in (jellyfin_pods.resources | map(attribute='status.phase') | list)

    - name: "Final Jellyfin status check"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: jellyfin
        label_selectors:
          - app=jellyfin
        kubeconfig: "{{ kubeconfig_path }}"
      register: jellyfin_final
      failed_when: >
        jellyfin_final.resources | length == 0 or
        (jellyfin_final.resources[0].status.phase not in ['Running', 'Pending'] if jellyfin_final.resources else true)

    - name: "Display final Jellyfin pod status for debugging"
      debug:
        msg: |
          Final Jellyfin Pod Status:
          {% if jellyfin_final.resources %}
          Pod: {{ jellyfin_final.resources[0].metadata.name }}
          Phase: {{ jellyfin_final.resources[0].status.phase }}
          Pod IP: {{ jellyfin_final.resources[0].status.podIP | default('No IP assigned') }}
          Node: {{ jellyfin_final.resources[0].spec.nodeName | default('Not scheduled') }}
          {% if jellyfin_final.resources[0].status.containerStatuses %}
          Main Container Ready: {{ jellyfin_final.resources[0].status.containerStatuses[0].ready | default('Unknown') }}
          Main Container State: {{ jellyfin_final.resources[0].status.containerStatuses[0].state.keys() | first if jellyfin_final.resources[0].status.containerStatuses[0].state else 'Unknown' }}
          {% endif %}
          {% if jellyfin_final.resources[0].status.initContainerStatuses is defined and jellyfin_final.resources[0].status.initContainerStatuses %}
          Init Container Status: {{ jellyfin_final.resources[0].status.initContainerStatuses | length }} containers
          {% endif %}
          {% else %}
          No Jellyfin pod found!
          {% endif %}

    - name: "Verify Jellyfin is scheduled on storage node"
      assert:
        that:
          - jellyfin_pods.resources | map(attribute='spec.nodeName') | list | intersect(['storagenodet3500', 'storage_nodes']) | length > 0
        fail_msg: "Jellyfin pod is not scheduled on the storage node"
        success_msg: "‚úÖ Jellyfin correctly scheduled on storage node"

    - name: "Wait for Jellyfin container to be ready (not just running)"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: jellyfin
        label_selectors:
          - app=jellyfin
        kubeconfig: "{{ kubeconfig_path }}"
      register: jellyfin_ready_check
      until: >
        jellyfin_ready_check.resources | length > 0 and
        jellyfin_ready_check.resources[0].status.phase == 'Running' and
        jellyfin_ready_check.resources[0].status.containerStatuses is defined and
        jellyfin_ready_check.resources[0].status.containerStatuses | length > 0 and
        jellyfin_ready_check.resources[0].status.containerStatuses[0].ready == true
      retries: 20
      delay: 15
      failed_when: false

    - name: "Test Jellyfin web interface"
      uri:
        url: "http://192.168.4.61:30096/web/#/home.html"
        method: GET
        return_content: yes
        status_code: [200, 302]  # 302 is acceptable for redirect to setup
      register: jellyfin_test
      retries: 10
      delay: 15
      failed_when: jellyfin_test.status not in [200, 302]

- name: "Final Smoke Test Summary"
  hosts: monitoring_nodes
  become: true
  tasks:
    - name: "Display comprehensive cluster status"
      debug:
        msg: |
          üéâ VMStation Kubernetes Cluster Verification Complete!
          
          ‚úÖ Cluster Status:
            - All 3 nodes are Ready
            - Control plane: 192.168.4.63 (masternode)
            - Storage node: 192.168.4.61 (storagenodet3500) 
            - Compute node: 192.168.4.62 (homelab)
          
          ‚úÖ Core Components:
            - CoreDNS is Running (DNS resolution)
            - Flannel CNI is Running (pod networking)
            
          ‚úÖ Monitoring Stack:
            - Prometheus: http://192.168.4.63:30090
            - Grafana: http://192.168.4.63:30300 (admin/admin)
            
          ‚úÖ Applications:
            - Jellyfin: http://192.168.4.61:30096
            
          üîß Next Steps:
            - Configure Jellyfin media libraries
            - Set up Grafana dashboards
            - Deploy additional applications
            
          üìù Access Information:
            - kubectl config: /etc/kubernetes/admin.conf
            - Cluster endpoint: https://192.168.4.63:6443
            
    - name: "Export verification timestamp"
      copy:
        content: |
          VMStation Kubernetes Cluster
          Verification completed: {{ ansible_date_time.iso8601 }}
          Cluster nodes: {{ groups['all'] | length }}
          Status: VERIFIED ‚úÖ
        dest: /tmp/vmstation-cluster-verification.txt
        owner: root
        group: root
        mode: '0644'

    - name: "Verification successful"
      debug:
        msg: "üöÄ Cluster verification completed successfully! All components are operational."