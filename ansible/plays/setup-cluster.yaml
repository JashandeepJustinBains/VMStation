---
# VMStation Simplified Kubernetes Cluster Setup
# Replaces the complex 2901-line setup_cluster.yaml with essential functionality

- name: "Setup Kubernetes Cluster - All Nodes"
  hosts: all
  become: true
  vars:
    kubernetes_version: "1.29"
    pod_network_cidr: "10.244.0.0/16"
  tasks:
    - name: "Update package cache (Debian/Ubuntu)"
      apt:
        update_cache: yes
      when: ansible_os_family == 'Debian'

    - name: "Install required packages (Debian/Ubuntu)"
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
        state: present
      when: ansible_os_family == 'Debian'

    - name: "Add Kubernetes GPG key"
      apt_key:
        url: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/Release.key"
        state: present
      when: ansible_os_family == 'Debian'

    - name: "Add Kubernetes repository"
      apt_repository:
        repo: "deb https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/ /"
        state: present
        filename: kubernetes
      when: ansible_os_family == 'Debian'

    - name: "Install Kubernetes packages"
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
          - containerd
        state: present
        update_cache: yes
      when: ansible_os_family == 'Debian'
      register: k8s_package_install
      retries: 3
      delay: 10
      until: k8s_package_install is succeeded

    - name: "Install packages for RHEL/CentOS"
      block:
        - name: "Add Kubernetes repository (RHEL/CentOS)"
          yum_repository:
            name: kubernetes
            description: Kubernetes
            baseurl: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/rpm/"
            gpgcheck: yes
            repo_gpgcheck: yes
            gpgkey: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/rpm/repodata/repomd.xml.key"
            enabled: yes

        - name: "Install Kubernetes packages (RHEL/CentOS)"
          package:
            name:
              - kubelet
              - kubeadm
              - kubectl
              - containerd
            state: present
          register: k8s_package_install_rhel
          retries: 3
          delay: 10
          until: k8s_package_install_rhel is succeeded
      when: ansible_os_family == 'RedHat'

    - name: "Hold Kubernetes packages (Debian/Ubuntu)"
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl
      when: ansible_os_family == 'Debian'

    - name: "Comprehensive Kubernetes package validation"
      block:
        - name: "Verify Kubernetes binaries are installed and executable"
          shell: |
            echo "=== Verifying Kubernetes Package Installation ==="
            for cmd in kubelet kubeadm kubectl; do
              if command -v "$cmd" >/dev/null 2>&1; then
                version=$($cmd --version 2>/dev/null | head -1 || echo "Version check failed")
                echo "✓ $cmd is installed: $version"
                # Verify binary is executable
                if [ -x "$(command -v "$cmd")" ]; then
                  echo "✓ $cmd is executable"
                else
                  echo "✗ $cmd is not executable"
                  exit 1
                fi
              else
                echo "✗ $cmd is not found in PATH"
                exit 1
              fi
            done
            
            # Verify containerd
            if command -v containerd >/dev/null 2>&1; then
              echo "✓ containerd is installed"
              if [ -x "$(command -v containerd)" ]; then
                echo "✓ containerd is executable"
              else
                echo "✗ containerd is not executable"
                exit 1
              fi
            else
              echo "✗ containerd is not found in PATH"
              exit 1
            fi
            
            echo "✅ All Kubernetes packages verified successfully"
          register: k8s_package_verification
          failed_when: k8s_package_verification.rc != 0

        - name: "Display package verification results"
          debug:
            msg: "{{ k8s_package_verification.stdout_lines | join('\n') }}"

        - name: "Verify kubelet systemd service unit exists"
          stat:
            path: /lib/systemd/system/kubelet.service
          register: kubelet_service_unit
          
        - name: "Alternative kubelet service unit location check"
          stat:
            path: /usr/lib/systemd/system/kubelet.service
          register: kubelet_service_unit_alt
          when: not kubelet_service_unit.stat.exists

        - name: "Third alternative kubelet service unit location check"
          stat:
            path: /etc/systemd/system/kubelet.service
          register: kubelet_service_unit_etc
          when: 
            - not kubelet_service_unit.stat.exists
            - not (kubelet_service_unit_alt.stat.exists | default(false))

        - name: "Search for kubelet service unit in all locations"
          shell: |
            find /lib/systemd/system /usr/lib/systemd/system /etc/systemd/system -name "kubelet.service" 2>/dev/null | head -1
          register: kubelet_service_search
          when: 
            - not kubelet_service_unit.stat.exists
            - not (kubelet_service_unit_alt.stat.exists | default(false))
            - not (kubelet_service_unit_etc.stat.exists | default(false))

        - name: "Remediate missing kubelet service unit"
          block:
            - name: "Attempt to reinstall kubelet package when service unit missing"
              apt:
                name:
                  - kubelet
                  - kubeadm
                  - kubectl
                state: present
                update_cache: yes
                force_apt_get: yes
              when: ansible_os_family == 'Debian'
              register: kubelet_reinstall
              retries: 2
              delay: 15
              until: kubelet_reinstall is succeeded
              
            - name: "Attempt to reinstall kubelet package when service unit missing (RHEL/CentOS)"
              package:
                name:
                  - kubelet
                  - kubeadm
                  - kubectl
                state: present
              when: ansible_os_family == 'RedHat'
              register: kubelet_reinstall_rhel
              retries: 2
              delay: 15
              until: kubelet_reinstall_rhel is succeeded
              
            - name: "Reload systemd daemon after package reinstallation"
              systemd:
                daemon_reload: yes
                
            - name: "Re-verify kubelet service unit after remediation"
              stat:
                path: /lib/systemd/system/kubelet.service
              register: kubelet_service_recheck
              
            - name: "Alternative kubelet service unit location re-check"
              stat:
                path: /usr/lib/systemd/system/kubelet.service
              register: kubelet_service_recheck_alt
              when: not kubelet_service_recheck.stat.exists
              
            - name: "Final kubelet service unit verification after remediation"
              fail:
                msg: |
                  kubelet.service unit file still not found after remediation attempt:
                  - Package reinstallation result: {{ kubelet_reinstall.changed | default(kubelet_reinstall_rhel.changed | default('unknown')) }}
                  - Systemd daemon reloaded: yes
                  - Locations checked: /lib/systemd/system/kubelet.service, /usr/lib/systemd/system/kubelet.service
                  This indicates a persistent package installation issue that requires manual intervention.
              when: 
                - not kubelet_service_recheck.stat.exists
                - not (kubelet_service_recheck_alt.stat.exists | default(false))
                
          when: 
            - not kubelet_service_unit.stat.exists
            - not (kubelet_service_unit_alt.stat.exists | default(false))
            - not (kubelet_service_unit_etc.stat.exists | default(false))
            - (kubelet_service_search.stdout | default('')) == ''

        - name: "Display kubelet service unit location"
          debug:
            msg: |
              kubelet service unit found at: 
              {% if kubelet_service_unit.stat.exists %}
              /lib/systemd/system/kubelet.service
              {% elif kubelet_service_unit_alt.stat.exists | default(false) %}
              /usr/lib/systemd/system/kubelet.service
              {% elif kubelet_service_unit_etc.stat.exists | default(false) %}
              /etc/systemd/system/kubelet.service
              {% elif kubelet_service_search.stdout | default('') != '' %}
              {{ kubelet_service_search.stdout }}
              {% endif %}
          when: >
            kubelet_service_unit.stat.exists or 
            (kubelet_service_unit_alt.stat.exists | default(false)) or
            (kubelet_service_unit_etc.stat.exists | default(false)) or
            (kubelet_service_search.stdout | default('')) != ''

        - name: "Verify containerd systemd service unit exists"
          stat:
            path: /lib/systemd/system/containerd.service
          register: containerd_service_unit

        - name: "Alternative containerd service unit location check"
          stat:
            path: /usr/lib/systemd/system/containerd.service
          register: containerd_service_unit_alt
          when: not containerd_service_unit.stat.exists

        - name: "Search for containerd service unit in all locations"
          shell: |
            find /lib/systemd/system /usr/lib/systemd/system /etc/systemd/system -name "containerd.service" 2>/dev/null | head -1
          register: containerd_service_search
          when: 
            - not containerd_service_unit.stat.exists
            - not (containerd_service_unit_alt.stat.exists | default(false))

        - name: "Verify containerd service unit was found"
          fail:
            msg: |
              containerd.service unit file not found in expected locations:
              - /lib/systemd/system/containerd.service  
              - /usr/lib/systemd/system/containerd.service
              - /etc/systemd/system/containerd.service
              Search result: {{ containerd_service_search.stdout | default('No containerd.service found') }}
              This indicates a failed containerd package installation.
              Please check if containerd package was properly installed.
          when:
            - not containerd_service_unit.stat.exists
            - not (containerd_service_unit_alt.stat.exists | default(false))
            - (containerd_service_search.stdout | default('')) == ''

    - name: "Disable swap"
      shell: |
        swapoff -a
        sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

    - name: "Load kernel modules"
      modprobe:
        name: "{{ item }}"
      loop:
        - overlay
        - br_netfilter

    - name: "Create kernel modules config"
      copy:
        content: |
          overlay
          br_netfilter
        dest: /etc/modules-load.d/k8s.conf

    - name: "Set sysctl parameters"
      sysctl:
        name: "{{ item.key }}"
        value: "{{ item.value }}"
        state: present
        reload: yes
      loop:
        - { key: 'net.bridge.bridge-nf-call-iptables', value: '1' }
        - { key: 'net.bridge.bridge-nf-call-ip6tables', value: '1' }
        - { key: 'net.ipv4.ip_forward', value: '1' }

    - name: "Configure containerd"
      block:
        - name: "Create containerd config directory"
          file:
            path: /etc/containerd
            state: directory

        - name: "Generate containerd config"
          shell: containerd config default > /etc/containerd/config.toml
          args:
            creates: /etc/containerd/config.toml

        - name: "Configure containerd cgroup driver"
          replace:
            path: /etc/containerd/config.toml
            regexp: 'SystemdCgroup = false'
            replace: 'SystemdCgroup = true'

        - name: "Create CNI directories before containerd starts"
          file:
            path: "{{ item }}"
            state: directory
            owner: root
            group: root
            mode: '0755'
          loop:
            - /opt/cni/bin
            - /etc/cni/net.d
            - /var/lib/cni/networks
            - /run/flannel

        - name: "Create placeholder CNI configuration before containerd starts"
          copy:
            content: |
              {
                "name": "cni0",
                "cniVersion": "0.3.1",
                "plugins": [
                  {
                    "type": "bridge",
                    "bridge": "cni0",
                    "isDefaultGateway": true,
                    "ipMasq": true,
                    "ipam": {
                      "type": "host-local",
                      "subnet": "10.244.0.0/16"
                    }
                  }
                ]
              }
            dest: /etc/cni/net.d/00-placeholder.conflist
            owner: root
            group: root
            mode: '0644'

        - name: "Start and enable containerd"
          systemd:
            name: containerd
            state: restarted
            enabled: yes

        - name: "Configure crictl for containerd"
          copy:
            content: |
              runtime-endpoint: unix:///run/containerd/containerd.sock
              image-endpoint: unix:///run/containerd/containerd.sock
              timeout: 10
              debug: false
            dest: /etc/crictl.yaml
            owner: root
            group: root
            mode: '0644'

        - name: "Ensure containerd socket permissions and group access"
          block:
            - name: "Create containerd group for socket access"
              group:
                name: containerd
                state: present

            - name: "Add root user to containerd group for socket access"
              user:
                name: root
                groups: containerd
                append: yes

            - name: "Set up containerd socket permissions monitoring"
              shell: |
                # Function to ensure proper socket permissions
                ensure_socket_perms() {
                  if [ -S /run/containerd/containerd.sock ]; then
                    chgrp containerd /run/containerd/containerd.sock 2>/dev/null || true
                    chmod 660 /run/containerd/containerd.sock 2>/dev/null || true
                    echo "✓ Containerd socket permissions updated"
                  else
                    echo "WARNING: Containerd socket not found"
                    return 1
                  fi
                }
                
                # Ensure socket permissions are correct
                ensure_socket_perms
              register: socket_perms_result
              failed_when: false

            - name: "Test crictl communication with containerd"
              shell: |
                # Test crictl communication with proper error handling
                if crictl version >/dev/null 2>&1; then
                  echo "✓ crictl communication successful"
                  crictl version
                  exit 0
                else
                  echo "WARNING: crictl communication failed"
                  echo "Attempting to fix socket permissions..."
                  
                  # Wait for containerd socket to be ready
                  retry_count=0
                  max_retries=10
                  while [ $retry_count -lt $max_retries ]; do
                    if [ -S /run/containerd/containerd.sock ]; then
                      chgrp containerd /run/containerd/containerd.sock 2>/dev/null || true
                      chmod 660 /run/containerd/containerd.sock 2>/dev/null || true
                      
                      if crictl version >/dev/null 2>&1; then
                        echo "✓ crictl communication restored after socket fix"
                        crictl version
                        exit 0
                      fi
                    fi
                    
                    echo "Waiting for containerd socket... ($((retry_count + 1))/$max_retries)"
                    sleep 2
                    ((retry_count++))
                  done
                  
                  echo "ERROR: crictl still cannot communicate with containerd"
                  exit 1
                fi
              register: crictl_test_result
              failed_when: crictl_test_result.rc != 0

            - name: "Display crictl communication status"
              debug:
                msg: |
                  Crictl Communication Status:
                  {{ crictl_test_result.stdout_lines | join('\n                  ') }}
                  
                  Socket permissions properly configured for worker node join.

        - name: "Wait for containerd to fully initialize"
          pause:
            seconds: 10

        - name: "Initialize containerd image filesystem"
          shell: |
            # Initialize the k8s.io namespace (used by kubelet)
            ctr namespace create k8s.io 2>/dev/null || true
            
            # Force containerd to properly detect and initialize image filesystem capacity
            # This prevents the "invalid capacity 0 on image filesystem" error
            ctr --namespace k8s.io images ls >/dev/null 2>&1 || true
            sleep 5
            
            # Verify containerd image filesystem is properly initialized
            if ! ctr --namespace k8s.io images ls >/dev/null 2>&1; then
              echo "WARNING: containerd image filesystem not fully initialized"
              # Try to reinitialize
              systemctl restart containerd
              sleep 10
              ctr namespace create k8s.io 2>/dev/null || true
              ctr --namespace k8s.io images ls >/dev/null 2>&1 || true
            fi
          failed_when: false

    - name: "Enable kubelet"
      systemd:
        name: kubelet
        enabled: yes
      register: kubelet_enable_result
      failed_when: false

    - name: "Verify kubelet service can be enabled"
      block:
        - name: "Check kubelet enable result"
          debug:
            msg: |
              kubelet enable result: {{ kubelet_enable_result.enabled | default('unknown') }}
              kubelet enable failed: {{ kubelet_enable_result.failed | default(false) }}
              
        - name: "Handle kubelet enable failure"
          block:
            - name: "Reload systemd daemon"
              systemd:
                daemon_reload: yes
                
            - name: "Retry kubelet enable after daemon reload"
              systemd:
                name: kubelet
                enabled: yes
              register: kubelet_enable_retry
              
            - name: "Final kubelet enable verification"
              fail:
                msg: |
                  Failed to enable kubelet service even after systemd daemon reload.
                  This indicates a serious kubelet installation issue.
                  kubelet enable error: {{ kubelet_enable_result.msg | default('Unknown error') }}
                  kubelet retry error: {{ kubelet_enable_retry.msg | default('Unknown error') }}
              when: kubelet_enable_retry.failed | default(false)
              
          when: kubelet_enable_result.failed | default(false)

        - name: "Verify kubelet service is properly enabled"
          shell: systemctl is-enabled kubelet
          register: kubelet_enabled_check
          failed_when: kubelet_enabled_check.stdout != "enabled"

    - name: "Ensure kubelet service directory exists"
      file:
        path: /etc/systemd/system/kubelet.service.d
        state: directory
        mode: '0755'

    - name: "Comprehensive CNI plugin installation for all nodes"
      block:
        - name: "Download and install Flannel CNI plugin binary"
          get_url:
            url: "https://github.com/flannel-io/cni-plugin/releases/download/v1.7.1-flannel1/flannel-amd64"
            dest: /opt/cni/bin/flannel
            mode: '0755'
            timeout: 60
            validate_certs: false
            use_proxy: false
          retries: 3
          delay: 10
          until: flannel_download is succeeded
          register: flannel_download
          failed_when: false

        - name: "Fallback: Download Flannel CNI plugin with curl"
          shell: |
            curl -fsSL "https://github.com/flannel-io/cni-plugin/releases/download/v1.7.1-flannel1/flannel-amd64" \
              -o /opt/cni/bin/flannel --retry 3 --retry-delay 10 --connect-timeout 30 --max-time 120
            chmod 755 /opt/cni/bin/flannel
          when: flannel_download.failed and ('cert_file' in flannel_download.msg or 'urllib3' in flannel_download.msg or 'ssl' in flannel_download.msg or 'certificate' in flannel_download.msg)
          register: flannel_curl_download

        - name: "Enhanced fallback: Download Flannel CNI plugin with wget"
          shell: |
            wget -O /opt/cni/bin/flannel "https://github.com/flannel-io/cni-plugin/releases/download/v1.7.1-flannel1/flannel-amd64" \
              --no-check-certificate --retry-connrefused --tries=3 --timeout=30 --dns-timeout=10
            chmod 755 /opt/cni/bin/flannel
          when: 
            - flannel_download.failed 
            - flannel_curl_download is defined and flannel_curl_download.failed
          register: flannel_wget_download
          failed_when: false

        - name: "Verify Flannel CNI plugin download succeeded"
          fail:
            msg: |
              Failed to download Flannel CNI plugin using all available methods.
              get_url result: {{ flannel_download.msg | default('Unknown error') }}
              curl result: {{ flannel_curl_download.msg | default('Not attempted') }}
              wget result: {{ flannel_wget_download.msg | default('Not attempted') }}
          when: 
            - flannel_download.failed 
            - (flannel_curl_download is not defined or flannel_curl_download.failed)
            - (flannel_wget_download is not defined or flannel_wget_download.failed)

        - name: "Post-download verification: Ensure flannel binary is executable and valid"
          command: test -x /opt/cni/bin/flannel
          register: flannel_binary_verification
          failed_when: false

        - name: "Remediate flannel binary verification failure"
          block:
            - name: "Diagnose flannel binary issue"
              shell: |
                echo "=== Flannel Binary Diagnostics ==="
                if [ -f /opt/cni/bin/flannel ]; then
                  echo "File exists: YES"
                  echo "File size: $(stat -c%s /opt/cni/bin/flannel 2>/dev/null || echo 'unknown')"
                  echo "File permissions: $(ls -la /opt/cni/bin/flannel 2>/dev/null || echo 'unknown')"
                  echo "File type: $(file /opt/cni/bin/flannel 2>/dev/null || echo 'unknown')"
                else
                  echo "File exists: NO"
                fi
              register: flannel_diagnostics
              
            - name: "Attempt to fix flannel binary permissions"
              file:
                path: /opt/cni/bin/flannel
                mode: '0755'
                owner: root
                group: root
              when: flannel_diagnostics.stdout is search("File exists. YES")
              register: flannel_permission_fix
              
            - name: "Retry flannel download when file missing or corrupted"
              shell: |
                rm -f /opt/cni/bin/flannel
                curl -fsSL "https://github.com/flannel-io/cni-plugin/releases/download/v1.7.1-flannel1/flannel-amd64" \
                  -o /opt/cni/bin/flannel --retry 2 --retry-delay 5 --connect-timeout 30 --max-time 120
                chmod 755 /opt/cni/bin/flannel
              when: >
                flannel_diagnostics.stdout is search("File exists. NO") or 
                flannel_diagnostics.stdout is search("File size. 0") or
                flannel_diagnostics.stdout is search("ASCII text")
              register: flannel_redownload
              
            - name: "Final flannel binary verification after remediation"
              command: test -x /opt/cni/bin/flannel
              register: flannel_final_verification
              failed_when: flannel_final_verification.rc != 0
              
            - name: "Display flannel remediation results"
              debug:
                msg: |
                  Flannel Binary Remediation Results:
                  {{ flannel_diagnostics.stdout }}
                  Permission fix applied: {{ flannel_permission_fix.changed | default('not needed') }}
                  Redownload performed: {{ flannel_redownload.changed | default('not needed') }}
                  Final verification: {{ 'PASSED' if flannel_final_verification.rc == 0 else 'FAILED' }}
                  
          when: flannel_binary_verification.rc != 0

        - name: "Download and install additional CNI plugins"
          unarchive:
            src: "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz"
            dest: /opt/cni/bin
            remote_src: yes
            creates: /opt/cni/bin/bridge
            owner: root
            group: root
            mode: '0755'
          retries: 3
          delay: 10
          register: cni_plugins_download
          until: cni_plugins_download is succeeded

        - name: "Verify essential CNI plugins are installed"
          shell: |
            #!/bin/bash
            echo "=== Verifying CNI Plugin Installation ==="
            required_plugins=("bridge" "host-local" "loopback" "flannel")
            all_found=true
            
            for plugin in "${required_plugins[@]}"; do
              if [ -f "/opt/cni/bin/$plugin" ] && [ -x "/opt/cni/bin/$plugin" ]; then
                echo "✓ $plugin plugin installed and executable"
              else
                echo "✗ $plugin plugin missing or not executable"
                all_found=false
              fi
            done
            
            if [ "$all_found" = "true" ]; then
              echo "✅ All required CNI plugins verified successfully"
            else
              echo "❌ Some CNI plugins are missing"
              exit 1
            fi
            
            # Show all installed CNI plugins
            echo ""
            echo "All installed CNI plugins:"
            ls -la /opt/cni/bin/
          args:
            executable: /bin/bash
          register: cni_verification
          failed_when: cni_verification.rc != 0

        - name: "Display CNI plugin verification results"
          debug:
            msg: "{{ cni_verification.stdout_lines | join('\n') }}"

    # Note: Removed static kubelet configuration that conflicted with kubeadm join process
    # kubeadm init (control plane) and kubeadm join (workers) will handle kubelet configuration
    # This prevents conflicts during the TLS bootstrap process

  handlers:
    - name: reload systemd
      systemd:
        daemon_reload: yes

# Worker Node Installation Verification
- name: "Comprehensive Worker Node Installation Verification"
  hosts: storage_nodes:compute_nodes
  become: true
  tasks:
    - name: "Verify complete worker node setup"
      block:
        - name: "Check all required Kubernetes services and packages"
          shell: |
            echo "=== Worker Node Installation Verification ==="
            echo "Node: {{ ansible_hostname }} ({{ ansible_default_ipv4.address }})"
            echo "Timestamp: $(date)"
            echo ""
            
            # Check package installation
            echo "=== Package Verification ==="
            all_packages_ok=true
            for pkg in kubelet kubeadm kubectl containerd; do
              if command -v "$pkg" >/dev/null 2>&1; then
                version=$($pkg --version 2>/dev/null | head -1 || echo "Version check failed")
                echo "✓ $pkg: $version"
              else
                echo "✗ $pkg: NOT FOUND"
                all_packages_ok=false
              fi
            done
            
            # Check service units
            echo ""
            echo "=== Service Unit Verification ==="
            for service in kubelet containerd; do
              if systemctl list-unit-files | grep -q "^${service}.service"; then
                status=$(systemctl is-enabled $service 2>/dev/null || echo "unknown")
                echo "✓ ${service}.service unit exists (enabled: $status)"
              else
                echo "✗ ${service}.service unit NOT FOUND"
                all_packages_ok=false
              fi
            done
            
            # Check CNI plugins
            echo ""
            echo "=== CNI Plugin Verification ==="
            cni_plugins_ok=true
            required_plugins=("bridge" "host-local" "loopback" "flannel")
            for plugin in "${required_plugins[@]}"; do
              if [ -f "/opt/cni/bin/$plugin" ] && [ -x "/opt/cni/bin/$plugin" ]; then
                echo "✓ CNI plugin: $plugin"
              else
                echo "✗ CNI plugin missing: $plugin"
                cni_plugins_ok=false
              fi
            done
            
            # Check directories
            echo ""
            echo "=== Directory Structure Verification ==="
            dirs_ok=true
            required_dirs=("/opt/cni/bin" "/etc/cni/net.d" "/var/lib/kubelet" "/etc/systemd/system/kubelet.service.d")
            for dir in "${required_dirs[@]}"; do
              if [ -d "$dir" ]; then
                echo "✓ Directory exists: $dir"
              else
                echo "✗ Directory missing: $dir"
                dirs_ok=false
              fi
            done
            
            # Check containerd configuration
            echo ""
            echo "=== Containerd Configuration Verification ==="
            containerd_ok=true
            if [ -f "/etc/containerd/config.toml" ]; then
              echo "✓ containerd config exists"
              if grep -q "SystemdCgroup = true" /etc/containerd/config.toml; then
                echo "✓ containerd cgroup driver configured"
              else
                echo "⚠ containerd cgroup driver may not be properly configured"
              fi
            else
              echo "✗ containerd config missing"
              containerd_ok=false
            fi
            
            # Overall status
            echo ""
            echo "=== Overall Status ==="
            if [ "$all_packages_ok" = "true" ] && [ "$cni_plugins_ok" = "true" ] && [ "$dirs_ok" = "true" ] && [ "$containerd_ok" = "true" ]; then
              echo "✅ Worker node installation verification PASSED"
              echo "Node is ready for cluster join"
              exit 0
            else
              echo "❌ Worker node installation verification FAILED"
              echo "Some components are missing or misconfigured"
              exit 1
            fi
          args:
            executable: /bin/bash
          register: worker_verification
          failed_when: worker_verification.rc != 0

        - name: "Display worker node verification results"
          debug:
            msg: "{{ worker_verification.stdout_lines | join('\n') }}"

# Control plane initialization
- name: "Initialize Kubernetes Control Plane"
  hosts: monitoring_nodes
  become: true
  vars:
    pod_network_cidr: "10.244.0.0/16"
    auth_mode: "{{ kubernetes_authorization_mode | default('Node,RBAC') }}"
    enable_fallback: "{{ kubernetes_authorization_fallback | default(false) }}"
  tasks:
    - name: "Check if cluster exists"
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeconfig

    - name: "Initialize cluster with secure authorization mode"
      command: >
        kubeadm init 
        --pod-network-cidr={{ pod_network_cidr }}
        --apiserver-advertise-address={{ ansible_default_ipv4.address }}
        --control-plane-endpoint={{ ansible_default_ipv4.address }}
      when: not kubeconfig.stat.exists
      register: kubeadm_init
      ignore_errors: "{{ enable_fallback | bool }}"

    - name: "Initialize cluster with AlwaysAllow fallback (if secure mode failed)"
      command: >
        kubeadm init 
        --pod-network-cidr={{ pod_network_cidr }}
        --apiserver-advertise-address={{ ansible_default_ipv4.address }}
        --control-plane-endpoint={{ ansible_default_ipv4.address }}
      when: 
        - not kubeconfig.stat.exists
        - enable_fallback | bool
        - kubeadm_init.failed | default(false)
      register: kubeadm_init_fallback

    - name: "Display authorization mode warning if fallback was used"
      debug:
        msg: |
          WARNING: Cluster initialized with --authorization-mode=AlwaysAllow
          This is less secure and should only be used for troubleshooting.
          Consider investigating why {{ auth_mode }} mode failed.
      when: 
        - enable_fallback | bool
        - kubeadm_init_fallback is defined
        - kubeadm_init_fallback is succeeded

    - name: "Setup kubeconfig for root"
      block:
        - name: "Create .kube directory"
          file:
            path: /root/.kube
            state: directory
            mode: '0755'

        - name: "Copy admin.conf"
          copy:
            src: /etc/kubernetes/admin.conf
            dest: /root/.kube/config
            mode: '0644'
            remote_src: yes

    - name: "Open firewall ports for Kubernetes"
      firewalld:
        port: "{{ item }}"
        permanent: yes
        state: enabled
        immediate: yes
      loop:
        - "6443/tcp"    # API server
        - "10250/tcp"   # Kubelet
        - "10251/tcp"   # kube-scheduler
        - "10252/tcp"   # kube-controller-manager
        - "8472/udp"    # Flannel VXLAN
      failed_when: false
      when: ansible_os_family == 'RedHat'

    - name: "Check if Flannel CNI is already installed"
      shell: |
        timeout 30 kubectl get namespace kube-flannel >/dev/null 2>&1
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_exists
      failed_when: false

    - name: "Run CNI bridge cleanup helper on control-plane (if present)"
      shell: |
        if [ -x /root/VMStation/scripts/fix_cni_bridge_conflict.sh ]; then
          /root/VMStation/scripts/fix_cni_bridge_conflict.sh || true
        elif [ -x /srv/monitoring_data/VMStation/scripts/fix_cni_bridge_conflict.sh ]; then
          /srv/monitoring_data/VMStation/scripts/fix_cni_bridge_conflict.sh || true
        else
          echo "CNI cleanup helper not found on control-plane; skipping"
        fi
      delegate_to: "{{ control_plane_host | default(groups['monitoring_nodes'][0]) }}"
      run_once: true
      failed_when: false

    - name: "Install Flannel CNI"
      shell: |
        timeout 60 kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_install
      retries: 3
      delay: 10
      until: flannel_install.rc == 0
      when: flannel_exists.rc != 0
      async: 120
      poll: 5

    - name: "Wait for Flannel DaemonSet to be created"
      shell: |
        timeout 30 kubectl get daemonset -n kube-flannel kube-flannel-ds
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_ds_check
      retries: 6
      delay: 10
      until: flannel_ds_check.rc == 0
      when: flannel_exists.rc != 0
      failed_when: false

    - name: "Ensure CoreDNS has correct replica count and proper scheduling"
      shell: |
        # Remove NoSchedule taint from control-plane to allow CoreDNS scheduling
        control_plane_nodes=$(timeout 30 kubectl get nodes -l node-role.kubernetes.io/control-plane --no-headers -o custom-columns=":metadata.name")
        for node in $control_plane_nodes; do
          echo "Removing NoSchedule taint from control-plane node: $node"
          timeout 30 kubectl taint node "$node" node-role.kubernetes.io/control-plane:NoSchedule- 2>/dev/null || echo "  Taint not present or already removed"
          timeout 30 kubectl taint node "$node" node-role.kubernetes.io/master:NoSchedule- 2>/dev/null || echo "  Legacy master taint not present"
        done
        
        # Check current CoreDNS replica count
        current_replicas=$(timeout 30 kubectl get deployment coredns -n kube-system -o jsonpath='{.spec.replicas}')
        if [ "$current_replicas" != "1" ]; then
          echo "Scaling CoreDNS from $current_replicas to 1 replica"
          timeout 60 kubectl scale deployment coredns -n kube-system --replicas=1
        else
          echo "CoreDNS already has correct replica count: $current_replicas"
        fi
        
        # Configure CoreDNS to prefer control-plane nodes
        echo "Patching CoreDNS deployment to prefer control-plane nodes..."
        timeout 60 kubectl patch deployment coredns -n kube-system -p '{
          "spec": {
            "template": {
              "spec": {
                "affinity": {
                  "nodeAffinity": {
                    "preferredDuringSchedulingIgnoredDuringExecution": [{
                      "weight": 100,
                      "preference": {
                        "matchExpressions": [{
                          "key": "node-role.kubernetes.io/control-plane",
                          "operator": "Exists"
                        }]
                      }
                    }]
                  }
                },
                "tolerations": [{
                  "key": "node-role.kubernetes.io/control-plane",
                  "operator": "Exists",
                  "effect": "NoSchedule"
                }]
              }
            }
          }
        }' || echo "CoreDNS patch failed - may already be configured"
        
        echo "CoreDNS configuration completed"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: coredns_scale
      failed_when: false

    - name: "Check Flannel namespace and resources"
      shell: |
        echo "Checking Flannel deployment status:"
        timeout 30 kubectl get namespace kube-flannel
        timeout 30 kubectl get all -n kube-flannel
        echo "Node status:"
        timeout 30 kubectl get nodes
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_status
      failed_when: false

    - name: "Check CNI plugins availability"
      shell: |
        echo "=== CNI Plugins Status ==="
        ls -la /opt/cni/bin/ | grep -E "(flannel|bridge|host-local|loopback)" || echo "CNI plugins not found"
        echo ""
        echo "=== CNI Configuration Directory ==="
        ls -la /etc/cni/net.d/ || echo "CNI config directory not found"
      register: cni_plugins_status
      failed_when: false

    - name: "Check containerd CNI configuration"
      shell: |
        echo "=== Containerd CNI Configuration ==="
        if [ -f /etc/containerd/config.toml ]; then
          grep -A5 -B5 "cni" /etc/containerd/config.toml || echo "No CNI config found in containerd.toml"
        else
          echo "containerd config.toml not found"
        fi
      register: containerd_cni_config
      failed_when: false

    - name: "Analyze CNI runtime status"
      shell: |
        # Check if CNI shows real network interfaces or just loopback
        if ip link show 2>/dev/null | grep -q "cni0\|flannel\|veth"; then
          echo "cni_has_real_network=true"
        else
          echo "cni_only_loopback=true"
        fi
      register: cni_runtime_analysis
      failed_when: false

    - name: "Apply Flannel remediation if needed"
      block:
        - name: "Reapply Flannel when CNI shows only loopback interface"
          shell: |
            echo "Detected CNI only showing loopback interface - reapplying Flannel"
            timeout 60 kubectl delete -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml || true
            sleep 10
            timeout 60 kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          register: flannel_reapply
          when: cni_runtime_analysis.stdout is defined and 'cni_only_loopback=true' in cni_runtime_analysis.stdout
          async: 180
          poll: 10

    - name: "Check Flannel DaemonSet status"
      shell: |
        echo "=== Flannel DaemonSet and Pod Status ==="
        timeout 30 kubectl get daemonset,pods -n kube-flannel -l app=flannel -o wide
        echo ""
        echo "=== Flannel Readiness Check ==="
        timeout 30 kubectl get daemonset -n kube-flannel kube-flannel-ds -o jsonpath='{.status.numberReady}' || echo "0"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_ds_status
      failed_when: false

    - name: "Display comprehensive CNI readiness status"
      debug:
        msg: |
          === CNI Readiness Status ===
          
          Control Plane Flannel Status:
          {{ flannel_status.stdout }}
          
          CNI Plugins Status:
          {{ cni_plugins_status.stdout }}
          
          Containerd CNI Configuration:
          {{ containerd_cni_config.stdout }}
          
          CNI Runtime Analysis:
          {{ cni_runtime_analysis.stdout }}
          
          Flannel DaemonSet Status:
          {{ flannel_ds_status.stdout }}
          
          {% if cni_runtime_analysis.stdout is defined and 'cni_only_loopback=true' in cni_runtime_analysis.stdout %}
          WARNING: CNI runtime only shows loopback interface!
          This indicates that Flannel CNI is not properly configured or running.
          
          Recommended actions:
          1. Check Flannel pod status: kubectl get pods -n kube-flannel
          2. Check Flannel logs: kubectl logs -n kube-flannel -l app=flannel
          3. Verify network configuration: kubectl get nodes -o wide
          4. Manual Flannel reapplication has been attempted automatically
          {% endif %}
          
          Note: Flannel pods may show CrashLoopBackOff until worker nodes join.
          This is expected behavior in a single-node control plane setup.
          
          CNI Configuration Ready: Worker nodes can now join the cluster.

    - name: "Post-Flannel CoreDNS validation and fix"
      block:
        - name: "Wait for CoreDNS to stabilize after Flannel setup"
          pause:
            seconds: 30

        - name: "Check CoreDNS pod status and fix if needed"
          shell: |
            echo "=== Post-Flannel CoreDNS Validation ==="
            
            # Check CoreDNS pod status
            coredns_pods=$(timeout 30 kubectl get pods -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[*].metadata.name}')
            
            if [ -z "$coredns_pods" ]; then
              echo "ERROR: No CoreDNS pods found!"
              exit 1
            fi
            
            issues_found=false
            cni_conflict_detected=false
            
            echo "CoreDNS pods status:"
            for pod in $coredns_pods; do
              pod_status=$(timeout 30 kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.phase}')
              pod_ip=$(timeout 30 kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.podIP}')
              
              echo "  $pod: Status=$pod_status, IP=${pod_ip:-<none>}"
              
              # Check for CNI bridge conflicts in pod events
              if [ "$pod_status" = "Pending" ] || [ "$pod_status" = "ContainerCreating" ]; then
                cni_events=$(timeout 30 kubectl get events --field-selector involvedObject.name="$pod" -n kube-system -o jsonpath='{.items[*].message}' 2>/dev/null || true)
                if echo "$cni_events" | grep -q "failed to set bridge addr.*already has an IP address different"; then
                  echo "    ↳ CNI bridge IP conflict detected for this pod"
                  cni_conflict_detected=true
                  issues_found=true
                elif echo "$cni_events" | grep -q "FailedCreatePodSandBox"; then
                  echo "    ↳ Pod sandbox creation failed"
                  issues_found=true
                else
                  echo "    ↳ Pod is still creating - may be normal"
                fi
              elif [ "$pod_status" = "Unknown" ] || [ -z "$pod_ip" ] || [ "$pod_ip" = "null" ]; then
                echo "    ↳ This pod has issues - will be restarted"
                issues_found=true
              fi
            done
            
            if [ "$cni_conflict_detected" = "true" ]; then
              echo "=== CNI Bridge Conflict Detected ==="
              echo "CoreDNS pods cannot start due to CNI bridge IP conflict."
              echo "This requires CNI bridge reset, which will be handled by post-deployment fixes."
              echo "Skipping CoreDNS rollout wait to avoid hanging the deployment."
              echo ""
              echo "The deployment will complete and run fix_cni_bridge_conflict.sh automatically."
              
              # Set a flag for the post-deployment logic to handle this
              echo "CNI_CONFLICT_DETECTED=true" > /tmp/cni_conflict_detected
              
            elif [ "$issues_found" = "true" ]; then
              echo "=== Fixing CoreDNS issues (non-CNI related) ==="
              
              # Delete stuck CoreDNS pods to force rescheduling
              for pod in $coredns_pods; do
                pod_status=$(timeout 30 kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.phase}')
                pod_ip=$(timeout 30 kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.podIP}')
                
                if [ "$pod_status" = "Unknown" ] || [ -z "$pod_ip" ] || [ "$pod_ip" = "null" ]; then
                  echo "Deleting stuck CoreDNS pod: $pod"
                  timeout 60 kubectl delete pod -n kube-system "$pod" --force --grace-period=0
                fi
              done
              
              # Wait for new pods to be scheduled (with timeout to prevent hanging)
              echo "Waiting for CoreDNS to be rescheduled (max 120 seconds to prevent hang)..."
              if timeout 120 kubectl rollout status deployment/coredns -n kube-system; then
                echo "✓ CoreDNS rollout completed successfully"
              else
                echo "WARNING: CoreDNS rollout timed out after 120 seconds"
                echo "This may indicate ongoing CNI issues that require post-deployment fixes"
              fi
              
              # Quick verification with limited retries to avoid hanging
              echo "Verifying new CoreDNS pods have IP addresses (quick check)..."
              for i in {1..3}; do
                all_have_ips=true
                current_coredns_pods=$(timeout 30 kubectl get pods -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[*].metadata.name}')
                
                for pod in $current_coredns_pods; do
                  pod_ip=$(timeout 30 kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.podIP}')
                  pod_status=$(timeout 30 kubectl get pod -n kube-system "$pod" -o jsonpath='{.status.phase}')
                  
                  if [ -z "$pod_ip" ] || [ "$pod_ip" = "null" ] || [ "$pod_status" != "Running" ]; then
                    all_have_ips=false
                    break
                  fi
                done
                
                if [ "$all_have_ips" = "true" ]; then
                  echo "✓ All CoreDNS pods now have IP addresses"
                  break
                fi
                
                echo "  Waiting for CoreDNS pods to get IPs... (attempt $i/3)"
                sleep 10
              done
              
              if [ "$all_have_ips" != "true" ]; then
                echo "WARNING: Some CoreDNS pods still don't have IPs after quick fixes"
                echo "Post-deployment fixes will handle remaining issues"
              fi
              
            else
              echo "✓ CoreDNS pods are healthy"
            fi
            
            echo "=== Final CoreDNS Status ==="
            timeout 30 kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          register: coredns_fix_result
          failed_when: false

        - name: "Display CoreDNS fix results"
          debug:
            msg: |
              CoreDNS Post-Flannel Validation Results:
              {{ coredns_fix_result.stdout_lines | join('\n              ') }}
              
              {% if coredns_fix_result.rc != 0 %}
              WARNING: CoreDNS validation/fix encountered issues.
              After cluster setup completes, run: ./scripts/fix_coredns_unknown_status.sh
              {% endif %}
              
              {% if coredns_fix_result.stdout is defined and 'CNI_CONFLICT_DETECTED=true' in coredns_fix_result.stdout %}
              INFO: CNI bridge conflict detected during deployment.
              The deployment will automatically run fix_cni_bridge_conflict.sh after cluster setup.
              This will resolve CoreDNS pod creation issues caused by CNI bridge IP conflicts.
              {% endif %}

    - name: "Wait for API server to be ready"
      wait_for:
        host: "{{ ansible_default_ipv4.address }}"
        port: 6443
        timeout: 300
        delay: 10
      
    - name: "Verify API server accessibility using kubectl"
      shell: kubectl get nodes --request-timeout=10s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: api_health
      retries: 10
      delay: 15
      until: api_health.rc == 0

    - name: "Validate kubernetes-admin RBAC permissions"
      shell: >
        timeout 30 kubectl auth can-i create secrets --namespace=kube-system
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: rbac_check
      failed_when: false

    - name: "Check current authorization mode"
      shell: >
        timeout 30 kubectl get pods -n kube-system kube-apiserver-* -o jsonpath='{.items[0].spec.containers[0].command}' | 
        grep -o '\--authorization-mode=[^[:space:]]*' | 
        cut -d= -f2
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: current_auth_mode
      failed_when: false

    - name: "Display current authorization mode"
      debug:
        msg: "Current Kubernetes authorization mode: {{ current_auth_mode.stdout | default('Unable to determine') }}"

    - name: "Fix kubernetes-admin RBAC if needed"
      shell: |
        timeout 60 kubectl create clusterrolebinding kubernetes-admin \
          --clusterrole=cluster-admin \
          --user=kubernetes-admin \
          --dry-run=client -o yaml | timeout 60 kubectl apply -f -
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: 
        - rbac_check.stdout != "yes"
        - current_auth_mode.stdout is defined
        - "'RBAC' in current_auth_mode.stdout"

    - name: "Fix API server authorization mode if using AlwaysAllow"
      block:
        - name: "Backup API server manifest"
          copy:
            src: /etc/kubernetes/manifests/kube-apiserver.yaml
            dest: /etc/kubernetes/manifests/kube-apiserver.yaml.backup
            remote_src: yes

        - name: "Update authorization mode from AlwaysAllow to Node,RBAC"
          replace:
            path: /etc/kubernetes/manifests/kube-apiserver.yaml
            regexp: '--authorization-mode=AlwaysAllow'
            replace: '--authorization-mode=Node,RBAC'

        - name: "Wait for API server to restart after authorization fix"
          wait_for:
            host: "{{ ansible_default_ipv4.address }}"
            port: 6443
            timeout: 300
            delay: 30

        - name: "Verify API server health after authorization fix"
          shell: kubectl get nodes --request-timeout=10s
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          register: api_health_after_fix
          retries: 10
          delay: 15
          until: api_health_after_fix.rc == 0

        - name: "Apply RBAC fix after authorization mode change"
          shell: |
            kubectl create clusterrolebinding kubernetes-admin \
              --clusterrole=cluster-admin \
              --user=kubernetes-admin \
              --dry-run=client -o yaml | kubectl apply -f -
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

      when: 
        - current_auth_mode.stdout is defined
        - "'AlwaysAllow' in current_auth_mode.stdout"

    - name: "Skip RBAC fix for AlwaysAllow mode (no longer needed after fix)"
      debug:
        msg: "Authorization mode fixed from AlwaysAllow to Node,RBAC"
      when: 
        - rbac_check.stdout != "yes"
        - current_auth_mode.stdout is defined
        - "'AlwaysAllow' in current_auth_mode.stdout"

    - name: "Wait for API server pod to be Ready"
      shell: |
        kubectl get pods -n kube-system -l component=kube-apiserver \
          -o jsonpath='{.items[0].status.conditions[?(@.type=="Ready")].status}' | grep -q "True"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: api_pod_ready
      retries: 20
      delay: 15
      until: api_pod_ready.rc == 0

    - name: "Check cluster-info configmap RBAC permissions"
      shell: >
        kubectl get clusterrole system:public-info-viewer >/dev/null 2>&1 &&
        kubectl get clusterrolebinding cluster-info >/dev/null 2>&1 &&
        echo "yes" || echo "no"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: cluster_info_rbac_check
      failed_when: false

    - name: "Create RBAC rule for anonymous access to cluster-info configmap"
      shell: |
        timeout 60 kubectl create clusterrole system:public-info-viewer \
          --verb=get --resource=configmaps \
          --resource-name=cluster-info \
          --dry-run=client -o yaml | timeout 60 kubectl apply -f -
        
        timeout 60 kubectl create clusterrolebinding cluster-info \
          --clusterrole=system:public-info-viewer \
          --group=system:unauthenticated \
          --group=system:authenticated \
          --dry-run=client -o yaml | timeout 60 kubectl apply -f -
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: cluster_info_rbac_check.stdout != "yes"

    - name: "Verify cluster-info configmap accessibility"
      shell: >
        kubectl get clusterrole system:public-info-viewer >/dev/null 2>&1 &&
        kubectl get clusterrolebinding cluster-info >/dev/null 2>&1 &&
        echo "yes" || echo "no"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: cluster_info_verification
      retries: 3
      delay: 5
      until: cluster_info_verification.stdout == "yes"

    - name: "Generate fresh join command for wiped workers"
      block:
        - name: "Check existing tokens and clean up old ones if needed"
          shell: |
            # List existing tokens
            existing_tokens=$(kubeadm token list --output=value | cut -d' ' -f1 | wc -l)
            echo "Existing tokens: $existing_tokens"
            
            # If too many tokens exist, clean up old ones (keep max 5)
            if [ "$existing_tokens" -gt 5 ]; then
              echo "Cleaning up old tokens..."
              kubeadm token list --output=value | cut -d' ' -f1 | head -n -3 | while read token; do
                kubeadm token delete "$token" 2>/dev/null || true
              done
            fi
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          register: token_cleanup

        - name: "Generate fresh join command with enhanced validation"
          shell: |
            # Generate new token with extended TTL for wiped worker joins
            kubeadm token create --ttl=2h --print-join-command
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          register: join_command
          retries: 3
          delay: 10
          until: join_command.rc == 0

        - name: "Validate join command contains correct control-plane IP"
          fail:
            msg: "Generated join command does not contain expected control-plane IP {{ ansible_default_ipv4.address }}"
          when: ansible_default_ipv4.address not in join_command.stdout

        - name: "Save enhanced join command for wiped workers"
          copy:
            content: |
              #!/bin/bash
              # Generated kubeadm join command for post-wipe workers
              # Command generated at: {{ ansible_date_time.iso8601 }}
              # Control-plane IP: {{ ansible_default_ipv4.address }}
              # Token TTL: 2 hours
              {{ join_command.stdout }} --node-name {{ inventory_hostname }} "$@"
            dest: /tmp/kubeadm-join.sh
            mode: '0755'

        - name: "Display join command info"
          debug:
            msg: |
              Join Command Generated:
              - Timestamp: {{ ansible_date_time.iso8601 }}
              - Control-plane: {{ ansible_default_ipv4.address }}:6443
              - Token TTL: 2 hours
              - Ready for wiped workers: Yes

# Join worker nodes
- name: "Join Worker Nodes"
  hosts: storage_nodes:compute_nodes
  become: true
  vars:
    # Inventory hostname of the control-plane (used for delegate_to)
    control_plane_host: "{{ groups['monitoring_nodes'][0] }}"
    # Control-plane reachable address: prefer explicit ansible_host from inventory, fall back to the inventory name
    control_plane_ip: "{{ hostvars[groups['monitoring_nodes'][0]]['ansible_host'] | default(groups['monitoring_nodes'][0]) }}"
  tasks:
    - name: "Detect post-wipe worker state"
      block:
        - name: "Check if node is joined"
          stat:
            path: /etc/kubernetes/kubelet.conf
          register: kubelet_conf

        - name: "Check for existing cluster artifacts"
          stat:
            path: /etc/kubernetes/pki/ca.crt
          register: cluster_artifacts

        - name: "Check for clean post-wipe state indicators"
          stat:
            path: "{{ item }}"
          loop:
            - /etc/kubernetes
            - /var/lib/kubelet
            - /etc/cni/net.d
            - /var/lib/containerd
          register: kubernetes_directories

        - name: "Detect if worker was aggressively wiped"
          set_fact:
            worker_was_wiped: >-
              {{
                not kubelet_conf.stat.exists and
                not cluster_artifacts.stat.exists and
                (kubernetes_directories.results | selectattr('stat.exists') | list | length == 0 or
                 kubernetes_directories.results | selectattr('stat.exists') | 
                 selectattr('stat.isdir') | map(attribute='stat.path') | 
                 map('dirname') | unique | list | length == kubernetes_directories.results | length)
              }}

        - name: "Display worker state detection"
          debug:
            msg: |
              Worker Node State Analysis:
              - Kubelet config exists: {{ kubelet_conf.stat.exists }}
              - Cluster artifacts exist: {{ cluster_artifacts.stat.exists }}
              - Post-wipe state detected: {{ worker_was_wiped }}
              - Node requires fresh join: {{ not kubelet_conf.stat.exists }}

    - name: "Validate control-plane readiness before worker join"
      block:
        - name: "Check control-plane API server accessibility"
          wait_for:
            host: "{{ control_plane_ip }}"
            port: 6443
            timeout: 60
          delegate_to: localhost
          
        - name: "Verify control-plane cluster status"
          shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get --raw /healthz
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          delegate_to: "{{ control_plane_host }}"
          register: api_health_check
          failed_when: api_health_check.rc != 0
          
        - name: "Ensure control-plane has proper RBAC configuration"
          shell: |
            kubectl get clusterrole system:public-info-viewer >/dev/null 2>&1 &&
            kubectl get clusterrolebinding cluster-info >/dev/null 2>&1 &&
            echo "rbac_ready" || echo "rbac_missing"
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          delegate_to: "{{ control_plane_host }}"
          register: control_plane_rbac_status
          
        - name: "Display control-plane readiness status"
          debug:
            msg: |
              Control-Plane Readiness Check:
              - API server accessible: {{ api_health_check.rc == 0 }}
              - API health status: {{ api_health_check.stdout | default('failed') }}
              - RBAC configuration: {{ control_plane_rbac_status.stdout }}
              - Ready for worker joins: {{ (api_health_check.rc == 0) and ('rbac_ready' in (control_plane_rbac_status.stdout | default(''))) }}
              
      when: not kubelet_conf.stat.exists

    - name: "Prepare wiped workers for fresh cluster join"
      block:
        - name: "Enhanced reset for workers that may have partial state"
          block:
            - name: "Stop and disable kubelet service"
              systemd:
                name: kubelet
                state: stopped
                enabled: no
              failed_when: false

            - name: "Stop containerd temporarily for thorough preparation"
              systemd:
                name: containerd
                state: stopped
              failed_when: false

            - name: "Reset kubeadm configuration (idempotent for wiped workers)"
              shell: kubeadm reset --force
              register: reset_result
              failed_when: false

            - name: "Clean up iptables rules (idempotent)"
              shell: |
                iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
              failed_when: false

            - name: "Ensure complete removal of any residual Kubernetes state"
              shell: |
                rm -rf /etc/cni/net.d/*
                rm -rf /var/lib/cni/
                rm -rf /var/lib/kubelet/*
                rm -rf /etc/kubernetes/*
                rm -rf /var/lib/etcd/*
              failed_when: false

            - name: "Reset systemd services for clean slate"
              shell: |
                systemctl reset-failed kubelet || true
                systemctl reset-failed containerd || true
                systemctl daemon-reload
              failed_when: false

            - name: "Restart containerd after cleanup"
              systemd:
                name: containerd
                state: started
                enabled: yes

          when: 
            - worker_was_wiped or (cluster_artifacts.stat.exists and not kubelet_conf.stat.exists)

        - name: "Display worker preparation status"
          debug:
            msg: |
              Worker Preparation Complete:
              - Worker was previously wiped: {{ worker_was_wiped }}
              - Required reset performed: {{ worker_was_wiped or (cluster_artifacts.stat.exists and not kubelet_conf.stat.exists) }}
              - Ready for fresh join: Yes

    - name: "Open firewall ports for worker nodes"
      firewalld:
        port: "{{ item }}"
        permanent: yes
        state: enabled
        immediate: yes
      loop:
        - "10250/tcp"   # Kubelet
        - "8472/udp"    # Flannel VXLAN
      failed_when: false
      when: 
        - ansible_os_family == 'RedHat'
        - not kubelet_conf.stat.exists

    - name: "Test connectivity to control plane API server"
      wait_for:
        host: "{{ control_plane_ip }}"
        port: 6443
        timeout: 60
      when: not kubelet_conf.stat.exists

    - name: "Copy join command from control plane"
      slurp:
        src: /tmp/kubeadm-join.sh
      register: join_command_content
      delegate_to: "{{ control_plane_host }}"
      when: not kubelet_conf.stat.exists
      
    - name: "Write join command to worker"
      copy:
        content: "{{ join_command_content.content | b64decode }}"
        dest: /tmp/kubeadm-join.sh
        mode: '0755'
      when: not kubelet_conf.stat.exists

    - name: "Copy enhanced join scripts to worker nodes"
      copy:
        src: "{{ item }}"
        dest: "/tmp/{{ item | basename }}"
        mode: '0755'
      loop:
        - "../../scripts/validate_join_prerequisites.sh"
        - "../../scripts/enhanced_kubeadm_join.sh"
      when: not kubelet_conf.stat.exists

    - name: "Enhanced kubeadm join with post-wipe worker support"
      block:
        - name: "Pre-join kubelet config validation and remediation"
          block:
            - name: "Check if kubelet config already exists"
              stat:
                path: /var/lib/kubelet/config.yaml
              register: existing_kubelet_config

            - name: "Validate existing kubelet config if present"
              shell: |
                echo "=== Existing Kubelet Configuration Validation ==="
                if [ -f /var/lib/kubelet/config.yaml ]; then
                  echo "kubelet config.yaml exists"
                  
                  # Check if kubelet config points to correct control plane
                  if [ -f /etc/kubernetes/kubelet.conf ]; then
                    config_server=$(grep "server:" /etc/kubernetes/kubelet.conf 2>/dev/null | awk '{print $2}' | sed 's|https://||' | cut -d':' -f1 || echo "")
                    expected_server="{{ control_plane_ip }}"
                    
                    if [ "$config_server" = "$expected_server" ]; then
                      echo "✓ kubelet config points to correct control plane: $config_server"
                      
                      # Check if kubelet is running properly with this config
                      if systemctl is-active kubelet >/dev/null 2>&1; then
                        if ! journalctl -u kubelet --no-pager --since "5 minutes ago" | grep -q "standalone"; then
                          echo "✓ kubelet is running in cluster mode - join may not be needed"
                          echo "KUBELET_ALREADY_JOINED=true"
                          exit 0
                        else
                          echo "WARNING: kubelet config exists but kubelet is in standalone mode"
                          echo "KUBELET_CONFIG_INVALID=true"
                        fi
                      else
                        echo "WARNING: kubelet config exists but kubelet is not running"
                        echo "KUBELET_CONFIG_INVALID=true"
                      fi
                    else
                      echo "WARNING: kubelet config points to wrong server: $config_server (expected: $expected_server)"
                      echo "KUBELET_CONFIG_INVALID=true"
                    fi
                  else
                    echo "WARNING: kubelet config.yaml exists but kubelet.conf missing"
                    echo "KUBELET_CONFIG_INVALID=true"
                  fi
                else
                  echo "kubelet config.yaml does not exist - fresh join needed"
                  echo "KUBELET_CONFIG_MISSING=true"
                fi
              register: kubelet_config_validation
              when: existing_kubelet_config.stat.exists

            - name: "Handle invalid kubelet configuration"
              block:
                - name: "Backup invalid kubelet configuration"
                  shell: |
                    timestamp=$(date +%Y%m%d-%H%M%S)
                    backup_dir="/tmp/kubelet-config-backup-$timestamp"
                    mkdir -p "$backup_dir"
                    
                    # Backup existing config files
                    [ -f /var/lib/kubelet/config.yaml ] && cp /var/lib/kubelet/config.yaml "$backup_dir/"
                    [ -f /etc/kubernetes/kubelet.conf ] && cp /etc/kubernetes/kubelet.conf "$backup_dir/"
                    [ -f /var/lib/kubelet/kubeadm-flags.env ] && cp /var/lib/kubelet/kubeadm-flags.env "$backup_dir/"
                    
                    echo "Kubelet config backed up to: $backup_dir"
                    echo "BACKUP_DIR=$backup_dir"

                - name: "Reset invalid kubelet configuration"
                  shell: |
                    echo "Resetting invalid kubelet configuration..."
                    
                    # Stop kubelet
                    systemctl stop kubelet 2>/dev/null || true
                    
                    # Remove invalid config files
                    rm -f /var/lib/kubelet/config.yaml
                    rm -f /etc/kubernetes/kubelet.conf
                    rm -f /etc/kubernetes/bootstrap-kubelet.conf
                    rm -f /var/lib/kubelet/kubeadm-flags.env
                    
                    # Reset kubelet systemd state
                    systemctl reset-failed kubelet 2>/dev/null || true
                    
                    echo "✓ Invalid kubelet configuration reset"
                  
              when: 
                - existing_kubelet_config.stat.exists
                - kubelet_config_validation.stdout is defined
                - '"KUBELET_CONFIG_INVALID=true" in kubelet_config_validation.stdout'

            - name: "Skip join if kubelet already properly joined"
              set_fact:
                skip_join_process: true
              when: 
                - existing_kubelet_config.stat.exists
                - kubelet_config_validation.stdout is defined
                - '"KUBELET_ALREADY_JOINED=true" in kubelet_config_validation.stdout'

        - name: "Copy pre-join validation script to worker"
          copy:
            src: "../../scripts/ansible_pre_join_validation.sh"
            dest: "/tmp/ansible_pre_join_validation.sh"
            mode: '0755'

        - name: "Pre-join validation for wiped workers"
          shell: "/tmp/ansible_pre_join_validation.sh {{ control_plane_ip }} {{ worker_was_wiped }}"
          register: pre_join_validation
          async: 180  # Increased from 120 to 180 seconds due to increased timeouts
          poll: 0     # Start async, we'll check status with async_status

        - name: "Wait for pre-join validation to complete"
          async_status:
            jid: "{{ pre_join_validation.ansible_job_id }}"
          register: pre_join_result
          until: pre_join_result.finished
          retries: 36  # 36 * 5 = 180 seconds maximum wait (increased from 24)
          delay: 5
          failed_when: pre_join_result.rc != 0

        - name: "Display pre-join validation results"
          debug:
            msg: |
              Pre-join validation completed:
              Exit code: {{ pre_join_result.rc }}
              Output: {{ pre_join_result.stdout_lines | join('\n              ') }}
              {% if pre_join_result.stderr_lines is defined and pre_join_result.stderr_lines %}
              Errors: {{ pre_join_result.stderr_lines | join('\n              ') }}
              {% endif %}

        - name: "Execute enhanced join process for wiped worker (bash)"
          shell: |
            # Run under bash -lc to preserve quoting and environment
            bash -lc '
            export MASTER_IP="{{ control_plane_ip }}"; 
            export JOIN_TIMEOUT=180; 
            export MAX_RETRIES=3; 
            export TOKEN_REFRESH_RETRIES=2; 
            JOIN_COMMAND=$(grep -v "^#" /tmp/kubeadm-join.sh | grep kubeadm | head -1 | sed "s/ \"\$@\"$//"); 
            echo "=== Enhanced Join for Post-Wipe Worker ==="; 
            echo "Join command: $JOIN_COMMAND"; 
            /tmp/enhanced_kubeadm_join.sh "$JOIN_COMMAND"'
          register: enhanced_join_result
          failed_when: false
          when: not (skip_join_process | default(false))

        - name: "If enhanced join failed, run simple kubeadm join script on worker"
          become: true
          delegate_to: "{{ inventory_hostname }}"
          shell: |
            # run the provided kubeadm join script under bash to preserve quoting
            bash -lc '/tmp/kubeadm-join.sh 2>&1 | tee /tmp/kubeadm-join.exec.log'
          register: simple_join
          failed_when: false
          changed_when: simple_join.rc == 0
          when: not (skip_join_process | default(false)) and (enhanced_join_result is not defined or enhanced_join_result.rc != 0)

        - name: "Wait for node to appear on control-plane by inventory name"
          delegate_to: "{{ control_plane_host }}"
          shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get node {{ inventory_hostname }} -o name || true
          register: node_by_name
          retries: 12
          delay: 10
          until: node_by_name.stdout != ""
          failed_when: false
          when: not (skip_join_process | default(false))

        - name: "Verify node becomes Ready on control-plane"
          delegate_to: "{{ control_plane_host }}"
          shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get node {{ inventory_hostname }} -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' || true
          register: node_ready_check_by_name
          retries: 12
          delay: 5
          until: node_ready_check_by_name.stdout == "True"
          failed_when: false
          when: not (skip_join_process | default(false))
          
        - name: "Display join skip message"
          debug:
            msg: |
              ✅ Kubelet already properly joined to cluster
              Skipping join process as node is already connected to control plane.
              Kubelet is running in cluster mode and configuration is valid.
          when: skip_join_process | default(false)

        - name: "Display enhanced join results for wiped worker"
          debug:
            msg: |
              Enhanced Join Results for Post-Wipe Worker:
              Pre-validation: {{ pre_join_result.stdout_lines | join('\n              ') }}
              
              Join execution:
              Exit Code: {{ enhanced_join_result.rc }}
              {% if enhanced_join_result.stdout %}
              Output: {{ enhanced_join_result.stdout }}
              {% endif %}
              {% if enhanced_join_result.stderr %}
              Errors: {{ enhanced_join_result.stderr }}
              {% endif %}
          when: not (skip_join_process | default(false))
          
        - name: "Handle join failures with comprehensive logging"
          block:
            - name: "Gather detailed failure diagnostics"
              shell: |
                echo "=== Join Failure Diagnostics ==="
                echo "Timestamp: $(date)"
                echo "Node: {{ ansible_hostname }}"
                echo ""
                
                echo "=== Join Log Files ==="
                find /tmp -name "kubeadm-join-*.log" -type f -exec echo "File: {}" \; -exec cat {} \; 2>/dev/null || echo "No join log files found"
                echo ""
                
                echo "=== Recent Kubelet Logs ==="
                journalctl -u kubelet --no-pager -l --since "30 minutes ago" | tail -50 || echo "No kubelet logs available"
                echo ""
                
                echo "=== Recent Containerd Logs ==="
                journalctl -u containerd --no-pager -l --since "30 minutes ago" | tail -50 || echo "No containerd logs available"
                echo ""
                
                echo "=== Kubelet Service Status ==="
                systemctl status kubelet --no-pager -l || true
                echo ""
                
                echo "=== Containerd Service Status ==="
                systemctl status containerd --no-pager -l || true
                echo ""
                
                echo "=== Configuration Files ==="
                echo "crictl.yaml:"
                cat /etc/crictl.yaml 2>/dev/null || echo "File not found"
                echo ""
                echo "kubelet config.yaml:"
                cat /var/lib/kubelet/config.yaml 2>/dev/null || echo "File not found"
                echo ""
                echo "kubelet.conf:"
                cat /etc/kubernetes/kubelet.conf 2>/dev/null || echo "File not found"
                echo ""
                
                echo "=== Socket Information ==="
                ls -la /run/containerd/ 2>/dev/null || echo "Containerd runtime directory not found"
                echo ""
                
                echo "=== CRI Status ==="
                crictl info 2>/dev/null || echo "CRI info failed"
                echo ""
              register: failure_diagnostics
              when: enhanced_join_result.failed | default(false)
              
            - name: "Create comprehensive failure report"
              fail:
                msg: |
                  ❌ Worker node join failed after all retry attempts
                  
                  Join Command: {{ enhanced_join_result.cmd | default('Unknown') }}
                  Exit Code: {{ enhanced_join_result.rc | default('Unknown') }}
                  
                  === DETAILED FAILURE DIAGNOSTICS ===
                  {{ failure_diagnostics.stdout | default('No diagnostics available') }}
                  
                  === REMEDIATION STEPS ===
                  1. Check containerd socket permissions:
                     sudo ls -la /run/containerd/
                     sudo chmod 660 /run/containerd/containerd.sock
                     sudo chgrp containerd /run/containerd/containerd.sock
                  
                  2. Test crictl communication:
                     sudo crictl version
                     sudo crictl info
                  
                  3. Check for containerd filesystem issues:
                     sudo ./scripts/manual_containerd_filesystem_fix.sh
                  
                  4. Gather comprehensive diagnostics:
                     sudo ./scripts/gather_worker_diagnostics.sh
                  
                  5. Manual join attempt:
                     Check control plane: kubeadm token create --print-join-command
                     Run enhanced join: sudo ./scripts/enhanced_kubeadm_join.sh "<join-command>"
                  
                  === LOG FILES TO REVIEW ===
                  - /tmp/kubeadm-join-*.log
                  - journalctl -u kubelet -f
                  - journalctl -u containerd -f
              when: enhanced_join_result.failed | default(false)
              
          when: 
            - not (skip_join_process | default(false))
            - enhanced_join_result.failed | default(false)

      when: not kubelet_conf.stat.exists

    - name: "Post-join validation and verification for wiped workers"
      block:
        - name: "Wait for kubelet to stabilize after join"
          pause:
            seconds: 15
            
        - name: "Enhanced kubelet validation for post-wipe workers"
          shell: |
            echo "=== Post-Join Kubelet Validation for Wiped Worker ==="
            echo "Timestamp: $(date)"
            echo ""
            
            # Check if kubelet is active
            if ! systemctl is-active kubelet >/dev/null 2>&1; then
              echo "ERROR: kubelet is not running after join"
              exit 1
            fi
            echo "✓ kubelet service is active"
            
            # Check kubelet logs for standalone mode indicators
            if journalctl -u kubelet --no-pager --since "10 minutes ago" | grep -q "standalone"; then
              echo "ERROR: kubelet is still in standalone mode"
              echo "This indicates the join failed to properly connect to control-plane"
              exit 1
            fi
            echo "✓ kubelet is not in standalone mode"
            
            # Check for successful cluster connection
            if journalctl -u kubelet --no-pager --since "10 minutes ago" | grep -q "Started kubelet"; then
              if ! journalctl -u kubelet --no-pager --since "10 minutes ago" | grep -q "No API server defined"; then
                echo "✓ kubelet successfully connected to cluster"
                
                # Additional validation for wiped workers
                if [ -f /etc/kubernetes/kubelet.conf ]; then
                  server_ip=$(grep "server:" /etc/kubernetes/kubelet.conf | awk '{print $2}' | cut -d':' -f1 | sed 's|https://||')
                  if [ "$server_ip" = "{{ control_plane_ip }}" ]; then
                    echo "✓ kubelet.conf points to correct control-plane: $server_ip"
                  else
                    echo "WARNING: kubelet.conf server mismatch - expected {{ control_plane_ip }}, got $server_ip"
                  fi
                else
                  echo "ERROR: kubelet.conf not found after successful join"
                  exit 1
                fi
                
                exit 0
              else
                echo "ERROR: kubelet started but no API server connection"
                exit 1
              fi
            else
              echo "WARNING: kubelet status unclear"
              exit 1
            fi
          register: kubelet_validation
          failed_when: kubelet_validation.rc != 0
          
        - name: "Display kubelet validation results"
          debug:
            msg: "{{ kubelet_validation.stdout_lines | join('\n') }}"

        - name: "Verify node appears in cluster from control plane"
          shell: |
            echo "=== Cluster Node Verification ==="
            echo "Checking if {{ ansible_hostname }} appears in cluster..."
            kubectl get nodes {{ ansible_hostname }} -o wide
            echo ""
            echo "Node status in cluster:"
            kubectl get nodes {{ ansible_hostname }} -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}'
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          delegate_to: "{{ control_plane_host }}"
          register: node_status
          retries: 10
          delay: 15
          until: node_status.rc == 0
          
        - name: "Display cluster integration status"
          debug:
            msg: |
              Cluster Integration Status:
              {{ node_status.stdout_lines | join('\n              ') }}
              
              Post-Wipe Worker Join Summary:
              ✓ Worker was successfully reset and cleaned
              ✓ Fresh join token generated with 2h TTL
              ✓ Enhanced join process completed successfully
              ✓ kubelet connected to control-plane (NOT standalone)
              ✓ Node registered in cluster and visible from control-plane
              ✓ Post-wipe worker join process COMPLETED
              
      when: not kubelet_conf.stat.exists

    - name: "Enhanced CNI readiness verification for worker nodes"
      block:
        - name: "Wait for Flannel DaemonSet to be ready on control plane"
          shell: |
            # Check Flannel DaemonSet readiness
            kubectl get daemonset -n kube-flannel kube-flannel-ds -o jsonpath='{.status.numberReady}' | grep -q '^[1-9]' || {
              echo "Flannel DaemonSet not ready yet"
              exit 1
            }
            
            # Verify at least one Flannel pod is running
            kubectl get pods -n kube-flannel -l app=flannel --field-selector=status.phase=Running | grep -q flannel || {
              echo "No Flannel pods running"
              exit 1
            }
            
            echo "Flannel DaemonSet ready on control plane"
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          delegate_to: "{{ control_plane_ip }}"
          register: flannel_ready_check
          retries: 10
          delay: 15
          until: flannel_ready_check.rc == 0

        - name: "Verify and validate CNI configuration on worker"
          block:
            - name: "Check CNI configuration file exists"
              stat:
                path: /etc/cni/net.d/10-flannel.conflist
              register: cni_config_exists

            - name: "Validate CNI configuration syntax"
              shell: |
                if [ -f /etc/cni/net.d/10-flannel.conflist ]; then
                  # Validate JSON syntax using python3
                  python3 -m json.tool /etc/cni/net.d/10-flannel.conflist >/dev/null || {
                    echo "CNI configuration has invalid JSON syntax"
                    exit 1
                  }
                  
                  # Verify it contains Flannel configuration
                  grep -q '"type": "flannel"' /etc/cni/net.d/10-flannel.conflist || {
                    echo "CNI configuration missing Flannel type"
                    exit 1
                  }
                  
                  echo "CNI configuration valid"
                else
                  echo "CNI configuration file missing"
                  exit 1
                fi
              register: cni_syntax_check
              failed_when: cni_syntax_check.rc != 0

        - name: "Enhanced pre-join CNI preparation"
          shell: |
            # Ensure CNI directories have proper permissions for containerd
            chmod 755 /etc/cni/net.d /opt/cni/bin
            chown root:root /etc/cni/net.d /opt/cni/bin
            
            # Create additional CNI runtime directories if missing
            mkdir -p /var/lib/cni/networks /var/lib/cni/results
            chmod 755 /var/lib/cni/networks /var/lib/cni/results
            
            # Ensure Flannel runtime directory exists
            mkdir -p /run/flannel
            chmod 755 /run/flannel
            
            echo "CNI preparation completed successfully"
          register: cni_preparation
          
      when: not kubelet_conf.stat.exists
      
    - name: "Install CNI plugins and configuration on worker nodes (required for kubelet)"
      block:
        - name: "Create CNI directories on worker nodes"
          file:
            path: "{{ item }}"
            state: directory
            owner: root
            group: root
            mode: '0755'
          loop:
            - /opt/cni/bin
            - /etc/cni/net.d
            - /var/lib/cni/networks
            - /var/lib/cni/results

        - name: "Download and install Flannel CNI plugin binary on worker nodes"
          get_url:
            url: "https://github.com/flannel-io/cni-plugin/releases/download/v1.7.1-flannel1/flannel-amd64"
            dest: /opt/cni/bin/flannel
            mode: '0755'
            timeout: 60
            validate_certs: false
            use_proxy: false
          retries: 3
          delay: 10
          until: flannel_download_worker is succeeded
          register: flannel_download_worker
          failed_when: false

        - name: "Fallback: Download Flannel CNI plugin with curl on worker nodes"
          shell: |
            curl -fsSL "https://github.com/flannel-io/cni-plugin/releases/download/v1.7.1-flannel1/flannel-amd64" \
              -o /opt/cni/bin/flannel --retry 3 --retry-delay 10 --connect-timeout 30 --max-time 120
            chmod 755 /opt/cni/bin/flannel
          when: flannel_download_worker.failed and ('cert_file' in flannel_download_worker.msg or 'urllib3' in flannel_download_worker.msg or 'ssl' in flannel_download_worker.msg or 'certificate' in flannel_download_worker.msg)
          register: flannel_curl_download_worker

        - name: "Enhanced fallback: Download Flannel CNI plugin with wget on worker nodes"
          shell: |
            wget -O /opt/cni/bin/flannel "https://github.com/flannel-io/cni-plugin/releases/download/v1.7.1-flannel1/flannel-amd64" \
              --no-check-certificate --retry-connrefused --tries=3 --timeout=30 --dns-timeout=10
            chmod 755 /opt/cni/bin/flannel
          when: 
            - flannel_download_worker.failed 
            - flannel_curl_download_worker is defined and flannel_curl_download_worker.failed
          register: flannel_wget_download_worker
          failed_when: false

        - name: "Verify Flannel CNI plugin download succeeded on worker nodes"
          fail:
            msg: |
              Failed to download Flannel CNI plugin on worker node using all available methods.
              get_url result: {{ flannel_download_worker.msg | default('Unknown error') }}
              curl result: {{ flannel_curl_download_worker.msg | default('Not attempted') }}
              wget result: {{ flannel_wget_download_worker.msg | default('Not attempted') }}
          when: 
            - flannel_download_worker.failed 
            - (flannel_curl_download_worker is not defined or flannel_curl_download_worker.failed)
            - (flannel_wget_download_worker is not defined or flannel_wget_download_worker.failed)

        - name: "Post-download verification: Ensure flannel binary is executable and valid on worker nodes"
          command: test -x /opt/cni/bin/flannel
          register: flannel_binary_verification_worker
          failed_when: flannel_binary_verification_worker.rc != 0

        - name: "Download and install additional CNI plugins on worker nodes"
          unarchive:
            src: "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz"
            dest: /opt/cni/bin
            remote_src: yes
            creates: /opt/cni/bin/bridge
          retries: 3

        - name: "Create basic CNI configuration for worker nodes BEFORE containerd restart"
          copy:
            content: |
              {
                "name": "cni0",
                "cniVersion": "0.3.1",
                "plugins": [
                  {
                    "type": "flannel",
                    "delegate": {
                      "hairpinMode": true,
                      "isDefaultGateway": true
                    }
                  },
                  {
                    "type": "portmap",
                    "capabilities": {
                      "portMappings": true
                    }
                  }
                ]
              }
            dest: /etc/cni/net.d/10-flannel.conflist
            owner: root
            group: root
            mode: '0644'

        - name: "Create Flannel subnet environment directory"
          file:
            path: /run/flannel
            state: directory
            mode: '0755'

      when: not kubelet_conf.stat.exists
      
    - name: "Prepare containerd for kubelet join"
      block:
        - name: "Restart containerd AFTER CNI configuration is ready"
          systemd:
            name: containerd
            state: restarted

        - name: "Wait for containerd to fully initialize"
          pause:
            seconds: 15

        - name: "Initialize containerd image filesystem for kubelet"
          shell: |
            # Initialize the k8s.io namespace (used by kubelet)
            ctr namespace create k8s.io 2>/dev/null || true
            
            # Force containerd to detect and initialize image filesystem capacity properly
            # This prevents the "invalid capacity 0 on image filesystem" error during kubelet TLS Bootstrap
            ctr --namespace k8s.io version >/dev/null 2>&1 || true
            ctr --namespace k8s.io images ls >/dev/null 2>&1 || true
            
            # Wait for image filesystem to be properly initialized
            sleep 5
            
            # Verify initialization was successful
            if ! ctr --namespace k8s.io images ls >/dev/null 2>&1; then
              echo "WARNING: containerd image filesystem not fully initialized, attempting recovery"
              # Restart containerd and retry
              systemctl restart containerd
              sleep 10
              ctr namespace create k8s.io 2>/dev/null || true
              ctr --namespace k8s.io images ls >/dev/null 2>&1 || true
            fi
            
            echo "containerd image filesystem initialization completed"
          register: containerd_image_init
          failed_when: false

        - name: "Display containerd image filesystem status"
          debug:
            msg: |
              Containerd Image Filesystem Initialization:
              {{ containerd_image_init.stdout }}
              
              This initialization prevents the 'invalid capacity 0 on image filesystem' 
              error that causes kubelet TLS Bootstrap failures.

        - name: "Verify containerd is ready for kubelet"
          shell: systemctl is-active containerd
          register: containerd_ready_check
          retries: 3
          delay: 5
          until: containerd_ready_check.stdout == "active"

      when: not kubelet_conf.stat.exists

    - name: "Starting kubeadm join process"
      debug:
        msg: "Initiating kubeadm join with enhanced retry logic"
      when: not kubelet_conf.stat.exists

    - name: "Prepare kubelet for join (ensure clean state)"
      block:
        - name: "Stop kubelet service before join"
          systemd:
            name: kubelet
            state: stopped
          failed_when: false

        - name: "Remove any stale kubelet configuration files"
          shell: |
            rm -f /var/lib/kubelet/config.yaml || true
            rm -f /var/lib/kubelet/kubeadm-flags.env || true
          failed_when: false

        - name: "Ensure kubelet service is enabled for post-join management"
          systemd:
            name: kubelet
            enabled: yes

        - name: "Wait for kubelet to fully stop"
          pause:
            seconds: 5
      when: not kubelet_conf.stat.exists

    - name: "Join cluster with retry logic"
      shell: timeout 120 /tmp/kubeadm-join.sh --ignore-preflight-errors=Port-10250,FileAvailable--etc-kubernetes-pki-ca.crt --v=5
      register: join_result
      retries: 2
      delay: 15
      when: not kubelet_conf.stat.exists
      failed_when: false

    - name: "Handle join failure with cleanup and retry"
      block:
        - name: "Capture kubelet logs for troubleshooting"
          shell: |
            echo "=== Kubelet service status ==="
            systemctl status kubelet --no-pager -l || true
            echo ""
            echo "=== Recent kubelet logs ==="
            journalctl -u kubelet --no-pager -l --since "5 minutes ago" || true
            echo ""
            echo "=== Containerd status ==="
            systemctl status containerd --no-pager -l || true
          register: join_failure_logs
          failed_when: false

        - name: "Display failure diagnostics"
          debug:
            msg: |
              === Join Failure Diagnostics ===
              {{ join_failure_logs.stdout }}
              
              === Join Command Output ===
              {% if join_result.stdout is defined %}
              STDOUT: {{ join_result.stdout }}
              {% endif %}
              {% if join_result.stderr is defined %}
              STDERR: {{ join_result.stderr }}
              {% endif %}
        - name: "Cleaning up after failed join"
          shell: |
            echo "=== Resetting node after failed join ==="
            kubeadm reset --force --v=5
          failed_when: false

        - name: "Stop services for comprehensive cleanup"
          shell: |
            systemctl stop kubelet || true
            systemctl disable kubelet || true
            systemctl stop containerd || true
          failed_when: false

        - name: "Clean up networking rules"
          shell: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X || true
          failed_when: false

        - name: "Remove Kubernetes state directories"
          shell: |
            rm -rf /etc/cni/net.d/* || true
            rm -rf /var/lib/cni/ || true
            rm -rf /var/lib/kubelet/* || true
            rm -rf /etc/kubernetes/* || true
            rm -rf /var/lib/etcd/* || true
          failed_when: false

        - name: "Reset systemd services"
          shell: |
            systemctl reset-failed kubelet || true
            systemctl reset-failed containerd || true
            systemctl daemon-reload
          failed_when: false

        - name: "Restart containerd and prepare for retry"
          systemd:
            name: containerd
            state: started
            enabled: yes

        - name: "Wait for containerd to be fully ready after restart"
          pause:
            seconds: 10

        - name: "Reinitialize containerd image filesystem after cleanup"
          shell: |
            # Initialize the k8s.io namespace (used by kubelet) after cleanup
            ctr namespace create k8s.io 2>/dev/null || true
            
            # Ensure containerd properly detects and initializes image filesystem capacity
            # This is critical after cleanup to prevent repeated "invalid capacity 0" errors
            ctr --namespace k8s.io version >/dev/null 2>&1 || true
            ctr --namespace k8s.io images ls >/dev/null 2>&1 || true
            
            # Extended wait after cleanup for filesystem detection
            sleep 10
            
            # Verify containerd image filesystem is ready for retry
            if ! ctr --namespace k8s.io images ls >/dev/null 2>&1; then
              echo "WARNING: containerd image filesystem still not ready after cleanup"
              # Final attempt to initialize
              ctr namespace create k8s.io 2>/dev/null || true
              ctr --namespace k8s.io images ls >/dev/null 2>&1 || true
            fi
            
            echo "containerd image filesystem reinitialized after cleanup"
          failed_when: false

        - name: "Reinitialize CNI configuration after cleanup"
          block:
            - name: "Recreate CNI directories"
              file:
                path: "{{ item }}"
                state: directory
                owner: root
                group: root
                mode: '0755'
              loop:
                - /etc/cni/net.d
                - /run/flannel

            - name: "Recreate basic CNI configuration"
              copy:
                content: |
                  {
                    "name": "cni0",
                    "cniVersion": "0.3.1",
                    "plugins": [
                      {
                        "type": "flannel",
                        "delegate": {
                          "hairpinMode": true,
                          "isDefaultGateway": true
                        }
                      },
                      {
                        "type": "portmap",
                        "capabilities": {
                          "portMappings": true
                        }
                      }
                    ]
                  }
                dest: /etc/cni/net.d/10-flannel.conflist
                owner: root
                group: root
                mode: '0644'

        - name: "Enable kubelet for kubeadm join"
          systemd:
            name: kubelet
            enabled: yes

        - name: "Verify containerd is running before retry"
          command: systemctl is-active containerd
          register: containerd_retry_check
          failed_when: containerd_retry_check.stdout != "active"

        - name: "Wait before retry to ensure system stability"
          pause:
            seconds: 20

        - name: "Prepare kubelet for retry join (ensure clean state)"
          block:
            - name: "Stop kubelet service before retry"
              systemd:
                name: kubelet
                state: stopped
              failed_when: false

            - name: "Remove any stale kubelet configuration files before retry"
              shell: |
                rm -f /var/lib/kubelet/config.yaml || true
                rm -f /var/lib/kubelet/kubeadm-flags.env || true
              failed_when: false

            - name: "Wait for kubelet to fully stop before retry"
              pause:
                seconds: 5

        - name: "Retry join after thorough cleanup"
          shell: timeout 120 /tmp/kubeadm-join.sh --ignore-preflight-errors=Port-10250,FileAvailable--etc-kubernetes-pki-ca.crt --v=5
          register: join_retry_result
          failed_when: join_retry_result.rc != 0

      when: 
        - not kubelet_conf.stat.exists
        - join_result is defined
        - (join_result.rc is defined and join_result.rc != 0) or (join_result.failed | default(false))

    - name: "Display join result"
      debug:
        msg: |
          Join attempt result:
          {% if join_result is defined and join_result.rc is defined %}
          Initial join - Return code: {{ join_result.rc }}
          {% if join_result.stderr is defined %}
          Initial join error: {{ join_result.stderr }}
          {% endif %}
          {% elif join_result is defined %}
          Initial join - Status: {{ join_result.get('msg', 'No return code available') }}
          {% endif %}
          {% if join_retry_result is defined and join_retry_result.rc is defined %}
          Retry join - Return code: {{ join_retry_result.rc }}
          {% if join_retry_result.stderr is defined %}
          Retry join error: {{ join_retry_result.stderr }}
          {% endif %}
          {% elif join_retry_result is defined %}
          Retry join - Status: {{ join_retry_result.get('msg', 'No return code available') }}
          {% endif %}
      when: not kubelet_conf.stat.exists

    - name: "Remove join command and cleanup temporary files"
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /tmp/kubeadm-join.sh
        - /tmp/validate_join_prerequisites.sh
        - /tmp/enhanced_kubeadm_join.sh
      when: not kubelet_conf.stat.exists

# Final cluster validation for post-wipe worker integration
- name: "Post-Wipe Worker Integration Validation"
  hosts: monitoring_nodes
  become: false
  tasks:
    - name: "Final cluster status check after post-wipe worker joins"
      shell: |
        echo "=== Final Cluster Status After Post-Wipe Worker Integration ==="
        echo "Timestamp: $(date)"
        echo ""
        
        echo "Cluster Nodes:"
        timeout 30 kubectl get nodes -o wide
        echo ""
        
        echo "Node Readiness Status:"
        timeout 30 kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions[3].type,READY:.status.conditions[3].status
        echo ""
        
        echo "Flannel DaemonSet Status:"
        timeout 30 kubectl get daemonset -n kube-flannel -o wide
        echo ""
        
        echo "Pod Network Status:"
        timeout 30 kubectl get pods -n kube-flannel -o wide
        echo ""
        
        # Count nodes by type
        total_nodes=$(timeout 30 kubectl get nodes --no-headers | wc -l)
        ready_nodes=$(timeout 30 kubectl get nodes --no-headers | grep " Ready " | wc -l)
        
        echo "Cluster Summary:"
        echo "- Total nodes: $total_nodes"
        echo "- Ready nodes: $ready_nodes"
        echo "- Control-plane: {{ groups['monitoring_nodes'] | length }}"
        echo "- Storage nodes: {{ groups['storage_nodes'] | length }}"
        echo "- Compute nodes: {{ groups['compute_nodes'] | length }}"
        echo ""
        
        if [ "$ready_nodes" -eq "$total_nodes" ] && [ "$ready_nodes" -gt 1 ]; then
          echo "✅ POST-WIPE WORKER INTEGRATION SUCCESSFUL"
          echo "All wiped workers successfully joined control-plane cluster"
        else
          echo "⚠️  Some nodes may still be joining or have issues"
          echo "Wait a few minutes and check: kubectl get nodes"
        fi
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: final_cluster_status
      
    - name: "Display final cluster integration summary"
      debug:
        msg: |
          {{ final_cluster_status.stdout_lines | join('\n          ') }}
          
          ====================================
          POST-WIPE WORKER JOIN PROCESS COMPLETE
          ====================================
          
          The enhanced worker join process has:
          ✓ Detected post-wipe worker states
          ✓ Validated control-plane readiness
          ✓ Generated fresh join tokens (2h TTL)
          ✓ Performed enhanced reset and cleanup
          ✓ Successfully joined workers to control-plane
          ✓ Verified kubelet cluster connectivity (NOT standalone)
          ✓ Confirmed node registration in cluster
          
          Workers are now managed by the control-plane using TLS certificates.
          No standalone mode detected - cluster formation successful!
