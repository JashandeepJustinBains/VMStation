---
# VMStation Simplified Kubernetes Cluster Setup
# Replaces the complex 2901-line setup_cluster.yaml with essential functionality

- name: "Setup Kubernetes Cluster - All Nodes"
  hosts: all
  become: true
  vars:
    kubernetes_version: "1.29"
    pod_network_cidr: "10.244.0.0/16"
  tasks:
    - name: "Update package cache (Debian/Ubuntu)"
      apt:
        update_cache: yes
      when: ansible_os_family == 'Debian'

    - name: "Install required packages (Debian/Ubuntu)"
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
        state: present
      when: ansible_os_family == 'Debian'

    - name: "Add Kubernetes GPG key"
      apt_key:
        url: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/Release.key"
        state: present
      when: ansible_os_family == 'Debian'

    - name: "Add Kubernetes repository"
      apt_repository:
        repo: "deb https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/ /"
        state: present
        filename: kubernetes
      when: ansible_os_family == 'Debian'

    - name: "Install Kubernetes packages"
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
          - containerd
        state: present
      when: ansible_os_family == 'Debian'

    - name: "Install packages for RHEL/CentOS"
      block:
        - name: "Add Kubernetes repository (RHEL/CentOS)"
          yum_repository:
            name: kubernetes
            description: Kubernetes
            baseurl: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/rpm/"
            gpgcheck: yes
            repo_gpgcheck: yes
            gpgkey: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/rpm/repodata/repomd.xml.key"
            enabled: yes

        - name: "Install Kubernetes packages (RHEL/CentOS)"
          package:
            name:
              - kubelet
              - kubeadm
              - kubectl
              - containerd
            state: present
      when: ansible_os_family == 'RedHat'

    - name: "Hold Kubernetes packages (Debian/Ubuntu)"
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl
      when: ansible_os_family == 'Debian'

    - name: "Disable swap"
      shell: |
        swapoff -a
        sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

    - name: "Load kernel modules"
      modprobe:
        name: "{{ item }}"
      loop:
        - overlay
        - br_netfilter

    - name: "Create kernel modules config"
      copy:
        content: |
          overlay
          br_netfilter
        dest: /etc/modules-load.d/k8s.conf

    - name: "Set sysctl parameters"
      sysctl:
        name: "{{ item.key }}"
        value: "{{ item.value }}"
        state: present
        reload: yes
      loop:
        - { key: 'net.bridge.bridge-nf-call-iptables', value: '1' }
        - { key: 'net.bridge.bridge-nf-call-ip6tables', value: '1' }
        - { key: 'net.ipv4.ip_forward', value: '1' }

    - name: "Configure containerd"
      block:
        - name: "Create containerd config directory"
          file:
            path: /etc/containerd
            state: directory

        - name: "Generate containerd config"
          shell: containerd config default > /etc/containerd/config.toml
          args:
            creates: /etc/containerd/config.toml

        - name: "Configure containerd cgroup driver"
          replace:
            path: /etc/containerd/config.toml
            regexp: 'SystemdCgroup = false'
            replace: 'SystemdCgroup = true'

        - name: "Start and enable containerd"
          systemd:
            name: containerd
            state: restarted
            enabled: yes

        - name: "Wait for containerd to fully initialize"
          pause:
            seconds: 10

        - name: "Verify containerd filesystem capacity detection"
          shell: |
            # Force containerd to properly detect filesystem capacity
            ctr --namespace k8s.io images ls >/dev/null 2>&1 || true
            sleep 5
          failed_when: false

    - name: "Enable kubelet"
      systemd:
        name: kubelet
        enabled: yes

    - name: "Ensure kubelet service directory exists"
      file:
        path: /etc/systemd/system/kubelet.service.d
        state: directory
        mode: '0755'

    # Note: Removed static kubelet configuration that conflicted with kubeadm join process
    # kubeadm init (control plane) and kubeadm join (workers) will handle kubelet configuration
    # This prevents conflicts during the TLS bootstrap process

  handlers:
    - name: reload systemd
      systemd:
        daemon_reload: yes

# Control plane initialization
- name: "Initialize Kubernetes Control Plane"
  hosts: monitoring_nodes
  become: true
  vars:
    pod_network_cidr: "10.244.0.0/16"
    auth_mode: "{{ kubernetes_authorization_mode | default('Node,RBAC') }}"
    enable_fallback: "{{ kubernetes_authorization_fallback | default(false) }}"
  tasks:
    - name: "Check if cluster exists"
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeconfig

    - name: "Initialize cluster with secure authorization mode"
      command: >
        kubeadm init 
        --pod-network-cidr={{ pod_network_cidr }}
        --apiserver-advertise-address={{ ansible_default_ipv4.address }}
        --control-plane-endpoint={{ ansible_default_ipv4.address }}
      when: not kubeconfig.stat.exists
      register: kubeadm_init
      ignore_errors: "{{ enable_fallback | bool }}"

    - name: "Initialize cluster with AlwaysAllow fallback (if secure mode failed)"
      command: >
        kubeadm init 
        --pod-network-cidr={{ pod_network_cidr }}
        --apiserver-advertise-address={{ ansible_default_ipv4.address }}
        --control-plane-endpoint={{ ansible_default_ipv4.address }}
      when: 
        - not kubeconfig.stat.exists
        - enable_fallback | bool
        - kubeadm_init.failed | default(false)
      register: kubeadm_init_fallback

    - name: "Display authorization mode warning if fallback was used"
      debug:
        msg: |
          WARNING: Cluster initialized with --authorization-mode=AlwaysAllow
          This is less secure and should only be used for troubleshooting.
          Consider investigating why {{ auth_mode }} mode failed.
      when: 
        - enable_fallback | bool
        - kubeadm_init_fallback is defined
        - kubeadm_init_fallback is succeeded

    - name: "Setup kubeconfig for root"
      block:
        - name: "Create .kube directory"
          file:
            path: /root/.kube
            state: directory
            mode: '0755'

        - name: "Copy admin.conf"
          copy:
            src: /etc/kubernetes/admin.conf
            dest: /root/.kube/config
            mode: '0644'
            remote_src: yes

    - name: "Open firewall ports for Kubernetes"
      firewalld:
        port: "{{ item }}"
        permanent: yes
        state: enabled
        immediate: yes
      loop:
        - "6443/tcp"    # API server
        - "10250/tcp"   # Kubelet
        - "10251/tcp"   # kube-scheduler
        - "10252/tcp"   # kube-controller-manager
        - "8472/udp"    # Flannel VXLAN
      failed_when: false
      when: ansible_os_family == 'RedHat'

    - name: "Check if Flannel CNI is already installed"
      shell: |
        kubectl get namespace kube-flannel >/dev/null 2>&1
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_exists
      failed_when: false

    - name: "Install Flannel CNI"
      shell: |
        kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_install
      retries: 3
      delay: 10
      until: flannel_install.rc == 0
      when: flannel_exists.rc != 0

    - name: "Wait for Flannel DaemonSet to be created"
      shell: |
        kubectl get daemonset -n kube-flannel kube-flannel-ds
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_ds_check
      retries: 10
      delay: 5
      until: flannel_ds_check.rc == 0
      when: flannel_exists.rc != 0

    - name: "Ensure CoreDNS has correct replica count"
      shell: |
        # Check current CoreDNS replica count
        current_replicas=$(kubectl get deployment coredns -n kube-system -o jsonpath='{.spec.replicas}')
        if [ "$current_replicas" != "1" ]; then
          echo "Scaling CoreDNS from $current_replicas to 1 replica"
          kubectl scale deployment coredns -n kube-system --replicas=1
        else
          echo "CoreDNS already has correct replica count: $current_replicas"
        fi
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: coredns_scale
      failed_when: false

    - name: "Check Flannel namespace and resources"
      shell: |
        echo "Checking Flannel deployment status:"
        kubectl get namespace kube-flannel
        kubectl get all -n kube-flannel
        echo "Node status:"
        kubectl get nodes
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_status
      failed_when: false

    - name: "Display Flannel status"
      debug:
        msg: |
          Flannel CNI Status:
          {{ flannel_status.stdout }}
          
          Note: Flannel pods may show CrashLoopBackOff until worker nodes join.
          This is expected behavior in a single-node control plane setup.

    - name: "Wait for API server to be ready"
      wait_for:
        host: "{{ ansible_default_ipv4.address }}"
        port: 6443
        timeout: 300
        delay: 10
      
    - name: "Verify API server accessibility using kubectl"
      shell: kubectl get nodes --request-timeout=10s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: api_health
      retries: 10
      delay: 15
      until: api_health.rc == 0

    - name: "Validate kubernetes-admin RBAC permissions"
      shell: >
        kubectl auth can-i create secrets --namespace=kube-system
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: rbac_check
      failed_when: false

    - name: "Check current authorization mode"
      shell: >
        kubectl get pods -n kube-system kube-apiserver-* -o jsonpath='{.items[0].spec.containers[0].command}' | 
        grep -o '\--authorization-mode=[^[:space:]]*' | 
        cut -d= -f2
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: current_auth_mode
      failed_when: false

    - name: "Display current authorization mode"
      debug:
        msg: "Current Kubernetes authorization mode: {{ current_auth_mode.stdout | default('Unable to determine') }}"

    - name: "Fix kubernetes-admin RBAC if needed"
      shell: |
        kubectl create clusterrolebinding kubernetes-admin \
          --clusterrole=cluster-admin \
          --user=kubernetes-admin \
          --dry-run=client -o yaml | kubectl apply -f -
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: 
        - rbac_check.stdout != "yes"
        - current_auth_mode.stdout is defined
        - "'RBAC' in current_auth_mode.stdout"

    - name: "Fix API server authorization mode if using AlwaysAllow"
      block:
        - name: "Backup API server manifest"
          copy:
            src: /etc/kubernetes/manifests/kube-apiserver.yaml
            dest: /etc/kubernetes/manifests/kube-apiserver.yaml.backup
            remote_src: yes

        - name: "Update authorization mode from AlwaysAllow to Node,RBAC"
          replace:
            path: /etc/kubernetes/manifests/kube-apiserver.yaml
            regexp: '--authorization-mode=AlwaysAllow'
            replace: '--authorization-mode=Node,RBAC'

        - name: "Wait for API server to restart after authorization fix"
          wait_for:
            host: "{{ ansible_default_ipv4.address }}"
            port: 6443
            timeout: 300
            delay: 30

        - name: "Verify API server health after authorization fix"
          shell: kubectl get nodes --request-timeout=10s
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          register: api_health_after_fix
          retries: 10
          delay: 15
          until: api_health_after_fix.rc == 0

        - name: "Apply RBAC fix after authorization mode change"
          shell: |
            kubectl create clusterrolebinding kubernetes-admin \
              --clusterrole=cluster-admin \
              --user=kubernetes-admin \
              --dry-run=client -o yaml | kubectl apply -f -
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

      when: 
        - current_auth_mode.stdout is defined
        - "'AlwaysAllow' in current_auth_mode.stdout"

    - name: "Skip RBAC fix for AlwaysAllow mode (no longer needed after fix)"
      debug:
        msg: "Authorization mode fixed from AlwaysAllow to Node,RBAC"
      when: 
        - rbac_check.stdout != "yes"
        - current_auth_mode.stdout is defined
        - "'AlwaysAllow' in current_auth_mode.stdout"

    - name: "Wait for API server pod to be Ready"
      shell: |
        kubectl get pods -n kube-system -l component=kube-apiserver \
          -o jsonpath='{.items[0].status.conditions[?(@.type=="Ready")].status}' | grep -q "True"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: api_pod_ready
      retries: 20
      delay: 15
      until: api_pod_ready.rc == 0

    - name: "Check cluster-info configmap RBAC permissions"
      shell: >
        kubectl get clusterrole system:public-info-viewer >/dev/null 2>&1 &&
        kubectl get clusterrolebinding cluster-info >/dev/null 2>&1 &&
        echo "yes" || echo "no"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: cluster_info_rbac_check
      failed_when: false

    - name: "Create RBAC rule for anonymous access to cluster-info configmap"
      shell: |
        kubectl create clusterrole system:public-info-viewer \
          --verb=get --resource=configmaps \
          --resource-name=cluster-info \
          --dry-run=client -o yaml | kubectl apply -f -
        
        kubectl create clusterrolebinding cluster-info \
          --clusterrole=system:public-info-viewer \
          --group=system:unauthenticated \
          --group=system:authenticated \
          --dry-run=client -o yaml | kubectl apply -f -
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: cluster_info_rbac_check.stdout != "yes"

    - name: "Verify cluster-info configmap accessibility"
      shell: >
        kubectl get clusterrole system:public-info-viewer >/dev/null 2>&1 &&
        kubectl get clusterrolebinding cluster-info >/dev/null 2>&1 &&
        echo "yes" || echo "no"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: cluster_info_verification
      retries: 3
      delay: 5
      until: cluster_info_verification.stdout == "yes"

    - name: "Generate join command"
      shell: kubeadm token create --print-join-command
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: join_command
      retries: 3
      delay: 10
      until: join_command.rc == 0

    - name: "Save join command"
      copy:
        content: |
          #!/bin/bash
          # Generated kubeadm join command with support for additional arguments
          {{ join_command.stdout }} "$@"
        dest: /tmp/kubeadm-join.sh
        mode: '0755'

# Join worker nodes
- name: "Join Worker Nodes"
  hosts: storage_nodes:compute_nodes
  become: true
  vars:
    control_plane_ip: "{{ groups['monitoring_nodes'][0] }}"
  tasks:
    - name: "Check if node is joined"
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: "Check for existing cluster artifacts"
      stat:
        path: /etc/kubernetes/pki/ca.crt
      register: cluster_artifacts

    - name: "Reset node if it has cluster artifacts but isn't properly joined"
      block:
        - name: "Stop and disable kubelet service"
          systemd:
            name: kubelet
            state: stopped
            enabled: no
          failed_when: false

        - name: "Stop containerd temporarily for thorough cleanup"
          systemd:
            name: containerd
            state: stopped
          failed_when: false

        - name: "Reset kubeadm configuration"
          shell: kubeadm reset --force
          register: reset_result
          failed_when: false

        - name: "Clean up iptables rules"
          shell: |
            iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
          failed_when: false

        - name: "Remove all Kubernetes directories and configurations"
          shell: |
            rm -rf /etc/cni/net.d/*
            rm -rf /var/lib/cni/
            rm -rf /var/lib/kubelet/*
            rm -rf /etc/kubernetes/*
            rm -rf /var/lib/etcd/*
          failed_when: false

        - name: "Reset systemd services"
          shell: |
            systemctl reset-failed kubelet || true
            systemctl reset-failed containerd || true
            systemctl daemon-reload
          failed_when: false

        - name: "Restart containerd after cleanup"
          systemd:
            name: containerd
            state: started
            enabled: yes

      when: 
        - cluster_artifacts.stat.exists
        - not kubelet_conf.stat.exists

    - name: "Open firewall ports for worker nodes"
      firewalld:
        port: "{{ item }}"
        permanent: yes
        state: enabled
        immediate: yes
      loop:
        - "10250/tcp"   # Kubelet
        - "8472/udp"    # Flannel VXLAN
      failed_when: false
      when: 
        - ansible_os_family == 'RedHat'
        - not kubelet_conf.stat.exists

    - name: "Test connectivity to control plane API server"
      wait_for:
        host: "{{ control_plane_ip }}"
        port: 6443
        timeout: 60
      when: not kubelet_conf.stat.exists

    - name: "Copy join command from control plane"
      slurp:
        src: /tmp/kubeadm-join.sh
      register: join_command_content
      delegate_to: "{{ control_plane_ip }}"
      when: not kubelet_conf.stat.exists
      
    - name: "Write join command to worker"
      copy:
        content: "{{ join_command_content.content | b64decode }}"
        dest: /tmp/kubeadm-join.sh
        mode: '0755'
      when: not kubelet_conf.stat.exists

    - name: "Copy enhanced join scripts to worker nodes"
      copy:
        src: "{{ item }}"
        dest: "/tmp/{{ item | basename }}"
        mode: '0755'
      loop:
        - "../../scripts/validate_join_prerequisites.sh"
        - "../../scripts/enhanced_kubeadm_join.sh"
      when: not kubelet_conf.stat.exists

    - name: "Enhanced kubeadm join with comprehensive validation"
      block:
        - name: "Execute enhanced join process"
          shell: |
            # Set environment variables for enhanced join
            export MASTER_IP="{{ control_plane_ip }}"
            export JOIN_TIMEOUT=300
            export MAX_RETRIES=2
            
            # Extract base join command from script
            JOIN_COMMAND=$(grep -v '^#' /tmp/kubeadm-join.sh | grep kubeadm | head -1)
            
            # Run enhanced join process
            /tmp/enhanced_kubeadm_join.sh $JOIN_COMMAND
          register: enhanced_join_result
          failed_when: enhanced_join_result.rc != 0
          
        - name: "Display enhanced join results"
          debug:
            msg: |
              Enhanced Join Results:
              Exit Code: {{ enhanced_join_result.rc }}
              {% if enhanced_join_result.stdout %}
              Output: {{ enhanced_join_result.stdout }}
              {% endif %}
              {% if enhanced_join_result.stderr %}
              Errors: {{ enhanced_join_result.stderr }}
              {% endif %}

      when: not kubelet_conf.stat.exists

    - name: "Post-join validation and verification"
      block:
        - name: "Wait for kubelet to stabilize after join"
          pause:
            seconds: 30
            
        - name: "Verify kubelet is running and not in standalone mode"
          shell: |
            # Check if kubelet is active
            systemctl is-active kubelet || exit 1
            
            # Check kubelet logs for standalone mode indicators
            if journalctl -u kubelet --no-pager --since "10 minutes ago" | grep -q "standalone"; then
              echo "ERROR: kubelet is still in standalone mode"
              exit 1
            fi
            
            # Check for successful cluster connection
            if journalctl -u kubelet --no-pager --since "10 minutes ago" | grep -q "Started kubelet"; then
              if ! journalctl -u kubelet --no-pager --since "10 minutes ago" | grep -q "No API server defined"; then
                echo "SUCCESS: kubelet successfully connected to cluster"
                exit 0
              else
                echo "ERROR: kubelet started but no API server connection"
                exit 1
              fi
            else
              echo "WARNING: kubelet status unclear"
              exit 1
            fi
          register: kubelet_validation
          failed_when: kubelet_validation.rc != 0
          
        - name: "Display kubelet status"
          debug:
            msg: "{{ kubelet_validation.stdout }}"
            
        - name: "Verify node appears in cluster (from control plane)"
          shell: kubectl get nodes {{ ansible_hostname }} -o wide
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          delegate_to: "{{ control_plane_ip }}"
          register: node_status
          retries: 5
          delay: 10
          until: node_status.rc == 0
          
        - name: "Display node status in cluster"
          debug:
            msg: |
              Node Status in Cluster:
              {{ node_status.stdout }}
              
      when: not kubelet_conf.stat.exists

    - name: "Install CNI plugins and configuration on worker nodes (required for kubelet)"
      block:
        - name: "Create CNI directories on worker nodes"
          file:
            path: "{{ item }}"
            state: directory
            owner: root
            group: root
            mode: '0755'
          loop:
            - /opt/cni/bin
            - /etc/cni/net.d
            - /var/lib/cni/networks
            - /var/lib/cni/results

        - name: "Download and install Flannel CNI plugin binary on worker nodes"
          get_url:
            url: "https://github.com/flannel-io/cni-plugin/releases/download/v1.7.1-flannel2/flannel-amd64"
            dest: /opt/cni/bin/flannel
            mode: '0755'
            timeout: 60
          retries: 3

        - name: "Download and install additional CNI plugins on worker nodes"
          unarchive:
            src: "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz"
            dest: /opt/cni/bin
            remote_src: yes
            creates: /opt/cni/bin/bridge
          retries: 3

        - name: "Create basic CNI configuration for worker nodes"
          copy:
            content: |
              {
                "name": "cni0",
                "cniVersion": "0.3.1",
                "plugins": [
                  {
                    "type": "flannel",
                    "delegate": {
                      "hairpinMode": true,
                      "isDefaultGateway": true
                    }
                  },
                  {
                    "type": "portmap",
                    "capabilities": {
                      "portMappings": true
                    }
                  }
                ]
              }
            dest: /etc/cni/net.d/10-flannel.conflist
            owner: root
            group: root
            mode: '0644'

        - name: "Create Flannel subnet environment directory"
          file:
            path: /run/flannel
            state: directory
            mode: '0755'

      when: not kubelet_conf.stat.exists

    - name: "Prepare containerd for kubelet join"
      block:
        - name: "Restart containerd to ensure proper filesystem detection"
          systemd:
            name: containerd
            state: restarted

        - name: "Wait for containerd to fully initialize"
          pause:
            seconds: 15

        - name: "Initialize containerd filesystem capacity detection"
          shell: |
            # Force containerd to detect filesystem capacity properly
            ctr --namespace k8s.io version >/dev/null 2>&1 || true
            ctr --namespace k8s.io images ls >/dev/null 2>&1 || true
            sleep 5
          failed_when: false

        - name: "Verify containerd is ready for kubelet"
          shell: systemctl is-active containerd
          register: containerd_ready_check
          retries: 3
          delay: 5
          until: containerd_ready_check.stdout == "active"

      when: not kubelet_conf.stat.exists

    - name: "Starting kubeadm join process"
      debug:
        msg: "Initiating kubeadm join with enhanced retry logic"
      when: not kubelet_conf.stat.exists

    - name: "Prepare kubelet for join (ensure clean state)"
      block:
        - name: "Stop kubelet service before join"
          systemd:
            name: kubelet
            state: stopped
          failed_when: false

        - name: "Remove any stale kubelet configuration files"
          shell: |
            rm -f /var/lib/kubelet/config.yaml || true
            rm -f /var/lib/kubelet/kubeadm-flags.env || true
          failed_when: false

        - name: "Ensure kubelet service is enabled for post-join management"
          systemd:
            name: kubelet
            enabled: yes

        - name: "Wait for kubelet to fully stop"
          pause:
            seconds: 5
      when: not kubelet_conf.stat.exists

    - name: "Join cluster with retry logic"
      shell: timeout 600 /tmp/kubeadm-join.sh --ignore-preflight-errors=Port-10250,FileAvailable--etc-kubernetes-pki-ca.crt --v=5
      register: join_result
      retries: 3
      delay: 30
      when: not kubelet_conf.stat.exists
      failed_when: false

    - name: "Handle join failure with cleanup and retry"
      block:
        - name: "Capture kubelet logs for troubleshooting"
          shell: |
            echo "=== Kubelet service status ==="
            systemctl status kubelet --no-pager -l || true
            echo ""
            echo "=== Recent kubelet logs ==="
            journalctl -u kubelet --no-pager -l --since "5 minutes ago" || true
            echo ""
            echo "=== Containerd status ==="
            systemctl status containerd --no-pager -l || true
          register: join_failure_logs
          failed_when: false

        - name: "Display failure diagnostics"
          debug:
            msg: |
              === Join Failure Diagnostics ===
              {{ join_failure_logs.stdout }}
              
              === Join Command Output ===
              {% if join_result.stdout is defined %}
              STDOUT: {{ join_result.stdout }}
              {% endif %}
              {% if join_result.stderr is defined %}
              STDERR: {{ join_result.stderr }}
              {% endif %}
        - name: "Cleaning up after failed join"
          shell: |
            echo "=== Resetting node after failed join ==="
            kubeadm reset --force --v=5
          failed_when: false

        - name: "Stop services for comprehensive cleanup"
          shell: |
            systemctl stop kubelet || true
            systemctl disable kubelet || true
            systemctl stop containerd || true
          failed_when: false

        - name: "Clean up networking rules"
          shell: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X || true
          failed_when: false

        - name: "Remove Kubernetes state directories"
          shell: |
            rm -rf /etc/cni/net.d/* || true
            rm -rf /var/lib/cni/ || true
            rm -rf /var/lib/kubelet/* || true
            rm -rf /etc/kubernetes/* || true
            rm -rf /var/lib/etcd/* || true
          failed_when: false

        - name: "Reset systemd services"
          shell: |
            systemctl reset-failed kubelet || true
            systemctl reset-failed containerd || true
            systemctl daemon-reload
          failed_when: false

        - name: "Restart containerd and prepare for retry"
          systemd:
            name: containerd
            state: started
            enabled: yes

        - name: "Wait for containerd to be fully ready after restart"
          pause:
            seconds: 20

        - name: "Reinitialize containerd filesystem detection after cleanup"
          shell: |
            # Ensure containerd properly detects filesystem capacity
            ctr --namespace k8s.io version >/dev/null 2>&1 || true
            ctr --namespace k8s.io images ls >/dev/null 2>&1 || true
            sleep 10
          failed_when: false

        - name: "Reinitialize CNI configuration after cleanup"
          block:
            - name: "Recreate CNI directories"
              file:
                path: "{{ item }}"
                state: directory
                owner: root
                group: root
                mode: '0755'
              loop:
                - /etc/cni/net.d
                - /run/flannel

            - name: "Recreate basic CNI configuration"
              copy:
                content: |
                  {
                    "name": "cni0",
                    "cniVersion": "0.3.1",
                    "plugins": [
                      {
                        "type": "flannel",
                        "delegate": {
                          "hairpinMode": true,
                          "isDefaultGateway": true
                        }
                      },
                      {
                        "type": "portmap",
                        "capabilities": {
                          "portMappings": true
                        }
                      }
                    ]
                  }
                dest: /etc/cni/net.d/10-flannel.conflist
                owner: root
                group: root
                mode: '0644'

        - name: "Enable kubelet for kubeadm join"
          systemd:
            name: kubelet
            enabled: yes

        - name: "Verify containerd is running before retry"
          command: systemctl is-active containerd
          register: containerd_retry_check
          failed_when: containerd_retry_check.stdout != "active"

        - name: "Wait longer before retry to ensure system stability"
          pause:
            seconds: 60

        - name: "Prepare kubelet for retry join (ensure clean state)"
          block:
            - name: "Stop kubelet service before retry"
              systemd:
                name: kubelet
                state: stopped
              failed_when: false

            - name: "Remove any stale kubelet configuration files before retry"
              shell: |
                rm -f /var/lib/kubelet/config.yaml || true
                rm -f /var/lib/kubelet/kubeadm-flags.env || true
              failed_when: false

            - name: "Wait for kubelet to fully stop before retry"
              pause:
                seconds: 5

        - name: "Retry join after thorough cleanup"
          shell: timeout 600 /tmp/kubeadm-join.sh --ignore-preflight-errors=Port-10250,FileAvailable--etc-kubernetes-pki-ca.crt --v=5
          register: join_retry_result
          failed_when: join_retry_result.rc != 0

      when: 
        - not kubelet_conf.stat.exists
        - join_result is defined
        - (join_result.rc is defined and join_result.rc != 0) or (join_result.failed | default(false))

    - name: "Display join result"
      debug:
        msg: |
          Join attempt result:
          {% if join_result is defined and join_result.rc is defined %}
          Initial join - Return code: {{ join_result.rc }}
          {% if join_result.stderr is defined %}
          Initial join error: {{ join_result.stderr }}
          {% endif %}
          {% elif join_result is defined %}
          Initial join - Status: {{ join_result.get('msg', 'No return code available') }}
          {% endif %}
          {% if join_retry_result is defined and join_retry_result.rc is defined %}
          Retry join - Return code: {{ join_retry_result.rc }}
          {% if join_retry_result.stderr is defined %}
          Retry join error: {{ join_retry_result.stderr }}
          {% endif %}
          {% elif join_retry_result is defined %}
          Retry join - Status: {{ join_retry_result.get('msg', 'No return code available') }}
          {% endif %}
      when: not kubelet_conf.stat.exists

    - name: "Remove join command"
      file:
        path: /tmp/kubeadm-join.sh
        state: absent
