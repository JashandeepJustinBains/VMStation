---
# VMStation Simplified Kubernetes Cluster Setup
# Replaces the complex 2901-line setup_cluster.yaml with essential functionality

- name: "Setup Kubernetes Cluster - All Nodes"
  hosts: all
  become: true
  vars:
    kubernetes_version: "1.29"
    pod_network_cidr: "10.244.0.0/16"
  tasks:
    - name: "Update package cache (Debian/Ubuntu)"
      apt:
        update_cache: yes
      when: ansible_os_family == 'Debian'

    - name: "Install required packages (Debian/Ubuntu)"
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
        state: present
      when: ansible_os_family == 'Debian'

    - name: "Add Kubernetes GPG key"
      apt_key:
        url: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/Release.key"
        state: present
      when: ansible_os_family == 'Debian'

    - name: "Add Kubernetes repository"
      apt_repository:
        repo: "deb https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/ /"
        state: present
        filename: kubernetes
      when: ansible_os_family == 'Debian'

    - name: "Install Kubernetes packages"
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
          - containerd
        state: present
      when: ansible_os_family == 'Debian'

    - name: "Install packages for RHEL/CentOS"
      block:
        - name: "Add Kubernetes repository (RHEL/CentOS)"
          yum_repository:
            name: kubernetes
            description: Kubernetes
            baseurl: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/rpm/"
            gpgcheck: yes
            repo_gpgcheck: yes
            gpgkey: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/rpm/repodata/repomd.xml.key"
            enabled: yes

        - name: "Install Kubernetes packages (RHEL/CentOS)"
          package:
            name:
              - kubelet
              - kubeadm
              - kubectl
              - containerd
            state: present
      when: ansible_os_family == 'RedHat'

    - name: "Hold Kubernetes packages (Debian/Ubuntu)"
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl
      when: ansible_os_family == 'Debian'

    - name: "Disable swap"
      shell: |
        swapoff -a
        sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

    - name: "Load kernel modules"
      modprobe:
        name: "{{ item }}"
      loop:
        - overlay
        - br_netfilter

    - name: "Create kernel modules config"
      copy:
        content: |
          overlay
          br_netfilter
        dest: /etc/modules-load.d/k8s.conf

    - name: "Set sysctl parameters"
      sysctl:
        name: "{{ item.key }}"
        value: "{{ item.value }}"
        state: present
        reload: yes
      loop:
        - { key: 'net.bridge.bridge-nf-call-iptables', value: '1' }
        - { key: 'net.bridge.bridge-nf-call-ip6tables', value: '1' }
        - { key: 'net.ipv4.ip_forward', value: '1' }

    - name: "Configure containerd"
      block:
        - name: "Create containerd config directory"
          file:
            path: /etc/containerd
            state: directory

        - name: "Generate containerd config"
          shell: containerd config default > /etc/containerd/config.toml
          args:
            creates: /etc/containerd/config.toml

        - name: "Configure containerd cgroup driver"
          replace:
            path: /etc/containerd/config.toml
            regexp: 'SystemdCgroup = false'
            replace: 'SystemdCgroup = true'

        - name: "Start and enable containerd"
          systemd:
            name: containerd
            state: restarted
            enabled: yes

    - name: "Enable kubelet"
      systemd:
        name: kubelet
        enabled: yes

# Control plane initialization
- name: "Initialize Kubernetes Control Plane"
  hosts: monitoring_nodes
  become: true
  vars:
    pod_network_cidr: "10.244.0.0/16"
    auth_mode: "{{ kubernetes_authorization_mode | default('Node,RBAC') }}"
    enable_fallback: "{{ kubernetes_authorization_fallback | default(false) }}"
  tasks:
    - name: "Check if cluster exists"
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeconfig

    - name: "Initialize cluster with secure authorization mode"
      command: >
        kubeadm init 
        --pod-network-cidr={{ pod_network_cidr }}
        --apiserver-advertise-address={{ ansible_default_ipv4.address }}
        --control-plane-endpoint={{ ansible_default_ipv4.address }}
        --authorization-mode={{ auth_mode }}
      when: not kubeconfig.stat.exists
      register: kubeadm_init
      ignore_errors: "{{ enable_fallback | bool }}"

    - name: "Initialize cluster with AlwaysAllow fallback (if secure mode failed)"
      command: >
        kubeadm init 
        --pod-network-cidr={{ pod_network_cidr }}
        --apiserver-advertise-address={{ ansible_default_ipv4.address }}
        --control-plane-endpoint={{ ansible_default_ipv4.address }}
        --authorization-mode=AlwaysAllow
      when: 
        - not kubeconfig.stat.exists
        - enable_fallback | bool
        - kubeadm_init.failed | default(false)
      register: kubeadm_init_fallback

    - name: "Display authorization mode warning if fallback was used"
      debug:
        msg: |
          WARNING: Cluster initialized with --authorization-mode=AlwaysAllow
          This is less secure and should only be used for troubleshooting.
          Consider investigating why {{ auth_mode }} mode failed.
      when: 
        - enable_fallback | bool
        - kubeadm_init_fallback is defined
        - kubeadm_init_fallback is succeeded

    - name: "Setup kubeconfig for root"
      block:
        - name: "Create .kube directory"
          file:
            path: /root/.kube
            state: directory
            mode: '0755'

        - name: "Copy admin.conf"
          copy:
            src: /etc/kubernetes/admin.conf
            dest: /root/.kube/config
            mode: '0644'
            remote_src: yes

    - name: "Install Flannel CNI"
      shell: |
        kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: kubeadm_init is changed

    - name: "Validate kubernetes-admin RBAC permissions"
      shell: >
        kubectl auth can-i create secrets --namespace=kube-system
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: rbac_check
      failed_when: false

    - name: "Check current authorization mode"
      shell: >
        kubectl get pods -n kube-system kube-apiserver-* -o jsonpath='{.items[0].spec.containers[0].command}' | 
        grep -o '\--authorization-mode=[^[:space:]]*' | 
        cut -d= -f2
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: current_auth_mode
      failed_when: false

    - name: "Display current authorization mode"
      debug:
        msg: "Current Kubernetes authorization mode: {{ current_auth_mode.stdout | default('Unable to determine') }}"

    - name: "Fix kubernetes-admin RBAC if needed"
      shell: |
        kubectl create clusterrolebinding kubernetes-admin \
          --clusterrole=cluster-admin \
          --user=kubernetes-admin \
          --dry-run=client -o yaml | kubectl apply -f -
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: 
        - rbac_check.stdout != "yes"
        - current_auth_mode.stdout is defined
        - "'RBAC' in current_auth_mode.stdout"

    - name: "Skip RBAC fix for AlwaysAllow mode"
      debug:
        msg: "Skipping RBAC fix - cluster is running in AlwaysAllow mode"
      when: 
        - rbac_check.stdout != "yes"
        - current_auth_mode.stdout is defined
        - "'AlwaysAllow' in current_auth_mode.stdout"

    - name: "Generate join command"
      shell: kubeadm token create --print-join-command
      register: join_command
      retries: 3
      delay: 10
      until: join_command.rc == 0

    - name: "Save join command"
      copy:
        content: "{{ join_command.stdout }}"
        dest: /tmp/kubeadm-join.sh
        mode: '0755'

# Join worker nodes
- name: "Join Worker Nodes"
  hosts: storage_nodes:compute_nodes
  become: true
  tasks:
    - name: "Check if node is joined"
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: "Copy join command from control plane"
      slurp:
        src: /tmp/kubeadm-join.sh
      register: join_command_content
      delegate_to: "{{ groups['monitoring_nodes'][0] }}"
      when: not kubelet_conf.stat.exists
      
    - name: "Write join command to worker"
      copy:
        content: "{{ join_command_content.content | b64decode }}"
        dest: /tmp/kubeadm-join.sh
        mode: '0755'
      when: not kubelet_conf.stat.exists

    - name: "Join cluster"
      shell: /tmp/kubeadm-join.sh
      when: not kubelet_conf.stat.exists

    - name: "Remove join command"
      file:
        path: /tmp/kubeadm-join.sh
        state: absent
