---
# VMStation Simplified Kubernetes Cluster Setup
# Replaces the complex 2901-line setup_cluster.yaml with essential functionality

- name: "Setup Kubernetes Cluster - All Nodes"
  hosts: all
  become: true
  vars:
    kubernetes_version: "1.29"
    pod_network_cidr: "10.244.0.0/16"
  tasks:
    - name: "Update package cache (Debian/Ubuntu)"
      apt:
        update_cache: yes
      when: ansible_os_family == 'Debian'

    - name: "Install required packages (Debian/Ubuntu)"
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
        state: present
      when: ansible_os_family == 'Debian'

    - name: "Add Kubernetes GPG key"
      apt_key:
        url: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/Release.key"
        state: present
      when: ansible_os_family == 'Debian'

    - name: "Add Kubernetes repository"
      apt_repository:
        repo: "deb https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/ /"
        state: present
        filename: kubernetes
      when: ansible_os_family == 'Debian'

    - name: "Install Kubernetes packages"
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
          - containerd
        state: present
      when: ansible_os_family == 'Debian'

    - name: "Install packages for RHEL/CentOS"
      block:
        - name: "Add Kubernetes repository (RHEL/CentOS)"
          yum_repository:
            name: kubernetes
            description: Kubernetes
            baseurl: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/rpm/"
            gpgcheck: yes
            repo_gpgcheck: yes
            gpgkey: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/rpm/repodata/repomd.xml.key"
            enabled: yes

        - name: "Install Kubernetes packages (RHEL/CentOS)"
          package:
            name:
              - kubelet
              - kubeadm
              - kubectl
              - containerd
            state: present
      when: ansible_os_family == 'RedHat'

    - name: "Hold Kubernetes packages (Debian/Ubuntu)"
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl
      when: ansible_os_family == 'Debian'

    - name: "Disable swap"
      shell: |
        swapoff -a
        sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

    - name: "Load kernel modules"
      modprobe:
        name: "{{ item }}"
      loop:
        - overlay
        - br_netfilter

    - name: "Create kernel modules config"
      copy:
        content: |
          overlay
          br_netfilter
        dest: /etc/modules-load.d/k8s.conf

    - name: "Set sysctl parameters"
      sysctl:
        name: "{{ item.key }}"
        value: "{{ item.value }}"
        state: present
        reload: yes
      loop:
        - { key: 'net.bridge.bridge-nf-call-iptables', value: '1' }
        - { key: 'net.bridge.bridge-nf-call-ip6tables', value: '1' }
        - { key: 'net.ipv4.ip_forward', value: '1' }

    - name: "Configure containerd"
      block:
        - name: "Create containerd config directory"
          file:
            path: /etc/containerd
            state: directory

        - name: "Generate containerd config"
          shell: containerd config default > /etc/containerd/config.toml
          args:
            creates: /etc/containerd/config.toml

        - name: "Configure containerd cgroup driver"
          replace:
            path: /etc/containerd/config.toml
            regexp: 'SystemdCgroup = false'
            replace: 'SystemdCgroup = true'

        - name: "Start and enable containerd"
          systemd:
            name: containerd
            state: restarted
            enabled: yes

    - name: "Enable kubelet"
      systemd:
        name: kubelet
        enabled: yes

# Control plane initialization
- name: "Initialize Kubernetes Control Plane"
  hosts: monitoring_nodes
  become: true
  vars:
    pod_network_cidr: "10.244.0.0/16"
    auth_mode: "{{ kubernetes_authorization_mode | default('Node,RBAC') }}"
    enable_fallback: "{{ kubernetes_authorization_fallback | default(false) }}"
  tasks:
    - name: "Check if cluster exists"
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeconfig

    - name: "Initialize cluster with secure authorization mode"
      command: >
        kubeadm init 
        --pod-network-cidr={{ pod_network_cidr }}
        --apiserver-advertise-address={{ ansible_default_ipv4.address }}
        --control-plane-endpoint={{ ansible_default_ipv4.address }}
        --authorization-mode={{ auth_mode }}
      when: not kubeconfig.stat.exists
      register: kubeadm_init
      ignore_errors: "{{ enable_fallback | bool }}"

    - name: "Initialize cluster with AlwaysAllow fallback (if secure mode failed)"
      command: >
        kubeadm init 
        --pod-network-cidr={{ pod_network_cidr }}
        --apiserver-advertise-address={{ ansible_default_ipv4.address }}
        --control-plane-endpoint={{ ansible_default_ipv4.address }}
        --authorization-mode=AlwaysAllow
      when: 
        - not kubeconfig.stat.exists
        - enable_fallback | bool
        - kubeadm_init.failed | default(false)
      register: kubeadm_init_fallback

    - name: "Display authorization mode warning if fallback was used"
      debug:
        msg: |
          WARNING: Cluster initialized with --authorization-mode=AlwaysAllow
          This is less secure and should only be used for troubleshooting.
          Consider investigating why {{ auth_mode }} mode failed.
      when: 
        - enable_fallback | bool
        - kubeadm_init_fallback is defined
        - kubeadm_init_fallback is succeeded

    - name: "Setup kubeconfig for root"
      block:
        - name: "Create .kube directory"
          file:
            path: /root/.kube
            state: directory
            mode: '0755'

        - name: "Copy admin.conf"
          copy:
            src: /etc/kubernetes/admin.conf
            dest: /root/.kube/config
            mode: '0644'
            remote_src: yes

    - name: "Open firewall ports for Kubernetes"
      firewalld:
        port: "{{ item }}"
        permanent: yes
        state: enabled
        immediate: yes
      loop:
        - "6443/tcp"    # API server
        - "10250/tcp"   # Kubelet
        - "10251/tcp"   # kube-scheduler
        - "10252/tcp"   # kube-controller-manager
        - "8472/udp"    # Flannel VXLAN
      failed_when: false
      when: ansible_os_family == 'RedHat'

    - name: "Install Flannel CNI"
      shell: |
        kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_install
      retries: 3
      delay: 10
      until: flannel_install.rc == 0

    - name: "Wait for Flannel DaemonSet to be created"
      shell: |
        kubectl get daemonset -n kube-flannel kube-flannel-ds
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_ds_check
      retries: 10
      delay: 5
      until: flannel_ds_check.rc == 0

    - name: "Check Flannel namespace and resources"
      shell: |
        echo "Checking Flannel deployment status:"
        kubectl get namespace kube-flannel
        kubectl get all -n kube-flannel
        echo "Node status:"
        kubectl get nodes
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_status
      failed_when: false

    - name: "Display Flannel status"
      debug:
        msg: |
          Flannel CNI Status:
          {{ flannel_status.stdout }}
          
          Note: Flannel pods may show CrashLoopBackOff until worker nodes join.
          This is expected behavior in a single-node control plane setup.

    - name: "Wait for API server to be ready"
      wait_for:
        host: "{{ ansible_default_ipv4.address }}"
        port: 6443
        timeout: 300
        delay: 10
      
    - name: "Verify API server accessibility using kubectl"
      shell: kubectl get nodes --request-timeout=10s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: api_health
      retries: 10
      delay: 15
      until: api_health.rc == 0

    - name: "Validate kubernetes-admin RBAC permissions"
      shell: >
        kubectl auth can-i create secrets --namespace=kube-system
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: rbac_check
      failed_when: false

    - name: "Check current authorization mode"
      shell: >
        kubectl get pods -n kube-system kube-apiserver-* -o jsonpath='{.items[0].spec.containers[0].command}' | 
        grep -o '\--authorization-mode=[^[:space:]]*' | 
        cut -d= -f2
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: current_auth_mode
      failed_when: false

    - name: "Display current authorization mode"
      debug:
        msg: "Current Kubernetes authorization mode: {{ current_auth_mode.stdout | default('Unable to determine') }}"

    - name: "Fix kubernetes-admin RBAC if needed"
      shell: |
        kubectl create clusterrolebinding kubernetes-admin \
          --clusterrole=cluster-admin \
          --user=kubernetes-admin \
          --dry-run=client -o yaml | kubectl apply -f -
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: 
        - rbac_check.stdout != "yes"
        - current_auth_mode.stdout is defined
        - "'RBAC' in current_auth_mode.stdout"

    - name: "Fix API server authorization mode if using AlwaysAllow"
      block:
        - name: "Backup API server manifest"
          copy:
            src: /etc/kubernetes/manifests/kube-apiserver.yaml
            dest: /etc/kubernetes/manifests/kube-apiserver.yaml.backup
            remote_src: yes

        - name: "Update authorization mode from AlwaysAllow to Node,RBAC"
          replace:
            path: /etc/kubernetes/manifests/kube-apiserver.yaml
            regexp: '--authorization-mode=AlwaysAllow'
            replace: '--authorization-mode=Node,RBAC'

        - name: "Wait for API server to restart after authorization fix"
          wait_for:
            host: "{{ ansible_default_ipv4.address }}"
            port: 6443
            timeout: 300
            delay: 30

        - name: "Verify API server health after authorization fix"
          shell: kubectl get nodes --request-timeout=10s
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          register: api_health_after_fix
          retries: 10
          delay: 15
          until: api_health_after_fix.rc == 0

        - name: "Apply RBAC fix after authorization mode change"
          shell: |
            kubectl create clusterrolebinding kubernetes-admin \
              --clusterrole=cluster-admin \
              --user=kubernetes-admin \
              --dry-run=client -o yaml | kubectl apply -f -
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

      when: 
        - current_auth_mode.stdout is defined
        - "'AlwaysAllow' in current_auth_mode.stdout"

    - name: "Skip RBAC fix for AlwaysAllow mode (no longer needed after fix)"
      debug:
        msg: "Authorization mode fixed from AlwaysAllow to Node,RBAC"
      when: 
        - rbac_check.stdout != "yes"
        - current_auth_mode.stdout is defined
        - "'AlwaysAllow' in current_auth_mode.stdout"

    - name: "Wait for API server pod to be Ready"
      shell: |
        kubectl get pods -n kube-system -l component=kube-apiserver \
          -o jsonpath='{.items[0].status.conditions[?(@.type=="Ready")].status}' | grep -q "True"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: api_pod_ready
      retries: 20
      delay: 15
      until: api_pod_ready.rc == 0

    - name: "Check cluster-info configmap RBAC permissions"
      shell: >
        kubectl get clusterrole system:public-info-viewer >/dev/null 2>&1 && echo "yes" || echo "no"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: cluster_info_rbac_check
      failed_when: false

    - name: "Create RBAC rule for anonymous access to cluster-info configmap"
      shell: |
        kubectl create clusterrole system:public-info-viewer \
          --verb=get --resource=configmaps \
          --resource-name=cluster-info \
          --dry-run=client -o yaml | kubectl apply -f -
        
        kubectl create clusterrolebinding cluster-info \
          --clusterrole=system:public-info-viewer \
          --group=system:unauthenticated \
          --group=system:authenticated \
          --dry-run=client -o yaml | kubectl apply -f -
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: cluster_info_rbac_check.stdout != "yes"

    - name: "Verify cluster-info configmap accessibility"
      shell: >
        kubectl get clusterrole system:public-info-viewer >/dev/null 2>&1 && 
        kubectl get clusterrolebinding cluster-info >/dev/null 2>&1 && 
        echo "yes" || echo "no"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: cluster_info_verification
      retries: 3
      delay: 5
      until: cluster_info_verification.stdout == "yes"

    - name: "Generate join command"
      shell: kubeadm token create --print-join-command
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: join_command
      retries: 3
      delay: 10
      until: join_command.rc == 0

    - name: "Save join command"
      copy:
        content: "{{ join_command.stdout }}"
        dest: /tmp/kubeadm-join.sh
        mode: '0755'

# Join worker nodes
- name: "Join Worker Nodes"
  hosts: storage_nodes:compute_nodes
  become: true
  vars:
    control_plane_ip: "{{ groups['monitoring_nodes'][0] }}"
  tasks:
    - name: "Check if node is joined"
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: "Open firewall ports for worker nodes"
      firewalld:
        port: "{{ item }}"
        permanent: yes
        state: enabled
        immediate: yes
      loop:
        - "10250/tcp"   # Kubelet
        - "8472/udp"    # Flannel VXLAN
      failed_when: false
      when: 
        - ansible_os_family == 'RedHat'
        - not kubelet_conf.stat.exists

    - name: "Test connectivity to control plane API server"
      wait_for:
        host: "{{ control_plane_ip }}"
        port: 6443
        timeout: 60
      when: not kubelet_conf.stat.exists

    - name: "Copy join command from control plane"
      slurp:
        src: /tmp/kubeadm-join.sh
      register: join_command_content
      delegate_to: "{{ control_plane_ip }}"
      when: not kubelet_conf.stat.exists
      
    - name: "Write join command to worker"
      copy:
        content: "{{ join_command_content.content | b64decode }}"
        dest: /tmp/kubeadm-join.sh
        mode: '0755'
      when: not kubelet_conf.stat.exists

    - name: "Join cluster with retry logic"
      shell: /tmp/kubeadm-join.sh
      register: join_result
      retries: 3
      delay: 30
      until: join_result.rc == 0
      when: not kubelet_conf.stat.exists

    - name: "Display join result"
      debug:
        msg: |
          Join attempt result:
          Return code: {{ join_result.rc | default('N/A') }}
          {% if join_result.stderr is defined %}
          Error output: {{ join_result.stderr }}
          {% endif %}
      when: not kubelet_conf.stat.exists

    - name: "Remove join command"
      file:
        path: /tmp/kubeadm-join.sh
        state: absent
