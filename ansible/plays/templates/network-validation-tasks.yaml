# Network Validation and Remediation Tasks for VMStation
# These tasks replace the functionality of post-deployment fix scripts
---
- name: "Validate and fix CNI bridge configuration"
  block:
    - name: "Check for CNI bridge IP conflicts"
      shell: |
        #!/bin/bash
        echo "=== CNI Bridge Validation ==="
        
        # Check if cni0 exists and get its current IP
        if ip addr show cni0 >/dev/null 2>&1; then
          current_ip=$(ip addr show cni0 | grep "inet " | awk '{print $2}' | head -1)
          echo "Current cni0 IP: ${current_ip:-none}"
          
          # Check recent events for CNI bridge conflicts
          cni_errors=$(kubectl get events --all-namespaces --field-selector reason=FailedCreatePodSandBox --sort-by='.lastTimestamp' -o custom-columns=MESSAGE:.message | grep -i "failed to set bridge addr.*already has an IP address" | tail -1 || echo "")
          
          if [ -n "$cni_errors" ]; then
            echo "CNI_CONFLICT_DETECTED=true"
            echo "Error: $cni_errors"
            exit 1
          fi
          
          echo "CNI_BRIDGE_OK=true"
        else
          echo "CNI_BRIDGE_MISSING=true"
        fi
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: cni_bridge_check
      failed_when: false
      delegate_to: "{{ groups['monitoring_nodes'][0] }}"

    - name: "Apply CNI bridge remediation if conflicts detected"
      block:
        - name: "Stop containerd temporarily for CNI reset"
          systemd:
            name: containerd
            state: stopped

        - name: "Remove conflicting CNI bridge"
          shell: |
            if ip link show cni0 >/dev/null 2>&1; then
              echo "Removing conflicting cni0 bridge"
              ip link set cni0 down 2>/dev/null || true
              ip link delete cni0 2>/dev/null || true
            fi
            
            # Clear CNI network state
            rm -rf /var/lib/cni/networks/* 2>/dev/null || true
          failed_when: false

        - name: "Restart containerd after CNI cleanup"
          systemd:
            name: containerd
            state: started

        - name: "Wait for containerd to stabilize"
          pause:
            seconds: 15
      when: 
        - cni_bridge_check.stdout is defined
        - '"CNI_CONFLICT_DETECTED=true" in cni_bridge_check.stdout'

- name: "Validate and fix CoreDNS scheduling"
  block:
    - name: "Check CoreDNS pod placement"
      shell: |
        # Check if CoreDNS is running on worker nodes instead of control plane
        coredns_on_workers=$(kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide | grep -v "{{ ansible_hostname }}" | grep -v "NAME" || echo "")
        
        if [ -n "$coredns_on_workers" ]; then
          echo "COREDNS_ON_WORKERS=true"
          echo "CoreDNS pods found on worker nodes:"
          echo "$coredns_on_workers"
          exit 1
        else
          echo "COREDNS_PLACEMENT_OK=true"
        fi
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: coredns_placement_check
      failed_when: false

    - name: "Apply enhanced CoreDNS scheduling configuration"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: coredns
            namespace: kube-system
          spec:
            template:
              spec:
                affinity:
                  nodeAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      nodeSelectorTerms:
                      - matchExpressions:
                        - key: node-role.kubernetes.io/control-plane
                          operator: Exists
                tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
                - key: "CriticalAddonsOnly"
                  operator: "Exists"
      when: 
        - coredns_placement_check.stdout is defined
        - '"COREDNS_ON_WORKERS=true" in coredns_placement_check.stdout'

    - name: "Force CoreDNS pod rescheduling if needed"
      shell: |
        kubectl rollout restart deployment/coredns -n kube-system
        kubectl rollout status deployment/coredns -n kube-system --timeout=120s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: 
        - coredns_placement_check.stdout is defined
        - '"COREDNS_ON_WORKERS=true" in coredns_placement_check.stdout'

- name: "Validate and fix kube-proxy configuration"
  block:
    - name: "Check for crashlooping kube-proxy pods"
      shell: |
        crashloop_pods=$(kubectl get pods -n kube-system -l k8s-app=kube-proxy | grep "CrashLoopBackOff" || echo "")
        
        if [ -n "$crashloop_pods" ]; then
          echo "KUBE_PROXY_CRASHLOOP=true"
          echo "Crashlooping kube-proxy pods:"
          echo "$crashloop_pods"
          
          # Check for iptables/nftables compatibility issues in logs
          pod_name=$(echo "$crashloop_pods" | head -1 | awk '{print $1}')
          nftables_errors=$(kubectl logs -n kube-system "$pod_name" --previous 2>/dev/null | grep -i "nf_tables.*incompatible" || echo "")
          
          if [ -n "$nftables_errors" ]; then
            echo "NFTABLES_COMPATIBILITY_ISSUE=true"
            echo "Error: $nftables_errors"
          fi
          
          exit 1
        else
          echo "KUBE_PROXY_OK=true"
        fi
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: kube_proxy_check
      failed_when: false

    - name: "Apply enhanced kube-proxy configuration"
      kubernetes.core.k8s:
        state: present
        src: "/home/runner/work/VMStation/VMStation/manifests/network/kube-proxy-configmap.yaml"
      when: 
        - kube_proxy_check.stdout is defined
        - '"KUBE_PROXY_CRASHLOOP=true" in kube_proxy_check.stdout'

    - name: "Restart kube-proxy DaemonSet"
      shell: |
        kubectl rollout restart daemonset/kube-proxy -n kube-system
        kubectl rollout status daemonset/kube-proxy -n kube-system --timeout=180s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: 
        - kube_proxy_check.stdout is defined
        - '"KUBE_PROXY_CRASHLOOP=true" in kube_proxy_check.stdout'

- name: "Validate Flannel network health"
  block:
    - name: "Check Flannel DaemonSet status"
      shell: |
        flannel_desired=$(kubectl get daemonset -n kube-flannel kube-flannel-ds -o jsonpath='{.status.desiredNumberScheduled}')
        flannel_ready=$(kubectl get daemonset -n kube-flannel kube-flannel-ds -o jsonpath='{.status.numberReady}')
        
        echo "Flannel pods: $flannel_ready/$flannel_desired ready"
        
        if [ "$flannel_ready" != "$flannel_desired" ] || [ "$flannel_ready" = "0" ]; then
          echo "FLANNEL_NOT_READY=true"
          
          # Check for completed Flannel pods (should always be running)
          completed_pods=$(kubectl get pods -n kube-flannel | grep "Completed" || echo "")
          if [ -n "$completed_pods" ]; then
            echo "FLANNEL_COMPLETED_PODS=true"
            echo "$completed_pods"
          fi
          
          exit 1
        else
          echo "FLANNEL_HEALTHY=true"
        fi
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_health_check
      failed_when: false

    - name: "Restart problematic Flannel pods"
      shell: |
        # Delete completed or crashlooping Flannel pods
        kubectl delete pods -n kube-flannel -l app=flannel --field-selector=status.phase=Succeeded --ignore-not-found=true
        
        # Delete crashlooping pods
        kubectl get pods -n kube-flannel -l app=flannel | grep -E "(CrashLoopBackOff|Error)" | awk '{print $1}' | xargs -r kubectl delete pod -n kube-flannel --force --grace-period=0
        
        # Wait for new pods to start
        sleep 30
        
        # Check if DaemonSet is now healthy
        kubectl rollout status daemonset/kube-flannel-ds -n kube-flannel --timeout=120s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: 
        - flannel_health_check.stdout is defined
        - '"FLANNEL_NOT_READY=true" in flannel_health_check.stdout'

- name: "Comprehensive network validation"
  block:
    - name: "Final network health check"
      shell: |
        echo "=== Final Network Validation ==="
        
        # Check all critical network components
        echo "CoreDNS status:"
        kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide
        
        echo ""
        echo "Flannel status:"
        kubectl get pods -n kube-flannel -o wide
        
        echo ""
        echo "kube-proxy status:"
        kubectl get pods -n kube-system -l k8s-app=kube-proxy -o wide
        
        echo ""
        echo "Node status:"
        kubectl get nodes -o wide
        
        # Count healthy components
        coredns_ready=$(kubectl get pods -n kube-system -l k8s-app=kube-dns --field-selector=status.phase=Running | grep -c "Running" || echo "0")
        flannel_ready=$(kubectl get pods -n kube-flannel -l app=flannel --field-selector=status.phase=Running | grep -c "Running" || echo "0")
        proxy_ready=$(kubectl get pods -n kube-system -l k8s-app=kube-proxy --field-selector=status.phase=Running | grep -c "Running" || echo "0")
        
        echo ""
        echo "Network component health:"
        echo "- CoreDNS running pods: $coredns_ready"
        echo "- Flannel running pods: $flannel_ready"  
        echo "- kube-proxy running pods: $proxy_ready"
        
        if [ "$coredns_ready" -gt 0 ] && [ "$flannel_ready" -gt 0 ] && [ "$proxy_ready" -gt 0 ]; then
          echo "NETWORK_VALIDATION_PASSED=true"
          echo "✅ All network components are healthy"
        else
          echo "NETWORK_VALIDATION_FAILED=true"
          echo "❌ Some network components are not healthy"
          exit 1
        fi
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: final_network_validation

    - name: "Display network validation results"
      debug:
        msg: |
          Network Validation Results:
          {{ final_network_validation.stdout_lines | join('\n          ') }}
          
          {% if final_network_validation.stdout is defined and 'NETWORK_VALIDATION_PASSED=true' in final_network_validation.stdout %}
          🎉 Network validation completed successfully!
          The cluster networking is ready for application deployment.
          {% else %}
          ⚠️  Network validation detected issues.
          Some components may need additional time to stabilize.
          {% endif %}