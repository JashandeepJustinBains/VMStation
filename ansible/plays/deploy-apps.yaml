---
# VMStation Application Deployment
# Deploys comprehensive monitoring stack and applications using manifest files

- name: "Deploy VMStation Monitoring Stack"
  hosts: monitoring_nodes
  become: false
  vars:
    monitoring_namespace: monitoring
    manifests_dir: "{{ playbook_dir }}/../../manifests"
  tasks:
    - name: "Create monitoring namespace"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        name: "{{ monitoring_namespace }}"
        api_version: v1
        kind: Namespace
        state: present
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Wait for Flannel CNI to be ready across all nodes"
      kubernetes.core.k8s_info:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        api_version: v1
        kind: Pod
        namespace: kube-flannel
        label_selectors:
          - app=flannel
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_pods
      until: >
        flannel_pods is defined and 
        flannel_pods.resources is defined and
        flannel_pods.resources | length > 0 and
        (flannel_pods.resources | selectattr('status.phase', 'equalto', 'Running') | list | length) == (flannel_pods.resources | length)
      retries: 24
      delay: 5
      failed_when: false

    # Deploy Node Exporter (system metrics collection)
    - name: "Deploy Node Exporter DaemonSet"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        src: "{{ manifests_dir }}/monitoring/node-exporter.yaml"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    # Deploy Kube State Metrics (Kubernetes object metrics)
    - name: "Deploy Kube State Metrics"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        src: "{{ manifests_dir }}/monitoring/kube-state-metrics.yaml"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    # Deploy Prometheus (metrics collection and alerting)
    - name: "Deploy Prometheus"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        src: "{{ manifests_dir }}/monitoring/prometheus.yaml"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    # Deploy Loki (log aggregation)
    - name: "Deploy Loki"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        src: "{{ manifests_dir }}/monitoring/loki.yaml"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    # Deploy Grafana (visualization and dashboards)
    - name: "Deploy Grafana"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        src: "{{ manifests_dir }}/monitoring/grafana.yaml"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    # Deploy Grafana Dashboards as ConfigMaps
    - name: "Create Grafana Dashboard ConfigMaps"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: "grafana-dashboard-{{ item.name }}"
            namespace: monitoring
          data:
            "{{ item.name }}.json": "{{ lookup('file', playbook_dir + '/../files/grafana_dashboards/' + item.file) }}"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      loop:
        - { name: "node", file: "node-dashboard.json" }
        - { name: "prometheus", file: "prometheus-dashboard.json" }
        - { name: "loki", file: "loki-dashboard.json" }
        - { name: "kubernetes-cluster", file: "kubernetes-cluster-dashboard.json" }

    # Deploy Kubernetes Dashboard
    - name: "Deploy Kubernetes Dashboard"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        src: https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      
    - name: "Patch Kubernetes Dashboard to run on monitoring node"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: kubernetes-dashboard
            namespace: kubernetes-dashboard
          spec:
            template:
              spec:
                nodeSelector:
                  node-role.kubernetes.io/control-plane: ""
                tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
                  
    - name: "Patch Dashboard Metrics Scraper to run on monitoring node"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: dashboard-metrics-scraper
            namespace: kubernetes-dashboard
          spec:
            template:
              spec:
                nodeSelector:
                  node-role.kubernetes.io/control-plane: ""
                tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Create Dashboard Admin User"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: admin-user
            namespace: kubernetes-dashboard
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Create Dashboard Admin ClusterRoleBinding"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: admin-user
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: cluster-admin
          subjects:
          - kind: ServiceAccount
            name: admin-user
            namespace: kubernetes-dashboard
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    # Wait for monitoring stack to be ready
    - name: "Wait for monitoring pods to be ready"
      block:
        - name: "Check CoreDNS status (informational only - kubeadm manages CoreDNS)"
          kubernetes.core.k8s_info:
            kubeconfig: /etc/kubernetes/admin.conf
            validate_certs: false
            api_version: v1
            kind: Pod
            namespace: kube-system
            label_selectors:
              - k8s-app=kube-dns
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          register: coredns_check
          failed_when: false

        - name: "Display CoreDNS status"
          debug:
            msg: >-
              {% if coredns_check is defined and coredns_check.resources is defined %}
              CoreDNS pods: {{ coredns_check.resources | length }} (managed by kubeadm, not modified by this playbook)
              {% else %}
              CoreDNS pod information unavailable (API call failed or returned no pods)
              {% endif %}
        
        - name: "Wait for Node Exporter DaemonSet"
          kubernetes.core.k8s_info:
            kubeconfig: /etc/kubernetes/admin.conf
            validate_certs: false
            api_version: v1
            kind: Pod
            namespace: "{{ monitoring_namespace }}"
            label_selectors:
              - app.kubernetes.io/name=node-exporter
            wait: true
            wait_condition:
              type: Ready
              status: "True"
            wait_timeout: 180
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          register: node_exporter_ready
          ignore_errors: yes

        - name: "Wait for Kube State Metrics"
          kubernetes.core.k8s_info:
            kubeconfig: /etc/kubernetes/admin.conf
            validate_certs: false
            api_version: v1
            kind: Pod
            namespace: "{{ monitoring_namespace }}"
            label_selectors:
              - app.kubernetes.io/name=kube-state-metrics
            wait: true
            wait_condition:
              type: Ready
              status: "True"
            wait_timeout: 180
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          register: kube_state_metrics_ready
          ignore_errors: yes

        - name: "Wait for core monitoring applications"
          kubernetes.core.k8s_info:
            kubeconfig: /etc/kubernetes/admin.conf
            validate_certs: false
            api_version: v1
            kind: Pod
            namespace: "{{ monitoring_namespace }}"
            label_selectors:
              - app.kubernetes.io/name={{ item }}
            wait: true
            wait_condition:
              type: Ready
              status: "True"
            wait_timeout: 180
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          loop:
            - prometheus
            - grafana
            - loki
          register: app_readiness_results
          ignore_errors: yes

        - name: "Troubleshoot stuck pods in monitoring namespace"
          block:
            - name: "Get all pods in monitoring namespace"
              ansible.builtin.shell: |
                kubectl get pods -n {{ monitoring_namespace }} -o jsonpath='{range .items[*]}{.metadata.name}\n{end}'
              args:
                executable: /bin/bash
              environment:
                KUBECONFIG: /etc/kubernetes/admin.conf
              register: monitoring_pods

            - name: "Describe all pods in monitoring namespace"
              ansible.builtin.shell: |
                for pod in $(echo "{{ monitoring_pods.stdout }}"); do
                  echo "\n===== DESCRIBE $pod ====="
                  kubectl describe pod -n {{ monitoring_namespace }} $pod
                done
              args:
                executable: /bin/bash
              environment:
                KUBECONFIG: /etc/kubernetes/admin.conf
              register: monitoring_pod_describes
              failed_when: false

            - name: "Show pod describe output"
              ansible.builtin.debug:
                var: monitoring_pod_describes.stdout_lines

            - name: "Show recent events in monitoring namespace"
              ansible.builtin.shell: |
                kubectl get events -n {{ monitoring_namespace }} --sort-by=.metadata.creationTimestamp | tail -30
              args:
                executable: /bin/bash
              environment:
                KUBECONFIG: /etc/kubernetes/admin.conf
              register: monitoring_events
              failed_when: false

            - name: "Show monitoring events output"
              ansible.builtin.debug:
                var: monitoring_events.stdout_lines
          when: app_readiness_results is defined and (app_readiness_results.results | selectattr('failed', 'defined') | selectattr('failed') | list | length > 0)

    - name: "Check pod status for troubleshooting"
      kubernetes.core.k8s_info:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        api_version: v1
        kind: Pod
        namespace: "{{ monitoring_namespace }}"
        label_selectors:
          - app.kubernetes.io/name={{ item.item }}
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      loop: "{{ app_readiness_results.results }}"
      when: item.failed | default(false)
      register: failed_pod_status
      ignore_errors: yes

    - name: "Display troubleshooting information for failed pods"
      debug:
        msg: |
          === Troubleshooting {{ item.item.item }} ===
          {% for pod in item.resources | default([]) %}
          Pod: {{ pod.metadata.name }}
          Status: {{ pod.status.phase | default('Unknown') }}
          {% if pod.status.conditions is defined %}
          Conditions:
          {% for condition in pod.status.conditions %}
          - {{ condition.type }}: {{ condition.status }} ({{ condition.reason | default('') }})
          {% endfor %}
          {% endif %}
          {% if pod.status.containerStatuses is defined %}
          Container Status:
          {% for container in pod.status.containerStatuses %}
          - {{ container.name }}: {{ container.state.keys() | first }}
          {% if container.state.waiting is defined %}
            Reason: {{ container.state.waiting.reason | default('Unknown') }}
            Message: {{ container.state.waiting.message | default('') }}
          {% endif %}
          {% endfor %}
          {% endif %}
          {% endfor %}
          
          Check with: kubectl describe pod -n {{ monitoring_namespace }} <pod-name>
          Check logs: kubectl logs -n {{ monitoring_namespace }} <pod-name>
      loop: "{{ failed_pod_status.results | default([]) }}"
      when: failed_pod_status is defined and item.resources is defined

    - name: "Display deployment status"
      debug:
        msg: |
          === VMStation Monitoring Stack Deployed ===
          
          Core Monitoring Services:
          - Prometheus: http://192.168.4.63:30090
          - Grafana: http://192.168.4.63:30300 (admin/admin)
          - Loki: http://192.168.4.63:31100
          
          Metrics Collectors:
          - Node Exporter: DaemonSet running on all nodes
          - Kube State Metrics: Kubernetes object metrics
          
          Dashboards Available in Grafana:
          - VMStation Kubernetes Cluster Overview
          - VMStation Node Metrics
          - VMStation Prometheus Metrics
          - VMStation Loki Logs
          
          To access Kubernetes Dashboard:
          1. kubectl -n kubernetes-dashboard create token admin-user
          2. kubectl proxy
          3. Access at: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
