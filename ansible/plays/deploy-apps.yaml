---
# VMStation Simplified Application Deployment
# Deploys essential monitoring and applications

- name: "Deploy VMStation Applications"
  hosts: monitoring_nodes
  become: false
  vars:
    monitoring_namespace: monitoring
    grafana_nodeport: 30300
    prometheus_nodeport: 30090
    loki_nodeport: 31100
    alertmanager_nodeport: 30903
  tasks:
    - name: "Create monitoring namespace"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        name: "{{ monitoring_namespace }}"
        api_version: v1
        kind: Namespace
        state: present
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Wait for Flannel CNI to be ready across all nodes"
      kubernetes.core.k8s_info:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        api_version: v1
        kind: Pod
        namespace: kube-flannel
        label_selectors:
          - app=flannel
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_pods
      until: >
        flannel_pods is defined and 
        flannel_pods.resources is defined and
        flannel_pods.resources | length > 0 and
        (flannel_pods.resources | selectattr('status.phase', 'equalto', 'Running') | list | length) == (flannel_pods.resources | length)
      retries: 24
      delay: 5
      failed_when: false

    - name: "Deploy Prometheus"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: prometheus
            namespace: "{{ monitoring_namespace }}"
            labels:
              app.kubernetes.io/name: prometheus
          spec:
            replicas: 1
            selector:
              matchLabels:
                app.kubernetes.io/name: prometheus
            template:
              metadata:
                labels:
                  app.kubernetes.io/name: prometheus
              spec:
                tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
                containers:
                - name: prometheus
                  image: prom/prometheus:latest
                  imagePullPolicy: IfNotPresent
                  ports:
                  - containerPort: 9090
                  args:
                    - --config.file=/etc/prometheus/prometheus.yml
                    - --storage.tsdb.path=/prometheus/
                    - --web.console.libraries=/etc/prometheus/console_libraries
                    - --web.console.templates=/etc/prometheus/consoles
                    - --web.enable-lifecycle
                  resources:
                    requests:
                      memory: "256Mi"
                      cpu: "100m"
                    limits:
                      memory: "1Gi"
                      cpu: "500m"
                  readinessProbe:
                    httpGet:
                      path: /-/ready
                      port: 9090
                    initialDelaySeconds: 30
                    periodSeconds: 10
                    timeoutSeconds: 5
                  livenessProbe:
                    httpGet:
                      path: /-/healthy
                      port: 9090
                    initialDelaySeconds: 60
                    periodSeconds: 30
                  volumeMounts:
                  - name: config-volume
                    mountPath: /etc/prometheus/
                  - name: storage-volume
                    mountPath: /prometheus/
                volumes:
                - name: config-volume
                  configMap:
                    name: prometheus-config
                - name: storage-volume
                  emptyDir: {}
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Create Prometheus ConfigMap"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: prometheus-config
            namespace: "{{ monitoring_namespace }}"
          data:
            prometheus.yml: |
              global:
                scrape_interval: 15s
                evaluation_interval: 15s
              
              scrape_configs:
              - job_name: 'kubernetes-nodes'
                kubernetes_sd_configs:
                - role: node
                relabel_configs:
                - source_labels: [__address__]
                  regex: '(.*):10250'
                  target_label: __address__
                  replacement: '${1}:9100'
              
              - job_name: 'kubernetes-pods'
                kubernetes_sd_configs:
                - role: pod
                relabel_configs:
                - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                  action: keep
                  regex: true
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Deploy Prometheus Service"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            name: prometheus
            namespace: "{{ monitoring_namespace }}"
          spec:
            type: NodePort
            selector:
              app.kubernetes.io/name: prometheus
            ports:
            - port: 9090
              targetPort: 9090
              nodePort: "{{ prometheus_nodeport }}"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Create Grafana Datasource Provisioning ConfigMap"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: grafana-datasources
            namespace: "{{ monitoring_namespace }}"
            labels:
              app.kubernetes.io/name: grafana
          data:
            prometheus-loki-datasources.yaml: |
              apiVersion: 1
              datasources:
                - name: Prometheus
                  type: prometheus
                  access: proxy
                  url: http://prometheus.{{ monitoring_namespace }}.svc.cluster.local:9090
                  isDefault: true
                  editable: true
                - name: Loki
                  type: loki
                  access: proxy
                  url: http://loki.{{ monitoring_namespace }}.svc.cluster.local:3100
                  editable: true
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Deploy Grafana"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: grafana
            namespace: "{{ monitoring_namespace }}"
            labels:
              app.kubernetes.io/name: grafana
          spec:
            replicas: 1
            selector:
              matchLabels:
                app.kubernetes.io/name: grafana
            template:
              metadata:
                labels:
                  app.kubernetes.io/name: grafana
              spec:
                tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
                containers:
                - name: grafana
                  image: grafana/grafana:latest
                  imagePullPolicy: IfNotPresent
                  ports:
                  - containerPort: 3000
                  env:
                  - name: GF_SECURITY_ADMIN_PASSWORD
                    value: "admin"
                  - name: GF_INSTALL_PLUGINS
                    value: ""
                  resources:
                    requests:
                      memory: "128Mi"
                      cpu: "100m"
                    limits:
                      memory: "512Mi"
                      cpu: "500m"
                  readinessProbe:
                    httpGet:
                      path: /api/health
                      port: 3000
                    initialDelaySeconds: 30
                    periodSeconds: 10
                    timeoutSeconds: 5
                  livenessProbe:
                    httpGet:
                      path: /api/health
                      port: 3000
                    initialDelaySeconds: 60
                    periodSeconds: 30
                  volumeMounts:
                  - name: storage
                    mountPath: /var/lib/grafana
                  - name: grafana-datasources
                    mountPath: /etc/grafana/provisioning/datasources/
                    readOnly: true
                volumes:
                - name: storage
                  emptyDir: {}
                - name: grafana-datasources
                  configMap:
                    name: grafana-datasources
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Deploy Grafana Service"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            name: grafana
            namespace: "{{ monitoring_namespace }}"
          spec:
            type: NodePort
            selector:
              app.kubernetes.io/name: grafana
            ports:
            - port: 3000
              targetPort: 3000
              nodePort: "{{ grafana_nodeport }}"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Deploy Loki"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: loki
            namespace: "{{ monitoring_namespace }}"
            labels:
              app.kubernetes.io/name: loki
          spec:
            replicas: 1
            selector:
              matchLabels:
                app.kubernetes.io/name: loki
            template:
              metadata:
                labels:
                  app.kubernetes.io/name: loki
              spec:
                tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
                containers:
                - name: loki
                  image: grafana/loki:latest
                  imagePullPolicy: IfNotPresent
                  ports:
                  - containerPort: 3100
                  args:
                    - -config.file=/etc/loki/local-config.yaml
                  resources:
                    requests:
                      memory: "128Mi"
                      cpu: "100m"
                    limits:
                      memory: "512Mi"
                      cpu: "500m"
                  readinessProbe:
                    httpGet:
                      path: /ready
                      port: 3100
                    initialDelaySeconds: 30
                    periodSeconds: 10
                    timeoutSeconds: 5
                  livenessProbe:
                    httpGet:
                      path: /ready
                      port: 3100
                    initialDelaySeconds: 60
                    periodSeconds: 30
                  volumeMounts:
                  - name: storage
                    mountPath: /tmp/loki
                volumes:
                - name: storage
                  emptyDir: {}
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Deploy Loki Service"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            name: loki
            namespace: "{{ monitoring_namespace }}"
          spec:
            type: NodePort
            selector:
              app.kubernetes.io/name: loki
            ports:
            - port: 3100
              targetPort: 3100
              nodePort: "{{ loki_nodeport }}"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Deploy Kubernetes Dashboard"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        src: https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      
    - name: "Patch Kubernetes Dashboard to run on monitoring node"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: kubernetes-dashboard
            namespace: kubernetes-dashboard
          spec:
            template:
              spec:
                nodeSelector:
                  node-role.kubernetes.io/control-plane: ""
                tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
                  
    - name: "Patch Dashboard Metrics Scraper to run on monitoring node"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: dashboard-metrics-scraper
            namespace: kubernetes-dashboard
          spec:
            template:
              spec:
                nodeSelector:
                  node-role.kubernetes.io/control-plane: ""
                tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Create Dashboard Admin User"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: admin-user
            namespace: kubernetes-dashboard
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Create Dashboard Admin ClusterRoleBinding"
      kubernetes.core.k8s:
        kubeconfig: /etc/kubernetes/admin.conf
        validate_certs: false
        state: present
        definition:
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: admin-user
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: cluster-admin
          subjects:
          - kind: ServiceAccount
            name: admin-user
            namespace: kubernetes-dashboard
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Wait for applications to be ready (with improved error handling)"
      block:
        - name: "Check CoreDNS status (informational only - kubeadm manages CoreDNS)"
          kubernetes.core.k8s_info:
            kubeconfig: /etc/kubernetes/admin.conf
            validate_certs: false
            api_version: v1
            kind: Pod
            namespace: kube-system
            label_selectors:
              - k8s-app=kube-dns
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          register: coredns_check
          failed_when: false

        - name: "Display CoreDNS status"
          debug:
            msg: >-
              {% if coredns_check is defined and coredns_check.resources is defined %}
              CoreDNS pods: {{ coredns_check.resources | length }} (managed by kubeadm, not modified by this playbook)
              {% else %}
              CoreDNS pod information unavailable (API call failed or returned no pods)
              {% endif %}
        
        - name: "Wait for applications to be ready with reduced timeout"
          kubernetes.core.k8s_info:
            kubeconfig: /etc/kubernetes/admin.conf
            validate_certs: false
            api_version: v1
            kind: Pod
            namespace: "{{ monitoring_namespace }}"
            label_selectors:
              - app.kubernetes.io/name={{ item }}
            wait: true
            wait_condition:
              type: Ready
              status: "True"
            wait_timeout: 180  # Lowered from 300 to 180 seconds (3 minutes)
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf
          loop:
            - prometheus
            - grafana
            - loki
          register: app_readiness_results
          ignore_errors: yes

        - name: "Troubleshoot stuck pods in monitoring namespace (describe and events)"
          block:
            - name: "Get all pods in monitoring namespace"
              ansible.builtin.shell: |
                kubectl get pods -n {{ monitoring_namespace }} -o jsonpath='{range .items[*]}{.metadata.name}\n{end}'
              args:
                executable: /bin/bash
              environment:
                KUBECONFIG: /etc/kubernetes/admin.conf
              register: monitoring_pods

            - name: "Describe all pods in monitoring namespace"
              ansible.builtin.shell: |
                for pod in $(echo "{{ monitoring_pods.stdout }}"); do
                  echo "\n===== DESCRIBE $pod ====="
                  kubectl describe pod -n {{ monitoring_namespace }} $pod
                done
              args:
                executable: /bin/bash
              environment:
                KUBECONFIG: /etc/kubernetes/admin.conf
              register: monitoring_pod_describes
              failed_when: false

            - name: "Show pod describe output"
              ansible.builtin.debug:
                var: monitoring_pod_describes.stdout_lines

            - name: "Show recent events in monitoring namespace"
              ansible.builtin.shell: |
                kubectl get events -n {{ monitoring_namespace }} --sort-by=.metadata.creationTimestamp | tail -30
              args:
                executable: /bin/bash
              environment:
                KUBECONFIG: /etc/kubernetes/admin.conf
              register: monitoring_events
              failed_when: false

            - name: "Show monitoring events output"
              ansible.builtin.debug:
                var: monitoring_events.stdout_lines
      when: app_readiness_results is defined and (app_readiness_results.results | selectattr('failed', 'defined') | selectattr('failed') | list | length > 0)
    - name: "Check pod status for troubleshooting"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: "{{ monitoring_namespace }}"
        label_selectors:
          - app.kubernetes.io/name={{ item.item }}
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      loop: "{{ app_readiness_results.results }}"
      when: item.failed | default(false)
      register: failed_pod_status
      ignore_errors: yes

    - name: "Display troubleshooting information for failed pods"
      debug:
        msg: |
          === Troubleshooting {{ item.item.item }} ===
          {% for pod in item.resources | default([]) %}
          Pod: {{ pod.metadata.name }}
          Status: {{ pod.status.phase | default('Unknown') }}
          {% if pod.status.conditions is defined %}
          Conditions:
          {% for condition in pod.status.conditions %}
          - {{ condition.type }}: {{ condition.status }} ({{ condition.reason | default('') }})
          {% endfor %}
          {% endif %}
          {% if pod.status.containerStatuses is defined %}
          Container Status:
          {% for container in pod.status.containerStatuses %}
          - {{ container.name }}: {{ container.state.keys() | first }}
          {% if container.state.waiting is defined %}
            Reason: {{ container.state.waiting.reason | default('Unknown') }}
            Message: {{ container.state.waiting.message | default('') }}
          {% endif %}
          {% endfor %}
          {% endif %}
          {% endfor %}
          
          Check with: kubectl describe pod -n {{ monitoring_namespace }} <pod-name>
          Check logs: kubectl logs -n {{ monitoring_namespace }} <pod-name>
      loop: "{{ failed_pod_status.results | default([]) }}"
      when: failed_pod_status is defined and item.resources is defined

    - name: "Display deployment status"
      debug:
        msg: |
          === VMStation Applications Deployed ===
          
          Services available at:
          - Prometheus: http://192.168.4.63:{{ prometheus_nodeport }}
          - Grafana: http://192.168.4.63:{{ grafana_nodeport }} (admin/admin)
          - Loki: http://192.168.4.63:{{ loki_nodeport }}
          
          To access Kubernetes Dashboard:
          1. kubectl -n kubernetes-dashboard create token admin-user
          2. kubectl proxy
          3. Navigate to: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/