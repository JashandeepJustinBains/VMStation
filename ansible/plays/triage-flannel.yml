---
- name: Triage Flannel and kube-proxy on problem node (homelab)
  hosts: homelab
  become: true
  gather_facts: yes
  vars:
    # set to true to attempt service restart and pod delete as remediation
    do_restart_services: false
    do_delete_pods: false

  tasks:
    - name: Print header
      ansible.builtin.debug:
        msg: "Collecting diagnostics for {{ inventory_hostname }} (do_restart_services={{ do_restart_services }}, do_delete_pods={{ do_delete_pods }})"

    - name: Record free memory and disk
      ansible.builtin.shell: |
        echo '--- free -h ---'
        free -h
        echo '--- df -h /var ---'
        df -h /var || true
      register: memdisk

    - name: Show mem/disk
      ansible.builtin.debug:
        var: memdisk.stdout_lines

    - name: Check for OOM or kernel kills in dmesg (last 500 lines)
      ansible.builtin.shell: dmesg --ctime | tail -n 500 | egrep -i "killed process|oom|out of memory" || true
      register: dmesg_oom

    - name: Show dmesg OOM snippets
      ansible.builtin.debug:
        var: dmesg_oom.stdout_lines

    - name: Get recent kernel messages (last 200)
      ansible.builtin.shell: journalctl -k -n 200 --no-pager || true
      register: journal_k

    - name: Show kernel journal snippet
      ansible.builtin.debug:
        var: journal_k.stdout_lines

    - name: Collect container runtime status
      ansible.builtin.shell: |
        systemctl status containerd --no-pager || true
        echo '--- journal (containerd) ---'
        journalctl -u containerd -n 200 --no-pager || true
      register: contd

    - name: Show containerd status
      ansible.builtin.debug:
        var: contd.stdout_lines

    - name: Collect kubelet status and recent logs
      ansible.builtin.shell: |
        systemctl status kubelet --no-pager || true
        echo '--- journal (kubelet) ---'
        journalctl -u kubelet -n 200 --no-pager || true
      register: klet

    - name: Show kubelet status
      ansible.builtin.debug:
        var: klet.stdout_lines

    - name: List CNI conf dir
      ansible.builtin.shell: ls -l /etc/cni/net.d || true
      register: cni_conf

    - name: Show CNI conf
      ansible.builtin.debug:
        var: cni_conf.stdout_lines

    - name: Show contents of 10-flannel* if present
      ansible.builtin.shell: cat /etc/cni/net.d/10-flannel.conflist || cat /etc/cni/net.d/10-flannel.conf || true
      register: flannel_conf

    - name: Show flannel cni file
      ansible.builtin.debug:
        var: flannel_conf.stdout_lines

    - name: List /opt/cni/bin and check flannel binary
      ansible.builtin.shell: ls -l /opt/cni/bin || true
      register: cni_bins

    - name: Show cni bins
      ansible.builtin.debug:
        var: cni_bins.stdout_lines

    - name: Show IP links and flannel interface details
      ansible.builtin.shell: ip -d link show flannel.1 2>/dev/null || ip -d link show | sed -n '1,200p'
      register: iplinks

    - name: Show ip links
      ansible.builtin.debug:
        var: iplinks.stdout_lines

    - name: Fetch kube-flannel pod name
      ansible.builtin.shell: kubectl -n kube-flannel get pods -o jsonpath="{.items[?(@.spec.nodeName=='{{ inventory_hostname }}')].metadata.name}" || true
      register: fl_pod
      changed_when: false

    - name: Show flannel pod name
      ansible.builtin.debug:
        var: fl_pod.stdout

    - name: Tail flannel pod logs (kube-flannel)
      ansible.builtin.shell: |
        set -o pipefail
        if [ -n "{{ fl_pod.stdout }}" ]; then
          kubectl -n kube-flannel logs {{ fl_pod.stdout }} -c kube-flannel --tail=500 || true
        else
          echo 'no flannel pod found on this node'
        fi
      register: fl_logs

    - name: Show flannel logs
      ansible.builtin.debug:
        var: fl_logs.stdout_lines

    - name: Tail kube-proxy pod logs (on node)
      ansible.builtin.shell: |
        set -o pipefail
        KP=$(kubectl -n kube-system get pods -o jsonpath="{.items[?(@.spec.nodeName=='{{ inventory_hostname }}')].metadata.name}" || true)
        if [ -n "$KP" ]; then
          kubectl -n kube-system logs $KP -c kube-proxy --tail=500 || true
        else
          echo 'no kube-proxy pod found on this node'
        fi
      register: kp_logs

    - name: Show kube-proxy logs
      ansible.builtin.debug:
        var: kp_logs.stdout_lines

    - name: Optionally restart containerd and kubelet (set do_restart_services=true to run)
      ansible.builtin.service:
        name: "{{ item }}"
        state: restarted
      loop:
        - containerd
        - kubelet
      when: do_restart_services

    - name: Optionally delete flannel pod to force fresh recreate (set do_delete_pods=true)
      ansible.builtin.shell: |
        if [ -n "{{ fl_pod.stdout }}" ]; then
          kubectl -n kube-flannel delete pod {{ fl_pod.stdout }} || true
        fi
      when: do_delete_pods

    - name: Optionally delete kube-proxy pod on node (set do_delete_pods=true)
      ansible.builtin.shell: |
        KP=$(kubectl -n kube-system get pods -o jsonpath="{.items[?(@.spec.nodeName=='{{ inventory_hostname }}')].metadata.name}" || true)
        if [ -n "$KP" ]; then
          kubectl -n kube-system delete pod $KP || true
        fi
      when: do_delete_pods

    - name: Final message
      ansible.builtin.debug:
        msg: "Triage play completed on {{ inventory_hostname }}. Review above outputs for OOM, containerd errors, and flannel/kube-proxy logs. If you enable do_restart_services/do_delete_pods the play performed remediation actions."
