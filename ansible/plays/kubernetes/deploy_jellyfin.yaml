---
# High-Availability Jellyfin Kubernetes Deployment
# Implements auto-scaling, hardware acceleration, and session affinity
- name: Deploy High-Availability Jellyfin on Kubernetes
  hosts: monitoring_nodes  # Deploy from control plane
  become: true
  vars:
    jellyfin_namespace: jellyfin
    jellyfin_hostname: "jellyfin.local"
    jellyfin_published_url: "http://192.168.4.61:30096"
    storage_node_hostname: "{{ groups['storage_nodes'][0] }}"
  # jellyfin_media_path may be set in group_vars/all.yml; if not present we use '/srv/media' via defaults in tasks
    
  pre_tasks:
    - name: Verify Kubernetes cluster is ready
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        kubeconfig: /root/.kube/config
      register: cluster_nodes
      
    - name: Initialize storage node candidate list
      set_fact:
        storage_node_candidates: "{{ [ storage_node_hostname ] }}"

    - name: Add inventory host's ansible_hostname to candidates
      set_fact:
        storage_node_candidates: "{{ (storage_node_candidates | default([])) + [ hostvars[groups['storage_nodes'][0]].ansible_hostname ] }}"
      when: groups['storage_nodes'] | length > 0 and hostvars[groups['storage_nodes'][0]] is defined and hostvars[groups['storage_nodes'][0]].ansible_hostname is defined

    - name: Add inventory host's default IPv4 to candidates
      set_fact:
        storage_node_candidates: "{{ (storage_node_candidates | default([])) + [ hostvars[groups['storage_nodes'][0]].ansible_default_ipv4.address ] }}"
      when: groups['storage_nodes'] | length > 0 and hostvars[groups['storage_nodes'][0]] is defined and hostvars[groups['storage_nodes'][0]].ansible_default_ipv4 is defined and hostvars[groups['storage_nodes'][0]].ansible_default_ipv4.address is defined

    - name: Add raw group inventory entry to candidates
      set_fact:
        storage_node_candidates: "{{ (storage_node_candidates | default([])) + [ groups['storage_nodes'][0] ] }}"
      when: groups['storage_nodes'] | length > 0

    - name: Validate media directory exists on storage node
      stat:
        path: "{{ jellyfin_media_path | default('/srv/media') }}"
      delegate_to: "{{ groups['storage_nodes'][0] }}"
      register: media_directory_stat
      when: not (jellyfin_skip_mount_verification | default(false))
      
    - name: Skip media directory validation (mount verification disabled)
      debug:
        msg: |
          Mount verification is disabled (jellyfin_skip_mount_verification=true).
          Skipping validation of media directory: {{ jellyfin_media_path | default('/srv/media') }}
          
          WARNING: Ensure the media directory exists on the storage node before deployment:
          - Path: {{ jellyfin_media_path | default('/srv/media') }}
          - Storage node: {{ groups['storage_nodes'][0] }}
          - Manual check: ssh {{ groups['storage_nodes'][0] }} 'ls -la {{ jellyfin_media_path | default('/srv/media') }}'
      when: jellyfin_skip_mount_verification | default(false)
      
    - name: Fail if media directory does not exist
      fail:
        msg: |
          Media directory '{{ jellyfin_media_path | default('/srv/media') }}' does not exist on storage node {{ groups['storage_nodes'][0] }}.
          
          Please ensure the directory exists and has proper permissions, or update the 
          'jellyfin_media_path' variable in your group_vars/all.yml to point to the correct location.
          
          Common media directory locations:
          - /srv/media (default, used by legacy NFS setup)
          - /mnt/media (if media is mounted from external source)
          
          To create the directory: mkdir -p {{ jellyfin_media_path | default('/srv/media') }}
          To set permissions: chown -R 1000:1000 {{ jellyfin_media_path | default('/srv/media') }}
          
          Alternatively, to skip this check entirely, set: jellyfin_skip_mount_verification: true
      when: not (jellyfin_skip_mount_verification | default(false)) and not media_directory_stat.stat.exists
      
    - name: Display media directory information
      debug:
        msg: |
          Media directory validation successful:
          Path: {{ jellyfin_media_path | default('/srv/media') }}
          Storage node: {{ groups['storage_nodes'][0] }}
          Directory exists: {{ media_directory_stat.stat.exists }}
          Owner: {{ media_directory_stat.stat.pw_name | default('unknown') }}:{{ media_directory_stat.stat.gr_name | default('unknown') }}
          Permissions: {{ media_directory_stat.stat.mode | default('unknown') }}
      when: not (jellyfin_skip_mount_verification | default(false)) and media_directory_stat.stat.exists

    - name: Deduplicate storage node candidates
      set_fact:
        storage_node_candidates: "{{ storage_node_candidates | default([]) | map('string') | list | unique }}"

    - name: Debug cluster nodes and storage candidates
      debug:
        msg: |
          Cluster nodes found:
          {% for node in cluster_nodes.resources %}
          - Name: {{ node.metadata.name }}
            Addresses: {{ node.status.addresses | map(attribute='address') | list }}
          {% endfor %}
          
          Storage node candidates: {{ storage_node_candidates }}
          Target storage node from inventory: {{ groups['storage_nodes'][0] }}

    - name: Force use of known correct hostname for storage node 192.168.4.61
      set_fact:
        storage_node_k8s_name: "storagenodet3500"
        storage_node_k8s_addresses: ["192.168.4.61"]
        use_forced_hostname: true
      when: groups['storage_nodes'][0] == '192.168.4.61'

    - name: Find matching Kubernetes node for storage node (for other IPs)
      set_fact:
        storage_node_k8s_name: "{{ item.metadata.name }}"
        storage_node_k8s_addresses: "{{ item.status.addresses | map(attribute='address') | list }}"
        use_forced_hostname: false
      loop: "{{ cluster_nodes.resources }}"
      when: >-
        groups['storage_nodes'][0] != '192.168.4.61' and
        ((item.metadata.name | lower) in (storage_node_candidates | map('lower') | list)
        or ((item.status.addresses | map(attribute='address') | list | intersect(storage_node_candidates) | length) > 0))

    - name: Debug hostname resolution method
      debug:
        msg: |
          Hostname resolution for storage node:
          - Inventory IP: {{ groups['storage_nodes'][0] }}
          - Kubernetes hostname: {{ storage_node_k8s_name }}
          - Method used: {% if use_forced_hostname | default(false) %}FORCED (known mapping){% else %}AUTOMATIC (cluster discovery){% endif %}
          - Node addresses: {{ storage_node_k8s_addresses | default([]) }}
          
          This ensures the nodeSelector and PV nodeAffinity use the exact same hostname.

    - name: Validate resolved hostname exists in cluster
      set_fact:
        hostname_exists_in_cluster: true
      loop: "{{ cluster_nodes.resources }}"
      when: item.metadata.name == storage_node_k8s_name

    - name: Fail if resolved hostname doesn't exist in cluster  
      fail:
        msg: |
          ERROR: Resolved storage node hostname '{{ storage_node_k8s_name }}' does not exist in cluster!
          
          Available cluster nodes:
          {% for node in cluster_nodes.resources %}
          - {{ node.metadata.name }}: {{ node.status.addresses | map(attribute='address') | list }}
          {% endfor %}
          
          Expected storage node: {{ groups['storage_nodes'][0] }}
          
          This is a critical issue that would cause pod scheduling failures.
          Please check your cluster configuration or update the hostname mapping.
      when: hostname_exists_in_cluster is not defined

    - name: Debug resolved storage node
      debug:
        msg: |
          Resolved storage node:
          - Kubernetes node name: {{ storage_node_k8s_name | default('NOT FOUND') }}
          - Node addresses: {{ storage_node_k8s_addresses | default([]) }}
          - Expected storage node IP: {{ groups['storage_nodes'][0] }}

    - name: Fail if storage node not found in cluster
      fail:
        msg: |
          Storage node not found in cluster (checked metadata.name and node addresses). 
          
          Expected storage node: {{ groups['storage_nodes'][0] }}
          Storage node candidates: {{ storage_node_candidates }}
          
          Available cluster nodes:
          {% for node in cluster_nodes.resources %}
          - {{ node.metadata.name }}: {{ node.status.addresses | map(attribute='address') | list }}
          {% endfor %}
          
          This usually means the storage node ({{ groups['storage_nodes'][0] }}) is not properly joined to the Kubernetes cluster.
          Please verify with: kubectl get nodes -o wide
      when: storage_node_k8s_name is not defined

    - name: Validate resolved storage node matches expected storage node
      fail:
        msg: |
          ERROR: Resolved storage node does not match expected storage node!
          
          Expected: {{ groups['storage_nodes'][0] }}
          Resolved: {{ storage_node_k8s_name }}
          Node addresses: {{ storage_node_k8s_addresses }}
          
          This means the nodeSelector will schedule pods on the wrong node.
          The Jellyfin deployment should run on {{ groups['storage_nodes'][0] }} but would run on {{ storage_node_k8s_name }}.
      when: >-
        storage_node_k8s_name is defined and 
        groups['storage_nodes'][0] not in storage_node_k8s_addresses and
        storage_node_k8s_name | lower != groups['storage_nodes'][0] | lower
      
    - name: Check if existing Podman Jellyfin is running
      shell: >-
        {% raw %}podman ps --filter name=jellyfin --format "table {{.Names}}\t{{.Status}}"{% endraw %}
      delegate_to: "{{ groups['storage_nodes'][0] }}"
      register: podman_jellyfin_status
      failed_when: false
      changed_when: false
      
    - name: Warn about existing Podman Jellyfin
      debug:
        msg: |
          WARNING: Found existing Podman Jellyfin container on storage node.
          Status: {{ podman_jellyfin_status.stdout }}
          This deployment will create a Kubernetes version.
          Consider stopping the Podman container after verifying the K8s deployment works.
      when: podman_jellyfin_status.rc == 0 and 'jellyfin' in podman_jellyfin_status.stdout

  tasks:
    - name: Ensure required Python packages for Kubernetes
      apt:
        name:
          - python3-kubernetes
          - python3-yaml
        state: present
        update_cache: yes
      when: ansible_facts['pkg_mgr'] == 'apt'

    - name: Fallback install kubernetes Python library via pip
      pip:
        name: 
          - kubernetes
          - PyYAML
        state: present
        executable: pip3
      when: ansible_facts['pkg_mgr'] != 'apt'

    - name: Create Jellyfin namespace
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: "{{ jellyfin_namespace }}"
            labels:
              name: "{{ jellyfin_namespace }}"
              app: jellyfin
              monitoring: enabled
        kubeconfig: /root/.kube/config

    - name: Check if Jellyfin Persistent Volumes already exist
      kubernetes.core.k8s_info:
        api_version: v1
        kind: PersistentVolume
        name: "{{ item }}"
        kubeconfig: /root/.kube/config
      loop:
        - jellyfin-media-pv
        - jellyfin-config-pv
      register: existing_pvs
      failed_when: false

    - name: Display existing PV status
      debug:
        msg: |
          Checking Persistent Volume status:
          {% for result in existing_pvs.results %}
          - {{ result.item }}: {% if result.resources %}EXISTS ({{ result.resources[0].status.phase | default('Unknown') }}){% else %}NOT FOUND{% endif %}
          {% endfor %}

    - name: Check if existing PVs have correct nodeAffinity
      set_fact:
        pv_nodeaffinity_issues: []
      
    - name: Validate existing PV nodeAffinity
      set_fact:
        pv_nodeaffinity_issues: "{{ pv_nodeaffinity_issues + [result.item] }}"
      loop: "{{ existing_pvs.results }}"
      when: 
        - result.resources | length > 0
        - result.resources[0].spec.nodeAffinity is defined
        - result.resources[0].spec.nodeAffinity.required is defined
        - result.resources[0].spec.nodeAffinity.required.nodeSelectorTerms is defined
        - storage_node_k8s_name not in (result.resources[0].spec.nodeAffinity.required.nodeSelectorTerms | map(attribute='matchExpressions') | flatten | selectattr('key', 'equalto', 'kubernetes.io/hostname') | map(attribute='values') | flatten | list)
      loop_control:
        loop_var: result

    - name: Display PV nodeAffinity validation results
      debug:
        msg: |
          PV nodeAffinity validation:
          - Target hostname: {{ storage_node_k8s_name }}
          - PVs with incorrect nodeAffinity: {{ pv_nodeaffinity_issues | default([]) }}
          {% if pv_nodeaffinity_issues | length > 0 %}
          
          WARNING: Found PVs with incorrect nodeAffinity. These will cause scheduling conflicts.
          The PVs need to be recreated with the correct hostname.
          {% endif %}

    - name: Remove PVs with incorrect nodeAffinity
      kubernetes.core.k8s:
        state: absent
        api_version: v1
        kind: PersistentVolume
        name: "{{ item }}"
        kubeconfig: /root/.kube/config
      loop: "{{ pv_nodeaffinity_issues }}"
      when: pv_nodeaffinity_issues | length > 0

    - name: Wait for PV deletion to complete
      kubernetes.core.k8s_info:
        api_version: v1
        kind: PersistentVolume
        name: "{{ item }}"
        kubeconfig: /root/.kube/config
        wait: true
        wait_condition:
          type: Deleted
        wait_timeout: 60
      loop: "{{ pv_nodeaffinity_issues }}"
      when: pv_nodeaffinity_issues | length > 0
      failed_when: false

    - name: Update existing PV list after deletions
      kubernetes.core.k8s_info:
        api_version: v1
        kind: PersistentVolume
        name: "{{ item }}"
        kubeconfig: /root/.kube/config
      loop:
        - jellyfin-media-pv
        - jellyfin-config-pv
      register: updated_existing_pvs
      failed_when: false

    - name: Skip PV creation (already exists)
      debug:
        msg: "Skipping creation of {{ item.name }} - PersistentVolume already exists with correct nodeAffinity"
      loop:
        - name: jellyfin-media-pv
        - name: jellyfin-config-pv
      when: (updated_existing_pvs.results | selectattr('item', 'equalto', item.name) | first).resources

    - name: Deploy Persistent Volumes (only if they don't exist)
      kubernetes.core.k8s:
        state: present
        definition: "{{ item.definition }}"
        kubeconfig: /root/.kube/config
      loop:
        - name: jellyfin-media-pv
          definition:
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: jellyfin-media-pv
              labels:
                app: jellyfin
                component: media
            spec:
              capacity:
                storage: 100Ti
              accessModes:
                - ReadWriteMany
              persistentVolumeReclaimPolicy: Retain
              storageClassName: jellyfin-media
              hostPath:
                path: "{{ jellyfin_media_path | default('/srv/media') }}"
                type: Directory
              nodeAffinity:
                required:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: kubernetes.io/hostname
                      operator: In
                      values:
                      - "{{ storage_node_k8s_name }}"
        - name: jellyfin-config-pv
          definition:
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: jellyfin-config-pv
              labels:
                app: jellyfin
                component: config
            spec:
              capacity:
                storage: 50Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Retain
              storageClassName: jellyfin-config
              hostPath:
                path: /mnt/jellyfin-config
                type: DirectoryOrCreate
              nodeAffinity:
                required:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: kubernetes.io/hostname
                      operator: In
                      values:
                      - "{{ storage_node_k8s_name }}"
      when: not (updated_existing_pvs.results | selectattr('item', 'equalto', item.name) | first).resources
      vars:
        existing_pv_check: "{{ updated_existing_pvs.results | selectattr('item', 'equalto', item.name) | first }}"

    - name: Deploy Persistent Volume Claims
      kubernetes.core.k8s:
        state: present
        definition: "{{ item }}"
        kubeconfig: /root/.kube/config
      loop:
        - apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            name: jellyfin-media-pvc
            namespace: "{{ jellyfin_namespace }}"
            labels:
              app: jellyfin
              component: media
          spec:
            accessModes:
              - ReadWriteMany
            resources:
              requests:
                storage: 100Ti
            storageClassName: jellyfin-media
        - apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            name: jellyfin-config-pvc
            namespace: "{{ jellyfin_namespace }}"
            labels:
              app: jellyfin
              component: config
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 50Gi
            storageClassName: jellyfin-config

    - name: Deploy Jellyfin ConfigMap
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: jellyfin-config
            namespace: "{{ jellyfin_namespace }}"
            labels:
              app: jellyfin
              component: config
          data:
            encoding.xml: |
              <?xml version="1.0" encoding="utf-8"?>
              <EncodingOptions xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
                <TranscodingTempPath>/config/transcoding-temp</TranscodingTempPath>
                <EncoderAppPath>/usr/lib/jellyfin-ffmpeg/ffmpeg</EncoderAppPath>
                <VaapiDevice>/dev/dri/renderD128</VaapiDevice>
                <EnableHardwareEncoding>true</EnableHardwareEncoding>
                <AllowHevcEncoding>true</AllowHevcEncoding>
                <EnableHardwareDecoding>true</EnableHardwareDecoding>
                <EnableDecodingColorDepth10Hevc>true</EnableDecodingColorDepth10Hevc>
                <EnableDecodingColorDepth10Vp9>true</EnableDecodingColorDepth10Vp9>
                <EnableIntelLowPowerH264HwEncoder>true</EnableIntelLowPowerH264HwEncoder>
                <EnableIntelLowPowerHevcHwEncoder>true</EnableIntelLowPowerHevcHwEncoder>
                <EnableHardwareFilterUpscaling>true</EnableHardwareFilterUpscaling>
                <AllowAv1Encoding>true</AllowAv1Encoding>
                <HardwareDecodingCodecs>
                  <string>h264</string>
                  <string>hevc</string>
                  <string>mpeg2video</string>
                  <string>mpeg4</string>
                  <string>vc1</string>
                  <string>vp8</string>
                  <string>vp9</string>
                  <string>av1</string>
                </HardwareDecodingCodecs>
                <MaxMuxingQueueSize>2048</MaxMuxingQueueSize>
                <EnableSubtitleExtraction>true</EnableSubtitleExtraction>
                <MaximumEncodingThreads>0</MaximumEncodingThreads>
              </EncodingOptions>
        kubeconfig: /root/.kube/config

    - name: Deploy Jellyfin Deployment
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: jellyfin
            namespace: "{{ jellyfin_namespace }}"
            labels:
              app: jellyfin
              component: media-server
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: jellyfin
                component: media-server
            template:
              metadata:
                labels:
                  app: jellyfin
                  component: media-server
              spec:
                nodeSelector:
                  kubernetes.io/hostname: "{{ storage_node_k8s_name }}"
                affinity:
                  podAntiAffinity:
                    preferredDuringSchedulingIgnoredDuringExecution:
                    - weight: 100
                      podAffinityTerm:
                        labelSelector:
                          matchExpressions:
                          - key: app
                            operator: In
                            values: ["jellyfin"]
                        topologyKey: kubernetes.io/hostname
                containers:
                - name: jellyfin
                  image: jellyfin/jellyfin:latest
                  imagePullPolicy: Always
                  ports:
                  - containerPort: 8096
                    name: http
                  - containerPort: 8920
                    name: https
                  - containerPort: 1900
                    name: discovery
                    protocol: UDP
                  - containerPort: 7359
                    name: auto-discovery
                    protocol: UDP
                  env:
                  - name: JELLYFIN_PublishedServerUrl
                    value: "{{ jellyfin_published_url }}"
                  - name: JELLYFIN_FFmpeg__analyzeduration
                    value: "200M"
                  - name: JELLYFIN_FFmpeg__probesize
                    value: "1G"
                  resources:
                    requests:
                      memory: "2Gi"
                      cpu: "500m"
                    limits:
                      memory: "2.5Gi"
                      cpu: "2000m"
                  volumeMounts:
                  - name: media
                    mountPath: /media
                    readOnly: true
                  - name: config
                    mountPath: /config
                  - name: dev-dri
                    mountPath: /dev/dri
                    readOnly: false
                  - name: jellyfin-config-files
                    mountPath: /config/config
                    readOnly: true
                  livenessProbe:
                    httpGet:
                      path: /health
                      port: 8096
                    initialDelaySeconds: 60
                    periodSeconds: 30
                    timeoutSeconds: 10
                    failureThreshold: 3
                  readinessProbe:
                    httpGet:
                      path: /health
                      port: 8096
                    initialDelaySeconds: 30
                    periodSeconds: 10
                    timeoutSeconds: 5
                    failureThreshold: 3
                  securityContext:
                    privileged: false
                    allowPrivilegeEscalation: true
                    capabilities:
                      add:
                      - SYS_ADMIN
                      drop:
                      - ALL
                    runAsUser: 0
                  startupProbe:
                    httpGet:
                      path: /health
                      port: 8096
                    initialDelaySeconds: 10
                    periodSeconds: 10
                    timeoutSeconds: 5
                    failureThreshold: 30
                volumes:
                - name: media
                  persistentVolumeClaim:
                    claimName: jellyfin-media-pvc
                - name: config
                  persistentVolumeClaim:
                    claimName: jellyfin-config-pvc
                - name: dev-dri
                  hostPath:
                    path: /dev/dri
                    type: Directory
                - name: jellyfin-config-files
                  configMap:
                    name: jellyfin-config
                restartPolicy: Always
                dnsPolicy: ClusterFirst
        kubeconfig: /root/.kube/config

    - name: Deploy Jellyfin Service
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            name: jellyfin-service
            namespace: "{{ jellyfin_namespace }}"
            labels:
              app: jellyfin
              component: media-server
            annotations:
              service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
          spec:
            selector:
              app: jellyfin
              component: media-server
            ports:
            - name: http
              port: 8096
              targetPort: 8096
              protocol: TCP
              nodePort: 30096
            - name: https
              port: 8920
              targetPort: 8920
              protocol: TCP
              nodePort: 30920
            - name: discovery
              port: 1900
              targetPort: 1900
              protocol: UDP
              nodePort: 31900
            - name: auto-discovery
              port: 7359
              targetPort: 7359
              protocol: UDP
              nodePort: 30735
            type: NodePort
            sessionAffinity: ClientIP
            sessionAffinityConfig:
              clientIP:
                timeoutSeconds: 10800
        kubeconfig: /root/.kube/config

    - name: Deploy Horizontal Pod Autoscaler
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: autoscaling/v2
          kind: HorizontalPodAutoscaler
          metadata:
            name: jellyfin-hpa
            namespace: "{{ jellyfin_namespace }}"
            labels:
              app: jellyfin
              component: media-server
          spec:
            scaleTargetRef:
              apiVersion: apps/v1
              kind: Deployment
              name: jellyfin
            minReplicas: 1
            maxReplicas: 3
            metrics:
            - type: Resource
              resource:
                name: cpu
                target:
                  type: Utilization
                  averageUtilization: 60
            - type: Resource
              resource:
                name: memory
                target:
                  type: Utilization
                  averageUtilization: 70
            behavior:
              scaleDown:
                stabilizationWindowSeconds: 300
                policies:
                - type: Percent
                  value: 50
                  periodSeconds: 60
              scaleUp:
                stabilizationWindowSeconds: 60
                policies:
                - type: Percent
                  value: 100
                  periodSeconds: 60
                selectPolicy: Max
        kubeconfig: /root/.kube/config

    - name: Deploy Monitoring ServiceMonitor
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: monitoring.coreos.com/v1
          kind: ServiceMonitor
          metadata:
            name: jellyfin-metrics
            namespace: "{{ jellyfin_namespace }}"
            labels:
              app: jellyfin
              component: media-server
              release: kube-prometheus-stack
          spec:
            selector:
              matchLabels:
                app: jellyfin
                component: media-server
            endpoints:
            - port: http
              interval: 30s
              path: /System/Info/Public
              scrapeTimeout: 10s
            namespaceSelector:
              matchNames:
              - "{{ jellyfin_namespace }}"
        kubeconfig: /root/.kube/config

    - name: Wait for Jellyfin deployment to be ready
      block:
        - name: Wait for Jellyfin pods to be ready
          kubernetes.core.k8s_info:
            api_version: v1
            kind: Pod
            namespace: "{{ jellyfin_namespace }}"
            label_selectors:
              - app=jellyfin
              - component=media-server
            wait: true
            wait_condition:
              type: Ready
              status: "True"
            wait_timeout: 600
            kubeconfig: /root/.kube/config
          register: jellyfin_pods_wait
          
        - name: Get Jellyfin deployment status
          kubernetes.core.k8s_info:
            api_version: apps/v1
            kind: Deployment
            name: jellyfin
            namespace: "{{ jellyfin_namespace }}"
            kubeconfig: /root/.kube/config
          register: jellyfin_deployment
          
      rescue:
        - name: Gather Jellyfin pods for debugging
          kubernetes.core.k8s_info:
            api_version: v1
            kind: Pod
            namespace: "{{ jellyfin_namespace }}"
            label_selectors:
              - app=jellyfin
            kubeconfig: /root/.kube/config
          register: jellyfin_debug_pods
          failed_when: false
          
        - name: Gather Jellyfin events for debugging
          kubernetes.core.k8s_info:
            api_version: v1
            kind: Event
            namespace: "{{ jellyfin_namespace }}"
            kubeconfig: /root/.kube/config
          register: jellyfin_debug_events
          failed_when: false
          
        - name: Gather Jellyfin deployment status for debugging
          kubernetes.core.k8s_info:
            api_version: apps/v1
            kind: Deployment
            name: jellyfin
            namespace: "{{ jellyfin_namespace }}"
            kubeconfig: /root/.kube/config
          register: jellyfin_debug_deployment
          failed_when: false
          
        - name: Gather Jellyfin PVC status for debugging
          kubernetes.core.k8s_info:
            api_version: v1
            kind: PersistentVolumeClaim
            namespace: "{{ jellyfin_namespace }}"
            kubeconfig: /root/.kube/config
          register: jellyfin_debug_pvcs
          failed_when: false
          
        - name: Display basic debug information
          debug:
            msg: |
              ==========================================
              JELLYFIN DEPLOYMENT FAILURE DEBUG INFO
              ==========================================
              
              Timeout: Jellyfin deployment failed to become ready within 10 minutes.
              Namespace: {{ jellyfin_namespace }}
              Storage Node: {{ storage_node_k8s_name | default('UNKNOWN') }}
              
        - name: Display pod debug information
          debug:
            msg: |
              POD STATUS:
              {% if jellyfin_debug_pods.resources is defined and jellyfin_debug_pods.resources | length > 0 %}
              {% for pod in jellyfin_debug_pods.resources %}
              - Pod: {{ pod.metadata.name | default('unknown') }}
                Status: {{ pod.status.phase | default('unknown') }}
                Node: {{ pod.spec.nodeName | default('unscheduled') }}
                {% if pod.status.containerStatuses is defined and pod.status.containerStatuses | length > 0 %}
                Containers:
                {% for container in pod.status.containerStatuses %}
                  - {{ container.name | default('unknown') }}: ready={{ container.ready | default('false') }}, restarts={{ container.restartCount | default('0') }}
                    {% if container.state.waiting is defined %}
                    Waiting: {{ container.state.waiting.reason | default('unknown') }}
                    {% if container.state.waiting.message is defined %}
                    Message: {{ container.state.waiting.message }}
                    {% endif %}
                    {% elif container.state.terminated is defined %}
                    Terminated: {{ container.state.terminated.reason | default('unknown') }} (exit {{ container.state.terminated.exitCode | default('unknown') }})
                    {% endif %}
                {% endfor %}
                {% else %}
                Container status: Not available
                {% endif %}
              {% endfor %}
              {% else %}
              No pods found or pod information unavailable
              {% endif %}
              
        - name: Display deployment debug information
          debug:
            msg: |
              DEPLOYMENT STATUS:
              {% if jellyfin_debug_deployment.resources is defined and jellyfin_debug_deployment.resources | length > 0 %}
              {% set deploy = jellyfin_debug_deployment.resources[0] %}
              - Replicas desired: {{ deploy.spec.replicas | default('unknown') }}
              - Replicas available: {{ deploy.status.availableReplicas | default('0') }}
              - Replicas ready: {{ deploy.status.readyReplicas | default('0') }}
              - Replicas updated: {{ deploy.status.updatedReplicas | default('0') }}
              {% if deploy.status.conditions is defined %}
              Conditions:
              {% for condition in deploy.status.conditions %}
              - {{ condition.type | default('unknown') }}: {{ condition.status | default('unknown') }} ({{ condition.reason | default('no reason') }})
              {% endfor %}
              {% endif %}
              {% else %}
              Deployment information unavailable
              {% endif %}
              
        - name: Display PVC debug information
          debug:
            msg: |
              PERSISTENT VOLUME CLAIMS:
              {% if jellyfin_debug_pvcs.resources is defined and jellyfin_debug_pvcs.resources | length > 0 %}
              {% for pvc in jellyfin_debug_pvcs.resources %}
              - {{ pvc.metadata.name | default('unknown') }}: {{ pvc.status.phase | default('unknown') }}
              {% endfor %}
              {% else %}
              No PVCs found or PVC information unavailable
              {% endif %}
              
        - name: Display events debug information
          debug:
            msg: |
              RECENT EVENTS:
              {% if jellyfin_debug_events.resources is defined and jellyfin_debug_events.resources | length > 0 %}
              {% for event in jellyfin_debug_events.resources[-10:] %}
              - {{ event.firstTimestamp | default(event.eventTime | default('unknown time')) }}: {{ event.reason | default('unknown') }} - {{ event.message | default('no message') }}
              {% endfor %}
              {% else %}
              No events found or event information unavailable
              {% endif %}
              
        - name: Fail with helpful error message
          fail:
            msg: |
              ==========================================
              JELLYFIN DEPLOYMENT FAILED
              ==========================================
              
              The Jellyfin deployment failed to become ready within 10 minutes.
              
              TROUBLESHOOTING STEPS:
              ==========================================
              
              1. Check pod status and logs:
                 kubectl get pods -n jellyfin
                 kubectl describe pods -n jellyfin
                 kubectl logs -n jellyfin -l app=jellyfin
              
              2. Check persistent volume status:
                 kubectl get pv | grep jellyfin
                 kubectl get pvc -n jellyfin
                 kubectl describe pvc -n jellyfin
              
              3. Check events for errors:
                 kubectl get events -n jellyfin --sort-by=.metadata.creationTimestamp
              
              4. Verify storage node is accessible:
                 kubectl get nodes -o wide
                 ssh {{ groups['storage_nodes'][0] }} 'ls -la {{ jellyfin_media_path | default('/srv/media') }}'
              
              5. Check hardware acceleration devices (if needed):
                 ssh {{ groups['storage_nodes'][0] }} 'ls -la /dev/dri/'
              
              6. Common solutions:
                 - Ensure media directory exists: mkdir -p {{ jellyfin_media_path | default('/srv/media') }}
                 - Check directory permissions: chown -R 1000:1000 {{ jellyfin_media_path | default('/srv/media') }}
                 - Verify storage node is in cluster: kubectl get nodes
                 - Check available resources: kubectl describe node {{ storage_node_k8s_name | default('storage-node') }}
              
              7. Manual cleanup if needed:
                 kubectl delete deployment jellyfin -n jellyfin
                 kubectl delete pvc jellyfin-config-pvc -n jellyfin
                 # Then re-run the deployment
              
              Please check the debug information above for specific error details.

    - name: Display deployment status
      debug:
        msg: |
          Jellyfin High-Availability Deployment Status:
          ================================================
          
          Namespace: {{ jellyfin_namespace }}
          Deployment: {{ jellyfin_deployment.resources[0].status.replicas | default(0) }} replicas
          Available: {{ jellyfin_deployment.resources[0].status.availableReplicas | default(0) }} replicas
          Scheduled on: {{ storage_node_k8s_name }} ({{ groups['storage_nodes'][0] }})
          
          Pods:
          {% for pod in jellyfin_pods_wait.resources %}
          - {{ pod.metadata.name }}: {{ pod.status.phase }} (Node: {{ pod.spec.nodeName | default('Pending') }})
          {% endfor %}
          
          Access URLs:
          - Primary HTTP: http://{{ groups['storage_nodes'][0] }}:30096 (via storage node)
          - Primary HTTPS: https://{{ groups['storage_nodes'][0] }}:30920 (via storage node)
          - Alternative access via any cluster node: http://{{ ansible_default_ipv4.address }}:30096
          
          Features Enabled:
          ✓ Auto-scaling (1-3 pods based on load)
          ✓ Session affinity for uninterrupted streaming
          ✓ Hardware acceleration support
          ✓ Large file upload support (50GB)
          ✓ Monitoring integration
          ✓ Resource constraints (2-2.5GB RAM per pod)
          
          Next Steps:
          1. Verify media access at the above URL
          2. Test auto-scaling by starting multiple streams
          3. Monitor resource usage in Grafana
          4. Stop old Podman container when satisfied: podman stop jellyfin

  post_tasks:
    - name: Create validation script
      copy:
        dest: /tmp/validate_jellyfin_ha.sh
        mode: '0755'
        content: |
          #!/bin/bash
          echo "=== Jellyfin HA Validation ==="
          echo "Checking Kubernetes resources..."
          
          kubectl get pods -n jellyfin
          kubectl get svc -n jellyfin
          kubectl get hpa -n jellyfin
          kubectl get pv | grep jellyfin
          kubectl get pvc -n jellyfin
          
          echo ""
          echo "Checking service endpoints..."
          echo "Testing storage node endpoint (primary): http://{{ groups['storage_nodes'][0] }}:30096/health"
          curl -s -I http://{{ groups['storage_nodes'][0] }}:30096/health || echo "Storage node service not ready yet"
          
          echo "Testing control plane endpoint (fallback): http://{{ ansible_default_ipv4.address }}:30096/health"  
          curl -s -I http://{{ ansible_default_ipv4.address }}:30096/health || echo "Control plane service not ready yet"
          
          echo ""
          echo "Checking HPA metrics..."
          kubectl top pods -n jellyfin 2>/dev/null || echo "Metrics not ready yet"
          
          echo ""
          echo "Access your Jellyfin server at:"
          echo "Primary URL (storage node): http://{{ groups['storage_nodes'][0] }}:30096"
          echo "Fallback URL (any node): http://{{ ansible_default_ipv4.address }}:30096"
          echo ""
          echo "Expected deployment location: {{ groups['storage_nodes'][0] }} (storage node)"
          echo "Pods should be scheduled on: {{ storage_node_k8s_name | default('UNKNOWN') }}"

    - name: Run validation
      shell: /tmp/validate_jellyfin_ha.sh
      register: validation_output
      
    - name: Show validation results
      debug:
        msg: "{{ validation_output.stdout_lines }}"