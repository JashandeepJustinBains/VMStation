---
# Kubernetes Cluster Setup Playbook
# Sets up monitoring_nodes as control plane and other nodes as workers
- name: Setup Kubernetes cluster with monitoring_nodes as control plane
  hosts: all
  become: true
  vars:
    kubernetes_version: "1.29"
    pod_network_cidr: "10.244.0.0/16"
    
  pre_tasks:
    - name: Remove bad kubernetes-upstream repo and clean yum/dnf cache (preflight)
      shell: |
        rm -f /etc/yum.repos.d/kubernetes-upstream.repo || true
        if command -v dnf >/dev/null 2>&1; then
          dnf clean all || true
        else
          yum clean all || true
        fi
      when: ansible_os_family == 'RedHat'
      ignore_errors: yes

  tasks:
    - name: Update apt cache
      apt:
        update_cache: yes
      when: ansible_os_family == 'Debian'

    - name: Install required packages
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gpg
        state: present
      when: ansible_os_family == 'Debian'

    - name: Add Kubernetes GPG key
      apt_key:
        url: https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/Release.key
        state: present
      when: ansible_os_family == 'Debian'

    - name: Add Kubernetes repository
      apt_repository:
        repo: "deb https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/ /"
        state: present
        filename: kubernetes
      when: ansible_os_family == 'Debian'

    - name: Update apt cache after adding repository
      apt:
        update_cache: yes
      when: ansible_os_family == 'Debian'

    - name: Install Kubernetes packages
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
      when: ansible_os_family == 'Debian'

    - name: Remove user-added kubernetes-upstream repo file if present (prevents broken metadata)
      file:
        path: /etc/yum.repos.d/kubernetes-upstream.repo
        state: absent
      when: ansible_os_family == 'RedHat'
      ignore_errors: yes

    - name: Ensure yum-utils is installed on RHEL-family systems
      package:
        name: yum-utils
        state: present
      when: ansible_os_family == 'RedHat'

    - name: Determine RHEL major version
      set_fact:
        rhel_major: "{{ ansible_distribution_major_version | int }}"
      when: ansible_os_family == 'RedHat'
    - block:
        - name: Add Kubernetes yum repository for RHEL-family systems (EL9/EL8)
          yum_repository:
            name: kubernetes
            description: Kubernetes repo
            baseurl: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/rpm/"
            gpgcheck: yes
            repo_gpgcheck: yes
            gpgkey: "https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/rpm/Release.key"
            enabled: yes
        - name: Update yum cache on RHEL-family systems (EL9/EL8)
          command: yum makecache -q
        - name: Install Kubernetes and containerd packages on RHEL-family systems (EL9/EL8)
          package:
            name:
              - kubelet
              - kubeadm
              - kubectl
              - containerd
            state: present
      when: ansible_os_family == 'RedHat' and (rhel_major | int) < 10

    - block:
        - name: Install required RHEL 10+ packages for Kubernetes
          package:
            name:
              - curl
              - wget
              - conntrack-tools
              - socat
              - iproute-tc
              - iptables
            state: present
          ignore_errors: yes
        
        - name: Fallback install conntrack if conntrack-tools not available
          package:
            name: conntrack
            state: present
          ignore_errors: yes
            
        - name: Add Docker CE repo for containerd (RHEL 10+ fallback)
          shell: |
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo
            elif command -v wget >/dev/null 2>&1; then
              wget -q -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo
            else
              dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo || true
            fi
          args:
            creates: /etc/yum.repos.d/docker-ce.repo
        
        - name: Ensure Docker CE repo file has correct permissions
          file:
            path: /etc/yum.repos.d/docker-ce.repo
            mode: '0644'
            owner: root
            group: root
        
        - name: Refresh package cache (dnf)
          command: dnf makecache --refresh
          
        - name: Try install containerd.io (RHEL 10+)
          package:
            name: containerd.io
            state: present
          register: containerd_install
          ignore_errors: yes
          
        - name: Fallback install containerd package name (containerd) if containerd.io not available
          package:
            name: containerd
            state: present
          when: containerd_install is failed
        - name: Ensure containerd directory exists
          file:
            path: /etc/containerd
            state: directory
            
        - name: Generate default containerd configuration (RHEL 10+)
          shell: containerd config default > /etc/containerd/config.toml
          args:
            creates: /etc/containerd/config.toml
          ignore_errors: yes
          
        - name: Configure containerd to use systemd cgroup driver (RHEL 10+)
          replace:
            path: /etc/containerd/config.toml
            regexp: 'SystemdCgroup = false'
            replace: 'SystemdCgroup = true'
          when: 
            - ansible_os_family == 'RedHat' 
            - (rhel_major | int) >= 10
          ignore_errors: yes
          
        - name: Start and enable containerd (RHEL 10+)
          systemd:
            name: containerd
            state: restarted
            enabled: yes
            daemon_reload: yes
        - name: Get latest stable Kubernetes version for RHEL 10+ (curl/wget fallback)
          shell: |
            set -o pipefail
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL "https://dl.k8s.io/release/stable-{{ kubernetes_version }}.txt" || true
            elif command -v wget >/dev/null 2>&1; then
              wget -qO- "https://dl.k8s.io/release/stable-{{ kubernetes_version }}.txt" || true
            else
              echo "";
            fi
          args:
            executable: /bin/bash
          register: k8s_stable_version
          changed_when: false
          ignore_errors: yes
          
        - name: Set Kubernetes version for download
          set_fact:
            k8s_download_version: >-
              {{ (k8s_stable_version.stdout | default('') ).strip() if (k8s_stable_version is defined and k8s_stable_version.stdout is defined and (k8s_stable_version.stdout | trim) != '') else 'v' + kubernetes_version + '.0' }}
            
        - name: Download kubeadm binary (RHEL 10+ fallback)
          get_url:
            url: "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubeadm"
            dest: /usr/bin/kubeadm
            mode: '0755'
            owner: root
            group: root
            validate_certs: false
            use_proxy: false
          register: kubeadm_download
          retries: 3
          delay: 5
          until: kubeadm_download is succeeded
          failed_when: false
          
        - name: Download kubeadm binary using shell fallback (RHEL 10+ urllib3 compatibility)
          shell: |
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubeadm" -o /usr/bin/kubeadm
            elif command -v wget >/dev/null 2>&1; then
              wget -q -O /usr/bin/kubeadm "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubeadm"
            else
              echo "Neither curl nor wget available for download"
              exit 1
            fi
            chmod 0755 /usr/bin/kubeadm
            chown root:root /usr/bin/kubeadm
          args:
            creates: /usr/bin/kubeadm
          when: kubeadm_download is failed and ('cert_file' in kubeadm_download.msg or 'urllib3' in kubeadm_download.msg)
          retries: 3
          delay: 5
          
        - name: Verify kubeadm binary was downloaded successfully
          stat:
            path: /usr/bin/kubeadm
          register: kubeadm_binary_check
          failed_when: not kubeadm_binary_check.stat.exists
          
        - name: Ensure kubeadm binary permissions are correct
          file:
            path: /usr/bin/kubeadm
            mode: '0755'
            owner: root
            group: root
          
        - name: Download kubectl binary (RHEL 10+ fallback)
          get_url:
            url: "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubectl"
            dest: /usr/bin/kubectl
            mode: '0755'
            owner: root
            group: root
            validate_certs: false
            use_proxy: false
          register: kubectl_download
          retries: 3
          delay: 5
          until: kubectl_download is succeeded
          failed_when: false
          
        - name: Download kubectl binary using shell fallback (RHEL 10+ urllib3 compatibility)
          shell: |
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubectl" -o /usr/bin/kubectl
            elif command -v wget >/dev/null 2>&1; then
              wget -q -O /usr/bin/kubectl "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubectl"
            else
              echo "Neither curl nor wget available for download"
              exit 1
            fi
            chmod 0755 /usr/bin/kubectl
            chown root:root /usr/bin/kubectl
          args:
            creates: /usr/bin/kubectl
          when: kubectl_download is failed and ('cert_file' in kubectl_download.msg or 'urllib3' in kubectl_download.msg)
          retries: 3
          delay: 5
          
        - name: Verify kubectl binary was downloaded successfully
          stat:
            path: /usr/bin/kubectl
          register: kubectl_binary_check
          failed_when: not kubectl_binary_check.stat.exists
          
        - name: Ensure kubectl binary permissions are correct
          file:
            path: /usr/bin/kubectl
            mode: '0755'
            owner: root
            group: root
          
        - name: Download kubelet binary (RHEL 10+ fallback)
          get_url:
            url: "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubelet"
            dest: /usr/bin/kubelet
            mode: '0755'
            owner: root
            group: root
            validate_certs: false
            use_proxy: false
          register: kubelet_download
          retries: 3
          delay: 5
          until: kubelet_download is succeeded
          failed_when: false
          
        - name: Download kubelet binary using shell fallback (RHEL 10+ urllib3 compatibility)
          shell: |
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubelet" -o /usr/bin/kubelet
            elif command -v wget >/dev/null 2>&1; then
              wget -q -O /usr/bin/kubelet "https://dl.k8s.io/release/{{ k8s_download_version }}/bin/linux/amd64/kubelet"
            else
              echo "Neither curl nor wget available for download"
              exit 1
            fi
            chmod 0755 /usr/bin/kubelet
            chown root:root /usr/bin/kubelet
          args:
            creates: /usr/bin/kubelet
          when: kubelet_download is failed and ('cert_file' in kubelet_download.msg or 'urllib3' in kubelet_download.msg)
          retries: 3
          delay: 5
          
        - name: Verify kubelet binary was downloaded successfully
          stat:
            path: /usr/bin/kubelet
          register: kubelet_binary_check
          failed_when: not kubelet_binary_check.stat.exists
          
        - name: Ensure kubelet binary permissions are correct
          file:
            path: /usr/bin/kubelet
            mode: '0755'
            owner: root
            group: root
        - name: Download crictl binary (RHEL 10+ fallback)
          get_url:
            url: "https://github.com/kubernetes-sigs/cri-tools/releases/download/v{{ kubernetes_version }}.0/crictl-v{{ kubernetes_version }}.0-linux-amd64.tar.gz"
            dest: /tmp/crictl.tar.gz
            validate_certs: false
            use_proxy: false
          register: crictl_download
          retries: 3
          delay: 5
          until: crictl_download is succeeded
          failed_when: false
          
        - name: Download crictl binary using shell fallback (RHEL 10+ urllib3 compatibility)
          shell: |
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL "https://github.com/kubernetes-sigs/cri-tools/releases/download/v{{ kubernetes_version }}.0/crictl-v{{ kubernetes_version }}.0-linux-amd64.tar.gz" -o /tmp/crictl.tar.gz
            elif command -v wget >/dev/null 2>&1; then
              wget -q -O /tmp/crictl.tar.gz "https://github.com/kubernetes-sigs/cri-tools/releases/download/v{{ kubernetes_version }}.0/crictl-v{{ kubernetes_version }}.0-linux-amd64.tar.gz"
            else
              echo "Neither curl nor wget available for download"
              exit 1
            fi
          args:
            creates: /tmp/crictl.tar.gz
          when: crictl_download is failed and ('cert_file' in crictl_download.msg or 'urllib3' in crictl_download.msg)
          retries: 3
          delay: 5
          
        - name: Extract and install crictl
          unarchive:
            src: /tmp/crictl.tar.gz
            dest: /usr/bin/
            remote_src: yes
            creates: /usr/bin/crictl
            
        - name: Ensure crictl is executable
          file:
            path: /usr/bin/crictl
            mode: '0755'
            owner: root
            group: root
            
        - name: Clean up crictl download
          file:
            path: /tmp/crictl.tar.gz
            state: absent
        - name: Install conntrack package (RHEL 10+ fallback)
          package:
            name: conntrack-tools
            state: present
          ignore_errors: yes
        - name: Install conntrack if conntrack-tools not available (RHEL 10+)
          package:
            name: conntrack
            state: present
          ignore_errors: yes
        - name: Create kubeadm-compatible kubelet systemd unit (RHEL 10+ fallback)
          copy:
            dest: /etc/systemd/system/kubelet.service
            content: |
              [Unit]
              Description=kubelet: The Kubernetes Node Agent
              Documentation=https://kubernetes.io/docs/
              Wants=network-online.target
              After=network-online.target containerd.service
              Requires=containerd.service

              [Service]
              Type=notify
              EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
              EnvironmentFile=-/etc/sysconfig/kubelet
              # KUBELET_KUBEADM_ARGS is written by kubeadm when joining
              ExecStart=/usr/bin/kubelet $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
              Restart=always
              RestartSec=10
              StartLimitBurst=3
              StartLimitInterval=60

              [Install]
              WantedBy=multi-user.target
              
        - name: Create kubelet dropin directory
          file:
            path: /etc/systemd/system/kubelet.service.d
            state: directory
            mode: '0755'
            
        - name: Create kubeadm kubelet dropin (join-compatible configuration)
          copy:
            dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
            content: |
              # Note: This dropin only works with kubeadm and kubelet v1.11+
              # Join-compatible kubelet configuration - lets kubeadm manage all configuration during join
              [Service]
              Environment="KUBELET_KUBECONFIG_ARGS="
              Environment="KUBELET_CONFIG_ARGS="
              # This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
              EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
              # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
              # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
              EnvironmentFile=-/etc/sysconfig/kubelet
              ExecStart=
              ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
              
        - name: Ensure /etc/sysconfig directory exists (RHEL 10+)
          file:
            path: /etc/sysconfig
            state: directory
            mode: '0755'
            
        - name: Ensure /etc/sysconfig/kubelet exists with systemd cgroup driver (RHEL 10+)
          copy:
            dest: /etc/sysconfig/kubelet
            content: |
              # Kubelet environment
              KUBELET_EXTRA_ARGS=--cgroup-driver=systemd --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock
            mode: '0644'
            
        - name: Reload systemd after kubelet unit and sysconfig changes (RHEL 10+)
          systemd:
            daemon_reload: yes
            
        - name: Enable kubelet service (RHEL 10+)
          systemd:
            name: kubelet
            enabled: yes
            
        - name: Verify kubelet binary works
          command: /usr/bin/kubelet --version
          register: kubelet_version_check
          changed_when: false
          
        - name: Display kubelet version
          debug:
            msg: "Kubelet version: {{ kubelet_version_check.stdout }}"
      when: ansible_os_family == 'RedHat' and (rhel_major | int) >= 10

    - name: Ensure SELinux is permissive on RHEL-family systems (required for some container setups)
      selinux:
        state: permissive
        policy: targeted
      when: ansible_os_family == 'RedHat' and (ansible_selinux is defined and ansible_selinux.status != 'Disabled')

    - name: Configure firewall for Kubernetes on RHEL systems
      block:
        - name: Check if firewalld is running
          systemd:
            name: firewalld
          register: firewalld_status
          ignore_errors: yes
          
        - name: Open Kubernetes ports in firewalld (control plane)
          firewalld:
            port: "{{ item }}"
            permanent: yes
            state: enabled
            immediate: yes
          loop:
            - "6443/tcp"    # Kubernetes API server
            - "2379-2380/tcp"  # etcd server client API
            - "10250/tcp"   # kubelet API
            - "10259/tcp"   # kube-scheduler
            - "10257/tcp"   # kube-controller-manager
          when: 
            - ansible_os_family == 'RedHat'
            - firewalld_status.status.ActiveState == 'active'
            - inventory_hostname in groups['monitoring_nodes']
          ignore_errors: yes
          
        - name: Open Kubernetes worker ports in firewalld
          firewalld:
            port: "{{ item }}"
            permanent: yes
            state: enabled
            immediate: yes
          loop:
            - "10250/tcp"   # kubelet API
            - "30000-32767/tcp"  # NodePort services
          when: 
            - ansible_os_family == 'RedHat'
            - firewalld_status.status.ActiveState == 'active'
            - inventory_hostname in groups['storage_nodes'] or inventory_hostname in groups['compute_nodes']
          ignore_errors: yes
          
        - name: Open CNI ports (Flannel) in firewalld
          firewalld:
            port: "{{ item }}"
            permanent: yes
            state: enabled
            immediate: yes
          loop:
            - "8285/udp"    # Flannel VXLAN
            - "8472/udp"    # Flannel VXLAN
          when: 
            - ansible_os_family == 'RedHat'
            - firewalld_status.status.ActiveState == 'active'
          ignore_errors: yes
      when: ansible_os_family == 'RedHat'

    - name: Hold Kubernetes packages
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl
      when: ansible_os_family == 'Debian'

    - name: Disable swap permanently
      replace:
        path: /etc/fstab
        regexp: '^([^#].*?\sswap\s.*)$'
        replace: '# \1'

    - name: Disable swap immediately
      command: swapoff -a
      changed_when: false

    - name: Load required kernel modules
      modprobe:
        name: "{{ item }}"
      loop:
        - overlay
        - br_netfilter

    - name: Create kernel modules config
      copy:
        content: |
          overlay
          br_netfilter
        dest: /etc/modules-load.d/k8s.conf

    - name: Set sysctl parameters for Kubernetes
      sysctl:
        name: "{{ item.key }}"
        value: "{{ item.value }}"
        state: present
        reload: yes
      loop:
        - { key: 'net.bridge.bridge-nf-call-iptables', value: '1' }
        - { key: 'net.bridge.bridge-nf-call-ip6tables', value: '1' }
        - { key: 'net.ipv4.ip_forward', value: '1' }

    - name: Install containerd
      apt:
        name: containerd
        state: present
      when: ansible_os_family == 'Debian'

    - name: Create containerd configuration directory
      file:
        path: /etc/containerd
        state: directory

    - name: Check if containerd binary exists
      stat:
        path: /usr/bin/containerd
      register: containerd_bin

    - name: Generate containerd configuration
      shell: containerd config default
      register: containerd_config
      when: containerd_bin.stat.exists

    - name: Write containerd configuration
      copy:
        content: "{{ containerd_config.stdout }}"
        dest: /etc/containerd/config.toml
      when: containerd_bin.stat.exists

    - name: Configure containerd to use systemd cgroup driver
      replace:
        path: /etc/containerd/config.toml
        regexp: 'SystemdCgroup = false'
        replace: 'SystemdCgroup = true'
      when: containerd_bin.stat.exists

    - name: Restart and enable containerd
      systemd:
        name: containerd
        state: restarted
        enabled: yes
      when: containerd_bin.stat.exists

    - name: Gather service facts
      service_facts:

    - name: Restart and enable kubelet (only if service exists)
      systemd:
        name: kubelet
        state: restarted
        enabled: yes
      when: "'kubelet.service' in ansible_facts.services"
      register: kubelet_restart_result
      ignore_errors: yes

    - name: Handle kubelet restart failure with comprehensive recovery
      block:
        - name: Collect initial diagnostics for kubelet failure
          shell: |
            echo "=== Kubelet Failure Diagnostics ==="
            echo "System: $(uname -a)"
            echo "Kubelet status:"
            systemctl status kubelet --no-pager || true
            echo "Recent kubelet logs:"
            journalctl -u kubelet -n 20 --no-pager || true
            echo "Container runtime status:"
            systemctl status containerd --no-pager || true
            echo "Kubelet configuration:"
            ls -la /etc/systemd/system/kubelet.service.d/ || true
            cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf || true
          register: initial_diagnostics
          ignore_errors: yes

        - name: Display initial diagnostics
          debug:
            var: initial_diagnostics.stdout_lines

        - name: Stop kubelet for clean restart
          systemd:
            name: kubelet
            state: stopped
          ignore_errors: yes

        - name: Verify container runtime is operational
          block:
            - name: Check containerd status and restart if needed
              systemd:
                name: containerd
                state: restarted
                enabled: yes
              register: containerd_restart
              ignore_errors: yes

            - name: Verify containerd socket accessibility
              shell: |
                if [ -S /run/containerd/containerd.sock ]; then
                  echo "containerd socket exists"
                  ls -la /run/containerd/containerd.sock
                else
                  echo "containerd socket missing"
                  exit 1
                fi
              register: containerd_socket_check
              ignore_errors: yes

            - name: Test container runtime connectivity
              shell: |
                # Test if containerd is responding
                timeout 10s ctr version || echo "containerd not responding"
              register: ctr_test
              ignore_errors: yes

        - name: Clear comprehensive kubelet state
          shell: |
            # Remove kubelet state that might prevent startup
            rm -rf /var/lib/kubelet/pki/kubelet.crt || true
            rm -rf /var/lib/kubelet/pki/kubelet.key || true
            rm -rf /var/lib/kubelet/config.yaml || true
            # Clear any leftover pod manifests that might cause conflicts
            rm -rf /etc/kubernetes/manifests/*.yaml || true
            # Clear kubelet cache and temporary files
            rm -rf /var/lib/kubelet/pods/* || true
            rm -rf /var/lib/kubelet/cache/* || true
            # Reset kubelet configuration if corrupted
            rm -rf /var/lib/kubelet/kubeconfig || true
          ignore_errors: yes

        # Removed: Create minimal kubelet config to allow startup (control plane)
        # This task was causing kubeadm init conflicts by creating a config.yaml that interferes
        # with kubeadm's initialization process. Let kubeadm manage the config.yaml during init.
        # The config.yaml will be properly created by kubeadm init and referenced post-init.

        # Removed: Create minimal kubelet config to allow startup (worker nodes - CA agnostic)
        # This task was causing kubeadm join failures by creating a config.yaml that conflicts
        # with kubeadm's bootstrap process. Let kubeadm manage the config.yaml during join.
        # The config.yaml will be properly created by kubeadm join and referenced post-join.

        - name: Regenerate kubelet service configuration
          block:
            - name: Create kubelet service drop-in directory
              file:
                path: /etc/systemd/system/kubelet.service.d
                state: directory
                mode: '0755'

            - name: Check if kubelet.conf exists for recovery mode config decision
              stat:
                path: /etc/kubernetes/kubelet.conf
              register: recovery_kubelet_conf_check

            - name: Generate enhanced kubelet service configuration (recovery mode - joined node)
              copy:
                content: |
                  # Note: This dropin only works with kubeadm and kubelet v1.11+
                  [Service]
                  Environment="KUBELET_KUBECONFIG_ARGS=--kubeconfig=/etc/kubernetes/kubelet.conf"
                  Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
                  # This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
                  EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
                  # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
                  # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
                  EnvironmentFile=-/etc/sysconfig/kubelet
                  ExecStart=
                  ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
                  Restart=always
                  StartLimitInterval=0
                  RestartSec=10
                dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
                mode: '0644'
              when: recovery_kubelet_conf_check.stat.exists

            - name: Generate enhanced kubelet service configuration (recovery mode - pre-join)
              copy:
                content: |
                  # Note: This dropin only works with kubeadm and kubelet v1.11+
                  [Service]
                  Environment="KUBELET_KUBECONFIG_ARGS="
                  Environment="KUBELET_CONFIG_ARGS="
                  # This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
                  EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
                  # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
                  # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
                  EnvironmentFile=-/etc/sysconfig/kubelet
                  ExecStart=
                  ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
                  Restart=always
                  StartLimitInterval=0
                  RestartSec=10
                dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
                mode: '0644'
              when: not recovery_kubelet_conf_check.stat.exists
                
            # Removed: Create kubelet kubeadm flags file to prevent environment variable warnings
            # This task was causing kubeadm join failures by creating a static kubeadm-flags.env
            # that conflicts with kubeadm's bootstrap process. Let kubeadm manage this file during join.
            # The kubelet systemd service will still reference this file via EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
            # but kubeadm will create it with the correct join-specific parameters.

            - name: Ensure /etc/sysconfig directory exists
              file:
                path: /etc/sysconfig
                state: directory
                mode: '0755'
                
            - name: Ensure clean /etc/sysconfig/kubelet without deprecated flags
              copy:
                dest: /etc/sysconfig/kubelet
                content: |
                  # Kubelet environment - no deprecated flags like --network-plugin
                  KUBELET_EXTRA_ARGS=--cgroup-driver=systemd --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock
                mode: '0644'

        - name: Ensure CNI infrastructure is available
          block:
            - name: Create CNI directories
              file:
                path: "{{ item }}"
                state: directory
                owner: root
                group: root
                mode: '0755'
              loop:
                - /opt/cni/bin
                - /etc/cni/net.d
                - /var/lib/cni/networks
                - /var/lib/cni/results
              ignore_errors: yes

            - name: Check if basic CNI plugins exist
              stat:
                path: /opt/cni/bin/bridge
              register: cni_plugins_check
              ignore_errors: yes

            - name: Download essential CNI plugins if missing
              unarchive:
                src: "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz"
                dest: /opt/cni/bin
                remote_src: yes
                owner: root
                group: root
                mode: '0755'
                creates: /opt/cni/bin/bridge
              when: not cni_plugins_check.stat.exists
              ignore_errors: yes
              retries: 2
              delay: 5

        - name: Verify kernel modules are loaded
          shell: |
            # Ensure required kernel modules are loaded
            modprobe overlay || true
            modprobe br_netfilter || true
            # Verify modules are loaded
            lsmod | grep -E "(overlay|br_netfilter)" || echo "Kernel modules check"
          ignore_errors: yes

        - name: Handle systemd start rate limiting
          block:
            - name: Reset systemd failure state for kubelet
              shell: |
                # Reset any systemd failure counters that might prevent restart
                systemctl reset-failed kubelet || true
                # Clear systemd rate limiting for kubelet service
                systemctl stop kubelet || true
                sleep 2
              ignore_errors: yes

            - name: Check for systemd rate limiting issues
              shell: |
                # Check if kubelet is being rate limited
                systemctl status kubelet | grep -i "start request repeated too quickly" || echo "No rate limiting detected"
              register: rate_limit_check
              ignore_errors: yes

            - name: Apply additional rate limiting fixes if detected
              shell: |
                # If rate limiting is detected, reset the service completely
                if echo "{{ rate_limit_check.stdout }}" | grep -qi "start request repeated too quickly"; then
                  echo "Rate limiting detected, applying comprehensive reset"
                  systemctl stop kubelet || true
                  systemctl reset-failed kubelet || true
                  # Wait longer to ensure systemd internal state clears
                  sleep 5
                else
                  echo "No rate limiting issues detected"
                fi
              when: rate_limit_check.stdout is defined
              ignore_errors: yes

        - name: Reload systemd daemon and dependencies
          systemd:
            daemon_reload: yes

        - name: Wait for system to stabilize after configuration changes
          pause:
            seconds: 8

        - name: Attempt kubelet restart after comprehensive cleanup
          systemd:
            name: kubelet
            state: started
            enabled: yes
          register: kubelet_recovery_result
          ignore_errors: yes

        - name: Collect post-recovery diagnostics
          shell: |
            echo "=== Post-Recovery Diagnostics ==="
            echo "Kubelet status after recovery:"
            systemctl status kubelet --no-pager || true
            echo "Recent kubelet logs after recovery:"
            journalctl -u kubelet -n 10 --no-pager || true
            echo "Container runtime connectivity:"
            ctr version || echo "containerd not accessible"
          register: post_recovery_diagnostics
          ignore_errors: yes

        - name: Display comprehensive recovery status
          debug:
            msg: |
              Kubelet recovery attempt: {{ 'SUCCESS' if kubelet_recovery_result is succeeded else 'FAILED' }}
              
              Recovery actions taken:
              - Container runtime restart: {{ 'OK' if containerd_restart is succeeded else 'FAILED' }}
              - Containerd socket check: {{ 'OK' if containerd_socket_check is succeeded else 'FAILED' }}
              - Service configuration regenerated
              - Kubelet state cleared
              - Kernel modules verified
              
              {% if kubelet_recovery_result is failed %}
              Troubleshooting steps:
              1. Check logs: journalctl -u kubelet -f
              2. Check container runtime: systemctl status containerd
              3. Verify network: ss -ltnp | grep :10250
              4. Check node configuration: /var/lib/kubelet/config.yaml
              {% endif %}

        - name: Save diagnostic logs for manual review
          copy:
            content: |
              {{ initial_diagnostics.stdout }}
              
              === Post-Recovery Diagnostics ===
              {{ post_recovery_diagnostics.stdout }}
            dest: "/tmp/kubelet-recovery-{{ inventory_hostname }}-{{ ansible_date_time.epoch }}.log"
          ignore_errors: yes

      when: 
        - "'kubelet.service' in ansible_facts.services"
        - kubelet_restart_result is defined
        - kubelet_restart_result.failed is defined
        - kubelet_restart_result.failed


# Control plane initialization (only on monitoring_nodes)
- name: Initialize Kubernetes control plane
  hosts: monitoring_nodes
  become: true
  vars:
    pod_network_cidr: "10.244.0.0/16"
  tasks:
    - name: Check if cluster is already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeconfig

    - name: Preflight detect listeners on API port 6443
      shell: |
        set -o pipefail
        (ss -ltnp '( sport = :6443 )' 2>/dev/null || true) || (ss -ltnp 2>/dev/null | grep ':6443' || true)
      args:
        executable: /bin/bash
      register: port_6443
      changed_when: false

    - name: Show port-6443 listener (if any)
      debug:
        var: port_6443.stdout_lines
      when: port_6443.stdout != ''

    - name: Attempt to free port 6443 if occupied by kube components
      block:
        - name: Stop kubelet to prevent static pod recreation
          systemd:
            name: kubelet
            state: stopped
          ignore_errors: true

        - name: Stop kube-apiserver systemd unit if present
          systemd:
            name: kube-apiserver
            state: stopped
          ignore_errors: true

        - name: Move kube-apiserver static manifest out of manifests (if present)
          shell: |
            if [ -f /etc/kubernetes/manifests/kube-apiserver.yaml ]; then
              mv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/kube-apiserver.yaml.backup || true
            fi
          ignore_errors: true

        - name: Kill any remaining kube-apiserver processes
          shell: pkill -f kube-apiserver || true
          ignore_errors: true

        - name: Re-check port 6443 after cleanup
          shell: ss -ltnp 2>/dev/null | grep ':6443' || true
          register: port_6443_after
          changed_when: false

        - name: Fail if port 6443 still occupied after attempts to free it
          fail:
            msg: |
              Port 6443 is still in use after attempting to stop kubelet/kube-apiserver and remove static manifests.
              Manual intervention required. Run the following on the target node to investigate:
                sudo ss -ltnp | grep ':6443'
                sudo ps -ef | grep kube-apiserver
                sudo systemctl status kubelet kube-apiserver
              If it's safe, stop the owning service or kill the process, or remove the static manifest under /etc/kubernetes/manifests.
          when: port_6443_after.stdout != ''
      when: port_6443.stdout != '' and not kubeconfig.stat.exists

    - name: Ensure /etc/kubernetes directory structure exists
      file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: '0755'
      loop:
        - /etc/kubernetes
        - /etc/kubernetes/manifests
        - /var/lib/kubelet

    - name: Initialize Kubernetes cluster
      command: >
        kubeadm init 
        --pod-network-cidr={{ pod_network_cidr }}
        --apiserver-advertise-address={{ ansible_default_ipv4.address }}
        --control-plane-endpoint={{ ansible_default_ipv4.address }}
      when: not kubeconfig.stat.exists
      register: kubeadm_init

    - name: Verify kubelet configuration after kubeadm init
      block:
        - name: Check if kubelet config file exists
          stat:
            path: /var/lib/kubelet/config.yaml
          register: kubelet_config_check

        - name: Check if kubelet kubeadm flags file exists  
          stat:
            path: /var/lib/kubelet/kubeadm-flags.env
          register: kubelet_flags_check

        - name: Restart and enable kubelet (only if service exists)
          systemd:
            name: kubelet
            state: restarted
            enabled: yes
          when: "'kubelet.service' in ansible_facts.services"
          register: kubelet_post_init_restart
          ignore_errors: yes

        - name: Collect initial diagnostics for kubelet failure
          shell: |
            echo "=== Kubelet Failure Diagnostics ==="
            echo "System: $(uname -a)"
            echo "Kubelet status:"
            systemctl status kubelet --no-pager || true
            echo "Recent kubelet logs:"
            journalctl -u kubelet -n 20 --no-pager || true
            echo "Container runtime status:"
            systemctl status containerd --no-pager || true
            echo "Kubelet configuration:"
            ls -la /etc/systemd/system/kubelet.service.d/ || true
            cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf || true
          register: initial_diagnostics
          ignore_errors: yes
          when: kubelet_post_init_restart is defined and kubelet_post_init_restart.failed

        - name: Display initial diagnostics
          debug:
            var: initial_diagnostics.stdout_lines
          when: kubelet_post_init_restart is defined and kubelet_post_init_restart.failed

        - name: Attempt kubelet restart after comprehensive cleanup
          systemd:
            name: kubelet
            state: restarted
            enabled: yes
          register: kubelet_recovery_result
          ignore_errors: yes
          when: kubelet_post_init_restart is defined and kubelet_post_init_restart.failed

      when: kubeadm_init is defined and not (kubeadm_init.failed | default(false))

    - name: Create .kube directory for root
      file:
        path: /root/.kube
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Copy admin.conf to root's kubeconfig
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        owner: root
        group: root
        mode: '0644'
        remote_src: yes

    - name: Copy custom Flannel manifest to control plane
      copy:
        src: "{{ playbook_dir }}/templates/kube-flannel-allnodes.yml"
        dest: /tmp/kube-flannel-allnodes.yml
        mode: '0644'

    - name: Install Flannel CNI (custom manifest - all nodes)
      shell: kubectl apply -f /tmp/kube-flannel-allnodes.yml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Ensure any existing tokens are cleaned up before creating new one
      shell: |
        echo "Cleaning up any existing bootstrap tokens..."
        kubeadm token list | awk 'NR>1 {print $1}' | xargs -r -I {} kubeadm token delete {} || true
      ignore_errors: yes

    - name: Get join command with fresh token (24h TTL)
      shell: kubeadm token create --print-join-command --ttl 24h
      register: join_command
      retries: 3
      delay: 5
      until: join_command.rc == 0

    - name: Validate join command contains required components
      fail:
        msg: "Generated join command is invalid or incomplete: {{ join_command.stdout }}"
      when: not (join_command.stdout | regex_search('kubeadm join.*--token.*--discovery-token-ca-cert-hash'))

    - name: Save join command to file with timestamp
      copy:
        content: |
          #!/bin/bash
          # Generated: {{ ansible_date_time.iso8601 }}
          # Valid for 24 hours from generation time
          # Control plane: {{ inventory_hostname }}:6443
          {{ join_command.stdout }}
        dest: /tmp/kubeadm-join.sh
        mode: '0755'

    - name: Fetch join command to local machine
      fetch:
        src: /tmp/kubeadm-join.sh
        dest: /tmp/kubeadm-join.sh
        flat: yes

# Join worker nodes
- name: Join worker nodes to cluster
  hosts: storage_nodes:compute_nodes
  become: true
  tasks:
    - name: Check if node is already joined
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: Validate kubelet.conf if it exists (enhanced post-spindown recovery check)
      block:
        - name: Test kubelet.conf validity and cluster connectivity
          shell: |
            if [ -f /etc/kubernetes/kubelet.conf ]; then
              # Check if the kubeconfig contains cluster server info
              if grep -q "server:" /etc/kubernetes/kubelet.conf && grep -q "certificate-authority-data:" /etc/kubernetes/kubelet.conf; then
                # Additional check: try to use the kubeconfig to verify connectivity
                if timeout 10s kubectl --kubeconfig=/etc/kubernetes/kubelet.conf cluster-info >/dev/null 2>&1; then
                  echo "valid-and-connected"
                else
                  echo "valid-but-disconnected"
                fi
              else
                echo "invalid"
              fi
            else
              echo "missing"
            fi
          register: kubelet_conf_validation
          changed_when: false

        - name: Force rejoin if kubelet.conf is invalid, disconnected, or references old cluster
          set_fact:
            kubelet_conf: 
              stat:
                exists: false
            force_rejoin_reason: "{{ kubelet_conf_validation.stdout }}"
          when: kubelet_conf_validation.stdout != "valid-and-connected"

        - name: Display kubelet.conf validation result
          debug:
            msg: |
              Kubelet configuration validation on {{ inventory_hostname }}:
              - File exists: {{ kubelet_conf.stat.exists }}
              - Validation result: {{ kubelet_conf_validation.stdout }}
              {% if kubelet_conf_validation.stdout != "valid-and-connected" %}
              - Action: Will trigger rejoin process (reason: {{ force_rejoin_reason | default('unknown') }})
              {% else %}
              - Action: Node appears properly joined, skipping rejoin process
              {% endif %}

      when: kubelet_conf.stat.exists

    - name: Clean up stale CNI state on worker nodes (prevent cert-manager conflicts)
      block:
        - name: Stop and disable kubelet if running (to prevent CNI conflicts)
          systemd:
            name: kubelet
            state: stopped
            enabled: no
          ignore_errors: yes

        - name: Remove any existing CNI network interfaces (cni0, cbr0, flannel.1)
          shell: |
            # Remove cni0 interface if it exists
            if ip link show cni0 2>/dev/null; then
              echo "Removing existing cni0 interface"
              ip link set cni0 down || true
              ip link delete cni0 || true
            fi
            
            # Remove cbr0 interface if it exists  
            if ip link show cbr0 2>/dev/null; then
              echo "Removing existing cbr0 interface"
              ip link set cbr0 down || true
              ip link delete cbr0 || true
            fi
            
            # Remove flannel.1 interface if it exists
            if ip link show flannel.1 2>/dev/null; then
              echo "Removing existing flannel.1 interface"
              ip link set flannel.1 down || true
              ip link delete flannel.1 || true
            fi
          ignore_errors: yes
          register: cni_cleanup_result

        - name: Clear existing CNI configuration files (preserve directory structure)
          shell: |
            # Remove any existing CNI configuration but preserve directories
            rm -rf /etc/cni/net.d/* || true
            rm -f /opt/cni/bin/flannel || true
            # Clear CNI plugin cache but preserve directories
            rm -rf /var/lib/cni/networks/* || true
            rm -rf /var/lib/cni/results/* || true
            # Ensure CNI directories exist with proper permissions after cleanup
            mkdir -p /opt/cni/bin /etc/cni/net.d /var/lib/cni/networks /var/lib/cni/results || true
            chmod 755 /opt/cni/bin /etc/cni/net.d /var/lib/cni/networks /var/lib/cni/results || true
            chown root:root /opt/cni/bin /etc/cni/net.d /var/lib/cni/networks /var/lib/cni/results || true
          ignore_errors: yes

        - name: Clear kubelet CNI state
          shell: |
            # Remove any existing kubelet CNI state
            rm -rf /var/lib/kubelet/pods/* || true
            rm -rf /var/lib/kubelet/plugins_registry/* || true
          ignore_errors: yes

        - name: Display CNI cleanup results
          debug:
            msg: |
              CNI cleanup completed on {{ inventory_hostname }}:
              {{ cni_cleanup_result.stdout_lines | default(['No interfaces to remove']) | join('\n') }}
      
      when: not kubelet_conf.stat.exists

    - name: Install CNI plugins and configuration on worker nodes (required for kubelet)
      block:
        - name: Create CNI directories on worker nodes
          file:
            path: "{{ item }}"
            state: directory
            owner: root
            group: root
            mode: '0755'
          loop:
            - /opt/cni/bin
            - /etc/cni/net.d
            - /var/lib/cni/networks
            - /var/lib/cni/results

        - name: Validate CNI directory permissions before download
          shell: |
            # Verify directories exist and are writable
            for dir in /opt/cni/bin /etc/cni/net.d /var/lib/cni/networks /var/lib/cni/results; do
              if [ ! -d "$dir" ]; then
                echo "Creating missing directory: $dir"
                mkdir -p "$dir" || exit 1
                chmod 755 "$dir" || exit 1
                chown root:root "$dir" || exit 1
              fi
              if [ ! -w "$dir" ]; then
                echo "Fixing permissions for directory: $dir"
                chmod 755 "$dir" || exit 1
                chown root:root "$dir" || exit 1
              fi
              echo "Directory $dir is ready (permissions: $(stat -c '%a' "$dir"), owner: $(stat -c '%U:%G' "$dir"))"
            done
          register: cni_dir_validation
          failed_when: false

        - name: Display CNI directory validation results
          debug:
            var: cni_dir_validation.stdout_lines

        - name: Set Flannel CNI destination path per node type
          set_fact:
            flannel_cni_dest: /opt/cni/bin/flannel

        - name: Pre-download validation for Flannel CNI binary
          shell: |
            # Ensure target directory exists and is writable
            target_dir="$(dirname "{{ flannel_cni_dest }}")"
            if [ ! -d "$target_dir" ]; then
              echo "Creating target directory: $target_dir"
              mkdir -p "$target_dir" || exit 1
              chmod 755 "$target_dir" || exit 1
              chown root:root "$target_dir" || exit 1
            fi
            
            if [ ! -w "$target_dir" ]; then
              echo "Target directory not writable, fixing permissions: $target_dir"
              chmod 755 "$target_dir" || exit 1
              chown root:root "$target_dir" || exit 1
            fi
            
            # Test write access by creating a temporary file
            test_file="$target_dir/.write_test_$$"
            if echo "test" > "$test_file" 2>/dev/null; then
              rm -f "$test_file"
              echo "Target directory is writable: $target_dir"
            else
              echo "ERROR: Cannot write to target directory: $target_dir"
              ls -la "$target_dir" || true
              exit 1
            fi
          register: flannel_pre_download_check
          failed_when: flannel_pre_download_check.rc != 0

        - name: Ensure download tools are available for Flannel installation
          package:
            name: 
              - curl
              - wget
            state: present
          ignore_errors: yes
          register: download_tools_install

        - name: Download and install Flannel CNI plugin binary (primary method)
          get_url:
            url: "https://github.com/flannel-io/flannel/releases/download/v0.25.2/flanneld-amd64"
            dest: "{{ flannel_cni_dest }}"
            mode: '0755'
            owner: root
            group: root
            timeout: 60
            validate_certs: false
            use_proxy: false
          register: flannel_download_primary
          retries: 3
          delay: 10
          until: flannel_download_primary is succeeded
          failed_when: false

        - name: Download Flannel CNI plugin binary using curl fallback (first fallback)
          shell: |
            curl -fsSL --connect-timeout 30 --max-time 300 --retry 3 --retry-delay 5 \
              "https://github.com/flannel-io/flannel/releases/download/v0.25.2/flanneld-amd64" \
              -o "{{ flannel_cni_dest }}"
            chmod 0755 "{{ flannel_cni_dest }}"
            chown root:root "{{ flannel_cni_dest }}"
          args:
            creates: "{{ flannel_cni_dest }}"
          when: flannel_download_primary is failed
          register: flannel_download_curl
          failed_when: false
          retries: 2
          delay: 15

        - name: Download Flannel CNI plugin binary using wget fallback (second fallback)
          shell: |
            wget --connect-timeout=30 --read-timeout=300 --tries=3 --waitretry=5 \
              -q -O "{{ flannel_cni_dest }}" \
              "https://github.com/flannel-io/flannel/releases/download/v0.25.2/flanneld-amd64"
            chmod 0755 "{{ flannel_cni_dest }}"
            chown root:root "{{ flannel_cni_dest }}"
          args:
            creates: "{{ flannel_cni_dest }}"
          when: 
            - flannel_download_primary is failed
            - flannel_download_curl is failed or flannel_download_curl is skipped
          register: flannel_download_wget
          failed_when: false
          retries: 2
          delay: 15

        - name: Alternative Flannel binary source (third fallback - different version if needed)
          shell: |
            # Try older version if current version fails
            if command -v curl >/dev/null 2>&1; then
              curl -fsSL --connect-timeout 30 --max-time 300 --retry 2 \
                "https://github.com/flannel-io/flannel/releases/download/v0.24.2/flanneld-amd64" \
                -o "{{ flannel_cni_dest }}" || \
              curl -fsSL --connect-timeout 30 --max-time 300 --retry 2 \
                "https://github.com/flannel-io/flannel/releases/download/v0.23.0/flanneld-amd64" \
                -o "{{ flannel_cni_dest }}"
            elif command -v wget >/dev/null 2>&1; then
              wget --connect-timeout=30 --read-timeout=300 --tries=2 \
                -q -O "{{ flannel_cni_dest }}" \
                "https://github.com/flannel-io/flannel/releases/download/v0.24.2/flanneld-amd64" || \
              wget --connect-timeout=30 --read-timeout=300 --tries=2 \
                -q -O "{{ flannel_cni_dest }}" \
                "https://github.com/flannel-io/flannel/releases/download/v0.23.0/flanneld-amd64"
            fi
            chmod 0755 "{{ flannel_cni_dest }}"
            chown root:root "{{ flannel_cni_dest }}"
          args:
            creates: "{{ flannel_cni_dest }}"
          when: 
            - flannel_download_primary is failed
            - flannel_download_curl is failed or flannel_download_curl is skipped
            - flannel_download_wget is failed or flannel_download_wget is skipped
          register: flannel_download_alt
          failed_when: false

        - name: Verify Flannel CNI plugin binary was downloaded successfully
          stat:
            path: "{{ flannel_cni_dest }}"
          register: flannel_binary_check

        - name: Collect Flannel download diagnostics if verification fails
          block:
            - name: Show download attempt results
              debug:
                msg: |
                  Flannel download diagnostics for {{ inventory_hostname }}:
                  Primary method: {{ 'SUCCESS' if flannel_download_primary is succeeded else 'FAILED - ' + (flannel_download_primary.msg | default('Unknown error')) }}
                  Curl fallback: {{ 'SUCCESS' if flannel_download_curl is succeeded else ('FAILED - ' + (flannel_download_curl.stderr | default('Unknown error'))) if flannel_download_curl is not skipped else 'SKIPPED' }}
                  Wget fallback: {{ 'SUCCESS' if flannel_download_wget is succeeded else ('FAILED - ' + (flannel_download_wget.stderr | default('Unknown error'))) if flannel_download_wget is not skipped else 'SKIPPED' }}
                  Alt version: {{ 'SUCCESS' if flannel_download_alt is succeeded else ('FAILED - ' + (flannel_download_alt.stderr | default('Unknown error'))) if flannel_download_alt is not skipped else 'SKIPPED' }}
                  
                  Network connectivity test:
            
            - name: Test network connectivity to GitHub
              uri:
                url: "https://github.com"
                method: HEAD
                timeout: 30
                validate_certs: false
              register: github_connectivity
              failed_when: false
              
            - name: Test DNS resolution
              shell: nslookup github.com || true
              register: dns_test
              changed_when: false
              
            - name: Show network diagnostics
              debug:
                msg: |
                  GitHub connectivity: {{ 'OK' if github_connectivity.status is defined and github_connectivity.status == 200 else 'FAILED' }}
                  DNS resolution: {{ 'OK' if 'github.com' in dns_test.stdout else 'FAILED' }}
                  Available tools: {{ 'curl' if ansible_facts['packages']['curl'] is defined else 'no curl' }}, {{ 'wget' if ansible_facts['packages']['wget'] is defined else 'no wget' }}
                  
            - name: List current /opt/cni/bin contents
              shell: ls -la /opt/cni/bin/ || echo "Directory does not exist"
              register: cni_dir_contents
              changed_when: false
              
            - name: Check directory permissions and disk space
              shell: |
                echo "=== Directory Permissions ==="
                ls -la /opt/cni/ || echo "/opt/cni not found"
                ls -la /opt/cni/bin/ || echo "/opt/cni/bin not found"
                echo "=== Directory Ownership ==="
                stat /opt/cni/bin/ || echo "/opt/cni/bin stat failed"
                echo "=== Disk Space ==="
                df -h /opt/cni/bin/ || echo "df failed"
                echo "=== Write Test ==="
                touch /opt/cni/bin/.write_test && rm -f /opt/cni/bin/.write_test && echo "Write test: PASSED" || echo "Write test: FAILED"
              register: dir_diagnostics
              changed_when: false
              
            - name: Show CNI directory contents
              debug:
                var: cni_dir_contents.stdout_lines
                
            - name: Show directory permission diagnostics
              debug:
                var: dir_diagnostics.stdout_lines
                
          when: not flannel_binary_check.stat.exists
          
        - name: Fail if Flannel binary still missing after all attempts
          fail:
            msg: |
              Failed to download Flannel CNI plugin binary to {{ flannel_cni_dest }} on {{ inventory_hostname }}.
              All download methods failed. Check network connectivity and GitHub access.
              Manual installation may be required.
              
              Troubleshooting steps:
              1. Verify target directory permissions: ls -la $(dirname {{ flannel_cni_dest }})
              2. Check available disk space: df -h $(dirname {{ flannel_cni_dest }})
              3. Test network connectivity: curl -I https://github.com
              4. Manual installation command:
                 ssh {{ inventory_hostname }} 'curl -fsSL https://github.com/flannel-io/flannel/releases/download/v0.25.2/flanneld-amd64 -o {{ flannel_cni_dest }} && chmod 755 {{ flannel_cni_dest }}'
              
              Common causes:
              - Directory permissions issues (ensure /opt/cni/bin is writable by root)
              - Insufficient disk space
              - Network connectivity problems
              - SELinux or security policies blocking downloads
          when: not flannel_binary_check.stat.exists

        - name: Ensure Flannel CNI plugin binary permissions are correct
          file:
            path: "{{ flannel_cni_dest }}"
            mode: '0755'
            owner: root
            group: root
          when: flannel_binary_check.stat.exists

        - name: Download and install additional CNI plugins on worker nodes
          unarchive:
            src: "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz"
            dest: /opt/cni/bin
            remote_src: yes
            owner: root
            group: root
            mode: '0755'
            creates: /opt/cni/bin/bridge
          register: cni_plugins_download
          retries: 3
          delay: 10
          until: cni_plugins_download is succeeded

        - name: Ensure /run/flannel directory exists on worker nodes
          file:
            path: /run/flannel
            state: directory
            owner: root
            group: root
            mode: '0755'

        - name: Create Flannel subnet configuration for worker nodes
          copy:
            content: |
              {
                "Network": "10.244.0.0/16",
                "EnableNFTables": false,
                "Backend": {
                  "Type": "vxlan"
                }
              }
            dest: /run/flannel/subnet.env
            owner: root
            group: root
            mode: '0644'

        - name: Create basic CNI configuration for worker nodes
          copy:
            content: |
              {
                "name": "cni0",
                "cniVersion": "0.3.1",
                "plugins": [
                  {
                    "type": "flannel",
                    "delegate": {
                      "hairpinMode": true,
                      "isDefaultGateway": true
                    }
                  },
                  {
                    "type": "portmap",
                    "capabilities": {
                      "portMappings": true
                    }
                  }
                ]
              }
            dest: /etc/cni/net.d/10-flannel.conflist
            owner: root
            group: root
            mode: '0644'

        - name: Verify CNI plugin installation on worker nodes
          stat:
            path: "{{ item }}"
          register: cni_files_check
          failed_when: not cni_files_check.stat.exists
          loop:
            - /opt/cni/bin/flannel
            - /opt/cni/bin/bridge
            - /opt/cni/bin/portmap
            - /etc/cni/net.d/10-flannel.conflist

        - name: Display CNI installation completion
          debug:
            msg: |
               CNI plugins and configuration installed on worker node {{ inventory_hostname }}
              - Flannel CNI plugin: /opt/cni/bin/flannel
              - Standard CNI plugins: /opt/cni/bin/
              - CNI configuration: /etc/cni/net.d/10-flannel.conflist
              
              Worker nodes now have the necessary CNI infrastructure without running Flannel daemon.

      when: not kubelet_conf.stat.exists

    - name: Pre-join validation for worker nodes
      block:
        - name: Verify containerd is running
          systemd:
            name: containerd
          register: containerd_status
          failed_when: containerd_status.status.ActiveState != 'active'
          
        - name: Verify kubelet binary exists and is executable
          stat:
            path: /usr/bin/kubelet
          register: kubelet_binary
          failed_when: not kubelet_binary.stat.exists or not kubelet_binary.stat.executable
          
        - name: Verify kubeadm binary exists and is executable
          stat:
            path: /usr/bin/kubeadm
          register: kubeadm_binary
          failed_when: not kubeadm_binary.stat.exists or not kubeadm_binary.stat.executable
          
        - name: Test kubeadm preflight checks (non-fatal; requires bootstrap token on control plane)
          command: kubeadm join phase preflight --dry-run || true
          register: preflight_check
          changed_when: false
          failed_when: false
          ignore_errors: yes
          
        - name: Display preflight results
          debug:
            msg: "Preflight check result: {{ preflight_check.rc }} - {{ preflight_check.stderr if preflight_check.rc != 0 else 'PASSED' }}"
            
        - name: Verify required kernel modules are loaded
          command: lsmod
          register: loaded_modules
          changed_when: false
          
        - name: Check for required kernel modules
          fail:
            msg: "Required kernel module {{ item }} is not loaded"
          when: item not in loaded_modules.stdout
          loop:
            - br_netfilter
            - overlay
          ignore_errors: yes
          
        - name: Check container runtime socket
          stat:
            path: /var/run/containerd/containerd.sock
          register: containerd_socket
          failed_when: not containerd_socket.stat.exists
          
        - name: Test container runtime connectivity
          command: crictl version
          register: crictl_test
          changed_when: false
          ignore_errors: yes
          
        - name: Display container runtime test
          debug:
            msg: "Container runtime test: {{ 'PASSED' if crictl_test.rc == 0 else 'FAILED - ' + crictl_test.stderr }}"
            
      when: not kubelet_conf.stat.exists

    - name: Validate join command freshness and content before use
      block:
        - name: Check if join command file exists and is recent
          stat:
            path: /tmp/kubeadm-join.sh
          register: join_file_stat
          delegate_to: localhost

        - name: Validate join command age and content
          shell: |
            if [ ! -f /tmp/kubeadm-join.sh ]; then
              echo "missing"
              exit 0
            fi
            
            # Check if file is older than 23 hours (tokens expire in 24h)
            if [ $(find /tmp/kubeadm-join.sh -mmin +1380 | wc -l) -gt 0 ]; then
              echo "expired"
              exit 0
            fi
            
            # Check if the join command contains required elements
            if grep -q "kubeadm join.*--token.*--discovery-token-ca-cert-hash" /tmp/kubeadm-join.sh; then
              echo "valid"
            else
              echo "malformed"
            fi
          register: join_command_validation
          delegate_to: localhost
          changed_when: false

        - name: Fail if join command is not available or expired
          fail:
            msg: |
              Join command issue detected: {{ join_command_validation.stdout }}
              - missing: Join command file not found
              - expired: Join command is older than 23 hours (tokens expire in 24h)
              - malformed: Join command doesn't contain required token/hash elements
              
              To resolve:
              1. Ensure the control plane is running
              2. Re-run the setup_cluster playbook to generate a fresh join command
              3. Or manually run: kubeadm token create --print-join-command on control plane
          when: join_command_validation.stdout != "valid"

      when: not kubelet_conf.stat.exists

    - name: Copy join command to worker nodes
      copy:
        src: /tmp/kubeadm-join.sh
        dest: /tmp/kubeadm-join.sh
        mode: '0755'
      when: not kubelet_conf.stat.exists

    - name: Backup existing /etc/kubernetes if ca.crt exists to avoid kubeadm preflight error
      shell: |
        if [ -f /etc/kubernetes/pki/ca.crt ]; then
          ts=$(date +%s)
          mv /etc/kubernetes /etc/kubernetes.backup.$ts || true
          echo "backed_up=/etc/kubernetes.backup.$ts"
        else
          echo "backed_up=none"
        fi
      register: backup_k8s
      changed_when: "'backed_up=' in backup_k8s.stdout and 'none' not in backup_k8s.stdout"
      failed_when: false
      when: not kubelet_conf.stat.exists

    - name: Prepare kubelet service for cluster join
      block:
        - name: Enable kubelet service before cluster join
          systemd:
            name: kubelet
            enabled: yes
            daemon_reload: yes
          
        - name: Stop kubelet service if running (to prevent port conflicts)
          systemd:
            name: kubelet
            state: stopped
          ignore_errors: yes

      when: not kubelet_conf.stat.exists

    - name: Clear kubelet config.yaml before join to allow kubeadm to create its own
      file:
        path: /var/lib/kubelet/config.yaml
        state: absent
      when: not kubelet_conf.stat.exists

    - name: Clean up any stale join state (post-spindown recovery)
      shell: |
        # Remove any leftover join artifacts that might conflict
        rm -f /etc/kubernetes/kubelet.conf.backup.* 2>/dev/null || true
        rm -f /var/lib/kubelet/kubeadm-flags.env 2>/dev/null || true
        
        # If this was triggered by invalid kubelet.conf, remove it to force fresh join
        if [ "{{ force_rejoin_reason | default('') }}" != "" ]; then
          echo "Removing invalid kubelet.conf (reason: {{ force_rejoin_reason | default('unknown') }})"
          rm -f /etc/kubernetes/kubelet.conf || true
          rm -f /etc/kubernetes/pki/ca.crt || true
        fi
        
        echo "Pre-join cleanup completed"
      when: not kubelet_conf.stat.exists
      changed_when: true
      
    - name: Join worker nodes to cluster with enhanced timeout handling
      block:
        - name: Pre-join kubelet readiness verification
          block:
            - name: Ensure kubelet service is stopped before join
              systemd:
                name: kubelet
                state: stopped
              ignore_errors: yes
              
            - name: Verify containerd socket is available
              wait_for:
                path: /var/run/containerd/containerd.sock
                timeout: 30
                
            - name: Test kubelet can start with current configuration
              shell: |
                # Try to start kubelet briefly to test configuration
                timeout 10 kubelet --config=/var/lib/kubelet/config.yaml --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf || true
              register: kubelet_test_start
              ignore_errors: yes
              failed_when: false
              
          when: not kubelet_conf.stat.exists
          
        - name: Attempt to join cluster (primary attempt with extended timeout)
          shell: timeout 600 /tmp/kubeadm-join.sh --v=5
          register: join_result_1
          failed_when: false
          when: not kubelet_conf.stat.exists
          
        - name: Wait and retry if first attempt failed or timed out
          block:
            - name: Analyze first attempt failure
              debug:
                msg: |
                  First join attempt failed:
                  Return Code: {{ join_result_1.rc | default('unknown') }}
                  Stderr: {{ join_result_1.stderr | default('no stderr') | truncate(500) }}
                  Stdout: {{ join_result_1.stdout | default('no stdout') | truncate(300) }}
                  
            - name: Check kubelet status for diagnostic information
              shell: |
                echo "=== Kubelet Status ==="
                systemctl status kubelet --no-pager -l || true
                echo "=== Kubelet Logs (last 10 lines) ==="
                journalctl -u kubelet --no-pager -l -n 10 || true
                echo "=== Kubelet Health Endpoint ==="
                curl -s http://localhost:10248/healthz || echo "Health endpoint not available"
              register: kubelet_diagnostic
              ignore_errors: yes
              
            - name: Display kubelet diagnostic information  
              debug:
                msg: "{{ kubelet_diagnostic.stdout }}"
              when: kubelet_diagnostic.stdout is defined
                  
            - name: Wait before retry (extended for timeout recovery)
              pause:
                seconds: 60
                
            - name: Reset any partial join state cleanly
              shell: |
                # Stop services gracefully
                systemctl stop kubelet || true
                # Reset kubeadm state without removing containerd setup
                kubeadm reset -f || true
                # Only remove kubernetes-specific configs, preserve containerd
                rm -rf /etc/kubernetes/kubelet.conf /etc/kubernetes/pki/ || true
                # Clear kubelet state but preserve systemd config
                rm -rf /var/lib/kubelet/config.yaml /var/lib/kubelet/kubeadm-flags.env || true
                # Remove any existing sysconfig/kubelet that might have deprecated flags
                rm -f /etc/sysconfig/kubelet || true
              ignore_errors: yes
              
            - name: Verify containerd is healthy before retry
              systemd:
                name: containerd
                state: restarted
                
            - name: Wait for containerd socket
              wait_for:
                path: /var/run/containerd/containerd.sock
                timeout: 30

            - name: Ensure /etc/sysconfig directory exists for retry
              file:
                path: /etc/sysconfig
                state: directory
                mode: '0755'
                
            - name: Recreate clean sysconfig/kubelet for retry attempt
              copy:
                dest: /etc/sysconfig/kubelet
                content: |
                  # Kubelet environment - no deprecated flags like --network-plugin
                  KUBELET_EXTRA_ARGS=--cgroup-driver=systemd --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock
                mode: '0644'
                
            - name: Attempt to join cluster (retry with extended timeout)
              shell: timeout 900 /tmp/kubeadm-join.sh --v=5
              register: join_result_2
              failed_when: false
              
          when: join_result_1 is defined and (join_result_1.rc | default(0)) != 0
          
        - name: Set final join result
          set_fact:
            join_result: "{{ join_result_2 if join_result_2 is defined else join_result_1 }}"
            
      when: not kubelet_conf.stat.exists

    - name: Enhanced error collection and diagnostics
      block:
        - name: Extract join target from script (for connectivity checks)
          shell: "awk '{print $3}' /tmp/kubeadm-join.sh"
          register: join_target
          changed_when: false
          failed_when: false
          
        - name: Test API server reachability from node
          uri:
            url: "https://{{ join_target.stdout }}/healthz"
            validate_certs: no
            timeout: 10
          register: api_health
          failed_when: false
          changed_when: false
          
        - name: Test basic network connectivity to control plane
          command: "ping -c 3 {{ join_target.stdout.split(':')[0] }}"
          register: ping_test
          failed_when: false
          changed_when: false
          
        - name: Collect enhanced system diagnostics
          shell: |
            echo "=== System Information ==="
            uname -a
            echo "=== Container Runtime Status ==="
            systemctl status containerd --no-pager || true
            crictl version || true
            crictl info || true
            echo "=== Kubelet Status ==="
            systemctl status kubelet --no-pager || true
            echo "=== Network Configuration ==="
            ip addr show || true
            ip route show || true
            echo "=== Firewall Status ==="
            if command -v firewall-cmd >/dev/null; then
              firewall-cmd --list-all || true
            fi
            if command -v iptables >/dev/null; then
              iptables -L -n || true
            fi
            echo "=== SELinux Status ==="
            if command -v getenforce >/dev/null; then
              getenforce || true
            fi
            echo "=== Kernel Modules ==="
            lsmod | grep -E "(br_netfilter|overlay)" || true
            echo "=== Disk Space ==="
            df -h || true
            echo "=== Memory ==="
            free -m || true
          register: system_diagnostics
          changed_when: false
          failed_when: false
          
        - name: Save enhanced diagnostics to file
          copy:
            content: |
              Join Result: {{ join_result }}
              API Health: {{ api_health }}
              Ping Test: {{ ping_test }}
              System Diagnostics:
              {{ system_diagnostics.stdout }}
            dest: /tmp/join-diagnostics.log
            
        - name: Save kubelet journal to a file for fetch
          shell: "journalctl -u kubelet -n 500 --no-pager > /tmp/kubelet-journal.log || true"
          changed_when: false
          
        - name: Save containerd journal to a file for fetch
          shell: "journalctl -u containerd -n 500 --no-pager > /tmp/containerd-journal.log || true"
          changed_when: false
          
        - name: Save kubeadm audit logs if they exist
          shell: "find /var/log -name '*kubeadm*' -exec cat {} \\; > /tmp/kubeadm-audit.log 2>/dev/null || echo 'No kubeadm logs found'"
          changed_when: false
          
        - name: Ensure debug logs directory exists locally
          file:
            path: ./debug_logs
            state: directory
          delegate_to: localhost
          become: no
          
        - name: Fetch comprehensive diagnostic logs
          fetch:
            src: "{{ item.src }}"
            dest: "{{ item.dest }}"
            flat: yes
          loop:
            - { src: "/tmp/join-diagnostics.log", dest: "./debug_logs/{{ inventory_hostname }}-join-diagnostics.log" }
            - { src: "/tmp/kubelet-journal.log", dest: "./debug_logs/{{ inventory_hostname }}-kubelet-journal.log" }
            - { src: "/tmp/containerd-journal.log", dest: "./debug_logs/{{ inventory_hostname }}-containerd-journal.log" }
            - { src: "/tmp/kubeadm-audit.log", dest: "./debug_logs/{{ inventory_hostname }}-kubeadm-audit.log" }
          ignore_errors: yes
      when: not kubelet_conf.stat.exists and (join_result is defined and (join_result.rc | default(0)) != 0)

    - name: Post-join verification
      block:
        - name: Verify node joined successfully
          stat:
            path: /etc/kubernetes/kubelet.conf
          register: final_kubelet_conf

        - name: Wait for kubelet to be ready
          systemd:
            name: kubelet
          register: kubelet_final_status
          retries: 12
          delay: 15
          until: kubelet_final_status.status.ActiveState == 'active'
          
        - name: Display join success message
          debug:
            msg: " Node {{ inventory_hostname }} successfully joined the Kubernetes cluster!"
          when: final_kubelet_conf.stat.exists
          
        - name: Display join failure message
          fail:
            msg: " Node {{ inventory_hostname }} failed to join the cluster. Check debug logs for details."
          when: not final_kubelet_conf.stat.exists and (join_result is defined and (join_result.rc | default(0)) != 0)
          
      when: not kubelet_conf.stat.exists

    - name: Verify worker node kubelet configuration after join
      block:
        - name: Check if kubelet config file exists after join
          stat:
            path: /var/lib/kubelet/config.yaml
          register: worker_kubelet_config_check

        - name: Check if worker kubelet kubeconfig exists
          stat:
            path: /etc/kubernetes/kubelet.conf
          register: worker_kubelet_kubeconfig_check

        - name: Verify kubelet is using kubeadm-generated configuration after join
          block:
            - name: Check that kubeadm created the necessary files
              stat:
                path: "{{ item }}"
              register: kubeadm_files
              failed_when: not kubeadm_files.stat.exists
              loop:
                - /etc/kubernetes/kubelet.conf
                - /var/lib/kubelet/config.yaml
                - /var/lib/kubelet/kubeadm-flags.env
              
            - name: Update kubelet systemd config after successful join
              copy:
                dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
                content: |
                  # Note: This dropin only works with kubeadm and kubelet v1.11+
                  [Service]
                  Environment="KUBELET_KUBECONFIG_ARGS=--kubeconfig=/etc/kubernetes/kubelet.conf"
                  Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
                  # This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
                  EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
                  # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
                  # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
                  EnvironmentFile=-/etc/sysconfig/kubelet
                  ExecStart=
                  ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
                  # Post-join config - no bootstrap config dependency
              register: post_join_config_update
              when: worker_kubelet_kubeconfig_check.stat.exists

        - name: Reload systemd daemon after kubelet config update
          systemd:
            daemon_reload: yes
          when: post_join_config_update is changed

        - name: Restart and enable kubelet on worker (only if join was successful)
          systemd:
            name: kubelet
            state: restarted
            enabled: yes
          when: "'kubelet.service' in ansible_facts.services"
          register: worker_kubelet_restart
          ignore_errors: yes

        - name: Handle worker kubelet startup failure (simplified recovery)
          block:
            - name: Check if worker kubelet kubeconfig is missing (post-spindown recovery)
              stat:
                path: /etc/kubernetes/kubelet.conf
              register: worker_kubelet_kubeconfig_missing

            - name: Display worker recovery status
              debug:
                msg: |
                  Worker kubelet recovery needed: {{ worker_kubelet_kubeconfig_missing.stat.exists == false }}
                  {% if worker_kubelet_kubeconfig_missing.stat.exists == false %}
                  Action required: Worker node needs to rejoin the cluster
                  The node must be reset and rejoined using kubeadm join command
                  {% endif %}

            - name: Set rejoin flag if kubeconfig is missing
              set_fact:
                worker_needs_rejoin: true
              when: worker_kubelet_kubeconfig_missing.stat.exists == false

          when: worker_kubelet_restart is defined and worker_kubelet_restart.failed

        - name: Worker kubelet diagnostics if restart failed
          shell: |
            echo "=== Worker Node Kubelet Diagnostics ==="
            echo "System: $(uname -a)"
            echo "Kubelet status:"
            systemctl status kubelet --no-pager || true
            echo "Recent kubelet logs:"
            journalctl -u kubelet -n 20 --no-pager || true
            echo "Container runtime status:"
            systemctl status containerd --no-pager || true
            echo "Kubelet configuration files:"
            ls -la /var/lib/kubelet/ || true
            ls -la /etc/kubernetes/ || true
          register: worker_diagnostics
          ignore_errors: yes
          when: (worker_kubelet_restart is defined and (worker_kubelet_restart.failed | default(false))) or (worker_kubelet_restart_retry is defined and (worker_kubelet_restart_retry.failed | default(false)))

        - name: Display worker kubelet diagnostics
          debug:
            var: worker_diagnostics.stdout_lines
          when: (worker_kubelet_restart is defined and (worker_kubelet_restart.failed | default(false))) or (worker_kubelet_restart_retry is defined and (worker_kubelet_restart_retry.failed | default(false)))

      when: kubelet_conf.stat.exists or (join_result is defined and (join_result.rc | default(0)) == 0)

    - name: Setup static manifests directory on worker nodes (for cert-manager compatibility)
      block:
        - name: Create /etc/kubernetes/manifests directory on worker nodes
          file:
            path: /etc/kubernetes/manifests
            state: directory
            owner: root
            group: root
            mode: '0755'
          
        - name: Display manifests directory setup completion
          debug:
            msg: " Created /etc/kubernetes/manifests directory on worker node {{ inventory_hostname }} for cert-manager compatibility"
            
      when: kubelet_conf.stat.exists or (join_result is defined and (join_result.rc | default(0)) == 0)

    - name: Remove join command file
      file:
        path: /tmp/kubeadm-join.sh
        state: absent
