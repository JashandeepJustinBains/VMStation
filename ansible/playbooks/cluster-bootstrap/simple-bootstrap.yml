---
# VMStation Kubernetes Cluster Bootstrap - Simple Version
# Alternative to the complex setup-cluster.yaml for basic kubeadm deployment

- name: "System Preparation for All Nodes"
  hosts: all
  become: true
  gather_facts: true
  roles:
    - system-prep
  tasks:
    - name: "Include existing comprehensive setup tasks"
      include_tasks: ../../plays/setup-cluster.yaml
      when: use_comprehensive_setup | default(false)

- name: "Initialize Kubernetes Control Plane"
  hosts: monitoring_nodes
  become: true
  vars:
    kubeadm_config_path: "/etc/kubernetes/kubeadm-config.yaml"
    kubeconfig_path: "/etc/kubernetes/admin.conf"
    join_command_file: "/tmp/kubeadm-join-command.sh"
  tasks:
    - name: "Check if cluster is already initialized"
      stat:
        path: "{{ kubeconfig_path }}"
      register: kubeconfig_exists

    - name: "Generate kubeadm configuration from template"
      template:
        src: "{{ playbook_dir }}/../../manifests/kubeadm-config.yaml.j2"
        dest: "{{ kubeadm_config_path }}"
        owner: root
        group: root
        mode: '0600'
      when: not kubeconfig_exists.stat.exists

    - name: "Initialize Kubernetes control plane with kubeadm"
      shell: |
        kubeadm init --config={{ kubeadm_config_path }} --upload-certs
      register: kubeadm_init_result
      when: not kubeconfig_exists.stat.exists
      failed_when: kubeadm_init_result.rc != 0

    - name: "Setup kubeconfig for root user"
      block:
        - name: "Create .kube directory"
          file:
            path: /root/.kube
            state: directory
            owner: root
            group: root
            mode: '0755'

        - name: "Copy kubeconfig"
          copy:
            src: "{{ kubeconfig_path }}"
            dest: /root/.kube/config
            owner: root
            group: root
            mode: '0600'
            remote_src: true

    - name: "Generate and save join command"
      block:
        - name: "Generate join command for worker nodes"
          shell: kubeadm token create --print-join-command
          register: join_command_output

        - name: "Save join command to file with secure permissions"
          copy:
            content: "{{ join_command_output.stdout }}"
            dest: "{{ join_command_file }}"
            owner: root
            group: root
            mode: '0600'

        - name: "Fetch join command for distribution to workers"
          fetch:
            src: "{{ join_command_file }}"
            dest: "/tmp/kubeadm-join-command-{{ inventory_hostname }}.sh"
            flat: yes

    - name: "Deploy CNI (Flannel)"
      kubernetes.core.k8s:
        state: present
        src: "{{ playbook_dir }}/../../manifests/cni/flannel.yaml"
        kubeconfig: "{{ kubeconfig_path }}"
      retries: 3
      delay: 10

    - name: "Wait for CoreDNS to be ready"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: kube-system
        label_selectors:
          - k8s-app=kube-dns
        kubeconfig: "{{ kubeconfig_path }}"
      register: coredns_pods
      until: 
        - coredns_pods.resources | length > 0
        - coredns_pods.resources | map(attribute='status.phase') | list | unique == ['Running']
      retries: 30
      delay: 10

- name: "Join Worker Nodes to Cluster"
  hosts: storage_nodes:compute_nodes
  become: true
  serial: 1
  vars:
    join_command_file: "/tmp/kubeadm-join-command.sh"
  tasks:
    - name: "Check if node is already joined"
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf_exists

    - name: "Copy join command to worker node"
      copy:
        src: "/tmp/kubeadm-join-command-{{ groups['monitoring_nodes'][0] }}.sh"
        dest: "{{ join_command_file }}"
        owner: root
        group: root
        mode: '0700'
      when: not kubelet_conf_exists.stat.exists
      delegate_to: localhost

    - name: "Join worker node to cluster"
      shell: "{{ join_command_file }}"
      register: join_result
      when: not kubelet_conf_exists.stat.exists
      retries: 3
      delay: 30
      until: join_result.rc == 0

    - name: "Verify kubelet is running"
      systemd:
        name: kubelet
        state: started
        enabled: yes

    - name: "Clean up join command file"
      file:
        path: "{{ join_command_file }}"
        state: absent

- name: "Deploy Applications"
  hosts: monitoring_nodes
  become: true
  vars:
    kubeconfig_path: "/etc/kubernetes/admin.conf"
  tasks:
    - name: "Deploy monitoring stack"
      kubernetes.core.k8s:
        state: present
        src: "{{ item }}"
        kubeconfig: "{{ kubeconfig_path }}"
      loop:
        - "{{ playbook_dir }}/../../manifests/monitoring/prometheus.yaml"
        - "{{ playbook_dir }}/../../manifests/monitoring/grafana.yaml"
      when: deploy_monitoring | default(true)

    - name: "Deploy Jellyfin"
      kubernetes.core.k8s:
        state: present
        src: "{{ playbook_dir }}/../../manifests/jellyfin/jellyfin.yaml"
        kubeconfig: "{{ kubeconfig_path }}"
      when: deploy_jellyfin | default(true)

    - name: "Wait for all applications to be ready"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: "{{ item }}"
        kubeconfig: "{{ kubeconfig_path }}"
      register: app_pods
      until: >
        app_pods.resources | length > 0 and
        app_pods.resources | map(attribute='status.phase') | list | select('equalto', 'Running') | list | length > 0
      retries: 20
      delay: 15
      loop:
        - monitoring
        - jellyfin
      ignore_errors: yes

- name: "Final Cluster Verification"
  hosts: monitoring_nodes
  become: true
  vars:
    kubeconfig_path: "/etc/kubernetes/admin.conf"
  tasks:
    - name: "Display cluster status"
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        kubeconfig: "{{ kubeconfig_path }}"
      register: cluster_nodes

    - name: "Show cluster summary"
      debug:
        msg: |
          ðŸŽ‰ VMStation Kubernetes Cluster Bootstrap Complete!
          
          Nodes: {{ cluster_nodes.resources | length }}
          {% for node in cluster_nodes.resources %}
          - {{ node.metadata.name }}: {{ node.status.conditions | selectattr('type', 'equalto', 'Ready') | map(attribute='status') | first }}
          {% endfor %}
          
          Services:
          - Kubernetes API: https://192.168.4.63:6443
          - Prometheus: http://192.168.4.63:30090
          - Grafana: http://192.168.4.63:30300 (admin/admin)
          - Jellyfin: http://192.168.4.61:30096
          
          Next steps:
          - Run verification: ansible-playbook verify-cluster.yml
          - Configure applications via web interfaces