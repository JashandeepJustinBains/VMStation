---
# Emergency fix playbook for RHEL 10 node (homelab) CrashLoopBackOff issues
# This applies the network-fix role and forces pod recreation
- name: Fix RHEL 10 network configuration and restart pods
  hosts: homelab
  become: yes
  gather_facts: yes
  
  tasks:
    - name: Display node information
      ansible.builtin.debug:
        msg: "Applying network fixes to {{ inventory_hostname }} ({{ ansible_distribution }} {{ ansible_distribution_version }})"

    - name: Apply network-fix role
      ansible.builtin.import_role:
        name: network-fix

    - name: Force restart kubelet after network configuration
      ansible.builtin.service:
        name: kubelet
        state: restarted
      when: ansible_os_family == 'RedHat'

- name: Apply updated Flannel manifest and force pod recreation
  hosts: masternode
  become: yes
  gather_facts: no
  vars:
    # Default repo root where the manifests live on the masternode. Update if your checkout
    # lives in a different location.
    repo_root: /srv/monitoring_data/VMStation
  
  tasks:
    - name: Apply updated Flannel manifest
      ansible.builtin.command: kubectl apply -f {{ repo_root }}/manifests/cni/flannel.yaml
      register: flannel_apply
      changed_when: "'configured' in flannel_apply.stdout or 'created' in flannel_apply.stdout"

    - name: Wait for DaemonSet to be updated
      ansible.builtin.command: kubectl rollout status daemonset/kube-flannel-ds -n kube-flannel --timeout=60s
      register: flannel_rollout
      failed_when: false
      changed_when: false

    - name: Force delete Flannel pod on homelab to trigger fresh start
      ansible.builtin.command: >
        kubectl delete pod -n kube-flannel
        -l app=flannel
        --field-selector spec.nodeName=homelab
        --grace-period=0 --force
      register: flannel_delete
      changed_when: "'deleted' in flannel_delete.stdout"
      failed_when: false

    - name: Wait for new Flannel pod to be created
      ansible.builtin.command: >
        kubectl wait --for=condition=Ready pod
        -n kube-flannel
        -l app=flannel
        --field-selector spec.nodeName=homelab
        --timeout=120s
      register: flannel_wait
      retries: 3
      delay: 10
      until: flannel_wait.rc == 0
      failed_when: flannel_wait.rc != 0

    - name: Check Flannel pod status on homelab
      ansible.builtin.command: kubectl get pods -n kube-flannel -l app=flannel --field-selector spec.nodeName=homelab -o wide
      register: flannel_status
      changed_when: false

    - name: Display Flannel pod status
      ansible.builtin.debug:
        msg: "{{ flannel_status.stdout_lines }}"

    - name: Check kube-proxy pod status on homelab
      ansible.builtin.command: kubectl get pods -n kube-system -l k8s-app=kube-proxy --field-selector spec.nodeName=homelab -o wide
      register: proxy_status
      changed_when: false

    - name: Display kube-proxy pod status
      ansible.builtin.debug:
        msg: "{{ proxy_status.stdout_lines }}"

    - name: Get Flannel pod logs
      ansible.builtin.shell: |
        POD=$(kubectl get pods -n kube-flannel -l app=flannel --field-selector spec.nodeName=homelab -o name | head -1)
        if [ -n "$POD" ]; then
          echo "=== Current logs ==="
          kubectl logs -n kube-flannel $POD -c kube-flannel --tail=100 || true
          echo ""
          echo "=== Previous logs (if crashed) ==="
          kubectl logs -n kube-flannel $POD -c kube-flannel --previous --tail=100 2>/dev/null || echo "No previous logs"
        fi
      register: flannel_logs
      changed_when: false

    - name: Display Flannel logs
      ansible.builtin.debug:
        msg: "{{ flannel_logs.stdout_lines }}"

    - name: Check for CrashLoopBackOff pods
      ansible.builtin.command: kubectl get pods -A --field-selector spec.nodeName=homelab
      register: all_pods
      changed_when: false

    - name: Display all pods on homelab
      ansible.builtin.debug:
        msg: "{{ all_pods.stdout_lines }}"

    - name: Force restart kube-proxy on homelab
      ansible.builtin.command: >
        kubectl delete pod -n kube-system
        -l k8s-app=kube-proxy
        --field-selector spec.nodeName=homelab
        --grace-period=0 --force
      register: proxy_delete
      changed_when: "'deleted' in proxy_delete.stdout"
      failed_when: false

    - name: Wait for new kube-proxy pod to be running
      ansible.builtin.command: >
        kubectl wait --for=condition=Ready pod
        -n kube-system
        -l k8s-app=kube-proxy
        --field-selector spec.nodeName=homelab
        --timeout=120s
      register: proxy_wait
      retries: 3
      delay: 10
      until: proxy_wait.rc == 0
      failed_when: proxy_wait.rc != 0

    - name: Get kube-proxy pod logs
      ansible.builtin.shell: |
        POD=$(kubectl get pods -n kube-system -l k8s-app=kube-proxy --field-selector spec.nodeName=homelab -o name | head -1)
        if [ -n "$POD" ]; then
          echo "=== Current kube-proxy logs ==="
          kubectl logs -n kube-system $POD --tail=100 || true
          echo ""
          echo "=== Previous kube-proxy logs (if crashed) ==="
          kubectl logs -n kube-system $POD --previous --tail=100 2>/dev/null || echo "No previous logs"
        fi
      register: proxy_logs
      changed_when: false

    - name: Display kube-proxy logs
      ansible.builtin.debug:
        msg: "{{ proxy_logs.stdout_lines }}"

    - name: Final check - all pods on homelab
      ansible.builtin.command: kubectl get pods -A --field-selector spec.nodeName=homelab
      register: final_pods
      changed_when: false

    - name: Display final pod status
      ansible.builtin.debug:
        msg: "{{ final_pods.stdout_lines }}"

    - name: Final validation - check for unhealthy pods
      ansible.builtin.shell: |
        if kubectl get pods -A --field-selector spec.nodeName=homelab | grep -E 'CrashLoopBackOff|Error|Completed'; then
          echo "FAILURE: Unhealthy pods detected"
          exit 1
        else
          echo "SUCCESS: All pods on homelab are healthy"
          exit 0
        fi
      register: health_check
      changed_when: false
      failed_when: false

    - name: Display health check result
      ansible.builtin.debug:
        msg: "{{ health_check.stdout }}"

    - name: Fail playbook if pods are unhealthy
      ansible.builtin.fail:
        msg: |
          ‚ùå FAILED: Pods on homelab are still not healthy after applying fixes.
          
          The fix script completed all tasks but pods are still showing:
          - CrashLoopBackOff (pod is repeatedly crashing)
          - Completed (DaemonSet pod exited when it should run continuously)
          - Error (pod failed to start)
          
          Please review the pod logs above for specific error messages.
          
          Common root causes:
          - kube-proxy: Missing iptables chains, conntrack issues, or iptables mode mismatch
          - Flannel: Network interface issues, API watch stream problems, or configuration errors
          
          Next debugging steps:
          1. Review kube-proxy logs above for "exit code 2" errors
          2. Review Flannel logs to see why it's exiting (if Completed status)
          3. Check iptables backend: Run 'update-alternatives --display iptables' on homelab
          4. Verify conntrack is working: Run 'conntrack -L' on homelab
          5. Check if Flannel interface exists: Run 'ip link show flannel.1' on homelab
      when: health_check.rc != 0

    - name: Final validation
      ansible.builtin.command: kubectl get nodes homelab -o wide
      register: node_status
      changed_when: false

    - name: Display node status
      ansible.builtin.debug:
        msg: "{{ node_status.stdout_lines }}"


- name: Collect host-level diagnostics from homelab
  hosts: homelab
  become: yes
  gather_facts: no

  tasks:
    - name: Collect last 200 lines of kubelet journal
      ansible.builtin.shell: journalctl -u kubelet --no-pager -n 200 || true
      register: homelab_kubelet
      changed_when: false

    - name: Show kubelet journal snippet
      ansible.builtin.debug:
        msg: "--- kubelet journal (homelab) ---\n{{ homelab_kubelet.stdout }}"

    - name: Collect last 200 lines of containerd journal
      ansible.builtin.shell: journalctl -u containerd --no-pager -n 200 || true
      register: homelab_containerd
      changed_when: false

    - name: Show containerd journal snippet
      ansible.builtin.debug:
        msg: "--- containerd journal (homelab) ---\n{{ homelab_containerd.stdout }}"

    - name: Dump nftables ruleset
      ansible.builtin.shell: nft list ruleset || true
      register: homelab_nft
      changed_when: false

    - name: Show nftables ruleset
      ansible.builtin.debug:
        msg: "--- nft ruleset (homelab) ---\n{{ homelab_nft.stdout }}"

    - name: Dump iptables-save (IPv4)
      ansible.builtin.shell: iptables-save -c || true
      register: homelab_iptables
      changed_when: false

    - name: Show iptables-save
      ansible.builtin.debug:
        msg: "--- iptables-save (homelab) ---\n{{ homelab_iptables.stdout }}"

    - name: Show last dmesg lines
      ansible.builtin.shell: dmesg --ctime | tail -n 200 || true
      register: homelab_dmesg
      changed_when: false

    - name: Show dmesg tail
      ansible.builtin.debug:
        msg: "--- dmesg (homelab) ---\n{{ homelab_dmesg.stdout }}"

    - name: Show /run/flannel/subnet.env if present
      ansible.builtin.shell: test -s /run/flannel/subnet.env && cat /run/flannel/subnet.env || echo 'no-subnet-env'
      register: homelab_subnet_env
      changed_when: false

    - name: Display flannel subnet.env
      ansible.builtin.debug:
        msg: "--- /run/flannel/subnet.env (homelab) ---\n{{ homelab_subnet_env.stdout }}"
