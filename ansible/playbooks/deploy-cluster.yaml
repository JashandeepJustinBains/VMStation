---
# =============================================================================
# VMStation Kubernetes Cluster - Deploy Playbook
# Idempotent deployment for mixed OS cluster (Debian + RHEL 10)
# =============================================================================

# -----------------------------------------------------------------------------
# PHASE 0: Install Kubernetes Binaries (Debian nodes only)
# -----------------------------------------------------------------------------
- name: Phase 0 - Install Kubernetes binaries
  hosts: monitoring_nodes:storage_nodes
  become: true
  gather_facts: true
  roles:
    - install-k8s-binaries

# -----------------------------------------------------------------------------
# PHASE 1: System Preparation
# -----------------------------------------------------------------------------
- name: Phase 1 - System preparation
  hosts: all
  become: true
  gather_facts: true
  tasks:
    - name: Ensure /etc/hosts has cluster nodes
      ansible.builtin.blockinfile:
        path: /etc/hosts
        block: |
          192.168.4.63 masternode
          192.168.4.61 storagenodet3500
          192.168.4.62 homelab
        marker: "# {mark} VMStation Cluster"
        create: yes

  roles:
    - preflight
    - network-fix

# -----------------------------------------------------------------------------
# PHASE 2: CNI Plugins Installation
# -----------------------------------------------------------------------------
- name: Phase 2 - Install CNI plugins
  hosts: all
  become: true
  gather_facts: true
  tasks:
    - name: Detect architecture
      ansible.builtin.set_fact:
        cni_arch: "{{ 'amd64' if ansible_architecture == 'x86_64' else ansible_architecture }}"

    - name: Check if CNI plugins installed
      ansible.builtin.stat:
        path: /opt/cni/bin/bridge
      register: cni_installed

    - name: Download CNI plugins (get_url)
      ansible.builtin.get_url:
        url: "https://github.com/containernetworking/plugins/releases/download/v1.8.0/cni-plugins-linux-{{ cni_arch }}-v1.8.0.tgz"
        dest: /tmp/cni-plugins.tgz
        mode: '0644'
        validate_certs: false
      when: not cni_installed.stat.exists
      register: cni_download
      ignore_errors: true

    - name: Download CNI plugins (curl fallback)
      ansible.builtin.shell: |
        curl -fsSL -o /tmp/cni-plugins.tgz \
          "https://github.com/containernetworking/plugins/releases/download/v1.8.0/cni-plugins-linux-{{ cni_arch }}-v1.8.0.tgz"
      args:
        creates: /tmp/cni-plugins.tgz
      when: 
        - not cni_installed.stat.exists
        - cni_download is failed or 'cert_file' in (cni_download.msg | default('')) or 'urllib3' in (cni_download.msg | default(''))

    - name: Verify CNI archive downloaded
      ansible.builtin.stat:
        path: /tmp/cni-plugins.tgz
      register: cni_archive
      when: not cni_installed.stat.exists

    - name: Ensure /opt/cni/bin directory exists
      ansible.builtin.file:
        path: /opt/cni/bin
        state: directory
        mode: '0755'
      when: not cni_installed.stat.exists

    - name: Extract CNI plugins
      ansible.builtin.unarchive:
        src: /tmp/cni-plugins.tgz
        dest: /opt/cni/bin
        remote_src: yes
      when: not cni_installed.stat.exists and cni_archive.stat.exists

# -----------------------------------------------------------------------------
# PHASE 3: Control Plane Initialization
# -----------------------------------------------------------------------------
- name: Phase 3 - Initialize control plane
  hosts: monitoring_nodes
  become: true
  tasks:
    - name: Check if control plane initialized
      ansible.builtin.stat:
        path: /etc/kubernetes/admin.conf
      register: admin_conf

    - name: Initialize control plane
      ansible.builtin.shell: |
        kubeadm init \
          --pod-network-cidr=10.244.0.0/16 \
          --service-cidr=10.96.0.0/12 \
          --upload-certs
      when: not admin_conf.stat.exists

    - name: Set KUBECONFIG
      ansible.builtin.lineinfile:
        path: /root/.bashrc
        line: 'export KUBECONFIG=/etc/kubernetes/admin.conf'
        create: yes

    - name: Wait for API server
      ansible.builtin.wait_for:
        host: 127.0.0.1
        port: 6443
        timeout: 120

# -----------------------------------------------------------------------------
# PHASE 4: Worker Node Join
# -----------------------------------------------------------------------------
# PHASE 4: Worker Node Join (Robust & Idempotent)
# -----------------------------------------------------------------------------
- name: Phase 4 - Join worker nodes
  hosts: storage_nodes:compute_nodes
  become: true
  vars:
    join_timeout: 600
    join_retries: 3
  tasks:
    - name: Check if node already joined
      ansible.builtin.stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: Skip join if already joined
      ansible.builtin.debug:
        msg: "{{ inventory_hostname }} already joined to cluster (kubelet.conf exists)"
      when: kubelet_conf.stat.exists

    # Pre-join cleanup and validation
    - name: Pre-join cleanup and validation
      when: not kubelet_conf.stat.exists
      block:
        - name: Stop kubelet service before cleanup
          ansible.builtin.systemd:
            name: kubelet
            state: stopped
          ignore_errors: true

        - name: Kill any existing kubeadm join processes
          ansible.builtin.shell: |
            pkill -f "kubeadm join" || true
            sleep 2
            pkill -9 -f "kubeadm join" || true
          ignore_errors: true

        - name: Clean up partial join artifacts
          ansible.builtin.file:
            path: "{{ item }}"
            state: absent
          loop:
            - /var/lib/kubelet/config.yaml
            - /var/lib/kubelet/kubeadm-flags.env
            - /etc/kubernetes/bootstrap-kubelet.conf
            - /etc/kubernetes/kubelet.conf
            - /etc/kubernetes/pki/ca.crt
          ignore_errors: true

        - name: Ensure directories exist with correct permissions
          ansible.builtin.file:
            path: "{{ item }}"
            state: directory
            mode: '0755'
            owner: root
            group: root
          loop:
            - /var/lib/kubelet
            - /etc/kubernetes
            - /etc/kubernetes/pki
            - /var/log/containers
            - /var/log/pods

        - name: Wait for containerd socket (required for join)
          ansible.builtin.wait_for:
            path: /var/run/containerd/containerd.sock
            state: present
            timeout: 300
          register: containerd_wait

        - name: Verify containerd is active
          ansible.builtin.systemd:
            name: containerd
            state: started
          register: containerd_status

        - name: Check kubeadm binary availability
          ansible.builtin.command: which kubeadm
          register: which_kubeadm
          changed_when: false
          failed_when: false

        - name: Install kubeadm if missing
          when: which_kubeadm.rc != 0
          block:
            - name: Run install-k8s-binaries role for missing kubeadm
              include_role:
                name: install-k8s-binaries

            - name: Verify kubeadm after installation
              ansible.builtin.command: which kubeadm
              register: kubeadm_verify
              failed_when: kubeadm_verify.rc != 0

        - name: Test connectivity to control plane
          ansible.builtin.wait_for:
            host: "{{ groups['monitoring_nodes'][0] }}"
            port: 6443
            timeout: 30
          register: api_connectivity

    # Generate fresh join command
    - name: Generate fresh join command
      ansible.builtin.shell: kubeadm token create --print-join-command
      delegate_to: "{{ groups['monitoring_nodes'][0] }}"
      register: join_command
      when: not kubelet_conf.stat.exists
      run_once: true
      retries: 3
      delay: 5
      until: join_command.rc == 0

    # Robust join with retries and logging
    - name: Join worker to cluster with retries
      when: not kubelet_conf.stat.exists
      block:
        - name: Attempt kubeadm join (with retries)
          ansible.builtin.shell: |
            set -o pipefail
            echo "[$(date)] Starting kubeadm join on {{ inventory_hostname }}" | tee -a /var/log/kubeadm-join.log
            {{ join_command.stdout }} --v=5 2>&1 | tee -a /var/log/kubeadm-join.log
            exit_code=${PIPESTATUS[0]}
            echo "[$(date)] kubeadm join exit code: $exit_code" | tee -a /var/log/kubeadm-join.log
            exit $exit_code
          args:
            executable: /bin/bash
          register: join_result
          retries: "{{ join_retries }}"
          delay: 30
          until: join_result.rc == 0

        - name: Wait for kubelet config to appear
          ansible.builtin.wait_for:
            path: /etc/kubernetes/kubelet.conf
            state: present
            timeout: 120
          register: kubelet_config_wait

        - name: Verify kubelet service starts successfully
          ansible.builtin.systemd:
            name: kubelet
            state: started
            enabled: true
          register: kubelet_start

        - name: Wait for kubelet to be ready
          ansible.builtin.shell: |
            timeout=60
            while [ $timeout -gt 0 ]; do
              if systemctl is-active --quiet kubelet && ! journalctl -u kubelet -n 10 --no-pager | grep -i "failed to load kubelet config"; then
                echo "kubelet is running and healthy"
                exit 0
              fi
              sleep 2
              timeout=$((timeout - 2))
            done
            echo "kubelet failed to become healthy"
            exit 1
          register: kubelet_health
          retries: 3
          delay: 10
          until: kubelet_health.rc == 0

      rescue:
        - name: Capture failure diagnostics
          ansible.builtin.shell: |
            echo "=== Join Failure Diagnostics ===" >> /var/log/kubeadm-join-failure.log
            echo "Date: $(date)" >> /var/log/kubeadm-join-failure.log
            echo "Host: {{ inventory_hostname }}" >> /var/log/kubeadm-join-failure.log
            echo "" >> /var/log/kubeadm-join-failure.log
            echo "=== kubelet status ===" >> /var/log/kubeadm-join-failure.log
            systemctl status kubelet --no-pager -l >> /var/log/kubeadm-join-failure.log 2>&1 || true
            echo "" >> /var/log/kubeadm-join-failure.log
            echo "=== kubelet journal (last 50 lines) ===" >> /var/log/kubeadm-join-failure.log
            journalctl -u kubelet -n 50 --no-pager >> /var/log/kubeadm-join-failure.log 2>&1 || true
            echo "" >> /var/log/kubeadm-join-failure.log
            echo "=== containerd status ===" >> /var/log/kubeadm-join-failure.log
            systemctl status containerd --no-pager -l >> /var/log/kubeadm-join-failure.log 2>&1 || true
            echo "" >> /var/log/kubeadm-join-failure.log
            echo "=== File system state ===" >> /var/log/kubeadm-join-failure.log
            ls -la /var/lib/kubelet/ >> /var/log/kubeadm-join-failure.log 2>&1 || true
            ls -la /etc/kubernetes/ >> /var/log/kubeadm-join-failure.log 2>&1 || true
            echo "" >> /var/log/kubeadm-join-failure.log
            echo "=== Network connectivity ===" >> /var/log/kubeadm-join-failure.log
            ss -tlnp | grep -E '6443|10250' >> /var/log/kubeadm-join-failure.log 2>&1 || true
          ignore_errors: true

        - name: Display failure information and exit
          ansible.builtin.fail:
            msg: |
              Worker join failed after {{ join_retries }} attempts.
              Diagnostic logs written to /var/log/kubeadm-join-failure.log
              Last join attempt output: {{ join_result.stdout | default('No output') }}
              Last join attempt error: {{ join_result.stderr | default('No error') }}

    # Final validation
    - name: Validate successful join
      when: not kubelet_conf.stat.exists
      block:
        - name: Verify kubelet config exists
          ansible.builtin.stat:
            path: /etc/kubernetes/kubelet.conf
          register: final_kubelet_conf
          failed_when: not final_kubelet_conf.stat.exists

        - name: Verify kubelet is running
          ansible.builtin.systemd:
            name: kubelet
          register: final_kubelet_status
          failed_when: final_kubelet_status.status.ActiveState != 'active'

        - name: Log successful join
          ansible.builtin.shell: |
            echo "[$(date)] Successfully joined {{ inventory_hostname }} to cluster" | tee -a /var/log/kubeadm-join.log
            echo "kubelet status: $(systemctl is-active kubelet)" | tee -a /var/log/kubeadm-join.log

    - name: Display join completion status
      ansible.builtin.debug:
        msg: "{{ inventory_hostname }} join status: {{ 'ALREADY_JOINED' if kubelet_conf.stat.exists else 'NEWLY_JOINED' }}"
# -----------------------------------------------------------------------------
# PHASE 5: Flannel CNI Deployment
# -----------------------------------------------------------------------------
- name: Phase 5 - Deploy Flannel CNI
  hosts: monitoring_nodes
  become: true
  tasks:
    - name: Apply Flannel manifest
      ansible.builtin.shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/cni/flannel.yaml
      register: flannel_apply
      changed_when: "'created' in flannel_apply.stdout or 'configured' in flannel_apply.stdout"

    - name: Wait for Flannel DaemonSet rollout
      ansible.builtin.shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-flannel rollout status daemonset/kube-flannel-ds --timeout=90s
      retries: 3
      delay: 10
      register: flannel_rollout
      until: flannel_rollout.rc == 0

    - name: Wait for all Flannel pods to be Ready (not just Running)
      ansible.builtin.shell: |
        set -e
        total_nodes=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes --no-headers | wc -l)
        ready_flannel=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-flannel get pods -l app=flannel --field-selector=status.phase=Running -o json | \
          jq '[.items[] | select(.status.conditions[] | select(.type=="Ready" and .status=="True"))] | length')
        echo "Flannel pods ready: $ready_flannel / $total_nodes"
        test "$ready_flannel" -eq "$total_nodes"
      retries: 20
      delay: 10
      register: flannel_ready
      until: flannel_ready.rc == 0

    - name: Verify subnet.env exists on this node
      ansible.builtin.stat:
        path: /run/flannel/subnet.env
      register: subnet_env_stat
      retries: 10
      delay: 5
      until: subnet_env_stat.stat.exists
      failed_when: not subnet_env_stat.stat.exists

    - name: Debug subnet.env status on this node
      ansible.builtin.debug:
        msg: "/run/flannel/subnet.env exists: {{ subnet_env_stat.stat.exists }} on {{ inventory_hostname }}"

    - name: Verify CNI config exists on all nodes via SSH
      ansible.builtin.stat:
        path: /etc/cni/net.d/10-flannel.conflist
      delegate_to: "{{ item }}"
      loop:
        - masternode
        - storagenodet3500
      register: cni_file_check
      failed_when: not cni_file_check.stat.exists

    - name: Verify CNI flannel binary exists on all nodes
      ansible.builtin.stat:
        path: /opt/cni/bin/flannel
      delegate_to: "{{ item }}"
      loop:
        - masternode
        - storagenodet3500
      register: cni_binary_check
      failed_when: not cni_binary_check.stat.exists

    - name: Wait for all nodes Ready
      ansible.builtin.shell: |
        total=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes --no-headers | wc -l)
        ready=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes --no-headers | grep ' Ready ' | wc -l)
        test $ready -eq $total
      retries: 20
      delay: 10
      register: nodes_ready
      until: nodes_ready.rc == 0

    - name: Uncordon all nodes
      ansible.builtin.shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o name | \
          xargs -n1 -r kubectl --kubeconfig=/etc/kubernetes/admin.conf uncordon
      changed_when: false
      ignore_errors: true

    - name: Remove control-plane taint
      ansible.builtin.shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule- || true
      changed_when: false

# -----------------------------------------------------------------------------
# PHASE 6: Validation
# -----------------------------------------------------------------------------
- name: Phase 6 - Validate deployment
  hosts: monitoring_nodes
  become: true
  tasks:
    - name: Wait for kube-system pods to stabilize
      ansible.builtin.shell: |
        set -e
        # Check if any kube-system pods are in CrashLoopBackOff
        CRASH=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-system --no-headers | grep -i 'CrashLoopBackOff' || true)
        if [ -z "$CRASH" ]; then
          echo "All kube-system pods stable (no CrashLoopBackOff)"
          exit 0
        else
          echo "Waiting for pods to stabilize: $(echo "$CRASH" | wc -l) pods in CrashLoopBackOff"
          exit 1
        fi
      retries: 30
      delay: 10
      register: pods_stable
      until: pods_stable.rc == 0
      changed_when: false

    - name: Validate kube-system pods are healthy (no CrashLoopBackOff)
      ansible.builtin.shell: |
        set -e
        # Get all kube-system pods with status
        CRASH=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-system --no-headers | grep -i crash || true)
        if [ -n "$CRASH" ]; then
          echo "ERROR: Found CrashLoopBackOff pods:"
          echo "$CRASH"
          echo ""
          echo "=== Collecting diagnostic logs ==="
          for pod in $(echo "$CRASH" | awk '{print $1}'); do
            echo "--- Pod: $pod ---"
            kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-system describe pod "$pod" || true
            echo "--- Current logs ---"
            kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-system logs "$pod" --tail=50 || true
            echo "--- Previous logs ---"
            kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-system logs "$pod" --previous --tail=50 || true
            echo ""
          done
          exit 1
        fi
        echo "All kube-system pods healthy (no CrashLoopBackOff)"
      register: crash_check
      changed_when: false
      failed_when: crash_check.rc != 0

    - name: Display cluster status
      ansible.builtin.shell: |
        echo "=== Nodes ==="
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o wide
        echo ""
        echo "=== kube-system pods ==="
        kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-system get pods -o wide
        echo ""
        echo "=== Flannel pods ==="
        kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-flannel get pods -o wide
      register: cluster_status
      changed_when: false

    - name: Show cluster status
      ansible.builtin.debug:
        var: cluster_status.stdout_lines

# -----------------------------------------------------------------------------
# PHASE 7: Application Deployment
# -----------------------------------------------------------------------------
- import_playbook: ../plays/deploy-apps.yaml
- import_playbook: ../plays/jellyfin.yml
