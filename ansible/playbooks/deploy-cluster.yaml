---
# =============================================================================
# VMStation Kubernetes Cluster Deployment
# Implements all 7 phases of the deployment specification
# Idempotent, bulletproof deployment for Debian kubeadm cluster
# =============================================================================

# =============================================================================
# Phase 0: System Preparation
# Install Kubernetes binaries, configure containerd, set up systemd services
# =============================================================================
- name: "Phase 0: System Preparation - Install Kubernetes Binaries"
  hosts: monitoring_nodes:storage_nodes
  gather_facts: true
  become: true
  tasks:
    - name: "Display Phase 0 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 0: System Preparation
          Target: {{ inventory_hostname }}
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    # Disable swap (required for Kubernetes)
    - name: "Disable swap"
      shell: swapoff -a
      changed_when: false

    - name: "Remove swap from fstab"
      lineinfile:
        path: /etc/fstab
        regexp: '.*swap.*'
        state: absent

    # Load required kernel modules
    - name: "Load kernel modules for containerd"
      modprobe:
        name: "{{ item }}"
        state: present
      loop:
        - overlay
        - br_netfilter

    - name: "Ensure modules load on boot"
      copy:
        dest: /etc/modules-load.d/kubernetes.conf
        content: |
          overlay
          br_netfilter

    # Configure sysctl parameters
    - name: "Configure sysctl for Kubernetes"
      sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        state: present
        reload: true
      loop:
        - { name: 'net.bridge.bridge-nf-call-iptables', value: '1' }
        - { name: 'net.bridge.bridge-nf-call-ip6tables', value: '1' }
        - { name: 'net.ipv4.ip_forward', value: '1' }

    # Install containerd with robust fallback logic
    - name: "Check if containerd is installed"
      stat:
        path: /usr/bin/containerd
      register: containerd_installed

    - name: "Install containerd (try containerd.io first)"
      apt:
        name: containerd.io
        state: present
        update_cache: true
      when: not containerd_installed.stat.exists
      ignore_errors: true
      register: containerd_io_install

    - name: "Install containerd (fallback to containerd package)"
      apt:
        name: containerd
        state: present
        update_cache: true
      when: 
        - not containerd_installed.stat.exists
        - containerd_io_install is failed

    # Configure containerd
    - name: "Create containerd config directory"
      file:
        path: /etc/containerd
        state: directory
        mode: '0755'

    - name: "Generate containerd default config"
      shell: containerd config default > /etc/containerd/config.toml
      args:
        creates: /etc/containerd/config.toml

    - name: "Enable SystemdCgroup in containerd"
      lineinfile:
        path: /etc/containerd/config.toml
        regexp: '^\s*SystemdCgroup\s*='
        line: '            SystemdCgroup = true'
        insertafter: '^\s*\[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options\]'

    - name: "Restart containerd"
      systemd:
        name: containerd
        state: restarted
        enabled: true
        daemon_reload: true

    # Configure crictl to use containerd
    - name: "Configure crictl runtime endpoint"
      copy:
        dest: /etc/crictl.yaml
        content: |
          runtime-endpoint: unix:///var/run/containerd/containerd.sock
          image-endpoint: unix:///var/run/containerd/containerd.sock
          timeout: 10
        mode: '0644'

    # Install Kubernetes binaries
    - name: "Add Kubernetes apt key"
      apt_key:
        url: https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/Release.key
        state: present
        keyring: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

    - name: "Add Kubernetes apt repository"
      apt_repository:
        repo: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/ /"
        state: present
        filename: kubernetes

    - name: "Install Kubernetes binaries"
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: true

    - name: "Hold Kubernetes packages at current version"
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl

    - name: "Enable kubelet service"
      systemd:
        name: kubelet
        enabled: true

    # Create required directories with proper permissions
    - name: "Create required Kubernetes directories"
      file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
      loop:
        - /opt/cni/bin
        - /etc/cni/net.d
        - /var/lib/kubelet

# =============================================================================
# Phase 1: Control Plane Initialization
# Initialize Kubernetes control plane with kubeadm on master node
# =============================================================================
- name: "Phase 1: Control Plane Initialization"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 1 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 1: Control Plane Initialization
          Target: {{ inventory_hostname }}
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Check if cluster is already initialized"
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeconfig_exists

    - name: "Initialize control plane (if not exists)"
      shell: |
        kubeadm init \
          --pod-network-cidr={{ pod_network_cidr }} \
          --service-cidr={{ service_network_cidr }} \
          --control-plane-endpoint={{ control_plane_endpoint }} \
          --upload-certs
      when: not kubeconfig_exists.stat.exists
      register: kubeadm_init

    - name: "Regenerate admin.conf with kubeadm (fixes authentication issues)"
      shell: kubeadm init phase kubeconfig admin
      when: kubeconfig_exists.stat.exists

    - name: "Create .kube directory for root"
      file:
        path: /root/.kube
        state: directory
        mode: '0755'

    - name: "Copy admin.conf to /root/.kube/config"
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        remote_src: true
        mode: '0600'

    - name: "Set KUBECONFIG environment variable globally"
      lineinfile:
        path: /etc/environment
        line: 'KUBECONFIG=/etc/kubernetes/admin.conf'
        create: yes

    - name: "Add KUBECONFIG to root's bash profile"
      lineinfile:
        path: /root/.bashrc
        line: 'export KUBECONFIG=/etc/kubernetes/admin.conf'
        create: yes

# =============================================================================
# Phase 2: Control Plane Validation
# Verify control plane components are running correctly
# =============================================================================
- name: "Phase 2: Control Plane Validation"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 2 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 2: Control Plane Validation
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Wait for API server to be ready"
      wait_for:
        host: "{{ ansible_host }}"
        port: 6443
        timeout: 120

    - name: "Verify control plane is responding"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf cluster-info
      register: cluster_info
      retries: 10
      delay: 10
      until: cluster_info.rc == 0

    - name: "Display control plane status"
      debug:
        msg: "{{ cluster_info.stdout_lines }}"

# =============================================================================
# Phase 3: Token Generation
# Generate fresh join tokens for worker nodes
# =============================================================================
- name: "Phase 3: Token Generation"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 3 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 3: Token Generation
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Generate join token"
      shell: kubeadm token create --print-join-command
      register: join_command
      retries: 3
      delay: 5
      until: join_command.rc == 0

    - name: "Store join command"
      set_fact:
        kubernetes_join_command: "{{ join_command.stdout }}"

    - name: "Display join command"
      debug:
        msg: "Join command: {{ kubernetes_join_command }}"

# =============================================================================
# Phase 4: CNI Deployment
# Deploy Flannel CNI plugin before worker nodes join
# =============================================================================
- name: "Phase 4: CNI Deployment - Flannel"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 4 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 4: CNI Deployment (Flannel)
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Check if CNI plugins are installed"
      shell: "ls /opt/cni/bin 2>/dev/null | wc -l"
      register: cni_plugin_count
      changed_when: false

    - name: "Download CNI plugins if missing"
      get_url:
        url: "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz"
        dest: /tmp/cni-plugins.tgz
        mode: '0644'
      when: cni_plugin_count.stdout | int < 5

    - name: "Extract CNI plugins"
      unarchive:
        src: /tmp/cni-plugins.tgz
        dest: /opt/cni/bin
        remote_src: yes
      when: cni_plugin_count.stdout | int < 5

    - name: "Check if Flannel is already deployed"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get namespace kube-flannel 2>/dev/null
      register: flannel_namespace
      failed_when: false
      changed_when: false

    - name: "Deploy Flannel CNI"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/cni/flannel.yaml
      when: flannel_namespace.rc != 0
      register: flannel_deploy

    - name: "Wait for Flannel DaemonSet to be available"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get daemonset -n kube-flannel kube-flannel-ds 2>/dev/null
      register: flannel_ds
      retries: 10
      delay: 5
      until: flannel_ds.rc == 0
      when: flannel_deploy is changed

    - name: "Display Flannel deployment status"
      debug:
        msg: "Flannel CNI deployed successfully"

# =============================================================================
# Phase 5: Worker Node Join
# Join worker nodes to cluster
# =============================================================================
- name: "Phase 5: Worker Node Join"
  hosts: storage_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 5 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 5: Worker Node Join
          Target: {{ inventory_hostname }}
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Check if node is already joined"
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: "Join worker node to cluster"
      shell: "{{ hostvars[groups['monitoring_nodes'][0]].kubernetes_join_command }}"
      when: not kubelet_conf.stat.exists
      register: join_result

    - name: "Wait for kubelet to start"
      systemd:
        name: kubelet
        state: started
        enabled: true
      when: not kubelet_conf.stat.exists

    - name: "Display join status"
      debug:
        msg: "{{ inventory_hostname }} successfully joined the cluster"
      when: not kubelet_conf.stat.exists

# =============================================================================
# Phase 6: Cluster Validation
# Verify all nodes are Ready and core components are running
# =============================================================================
- name: "Phase 6: Cluster Validation"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 6 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 6: Cluster Validation
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Wait for nodes to be Ready"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o json | jq -r '.items[] | select(.status.conditions[] | select(.type=="Ready" and .status=="True")) | .metadata.name' | wc -l
      register: ready_nodes
      retries: 20
      delay: 10
      until: ready_nodes.stdout | int >= 2

    - name: "Get node status"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o wide
      register: nodes_status

    - name: "Display node status"
      debug:
        msg: "{{ nodes_status.stdout_lines }}"

    - name: "Wait for CoreDNS to be Running"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[*].status.phase}' | grep -c Running
      register: coredns_pods
      retries: 15
      delay: 10
      until: coredns_pods.stdout | int >= 1

    - name: "Display validation success"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          ✅ Cluster Validation Successful
          - Ready Nodes: {{ ready_nodes.stdout }}
          - CoreDNS Pods: {{ coredns_pods.stdout }}
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# =============================================================================
# Phase 7: Application Deployment
# Deploy monitoring stack (Prometheus, Grafana, Loki)
# =============================================================================
- name: "Phase 7: Application Deployment - Monitoring Stack"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 7 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 7: Application Deployment (Monitoring)
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Create monitoring namespace"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf create namespace monitoring --dry-run=client -o yaml | kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f -

    - name: "Deploy Prometheus"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/monitoring/prometheus.yaml

    - name: "Deploy Grafana"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/monitoring/grafana.yaml

    - name: "Wait for Prometheus to be ready"
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/prometheus \
          -n monitoring \
          --timeout=120s
      retries: 5
      delay: 10
      register: prometheus_ready
      until: prometheus_ready.rc == 0
      failed_when: false

    - name: "Wait for Grafana to be ready"
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/grafana \
          -n monitoring \
          --timeout=120s
      retries: 5
      delay: 10
      register: grafana_ready
      until: grafana_ready.rc == 0
      failed_when: false

    - name: "Display monitoring stack status"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get all -n monitoring
      register: monitoring_status

    - name: "Display monitoring deployment result"
      debug:
        msg: "{{ monitoring_status.stdout_lines }}"

    - name: "Display deployment complete message"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          ✅ VMStation Kubernetes Cluster Deployment Complete!
          
          Cluster Status:
          - Control Plane: {{ groups['monitoring_nodes'][0] }}
          - Worker Nodes: {{ groups['storage_nodes'] | join(', ') }}
          - CNI: Flannel
          - Monitoring: Prometheus, Grafana
          
          Next Steps:
          - Access Grafana at http://{{ hostvars[groups['monitoring_nodes'][0]].ansible_host }}:30000
          - Check cluster: kubectl get nodes
          - View pods: kubectl get pods -A
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
