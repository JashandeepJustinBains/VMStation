---
# Consolidated cluster deploy playbook: preflight -> apply network fixes -> deploy manifests -> validate
- hosts: all
  gather_facts: true
  roles:
    - system-prep
    - preflight
    - network-fix

- hosts: masternode
  become: true
  tasks:
    - name: Initialize Kubernetes control plane if not already initialized (idempotent)
      ansible.builtin.shell: |
        if [ ! -f /etc/kubernetes/admin.conf ]; then
          echo "Running kubeadm init to bootstrap control plane..."
          kubeadm init --pod-network-cidr=10.244.0.0/16 --upload-certs
        else
          echo "Control plane already initialized. Skipping kubeadm init."
        fi
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: kubeadm_init_result
      ignore_errors: true

    - name: Show kubeadm init output (if any)
      ansible.builtin.debug:
        var: kubeadm_init_result.stdout_lines
      when: kubeadm_init_result is defined and kubeadm_init_result.stdout_lines is defined

    - name: Wait for Kubernetes API server to be available
      ansible.builtin.shell: |
        for i in {1..24}; do
          if kubectl version --request-timeout=5s >/dev/null 2>&1; then
            echo "API server is up"
            exit 0
          fi
          echo "Waiting for API server... ($i/24)"
          sleep 5
        done
        echo "ERROR: API server did not become available in time" >&2
        exit 1
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
    - block:
        - name: Ensure artifacts dir exists on controller
          delegate_to: localhost
          run_once: true
          ansible.builtin.file:
            path: "{{ playbook_dir }}/../ansible/artifacts"
            state: directory

        - name: Apply CNI manifests (flannel) with immutable-selector recovery
          ansible.builtin.shell: |
            set -o errexit -o nounset -o pipefail
            echo "Applying flannel manifests"
            if kubectl apply -f manifests/cni/flannel.yaml; then
              echo "flannel applied"
            else
              echo "Initial apply failed, attempting safe recovery: delete existing DaemonSet and re-apply"
              # delete the DaemonSet if present (handles immutable selector changes)
              kubectl -n kube-flannel delete daemonset kube-flannel-ds --ignore-not-found
              kubectl apply -f manifests/cni/flannel.yaml
            fi
          args:
            chdir: "{{ playbook_dir }}/../../"
            executable: /bin/bash
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

        - name: Wait for flannel daemonset to be ready
          ansible.builtin.shell: |
            kubectl -n kube-flannel rollout status daemonset/kube-flannel-ds --timeout=180s
          args:
            executable: /bin/bash
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

        - name: Set flannel_cni_nodes variable (all nodes except masternode)
          set_fact:
            flannel_cni_nodes: "{{ groups['all'] | difference(['masternode']) }}"

        - name: Verify Flannel CNI config created on all nodes
          ansible.builtin.shell: |
            # Wait up to 60s for CNI config to appear on all nodes
            nodes=({{ flannel_cni_nodes | join(' ') }})
            for i in {1..12}; do
              missing=0
              for node in "${nodes[@]}"; do
                if ! ssh "$node" "test -f /etc/cni/net.d/10-flannel.conflist"; then
                  missing=$((missing + 1))
                fi
              done
              if [ "$missing" -eq 0 ]; then
                echo "Flannel CNI config verified on all nodes"
                exit 0
              fi
              echo "Waiting for CNI config on $missing nodes ($i/12)..."
              sleep 5
            done
            echo "ERROR: Flannel CNI config missing on some nodes after 60s"
            exit 1
          args:
            executable: /bin/bash
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

        - name: Ensure kube-proxy-ready
          ansible.builtin.shell: |
            kubectl -n kube-system get pods -l 'k8s-app=kube-proxy' -o jsonpath='{.items[*].status.phase}'
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

      rescue:
        - name: Run diagnostics role on failure
          import_role:
            name: diagnostics

    - name: Install idle-sleep timer and script on masternode
      import_role:
        name: idle-sleep

    - name: Uncordon any nodes that are still cordoned (idempotent)
      ansible.builtin.shell: |
        set -eux
        # list nodes that have spec.unschedulable=true
        nodes=$(kubectl get nodes -o jsonpath='{range .items[?(@.spec.unschedulable==true)]}{.metadata.name} "\n"{end}')
        if [ -n "$nodes" ]; then
          for n in $nodes; do
            echo "Uncordoning $n"
            kubectl uncordon "$n" || true
          done
        else
          echo "No cordoned nodes found"
        fi
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf


    - name: Uncordon all nodes after reset/deploy (make schedulable)
      ansible.builtin.shell: |
        set -eux
        kubectl get nodes --no-headers | awk '{print $1}' | xargs -r -n1 kubectl uncordon || true
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Remove common control-plane NoSchedule taints if present (idempotent)
      ansible.builtin.shell: |
        set -eux
        # remove control-plane/master taints that can block application scheduling in small clusters
        kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule- || true
        kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule- || true
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Ensure masternode has control-plane label for monitoring scheduling
      ansible.builtin.shell: |
        set -eux
        kubectl label node masternode 'node-role.kubernetes.io/control-plane=' --overwrite || true
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Validate flannel pods are stable before proceeding (external script)
      ansible.builtin.command:
        cmd: "{{ playbook_dir }}/../scripts/check_flannel_stability.sh"
      args:
        chdir: "{{ playbook_dir }}/.."
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_stability_check

    - name: Show flannel stability result
      ansible.builtin.debug:
        var: flannel_stability_check.stdout_lines

    - name: Wait for all nodes to become Ready after Flannel deploy
      ansible.builtin.shell: |
        for i in {1..24}; do
          notready=$(kubectl get nodes --no-headers | awk '{print $2}' | grep -v '^Ready$' | wc -l)
          if [ "$notready" -eq 0 ]; then
            echo "All nodes are Ready."
            exit 0
          fi
          echo "Waiting for nodes to become Ready ($i/24)..."
          sleep 5
        done
        echo "ERROR: Some nodes are still NotReady after waiting."
        kubectl get nodes -o wide
        exit 1
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

# Import application deployments (monitoring, Jellyfin etc.) so the top-level deploy covers apps as well
- import_playbook: ../plays/deploy-apps.yaml
  when: monitoring_enabled | default(true)

- import_playbook: ../plays/jellyfin.yml
  when: jellyfin_enabled | default(true)
