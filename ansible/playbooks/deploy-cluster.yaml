---
# =============================================================================
# VMStation Kubernetes Cluster Deployment
# Implements all 7 phases of the deployment specification
# Idempotent, bulletproof deployment for Debian kubeadm cluster
# =============================================================================

# =============================================================================
# Phase 0: System Preparation
# Install Kubernetes binaries, configure containerd, set up systemd services
# =============================================================================
- name: "Phase 0: System Preparation - Install Kubernetes Binaries"
  hosts: monitoring_nodes:storage_nodes
  gather_facts: true
  become: true
  tasks:
    - name: "Display Phase 0 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 0: System Preparation
          Target: {{ inventory_hostname }}
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    # Disable swap (required for Kubernetes)
    - name: "Disable swap"
      shell: swapoff -a
      changed_when: false

    - name: "Remove swap from fstab"
      lineinfile:
        path: /etc/fstab
        regexp: '.*swap.*'
        state: absent

    # Load required kernel modules
    - name: "Load kernel modules for containerd"
      modprobe:
        name: "{{ item }}"
        state: present
      loop:
        - overlay
        - br_netfilter

    - name: "Ensure modules load on boot"
      copy:
        dest: /etc/modules-load.d/kubernetes.conf
        content: |
          overlay
          br_netfilter

    # Configure sysctl parameters
    - name: "Configure sysctl for Kubernetes"
      sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        state: present
        reload: true
      loop:
        - { name: 'net.bridge.bridge-nf-call-iptables', value: '1' }
        - { name: 'net.bridge.bridge-nf-call-ip6tables', value: '1' }
        - { name: 'net.ipv4.ip_forward', value: '1' }

    # Install containerd with robust fallback logic
    - name: "Check if containerd is installed"
      stat:
        path: /usr/bin/containerd
      register: containerd_installed

    - name: "Install containerd (try containerd.io first)"
      apt:
        name: containerd.io
        state: present
        update_cache: true
      when: not containerd_installed.stat.exists
      ignore_errors: true
      register: containerd_io_install

    - name: "Install containerd (fallback to containerd package)"
      apt:
        name: containerd
        state: present
        update_cache: true
      when: 
        - not containerd_installed.stat.exists
        - containerd_io_install is failed

    # Configure containerd
    - name: "Create containerd config directory"
      file:
        path: /etc/containerd
        state: directory
        mode: '0755'

    - name: "Generate containerd default config"
      shell: containerd config default > /etc/containerd/config.toml
      args:
        creates: /etc/containerd/config.toml

    - name: "Enable SystemdCgroup in containerd"
      lineinfile:
        path: /etc/containerd/config.toml
        regexp: '^\s*SystemdCgroup\s*='
        line: '            SystemdCgroup = true'
        insertafter: '^\s*\[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options\]'

    - name: "Restart containerd"
      systemd:
        name: containerd
        state: restarted
        enabled: true
        daemon_reload: true

    # Install Kubernetes binaries
    - name: "Add Kubernetes apt key"
      apt_key:
        url: https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/Release.key
        state: present
        keyring: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

    - name: "Add Kubernetes apt repository"
      apt_repository:
        repo: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/ /"
        state: present
        filename: kubernetes

    - name: "Install Kubernetes binaries"
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: true

    - name: "Hold Kubernetes packages at current version"
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl

    - name: "Enable kubelet service"
      systemd:
        name: kubelet
        enabled: true

# =============================================================================
# Phase 1: Control Plane Initialization
# Initialize Kubernetes control plane with kubeadm on master node
# =============================================================================
- name: "Phase 1: Control Plane Initialization"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 1 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 1: Control Plane Initialization
          Target: {{ inventory_hostname }}
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Check if cluster is already initialized"
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeconfig_exists

    - name: "Initialize control plane (if not exists)"
      shell: |
        kubeadm init \
          --pod-network-cidr={{ pod_network_cidr }} \
          --service-cidr={{ service_network_cidr }} \
          --control-plane-endpoint={{ control_plane_endpoint }} \
          --upload-certs
      when: not kubeconfig_exists.stat.exists
      register: kubeadm_init

    - name: "Create .kube directory for root"
      file:
        path: /root/.kube
        state: directory
        mode: '0755'

    - name: "Copy admin.conf to .kube/config"
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        remote_src: true
        mode: '0600'

    - name: "Generate admin kubeconfig with proper RBAC (O=system:masters)"
      shell: |
        kubectl config set-credentials admin \
          --client-certificate=/etc/kubernetes/pki/admin.crt \
          --client-key=/etc/kubernetes/pki/admin.key \
          --embed-certs=true
        
        kubectl config set-context admin@kubernetes \
          --cluster=kubernetes \
          --user=admin
        
        kubectl config use-context admin@kubernetes
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

# =============================================================================
# Phase 2: Control Plane Validation
# Verify control plane components are running correctly
# =============================================================================
- name: "Phase 2: Control Plane Validation"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 2 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 2: Control Plane Validation
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Wait for API server to be ready"
      wait_for:
        host: "{{ ansible_host }}"
        port: 6443
        timeout: 120

    - name: "Verify control plane components are running"
      shell: |
        count=0
        if crictl ps 2>/dev/null | grep -q kube-apiserver; then count=`expr $count + 1`; fi
        if crictl ps 2>/dev/null | grep -q kube-controller-manager; then count=`expr $count + 1`; fi
        if crictl ps 2>/dev/null | grep -q kube-scheduler; then count=`expr $count + 1`; fi
        if crictl ps 2>/dev/null | grep -q etcd; then count=`expr $count + 1`; fi
        if kubectl cluster-info >/dev/null 2>&1; then count=`expr $count + 1`; fi
        echo $count
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: control_plane_check
      retries: 20
      delay: 15
      until: control_plane_check.stdout | int >= 4
      failed_when: false

    - name: "Alternative: Check control plane via systemd (if container check fails)"
      when: control_plane_check.stdout | int < 4
      shell: |
        count=0
        if systemctl is-active --quiet kubelet; then count=`expr $count + 1`; fi
        if systemctl is-active --quiet containerd; then count=`expr $count + 1`; fi
        if curl -k --max-time 5 https://localhost:6443/healthz >/dev/null 2>&1; then count=`expr $count + 1`; fi
        echo $count
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: systemd_check
      retries: 10
      delay: 10
      until: systemd_check.stdout | int >= 2

    - name: "Set final control plane status"
      set_fact:
        control_plane_ready: "{{ (control_plane_check.stdout | int >= 4) or (systemd_check.stdout | int >= 2) }}"

    - name: "Display control plane status"
      debug:
        msg: |
          Control plane validation results:
          - Container check: {{ control_plane_check.stdout | default('0') }} components running
          - Systemd check: {{ systemd_check.stdout | default('0') }} services running
          - Overall status: {{ 'READY' if control_plane_ready else 'NOT READY' }}

# =============================================================================
# Phase 3: Token Generation
# Generate fresh join tokens for worker nodes
# =============================================================================
- name: "Phase 3: Token Generation"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 3 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 3: Token Generation
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Ensure kubeadm binary available (locate or install)"
      block:
        - name: "Locate kubeadm in PATH"
          command: command -v kubeadm
          register: kubeadm_path
          changed_when: false
          failed_when: false

        - name: "Install kubeadm if not found (Debian)"
          apt:
            name: kubeadm
            state: present
            update_cache: yes
          when: kubeadm_path.rc != 0
          register: kubeadm_install
          ignore_errors: true

        - name: "Re-locate kubeadm after install"
          command: command -v kubeadm
          register: kubeadm_path_after
          changed_when: false
          failed_when: false
          when: kubeadm_path.rc != 0

        - name: "Set kubeadm full path fact"
          set_fact:
            kubeadm_full_path: "{{ kubeadm_path.stdout | default(kubeadm_path_after.stdout | default('/usr/bin/kubeadm')) }}"

      rescue:
        - name: "Fail clearly if kubeadm is not available"
          fail:
            msg: "kubeadm binary not found and automatic install failed. Ensure kubeadm is installed on the control plane node."

    - name: "Ensure admin kubeconfig is present for root"
      block:
        - name: "Create .kube directory for root (idempotent)"
          file:
            path: /root/.kube
            state: directory
            mode: '0700'

        - name: "Copy admin.conf to /root/.kube/config"
          copy:
            src: /etc/kubernetes/admin.conf
            dest: /root/.kube/config
            remote_src: true
            owner: root
            group: root
            mode: '0600'
          register: copy_admin_conf
          failed_when: false

      when: kubeadm_full_path is defined

    - name: "Wait for API server health before creating token"
      block:
        - name: "Check API health (loop until healthy)"
          shell: |
            set -o pipefail
            curl -k --max-time 5 -fsS https://127.0.0.1:6443/healthz || exit 1
          register: api_health
          retries: 12
          delay: 5
          until: api_health.rc == 0
          changed_when: false
          failed_when: api_health.rc != 0

      when: kubeadm_full_path is defined

    - name: "Generate join token (run once on control plane)"
      shell: "{{ kubeadm_full_path }} token create --print-join-command"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: join_command
      retries: 6
      delay: 10
      until: join_command.rc == 0
      run_once: true

    - name: "Store join command"
      set_fact:
        kubernetes_join_command: "{{ join_command.stdout }}"

    - name: "Persist join command to disk on control plane"
      copy:
        dest: /etc/kubernetes/join-command.txt
        content: "{{ kubernetes_join_command }}\n"
        owner: root
        group: root
        mode: '0640'
      run_once: true

    - name: "Display join command (for debugging)"
      debug:
        msg: "Join command: {{ kubernetes_join_command }}"

# =============================================================================
# Phase 4: Worker Node Join (Critical Phase)
# Join worker nodes to cluster with comprehensive error handling
# =============================================================================
- name: "Phase 4: Worker Node Join - Debian Workers Only"
  hosts: storage_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 4 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 4: Worker Node Join
          Target: {{ inventory_hostname }}
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Check if node is already joined"
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: "Skip join if already joined"
      debug:
        msg: "{{ inventory_hostname }} already joined to cluster"
      when: kubelet_conf.stat.exists

    # Pre-join cleanup
    - name: "Kill any hanging kubeadm join processes"
      shell: |
        pkill -f "kubeadm join" || true
        sleep 2
        pkill -9 -f "kubeadm join" || true
      when: not kubelet_conf.stat.exists

    - name: "Remove partial join state"
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/kubernetes/kubelet.conf
        - /etc/kubernetes/pki
        - /var/lib/kubelet/pki
      when: not kubelet_conf.stat.exists

    # Prerequisites validation
    - name: "Verify containerd socket exists"
      stat:
        path: /var/run/containerd/containerd.sock
      register: containerd_socket
      when: not kubelet_conf.stat.exists

    - name: "Fail if containerd socket missing"
      fail:
        msg: "Containerd socket not found at /var/run/containerd/containerd.sock"
      when: 
        - not kubelet_conf.stat.exists
        - not containerd_socket.stat.exists

    - name: "Verify kubeadm binary exists"
      stat:
        path: /usr/bin/kubeadm
      register: kubeadm_binary
      when: not kubelet_conf.stat.exists

    - name: "Fail if kubeadm binary missing"
      fail:
        msg: "kubeadm binary not found at /usr/bin/kubeadm"
      when:
        - not kubelet_conf.stat.exists
        - not kubeadm_binary.stat.exists

    - name: "Test connectivity to control plane"
      wait_for:
        host: "{{ hostvars[groups['monitoring_nodes'][0]].ansible_host }}"
        port: 6443
        timeout: 30
      when: not kubelet_conf.stat.exists

    # Execute join with retry logic
    - name: "Join worker node to cluster"
      shell: "{{ hostvars[groups['monitoring_nodes'][0]].kubernetes_join_command }}"
      when: not kubelet_conf.stat.exists
      register: join_result
      retries: 3
      delay: 10
      until: join_result.rc == 0
      failed_when: false

    # Log success
    - name: "Log successful join"
      copy:
        dest: /var/log/kubeadm-join.log
        content: |
          Join successful at {{ ansible_date_time.iso8601 }}
          Node: {{ inventory_hostname }}
          Command: {{ hostvars[groups['monitoring_nodes'][0]].kubernetes_join_command }}
          Result: {{ join_result.stdout }}
      when: 
        - not kubelet_conf.stat.exists
        - join_result.rc == 0

    # Handle failure
    - name: "Capture failure diagnostics"
      block:
        - name: "Get kubelet service status"
          shell: systemctl status kubelet || true
          register: kubelet_status

        - name: "Get kubelet logs"
          shell: journalctl -u kubelet -n 50 --no-pager || true
          register: kubelet_logs

        - name: "Get containerd logs"
          shell: journalctl -u containerd -n 50 --no-pager || true
          register: containerd_logs

        - name: "Check network connectivity"
          shell: |
            ss -tlnp | grep -E '6443|10250' || true
            ip route show || true
          register: network_status

        - name: "Write failure diagnostics"
          copy:
            dest: /var/log/kubeadm-join-failure.log
            content: |
              Join failed at {{ ansible_date_time.iso8601 }}
              Node: {{ inventory_hostname }}
              Command: {{ hostvars[groups['monitoring_nodes'][0]].kubernetes_join_command }}
              
              === Join Result ===
              {{ join_result.stdout }}
              {{ join_result.stderr }}
              
              === Kubelet Status ===
              {{ kubelet_status.stdout }}
              
              === Kubelet Logs ===
              {{ kubelet_logs.stdout }}
              
              === Containerd Logs ===
              {{ containerd_logs.stdout }}
              
              === Network Status ===
              {{ network_status.stdout }}

        - name: "Fail with diagnostic information"
          fail:
            msg: |
              Worker join failed. Diagnostics saved to /var/log/kubeadm-join-failure.log
              Check the log file for details.
      when:
        - not kubelet_conf.stat.exists
        - join_result.rc != 0

    # Health validation
    - name: "Wait for kubelet config file"
      wait_for:
        path: /etc/kubernetes/kubelet.conf
        timeout: 120
      when: 
        - not kubelet_conf.stat.exists
        - join_result.rc == 0

    - name: "Ensure kubelet is running"
      systemd:
        name: kubelet
        state: started
        enabled: true
      when:
        - not kubelet_conf.stat.exists
        - join_result.rc == 0

    - name: "Verify kubelet health"
      systemd:
        name: kubelet
        state: started
      register: kubelet_health
      when:
        - not kubelet_conf.stat.exists
        - join_result.rc == 0

# =============================================================================
# Phase 5: CNI Deployment
# Deploy Flannel CNI plugin for pod networking
# =============================================================================
- name: "Phase 5: CNI Deployment - Flannel"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 5 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 5: CNI Deployment (Flannel)
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Check if Flannel is already deployed"
      shell: kubectl get namespace kube-flannel
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_namespace
      failed_when: false
      changed_when: false

    - name: "Deploy Flannel CNI"
      shell: kubectl apply -f {{ playbook_dir }}/../../manifests/cni/flannel.yaml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      when: flannel_namespace.rc != 0

    - name: "Wait for Flannel pods to be ready"
      shell: |
        kubectl wait --for=condition=ready pod \
          -l app=flannel \
          -n kube-flannel \
          --timeout=120s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      retries: 5
      delay: 10
      register: flannel_ready
      until: flannel_ready.rc == 0

# =============================================================================
# Phase 6: Cluster Validation
# Verify all nodes are Ready and core components are running
# =============================================================================
- name: "Phase 6: Cluster Validation"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 6 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 6: Cluster Validation
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Get node status"
      shell: kubectl get nodes -o wide
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: nodes_status

    - name: "Display node status"
      debug:
        msg: "{{ nodes_status.stdout_lines }}"

    - name: "Verify all nodes are Ready"
      shell: kubectl get nodes -o json | jq -r '.items[] | select(.status.conditions[] | select(.type=="Ready" and .status=="True")) | .metadata.name' | wc -l
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: ready_nodes
      retries: 10
      delay: 10
      until: ready_nodes.stdout | int >= 2

    - name: "Get system pods status"
      shell: kubectl get pods -n kube-system -o wide
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: system_pods

    - name: "Display system pods"
      debug:
        msg: "{{ system_pods.stdout_lines }}"

    - name: "Verify CoreDNS is running"
      shell: kubectl get pods -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[*].status.phase}' | grep -c Running
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: coredns_pods
      retries: 10
      delay: 10
      until: coredns_pods.stdout | int >= 1

    - name: "Display validation success"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          ✅ Cluster Validation Successful
          - Ready Nodes: {{ ready_nodes.stdout }}
          - CoreDNS Pods: {{ coredns_pods.stdout }}
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# =============================================================================
# Phase 7: Application Deployment
# Deploy monitoring stack (Prometheus, Grafana, Loki)
# =============================================================================
- name: "Phase 7: Application Deployment - Monitoring Stack"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 7 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 7: Application Deployment (Monitoring)
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Create monitoring namespace"
      shell: kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Deploy Prometheus"
      shell: kubectl apply -f {{ playbook_dir }}/../../manifests/monitoring/prometheus.yaml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Deploy Grafana"
      shell: kubectl apply -f {{ playbook_dir }}/../../manifests/monitoring/grafana.yaml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: "Wait for Prometheus to be ready"
      shell: |
        kubectl wait --for=condition=available deployment/prometheus \
          -n monitoring \
          --timeout=120s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      retries: 5
      delay: 10
      register: prometheus_ready
      until: prometheus_ready.rc == 0
      failed_when: false

    - name: "Wait for Grafana to be ready"
      shell: |
        kubectl wait --for=condition=available deployment/grafana \
          -n monitoring \
          --timeout=120s
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      retries: 5
      delay: 10
      register: grafana_ready
      until: grafana_ready.rc == 0
      failed_when: false

    - name: "Display monitoring stack status"
      shell: kubectl get all -n monitoring
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: monitoring_status

    - name: "Display monitoring deployment result"
      debug:
        msg: "{{ monitoring_status.stdout_lines }}"

    - name: "Display deployment complete message"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          ✅ VMStation Kubernetes Cluster Deployment Complete!
          
          Cluster Status:
          - Control Plane: {{ groups['monitoring_nodes'][0] }}
          - Worker Nodes: {{ groups['storage_nodes'] | join(', ') }}
          - CNI: Flannel
          - Monitoring: Prometheus, Grafana
          
          Next Steps:
          - Access Grafana at http://{{ hostvars[groups['monitoring_nodes'][0]].ansible_host }}:30000
          - Check cluster: kubectl get nodes
          - View pods: kubectl get pods -A
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
