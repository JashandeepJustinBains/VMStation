---
# --- Preflight: Ensure CNI plugins are present on all nodes ---
- hosts: all
  become: true
  tasks:
    - name: Ensure /opt/cni/bin directory exists
      ansible.builtin.file:
        path: /opt/cni/bin
        state: directory
        mode: '0755'

    - name: Check if loopback CNI plugin exists
      ansible.builtin.stat:
        path: /opt/cni/bin/loopback
      register: loopback_cni

    - name: Download and install CNI plugins if missing
      block:
        - name: Download CNI plugins tarball (Debian/Ubuntu)
          ansible.builtin.get_url:
            url: https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz
            dest: /tmp/cni-plugins.tgz
            mode: '0644'
          when:
            - not loopback_cni.stat.exists
            - ansible_facts['os_family'] == 'Debian'

        - name: Download CNI plugins tarball (RHEL/Fallback)
          ansible.builtin.shell: |
            curl -L --retry 5 --retry-delay 2 -o /tmp/cni-plugins.tgz https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz
          args:
            creates: /tmp/cni-plugins.tgz
          when:
            - not loopback_cni.stat.exists
            - ansible_facts['os_family'] == 'RedHat'

        - name: Extract CNI plugins to /opt/cni/bin
          ansible.builtin.unarchive:
            src: /tmp/cni-plugins.tgz
            dest: /opt/cni/bin/
            remote_src: yes
          when: not loopback_cni.stat.exists

        - name: Ensure CNI plugins are executable
          ansible.builtin.file:
            path: /opt/cni/bin
            recurse: yes
            mode: '0755'
          when: not loopback_cni.stat.exists

    # CA trust tasks moved after kubeadm init
# Consolidated cluster deploy playbook: preflight -> apply network fixes -> deploy manifests -> validate

- hosts: all
  gather_facts: true
  roles:
    - system-prep
    - preflight
    - network-fix

# --- Pre-flight cluster health checks ---
- hosts: masternode
  become: true
  tasks:
    - name: Pre-flight | Warn if any nodes are NotReady (do not fail, just warn)
      ansible.builtin.shell: |
        notready=$(kubectl get nodes --no-headers | awk '{print $2}' | grep -v '^Ready$' | wc -l)
        if [ "$notready" -ne 0 ]; then
          echo "WARNING: Some nodes are NotReady before deployment. This is expected before CNI/Flannel is deployed."
          kubectl get nodes -o wide
        else
          echo "All nodes are Ready."
        fi
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Pre-flight | Warn if any nodes have NoSchedule taints (do not fail, just warn)
      ansible.builtin.shell: |
        tainted=$(kubectl get nodes -o json | jq '[.items[] | select(.spec.taints and ([.spec.taints[] | select(.effect=="NoSchedule")] | length) > 0)] | length')
        if [ "$tainted" -ne 0 ]; then
          echo "WARNING: Some nodes have NoSchedule taints before deployment."
          kubectl get nodes -o json | jq '.items[] | {name: .metadata.name, taints: .spec.taints}'
        else
          echo "No NoSchedule taints found."
        fi
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Pre-flight | Print system date/time
      ansible.builtin.shell: date
      args:
        executable: /bin/bash
      register: system_date
      changed_when: false

    - name: Pre-flight | Show system date/time
      ansible.builtin.debug:
        var: system_date.stdout

    - name: Pre-flight | Check /etc/kubernetes/admin.conf exists and print summary
      ansible.builtin.shell: |
        if [ -f /etc/kubernetes/admin.conf ]; then
          echo "admin.conf present."
          grep 'certificate-authority-data' /etc/kubernetes/admin.conf || true
        else
          echo "admin.conf MISSING!"
        fi
      args:
        executable: /bin/bash
      register: admin_conf_check
      changed_when: false

    - name: Pre-flight | Show admin.conf check
      ansible.builtin.debug:
        var: admin_conf_check.stdout_lines

    - name: Pre-flight | Check /etc/kubernetes/pki/ca.crt exists and print cert details
      ansible.builtin.shell: |
        if [ -f /etc/kubernetes/pki/ca.crt ]; then
          echo "ca.crt present."
          openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -text | grep -E 'Issuer:|Subject:|Not Before:|Not After :'
        else
          echo "ca.crt MISSING!"
        fi
      args:
        executable: /bin/bash
      register: ca_crt_check
      changed_when: false


    - name: Pre-flight | Show ca.crt check
      ansible.builtin.debug:
        var: ca_crt_check.stdout_lines

# --- Control plane initialization and CA trust ---
- hosts: masternode
  become: true
  tasks:
    - name: Initialize Kubernetes control plane if not already initialized (idempotent)
      ansible.builtin.shell: |
        if [ ! -f /etc/kubernetes/admin.conf ]; then
          echo "Running kubeadm init to bootstrap control plane..."
          kubeadm init --pod-network-cidr=10.244.0.0/16 --upload-certs
        else
          echo "Control plane already initialized. Skipping kubeadm init."
        fi
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: kubeadm_init_result
      ignore_errors: true

    - name: Show kubeadm init output (if any)
      ansible.builtin.debug:
        var: kubeadm_init_result.stdout_lines
      when: kubeadm_init_result is defined and kubeadm_init_result.stdout_lines is defined

    - name: Wait for Kubernetes API server to be available
      ansible.builtin.shell: |
        for i in {1..24}; do
          if kubectl version --request-timeout=5s >/dev/null 2>&1; then
            echo "API server is up"
            exit 0
          fi
          echo "Waiting for API server... ($i/24)"
          sleep 5
        done
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Trust Kubernetes CA system-wide (Debian/Ubuntu, masternode only)
      ansible.builtin.copy:
        src: /etc/kubernetes/pki/ca.crt
        dest: /usr/local/share/ca-certificates/kubernetes-ca.crt
        remote_src: yes
        mode: '0644'
      when:
        - ansible_facts['os_family'] == 'Debian'
        - inventory_hostname == 'masternode'
        - ansible.builtin.stat:
            path: /etc/kubernetes/pki/ca.crt
          register: ca_crt_stat
        - ca_crt_stat.stat.exists | default(false)

    - name: Update CA certificates (Debian/Ubuntu, masternode only)
      ansible.builtin.shell: update-ca-certificates
      when:
        - ansible_facts['os_family'] == 'Debian'
        - inventory_hostname == 'masternode'
        - ansible.builtin.stat:
            path: /usr/local/share/ca-certificates/kubernetes-ca.crt
          register: ca_cert_stat
        - ca_cert_stat.stat.exists | default(false)

    - name: Trust Kubernetes CA system-wide (RHEL, masternode only)
      ansible.builtin.copy:
        src: /etc/kubernetes/pki/ca.crt
        dest: /etc/pki/ca-trust/source/anchors/kubernetes-ca.crt
        remote_src: yes
        mode: '0644'
      when:
        - ansible_facts['os_family'] == 'RedHat'
        - inventory_hostname == 'masternode'
        - ansible.builtin.stat:
            path: /etc/kubernetes/pki/ca.crt
          register: ca_crt_stat_rh
        - ca_crt_stat_rh.stat.exists | default(false)

    - name: Update CA certificates (RHEL, masternode only)
      ansible.builtin.shell: update-ca-trust extract
      when:
        - ansible_facts['os_family'] == 'RedHat'
        - inventory_hostname == 'masternode'
        - ansible.builtin.stat:
            path: /etc/pki/ca-trust/source/anchors/kubernetes-ca.crt
          register: ca_cert_stat_rh
        - ca_cert_stat_rh.stat.exists | default(false)
- hosts: storagenodet3500,homelab
  become: true
  tasks:
    - name: Check if node is already joined (kubelet kubeconfig exists)
      ansible.builtin.stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: Fetch kubeadm join command from masternode
      ansible.builtin.shell: |
        kubeadm token create --print-join-command
      args:
        executable: /bin/bash
      delegate_to: masternode
      register: join_cmd
      when: not kubelet_conf.stat.exists

    - name: Join cluster if not already joined
      ansible.builtin.shell: |
        set -eux
        {{ join_cmd.stdout }} --ignore-preflight-errors=all
      args:
        executable: /bin/bash
      when: not kubelet_conf.stat.exists

    - name: Wait for node to appear in kubectl get nodes
      ansible.builtin.shell: |
        for i in {1..24}; do
          if kubectl get nodes | grep -q "$(hostname)"; then
            echo "Node joined successfully"
            exit 0
          fi
          echo "Waiting for node to join cluster ($i/24)..."
          sleep 5
        done
        echo "ERROR: Node did not join cluster in time" >&2
        exit 1
      args:
        executable: /bin/bash
      delegate_to: masternode
      when: not kubelet_conf.stat.exists

    - name: Debug join status
      ansible.builtin.debug:
        msg: "Node join attempted if not already joined. kubelet.conf exists: {{ kubelet_conf.stat.exists }}"

- hosts: masternode
  become: true
  tasks:
    - name: Initialize Kubernetes control plane if not already initialized (idempotent)
      ansible.builtin.shell: |
        if [ ! -f /etc/kubernetes/admin.conf ]; then
          echo "Running kubeadm init to bootstrap control plane..."
          kubeadm init --pod-network-cidr=10.244.0.0/16 --upload-certs
        else
          echo "Control plane already initialized. Skipping kubeadm init."
        fi
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: kubeadm_init_result
      ignore_errors: true

    - name: Show kubeadm init output (if any)
      ansible.builtin.debug:
        var: kubeadm_init_result.stdout_lines
      when: kubeadm_init_result is defined and kubeadm_init_result.stdout_lines is defined

    - name: Wait for Kubernetes API server to be available
      ansible.builtin.shell: |
        for i in {1..24}; do
          if kubectl version --request-timeout=5s >/dev/null 2>&1; then
            echo "API server is up"
            exit 0
          fi
          echo "Waiting for API server... ($i/24)"
          sleep 5
        done
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
    - block:
        - name: Ensure artifacts dir exists on controller
          delegate_to: localhost
          run_once: true
          ansible.builtin.file:
            path: "{{ playbook_dir }}/../ansible/artifacts"
            state: directory

        - name: Apply CNI manifests (flannel) with immutable-selector recovery
          ansible.builtin.shell: |
            set -o errexit -o nounset -o pipefail
            echo "Applying flannel manifests"
            if kubectl apply -f manifests/cni/flannel.yaml; then
              echo "flannel applied"
            else
              echo "Initial apply failed, attempting safe recovery: delete existing DaemonSet and re-apply"
              # delete the DaemonSet if present (handles immutable selector changes)
              kubectl -n kube-flannel delete daemonset kube-flannel-ds --ignore-not-found
              kubectl apply -f manifests/cni/flannel.yaml
            fi
          args:
            chdir: "{{ playbook_dir }}/../../"
            executable: /bin/bash
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf


        - name: Wait for flannel daemonset to be ready
          ansible.builtin.shell: |
            kubectl -n kube-flannel rollout status daemonset/kube-flannel-ds --timeout=180s
          args:
            executable: /bin/bash
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

        - name: Verify Flannel pods are running on all nodes (Kubernetes-native check)
          ansible.builtin.shell: |
            for i in {1..18}; do
              total_nodes=$(kubectl get nodes --no-headers | wc -l)
              running_flannel=$(kubectl -n kube-flannel get pods -l app=flannel -o json | jq '[.items[] | select(.status.phase=="Running")] | length')
              if [ "$running_flannel" -eq "$total_nodes" ]; then
                echo "Flannel pods verified: $running_flannel/$total_nodes nodes"
                exit 0
              fi
              echo "Waiting for Flannel pods: $running_flannel/$total_nodes nodes ready ($i/18)..."
              sleep 5
            done
            echo "ERROR: Not all Flannel pods are running after 90s"
            kubectl -n kube-flannel get pods -o wide
            exit 1
          args:
            executable: /bin/bash
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

        - name: Check CNI config presence on all nodes (robust diagnostic)
          ansible.builtin.shell: |
            for node in $(kubectl get nodes -o name | cut -d/ -f2); do
              echo "--- $node ---"
              ssh $node 'ls -l /etc/cni/net.d/ || echo "CNI config dir missing"'
              ssh $node 'cat /etc/cni/net.d/10-flannel.conflist 2>/dev/null || echo "10-flannel.conflist missing"'
            done
          args:
            executable: /bin/bash
          ignore_errors: true
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

        - name: Check kube-proxy pod status and auto-recover CrashLoopBackOff (RHEL 10)
          ansible.builtin.shell: |
            for node in $(kubectl get nodes -o name | cut -d/ -f2); do
              os=$(kubectl get node $node -o jsonpath='{.status.nodeInfo.osImage}')
              if echo "$os" | grep -qi 'Red Hat Enterprise Linux 10'; then
                pod=$(kubectl -n kube-system get pods -o wide | awk '$1 ~ /^kube-proxy/ && $7 == "'$node'" {print $1}')
                status=$(kubectl -n kube-system get pod $pod -o jsonpath='{.status.phase}')
                if [ "$status" = "CrashLoopBackOff" ]; then
                  echo "Restarting kube-proxy pod on $node (RHEL 10) due to CrashLoopBackOff..."
                  kubectl -n kube-system delete pod $pod
                fi
              fi
            done
          args:
            executable: /bin/bash
          ignore_errors: true
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

        - name: Wait for kube-proxy pods to be Running on all nodes
          ansible.builtin.shell: |
            for i in {1..18}; do
              total_nodes=$(kubectl get nodes --no-headers | wc -l)
              running_kp=$(kubectl -n kube-system get pods -l k8s-app=kube-proxy -o json | jq '[.items[] | select(.status.phase=="Running")] | length')
              if [ "$running_kp" -eq "$total_nodes" ]; then
                echo "kube-proxy pods verified: $running_kp/$total_nodes nodes"
                exit 0
              fi
              echo "Waiting for kube-proxy pods: $running_kp/$total_nodes nodes ready ($i/18)..."
              sleep 5
            done
            echo "ERROR: Not all kube-proxy pods are running after 90s"
            kubectl -n kube-system get pods -l k8s-app=kube-proxy -o wide
            exit 1
          args:
            executable: /bin/bash
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

        - name: Wait for all nodes to become Ready after Flannel and kube-proxy
          ansible.builtin.shell: |
            for i in {1..24}; do
              notready=$(kubectl get nodes --no-headers | awk '{print $2}' | grep -v '^Ready$' | wc -l)
              if [ "$notready" -eq 0 ]; then
                echo "All nodes are Ready."
                exit 0
              fi
              echo "Waiting for nodes to become Ready ($i/24)..."
              sleep 5
            done
            echo "ERROR: Some nodes are still NotReady after waiting."
            kubectl get nodes -o wide
            exit 1
          args:
            executable: /bin/bash
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

      rescue:
        - name: Run diagnostics role on failure
          import_role:
            name: diagnostics

    - name: Install idle-sleep timer and script on masternode
      import_role:
        name: idle-sleep

    - name: Uncordon any nodes that are still cordoned (idempotent)
      ansible.builtin.shell: |
        set -eux
        # list nodes that have spec.unschedulable=true
        nodes=$(kubectl get nodes -o jsonpath='{range .items[?(@.spec.unschedulable==true)]}{.metadata.name} "\n"{end}')
        if [ -n "$nodes" ]; then
          for n in $nodes; do
            echo "Uncordoning $n"
            kubectl uncordon "$n" || true
          done
        else
          echo "No cordoned nodes found"
        fi
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf


    - name: Uncordon all nodes after reset/deploy (make schedulable)
      ansible.builtin.shell: |
        set -eux
        kubectl get nodes --no-headers | awk '{print $1}' | xargs -r -n1 kubectl uncordon || true
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Remove common control-plane NoSchedule taints if present (idempotent)
      ansible.builtin.shell: |
        set -eux
        # remove control-plane/master taints that can block application scheduling in small clusters
        kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule- || true
        kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule- || true
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Ensure masternode has control-plane label for monitoring scheduling
      ansible.builtin.shell: |
        set -eux
        kubectl label node masternode 'node-role.kubernetes.io/control-plane=' --overwrite || true
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Validate flannel pods are stable before proceeding (external script)
      ansible.builtin.command:
        cmd: "{{ playbook_dir }}/../scripts/check_flannel_stability.sh"
      args:
        chdir: "{{ playbook_dir }}/.."
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_stability_check

    - name: Show flannel stability result
      ansible.builtin.debug:
        var: flannel_stability_check.stdout_lines

    - name: Wait for all nodes to become Ready after Flannel deploy
      ansible.builtin.shell: |
        for i in {1..24}; do
          notready=$(kubectl get nodes --no-headers | awk '{print $2}' | grep -v '^Ready$' | wc -l)
          if [ "$notready" -eq 0 ]; then
            echo "All nodes are Ready."
            exit 0
          fi
          echo "Waiting for nodes to become Ready ($i/24)..."
          sleep 5
        done
        echo "ERROR: Some nodes are still NotReady after waiting."
        kubectl get nodes -o wide
        exit 1
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

# --- Post-deploy: Summarize non-Ready pods and provide actionable diagnostics ---
  become: true
  tasks:
    - name: Get all non-Ready pods
      ansible.builtin.shell: |
        kubectl get pods --all-namespaces --field-selector=status.phase!=Running -o wide --kubeconfig /etc/kubernetes/admin.conf --insecure-skip-tls-verify
        echo "\nFor details on a pod, run: kubectl describe pod <pod> -n <namespace> --kubeconfig /etc/kubernetes/admin.conf --insecure-skip-tls-verify"
        echo "For logs: kubectl logs <pod> -n <namespace> --kubeconfig /etc/kubernetes/admin.conf --insecure-skip-tls-verify"
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: nonready_pods
      failed_when: false

    - name: Show non-Ready pod summary
      ansible.builtin.debug:
        var: nonready_pods.stdout_lines

# Import application deployments (monitoring, Jellyfin etc.) so the top-level deploy covers apps as well
- import_playbook: ../plays/deploy-apps.yaml
  when: monitoring_enabled | default(true)

- import_playbook: ../plays/jellyfin.yml
  when: jellyfin_enabled | default(true)
