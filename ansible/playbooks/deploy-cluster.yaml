---
# Consolidated cluster deploy playbook: preflight -> apply network fixes -> deploy manifests -> validate
- hosts: all
  gather_facts: false
  roles:
    - system-prep
    - preflight
    - network-fix

- hosts: masternode
  become: true
  tasks:
    - block:
        - name: Ensure artifacts dir exists on controller
          delegate_to: localhost
          run_once: true
          ansible.builtin.file:
            path: "{{ playbook_dir }}/../ansible/artifacts"
            state: directory

        - name: Apply CNI manifests (flannel) with immutable-selector recovery
          ansible.builtin.shell: |
            set -o errexit -o nounset -o pipefail
            echo "Applying flannel manifests"
            if kubectl apply -f manifests/cni/flannel.yaml; then
              echo "flannel applied"
            else
              echo "Initial apply failed, attempting safe recovery: delete existing DaemonSet and re-apply"
              # delete the DaemonSet if present (handles immutable selector changes)
              kubectl -n kube-flannel delete daemonset kube-flannel-ds --ignore-not-found
              kubectl apply -f manifests/cni/flannel.yaml
            fi
          args:
            chdir: "{{ playbook_dir }}/../../"
            executable: /bin/bash
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

        - name: Wait for flannel daemonset to be ready
          ansible.builtin.shell: |
            kubectl -n kube-flannel rollout status daemonset/kube-flannel-ds --timeout=180s
          args:
            executable: /bin/bash
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

        - name: Ensure kube-proxy-ready
          ansible.builtin.shell: |
            kubectl -n kube-system get pods -l 'k8s-app=kube-proxy' -o jsonpath='{.items[*].status.phase}'
          environment:
            KUBECONFIG: /etc/kubernetes/admin.conf

      rescue:
        - name: Run diagnostics role on failure
          import_role:
            name: diagnostics

    - name: Install idle-sleep timer and script on masternode
      import_role:
        name: idle-sleep

    - name: Uncordon any nodes that are still cordoned (idempotent)
      ansible.builtin.shell: |
        set -eux
        # list nodes that have spec.unschedulable=true
        nodes=$(kubectl get nodes -o jsonpath='{range .items[?(@.spec.unschedulable==true)]}{.metadata.name} "\n"{end}')
        if [ -n "$nodes" ]; then
          for n in $nodes; do
            echo "Uncordoning $n"
            kubectl uncordon "$n" || true
          done
        else
          echo "No cordoned nodes found"
        fi
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Remove common control-plane NoSchedule taints if present (idempotent)
      ansible.builtin.shell: |
        set -eux
        # remove control-plane/master taints that can block application scheduling in small clusters
        kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule- || true
        kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule- || true
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Ensure masternode has control-plane label for monitoring scheduling
      ansible.builtin.shell: |
        set -eux
        kubectl label node masternode 'node-role.kubernetes.io/control-plane=' --overwrite || true
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

    - name: Validate flannel pods are stable before proceeding
      ansible.builtin.shell: |
        set -eux
        echo "Checking flannel pod stability..."
        # wait up to 120s for all flannel pods to be Running and not restarting
        for i in {1..24}; do
          crash_count=$(kubectl get pods -n kube-flannel -l app=flannel -o jsonpath='{range .items[*]}{.status.containerStatuses[0].restartCount}{"\n"}{end}' | awk '{if($1>3)print}' | wc -l)
          pending_count=$(kubectl get pods -n kube-flannel -l app=flannel --field-selector=status.phase!=Running --no-headers 2>/dev/null | wc -l)
          
          if [ "$crash_count" -eq 0 ] && [ "$pending_count" -eq 0 ]; then
            echo "All flannel pods stable"
            exit 0
          fi
          
          echo "Waiting for flannel stability (attempt $i/24): crashes=$crash_count pending=$pending_count"
          sleep 5
        done
        
        echo "WARNING: Flannel pods may be unstable after 120s wait"
        kubectl get pods -n kube-flannel -o wide
        exit 0  # don't fail deploy, but warn
      args:
        executable: /bin/bash
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      register: flannel_stability_check

    - name: Show flannel stability result
      ansible.builtin.debug:
        var: flannel_stability_check.stdout_lines

# Import application deployments (monitoring, Jellyfin etc.) so the top-level deploy covers apps as well
- import_playbook: ../plays/deploy-apps.yaml
  when: monitoring_enabled | default(true)

- import_playbook: ../plays/jellyfin.yml
  when: jellyfin_enabled | default(true)
