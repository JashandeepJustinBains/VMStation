---
# =============================================================================
# VMStation Kubernetes Cluster Deployment
# Implements all 7 phases of the deployment specification
# Idempotent, bulletproof deployment for Debian kubeadm cluster
# =============================================================================

# =============================================================================
# Phase 0: System Preparation
# Install Kubernetes binaries, configure containerd, set up systemd services
# =============================================================================
- name: "Phase 0: System Preparation - Install Kubernetes Binaries"
  hosts: monitoring_nodes:storage_nodes
  gather_facts: true
  become: true
  tasks:
    - name: "Set default flags"
      set_fact:
        ipmi_required: "{{ ipmi_required | default(false) }}"
        # Optional Wake-on-LAN validation (disabled by default). Set to true in inventory/group_vars to run.
        wol_test: "{{ wol_test | default(false) }}"
        # Default WoL target list - override in inventory/group_vars with your MACs/IPs if needed.
        wol_targets: "{{ wol_targets | default([{'name':'storagenodet3500','ip':'192.168.4.61','mac':'b8:ac:6f:7e:6c:9d','user':'root'},{'name':'homelab','ip':'192.168.4.62','mac':'d0:94:66:30:d6:63','user':'jashandeepjustinbains'}]) }}"
      tags: always

    - name: "Display Phase 0 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# =============================================================================
# Phase 8: Optional Wake-on-LAN Validation (runs only when wol_test: true)
# This validates sleep -> wake behavior for a list of homelab nodes defined in `wol_targets`.
# Configure `wol_test: true` and `wol_targets` in inventory/group_vars to enable.
# =============================================================================
- name: "Phase 8: Wake-on-LAN Validation"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Skip WoL tests unless explicitly enabled"
      debug:
        msg: "Wake-on-LAN validation is disabled. Set wol_test: true to run."
      when: not wol_test | bool

    - name: "Run WoL validation sequence (only when enabled)"
      block:
        - name: "Build wol_targets from inventory when not explicitly provided"
          set_fact:
            wol_targets: >-
              [{% for h in (groups['monitoring_nodes'] + groups['storage_nodes'] + groups['compute_nodes']) | unique %}{% if hostvars[h].wol_mac is defined %}{'name':'{{ h }}','ip':'{{ hostvars[h].ansible_host }}','mac':'{{ hostvars[h].wol_mac.split()[0] }}','user':'{{ hostvars[h].ansible_user }}' }{% if not loop.last %},{% endif %}{% endif %}{% endfor %}]
          when: wol_targets is not defined

        - name: "Ensure wakeonlan is installed on control node (Debian family)"
          apt:
            name: wakeonlan
            state: present
          when: ansible_facts['os_family'] == 'Debian'

        - name: "Create sleep helper script on target nodes"
          copy:
            dest: /usr/local/bin/vmstation-sleep-ansible.sh
            mode: '0755'
            content: |
              #!/bin/sh
              # Minimal sleep helper: stop kubelet briefly and sleep to simulate node sleep
              systemctl stop kubelet || true
              sleep 10
              # Note: Do not actually power off control-plane nodes in automated tests

        - name: "Trigger sleep helper on each wol target (run as background)"
          shell: "ssh {{ item.user }}@{{ item.ip }} 'nohup /usr/local/bin/vmstation-sleep-ansible.sh >/tmp/vmstation-sleep.log 2>&1 &'"
          loop: "{{ wol_targets }}"
          delegate_to: localhost
          ignore_errors: yes
          register: wol_trigger_result

        - name: "Send WoL magic packets from control-plane"
          shell: "wakeonlan {{ item.mac }}"
          loop: "{{ wol_targets }}"
          register: wol_results
          delegate_to: localhost

        - name: "Wait for node SSH to return (timeout 120s)"
          wait_for:
            host: "{{ item.ip }}"
            port: 22
            timeout: 120
          loop: "{{ wol_targets }}"
          delegate_to: localhost
          ignore_errors: yes
          register: wol_wait_result

        - name: "Collect wake results"
          set_fact:
            wol_report: "{{ wol_report | default([]) + [ { 'name': item.name, 'ip': item.ip, 'mac': item.mac, 'wol_out': (wol_results.results | selectattr('item.mac','equalto', item.mac) | list)[0].stdout | default(''), 'ssh_accessible': (wol_wait_result.results | selectattr('item.ip','equalto', item.ip) | list)[0].failed | default(false) | ternary('no', 'yes') } ] }}"
          loop: "{{ wol_targets }}"

        - name: "Show WoL report"
          debug:
            var: wol_report
      when: wol_test | bool
    # Disable swap (required for Kubernetes)
    - name: "Disable swap"
      shell: swapoff -a
      changed_when: false

    - name: "Remove swap from fstab"
      lineinfile:
        path: /etc/fstab
        regexp: '.*swap.*'
        state: absent

    # Load required kernel modules
    - name: "Load kernel modules for containerd"
      modprobe:
        name: "{{ item }}"
        state: present
      loop:
        - overlay
        - br_netfilter

    - name: "Ensure modules load on boot"
      copy:
        dest: /etc/modules-load.d/kubernetes.conf
        content: |
          overlay
          br_netfilter

    # Configure sysctl parameters
    - name: "Configure sysctl for Kubernetes"
      sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        state: present
        reload: true
      loop:
        - { name: 'net.bridge.bridge-nf-call-iptables', value: '1' }
        - { name: 'net.bridge.bridge-nf-call-ip6tables', value: '1' }
        - { name: 'net.ipv4.ip_forward', value: '1' }

    # Install containerd with robust fallback logic
    - name: "Check if containerd is installed"
      stat:
        path: /usr/bin/containerd
      register: containerd_installed

    - name: "Install containerd (try containerd.io first)"
      apt:
        name: containerd.io
        state: present
        update_cache: true
      when: not containerd_installed.stat.exists
      ignore_errors: true
      register: containerd_io_install

    - name: "Install containerd (fallback to containerd package)"
      apt:
        name: containerd
        state: present
        update_cache: true
      when: 
        - not containerd_installed.stat.exists
        - containerd_io_install is failed

    # Configure containerd
    - name: "Create containerd config directory"
      file:
        path: /etc/containerd
        state: directory
        mode: '0755'

    - name: "Generate containerd default config"
      shell: containerd config default > /etc/containerd/config.toml
      args:
        creates: /etc/containerd/config.toml

    - name: "Enable SystemdCgroup in containerd"
      lineinfile:
        path: /etc/containerd/config.toml
        regexp: '^\s*SystemdCgroup\s*='
        line: '            SystemdCgroup = true'
        insertafter: '^\s*\[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options\]'

    - name: "Restart containerd"
      systemd:
        name: containerd
        state: restarted
        enabled: true
        daemon_reload: true

    # Configure crictl to use containerd
    - name: "Configure crictl runtime endpoint"
      copy:
        dest: /etc/crictl.yaml
        content: |
          runtime-endpoint: unix:///var/run/containerd/containerd.sock
          image-endpoint: unix:///var/run/containerd/containerd.sock
          timeout: 10
        mode: '0644'

    # Install Kubernetes binaries
    - name: "Add Kubernetes apt key"
      apt_key:
        url: https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/Release.key
        state: present
        keyring: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

    - name: "Add Kubernetes apt repository"
      apt_repository:
        repo: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/ /"
        state: present
        filename: kubernetes

    - name: "Install Kubernetes binaries"
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: true

    - name: "Hold Kubernetes packages at current version"
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl

    - name: "Enable kubelet service"
      systemd:
        name: kubelet
        enabled: true

    # Create required directories with proper permissions
    - name: "Create required Kubernetes directories"
      file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
      loop:
        - /opt/cni/bin
        - /etc/cni/net.d
        - /var/lib/kubelet

    # Install CNI plugins on all nodes (required for pod networking)
    - name: "Check if CNI plugins are installed"
      shell: "ls /opt/cni/bin 2>/dev/null | wc -l"
      register: cni_plugin_count
      changed_when: false

    - name: "Download CNI plugins if missing"
      get_url:
        url: "https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz"
        dest: /tmp/cni-plugins.tgz
        mode: '0644'
      when: cni_plugin_count.stdout | int < 5

    - name: "Extract CNI plugins"
      unarchive:
        src: /tmp/cni-plugins.tgz
        dest: /opt/cni/bin
        remote_src: yes
      when: cni_plugin_count.stdout | int < 5

# =============================================================================
# Phase 1: Control Plane Initialization
# Initialize Kubernetes control plane with kubeadm on master node
# =============================================================================
- name: "Phase 1: Control Plane Initialization"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 1 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 1: Control Plane Initialization
          Target: {{ inventory_hostname }}
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Check if cluster is already initialized"
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeconfig_exists

    - name: "Initialize control plane (if not exists)"
      shell: |
        kubeadm init \
          --pod-network-cidr={{ pod_network_cidr }} \
          --service-cidr={{ service_network_cidr }} \
          --control-plane-endpoint={{ control_plane_endpoint }} \
          --upload-certs
      when: not kubeconfig_exists.stat.exists
      register: kubeadm_init

    - name: "Regenerate admin.conf with kubeadm (fixes authentication issues)"
      shell: kubeadm init phase kubeconfig admin
      when: kubeconfig_exists.stat.exists

    - name: "Create .kube directory for root"
      file:
        path: /root/.kube
        state: directory
        mode: '0755'

    - name: "Copy admin.conf to /root/.kube/config"
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        remote_src: true
        mode: '0600'

    - name: "Set KUBECONFIG environment variable globally"
      lineinfile:
        path: /etc/environment
        line: 'KUBECONFIG=/etc/kubernetes/admin.conf'
        create: yes

    - name: "Add KUBECONFIG to root's bash profile"
      lineinfile:
        path: /root/.bashrc
        line: 'export KUBECONFIG=/etc/kubernetes/admin.conf'
        create: yes

# =============================================================================
# Phase 2: Control Plane Validation
# Verify control plane components are running correctly
# =============================================================================
- name: "Phase 2: Control Plane Validation"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 2 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 2: Control Plane Validation
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Wait for API server to be ready"
      wait_for:
        host: "{{ ansible_host }}"
        port: 6443
        timeout: 120

    - name: "Verify control plane is responding"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf cluster-info
      register: cluster_info
      retries: 10
      delay: 10
      until: cluster_info.rc == 0

    - name: "Display control plane status"
      debug:
        msg: "{{ cluster_info.stdout_lines }}"

# =============================================================================
# Phase 3: Token Generation
# Generate fresh join tokens for worker nodes
# =============================================================================
- name: "Phase 3: Token Generation"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 3 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 3: Token Generation
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Generate join token"
      shell: kubeadm token create --print-join-command
      register: join_command
      retries: 3
      delay: 5
      until: join_command.rc == 0

    - name: "Store join command"
      set_fact:
        kubernetes_join_command: "{{ join_command.stdout }}"

    - name: "Display join command"
      debug:
        msg: "Join command: {{ kubernetes_join_command }}"

# =============================================================================
# Phase 4: CNI Deployment
# Deploy Flannel CNI plugin before worker nodes join
# =============================================================================
- name: "Phase 4: CNI Deployment - Flannel"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 4 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 4: CNI Deployment (Flannel)
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Check if Flannel is already deployed"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get namespace kube-flannel 2>/dev/null
      register: flannel_namespace
      failed_when: false
      changed_when: false

    - name: "Deploy Flannel CNI"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/cni/flannel.yaml
      when: flannel_namespace.rc != 0
      register: flannel_deploy

    - name: "Wait for Flannel DaemonSet to be available"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get daemonset -n kube-flannel kube-flannel-ds 2>/dev/null
      register: flannel_ds
      retries: 10
      delay: 5
      until: flannel_ds.rc == 0
      when: flannel_deploy is changed

    - name: "Display Flannel deployment status"
      debug:
        msg: "Flannel CNI deployed successfully"

# =============================================================================
# Phase 5: Worker Node Join
# Join worker nodes to cluster
# =============================================================================
- name: "Phase 5: Worker Node Join"
  hosts: storage_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 5 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 5: Worker Node Join
          Target: {{ inventory_hostname }}
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Check if node is already joined"
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: "Join worker node to cluster"
      shell: "{{ hostvars[groups['monitoring_nodes'][0]].kubernetes_join_command }}"
      when: not kubelet_conf.stat.exists
      register: join_result

    - name: "Wait for kubelet to start"
      systemd:
        name: kubelet
        state: started
        enabled: true
      when: not kubelet_conf.stat.exists

    - name: "Display join status"
      debug:
        msg: "{{ inventory_hostname }} successfully joined the cluster"
      when: not kubelet_conf.stat.exists

# =============================================================================
# Phase 6: Cluster Validation
# Verify all nodes are Ready and core components are running
# =============================================================================
- name: "Phase 6: Cluster Validation"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 6 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 6: Cluster Validation
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Wait for nodes to be Ready"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o json | jq -r '.items[] | select(.status.conditions[] | select(.type=="Ready" and .status=="True")) | .metadata.name' | wc -l
      register: ready_nodes
      retries: 20
      delay: 10
      until: ready_nodes.stdout | int >= 2

    - name: "Get node status"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o wide
      register: nodes_status

    - name: "Display node status"
      debug:
        msg: "{{ nodes_status.stdout_lines }}"

    - name: "Wait for CoreDNS to be Running"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[*].status.phase}' | grep -c Running
      register: coredns_pods
      retries: 15
      delay: 10
      until: coredns_pods.stdout | int >= 1

    - name: "Display validation success"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          ✅ Cluster Validation Successful
          - Ready Nodes: {{ ready_nodes.stdout }}
          - CoreDNS Pods: {{ coredns_pods.stdout }}
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# =============================================================================
# Phase 7: Application Deployment
# Deploy monitoring stack (Prometheus, Grafana, Loki)
# =============================================================================
- name: "Phase 7: Application Deployment - Monitoring Stack"
  hosts: monitoring_nodes
  gather_facts: false
  become: true
  tasks:
    - name: "Display Phase 7 banner"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Phase 7: Application Deployment (Monitoring)
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: "Create monitoring namespace"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf create namespace monitoring --dry-run=client -o yaml | kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f -

    - name: "Apply monitoring PersistentVolumes and PVCs (prometheus/grafana/loki/promtail)"
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/monitoring/prometheus-pv.yaml || true
        kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/monitoring/grafana-pv.yaml || true
        kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/monitoring/loki-pv.yaml || true
        kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/monitoring/promtail-pv.yaml || true
      register: apply_pvs
      failed_when: false

    - name: "Fail if IPMI credentials required but not provided"
      fail:
        msg: "IPMI credentials not provided. Set ipmi_username and ipmi_password in ansible/inventory/group_vars/secrets.yml (ansible-vault) or create the secret manually."
      when: ipmi_required | default(false) and (ipmi_username is not defined or ipmi_password is not defined)

    - name: "Ensure ipmi credentials secret exists in monitoring namespace"
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: ipmi-credentials
            namespace: monitoring
          type: Opaque
          stringData:
            username: "{{ ipmi_username }}"
            password: "{{ ipmi_password }}"
      when: ipmi_username is defined and ipmi_password is defined
      no_log: true

    - name: "Deploy Node Exporter (system metrics)"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/monitoring/node-exporter.yaml

    - name: "Deploy Kube State Metrics (Kubernetes object state)"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/monitoring/kube-state-metrics.yaml

    - name: "Deploy Loki and Promtail (log aggregation and collection)"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/monitoring/loki.yaml

    - name: "Deploy IPMI Exporter"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/monitoring/ipmi-exporter.yaml

    - name: "Scale up remote IPMI exporter if credentials are available"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf scale deployment ipmi-exporter-remote -n monitoring --replicas=1
      when: ipmi_username is defined and ipmi_password is defined
      failed_when: false

    - name: "Deploy Prometheus"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/monitoring/prometheus.yaml

    - name: "Deploy Grafana"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/monitoring/grafana.yaml

    - name: "Wait for Node Exporter DaemonSet to be ready"
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf rollout status daemonset/node-exporter \
          -n monitoring \
          --timeout=120s
      retries: 3
      delay: 10
      register: node_exporter_ready
      until: node_exporter_ready.rc == 0
      failed_when: false

    - name: "Wait for Kube State Metrics to be ready"
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/kube-state-metrics \
          -n monitoring \
          --timeout=120s
      retries: 3
      delay: 10
      register: kube_state_metrics_ready
      until: kube_state_metrics_ready.rc == 0
      failed_when: false

    - name: "Wait for Loki to be ready"
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/loki \
          -n monitoring \
          --timeout=120s
      retries: 3
      delay: 10
      register: loki_ready
      until: loki_ready.rc == 0
      failed_when: false

    - name: "Wait for Promtail DaemonSet to be ready"
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf rollout status daemonset/promtail \
          -n monitoring \
          --timeout=120s
      retries: 3
      delay: 10
      register: promtail_ready
      until: promtail_ready.rc == 0
      failed_when: false

    - name: "Wait for Prometheus to be ready"
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/prometheus \
          -n monitoring \
          --timeout=120s
      retries: 5
      delay: 10
      register: prometheus_ready
      until: prometheus_ready.rc == 0
      failed_when: false

    - name: "Wait for Grafana to be ready"
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/grafana \
          -n monitoring \
          --timeout=120s
      retries: 5
      delay: 10
      register: grafana_ready
      until: grafana_ready.rc == 0
      failed_when: false

    - name: "Wait for Blackbox Exporter to be ready"
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/blackbox-exporter \
          -n monitoring \
          --timeout=120s
      register: blackbox_ready
      retries: 5
      delay: 10
      until: blackbox_ready.rc == 0
      failed_when: false

    - name: "Deploy Jellyfin media server"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ playbook_dir }}/../../manifests/jellyfin/jellyfin.yaml
      register: jellyfin_deploy
      failed_when: false

    - name: "Wait for Jellyfin pod to be ready"
      shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=ready pod/jellyfin \
          -n jellyfin \
          --timeout=300s
      retries: 3
      delay: 10
      register: jellyfin_ready
      until: jellyfin_ready.rc == 0
      failed_when: false

    - name: "Display monitoring stack status"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get all -n monitoring
      register: monitoring_status

    - name: "Display Jellyfin status"
      shell: kubectl --kubeconfig=/etc/kubernetes/admin.conf get all -n jellyfin -o wide
      register: jellyfin_status
      failed_when: false

    - name: "Display monitoring deployment result"
      debug:
        msg: "{{ monitoring_status.stdout_lines }}"

    - name: "Display Jellyfin deployment result"
      debug:
        msg: "{{ jellyfin_status.stdout_lines }}"
      when: jellyfin_status.rc == 0
    
    - name: "Verify Grafana endpoint is accessible"
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:30300/api/health"
        method: GET
        status_code: 200
        timeout: 10
      register: grafana_health
      retries: 3
      delay: 5
      until: grafana_health is succeeded
      failed_when: false
    
    - name: "Verify Prometheus endpoint is accessible"
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:30090/-/healthy"
        method: GET
        status_code: 200
        timeout: 10
      register: prometheus_health
      retries: 3
      delay: 5
      until: prometheus_health is succeeded
      failed_when: false
    
    - name: "Display endpoint health status"
      debug:
        msg: |
          Monitoring Endpoints Health:
          - Grafana:    {{ 'OK' if grafana_health is succeeded else 'NOT ACCESSIBLE' }}
          - Prometheus: {{ 'OK' if prometheus_health is succeeded else 'NOT ACCESSIBLE' }}
          
          {{ 'NOTE: If endpoints are not accessible, wait 30-60 seconds for pods to fully start' if (grafana_health is not succeeded or prometheus_health is not succeeded) else '' }}

    - name: "Display deployment complete message"
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          ✅ VMStation Kubernetes Cluster Deployment Complete!
          
          Cluster Status:
          - Control Plane: {{ groups['monitoring_nodes'][0] }}
          - Worker Nodes: {{ groups['storage_nodes'] | join(', ') }}
          - CNI: Flannel
          - Monitoring: Prometheus, Grafana
          - Media Server: Jellyfin (on {{ groups['storage_nodes'][0] }})
          
          Access URLs:
          - Grafana:    http://{{ hostvars[groups['monitoring_nodes'][0]].ansible_host }}:30300
          - Prometheus: http://{{ hostvars[groups['monitoring_nodes'][0]].ansible_host }}:30090
          - Jellyfin:   http://{{ hostvars[groups['storage_nodes'][0]].ansible_host }}:30096
          
          Next Steps:
          - Check cluster: kubectl get nodes
          - View pods: kubectl get pods -A
          - Test monitoring: curl http://{{ hostvars[groups['monitoring_nodes'][0]].ansible_host }}:30300
          - Test Jellyfin: curl http://{{ hostvars[groups['storage_nodes'][0]].ansible_host }}:30096/health
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
