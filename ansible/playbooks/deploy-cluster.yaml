---
# =============================================================================
# VMStation Kubernetes Cluster - Deploy Playbook
# Gold-standard, idempotent, OS-aware deployment
# Guarantees: No CrashLoopBackOff, No CoreDNS failures, All nodes Ready
# =============================================================================

# -----------------------------------------------------------------------------
# PHASE 1: System Preparation (All Nodes)
# Prepare system prerequisites before any Kubernetes components start
# -----------------------------------------------------------------------------
- name: Phase 1 - System preparation on all nodes
  hosts: all
  become: true
  gather_facts: true
  roles:
    - system-prep
    - preflight
    - network-fix

# -----------------------------------------------------------------------------
# PHASE 2: CNI Plugins Installation (All Nodes)
# Install CNI plugins before cluster initialization
# -----------------------------------------------------------------------------
- name: Phase 2 - Install CNI plugins on all nodes
  hosts: all
  become: true
  tasks:
    - name: Ensure /opt/cni/bin directory exists
      ansible.builtin.file:
        path: /opt/cni/bin
        state: directory
        mode: '0755'

    - name: Check if CNI plugins are already installed
      ansible.builtin.stat:
        path: /opt/cni/bin/loopback
      register: cni_installed

    - name: Download CNI plugins (if not present)
      ansible.builtin.get_url:
        url: https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz
        dest: /tmp/cni-plugins.tgz
        mode: '0644'
      when:
        - not cni_installed.stat.exists
        - ansible_os_family != 'RedHat'
      
    - name: Download CNI plugins with curl (RHEL fallback)
      ansible.builtin.shell: |
        curl -L -o /tmp/cni-plugins.tgz https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz
      args:
        creates: /tmp/cni-plugins.tgz
        executable: /bin/bash
      when:
        - not cni_installed.stat.exists
        - ansible_os_family == 'RedHat'

    - name: Extract CNI plugins
      ansible.builtin.unarchive:
        src: /tmp/cni-plugins.tgz
        dest: /opt/cni/bin/
        remote_src: yes
      when: not cni_installed.stat.exists

    - name: Ensure CNI plugins are executable
      ansible.builtin.file:
        path: /opt/cni/bin
        recurse: yes
        mode: '0755'

# -----------------------------------------------------------------------------
# PHASE 3: Control Plane Initialization
# Initialize Kubernetes control plane on masternode (idempotent)
# -----------------------------------------------------------------------------
- name: Phase 3 - Initialize Kubernetes control plane
  hosts: monitoring_nodes
  become: true
  tasks:
    - name: Check if control plane is already initialized
      ansible.builtin.stat:
        path: /etc/kubernetes/admin.conf
      register: admin_conf

    - name: Initialize control plane with kubeadm
      ansible.builtin.shell: |
        kubeadm init \
          --pod-network-cidr={{ pod_network_cidr }} \
          --service-cidr={{ service_network_cidr }} \
          --upload-certs \
          --control-plane-endpoint={{ control_plane_endpoint }}
      args:
        executable: /bin/bash
      when: not admin_conf.stat.exists
      register: kubeadm_init

    - name: Display kubeadm init output
      ansible.builtin.debug:
        var: kubeadm_init.stdout_lines
      when: kubeadm_init is changed

    - name: Wait for API server to be ready
      ansible.builtin.shell: |
        for i in {1..30}; do
          if kubectl --kubeconfig=/etc/kubernetes/admin.conf version &>/dev/null; then
            echo "API server ready"
            exit 0
          fi
          echo "Waiting for API server... ($i/30)"
          sleep 2
        done
        echo "API server not ready after 60 seconds"
        exit 1
      args:
        executable: /bin/bash

    - name: Ensure KUBECONFIG is set for root
      ansible.builtin.lineinfile:
        path: /root/.bashrc
        line: 'export KUBECONFIG=/etc/kubernetes/admin.conf'
        create: yes

    - name: Set KUBECONFIG for current session
      ansible.builtin.set_fact:
        ansible_env_kubeconfig: /etc/kubernetes/admin.conf

# -----------------------------------------------------------------------------
# PHASE 4: Worker Node Join
# Join worker nodes to cluster (idempotent)
# -----------------------------------------------------------------------------
- name: Phase 4 - Join worker nodes to cluster
  hosts: storage_nodes:compute_nodes
  become: true
  tasks:
    - name: Check if node is already joined
      ansible.builtin.stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: Generate join token on control plane
      ansible.builtin.shell: kubeadm token create --print-join-command
      args:
        executable: /bin/bash
      delegate_to: "{{ groups['monitoring_nodes'][0] }}"
      register: join_command
      when: not kubelet_conf.stat.exists
      run_once: true

    - name: Join worker node to cluster
      ansible.builtin.shell: "{{ join_command.stdout }} --ignore-preflight-errors=all"
      args:
        executable: /bin/bash
      when: not kubelet_conf.stat.exists

    - name: Wait for kubelet to start
      ansible.builtin.systemd:
        name: kubelet
        state: started
      when: not kubelet_conf.stat.exists

# -----------------------------------------------------------------------------
# PHASE 5: Flannel CNI Deployment
# Deploy Flannel CNI and wait for all pods to be Running
# CRITICAL: Ensure host networking is stable before Flannel deployment
# -----------------------------------------------------------------------------
- name: Phase 5 - Deploy Flannel CNI
  hosts: monitoring_nodes
  become: true
  tasks:
    - name: Ensure all nodes have stable networking before Flannel deployment
      ansible.builtin.shell: |
        # Verify all nodes are reachable and network is stable
        nodes=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}')
        
        echo "Verifying network stability for nodes: $nodes"
        all_stable=true
        
        for ip in $nodes; do
          # Ping test
          if ping -c 3 -W 2 $ip &>/dev/null; then
            echo "✓ Node $ip is reachable"
          else
            echo "✗ Node $ip is not reachable"
            all_stable=false
          fi
        done
        
        if [ "$all_stable" = "true" ]; then
          echo "All nodes have stable networking"
          exit 0
        else
          echo "Some nodes have unstable networking"
          exit 1
        fi
      args:
        executable: /bin/bash
      register: network_stability
      retries: 5
      delay: 10
      until: network_stability.rc == 0

    - name: Check if Flannel is already deployed
      ansible.builtin.shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf \
          -n kube-flannel get daemonset kube-flannel-ds &>/dev/null && echo "exists" || echo "missing"
      args:
        executable: /bin/bash
      register: flannel_check
      changed_when: false

    - name: Apply Flannel CNI manifest
      ansible.builtin.shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf \
          apply -f {{ playbook_dir }}/../../manifests/cni/flannel.yaml
      args:
        executable: /bin/bash
      when: flannel_check.stdout == "missing"

    - name: Wait for Flannel DaemonSet to be ready
      ansible.builtin.shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf \
          -n kube-flannel rollout status daemonset/kube-flannel-ds --timeout=240s
      args:
        executable: /bin/bash
      retries: 2
      delay: 10
      register: flannel_rollout
      until: flannel_rollout.rc == 0

    - name: Verify Flannel pods are Running on all nodes
      ansible.builtin.shell: |
        total_nodes=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes --no-headers | wc -l)
        running_pods=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf \
          -n kube-flannel get pods -l app=flannel --field-selector=status.phase=Running --no-headers | wc -l)
        
        if [ "$running_pods" -eq "$total_nodes" ]; then
          echo "All Flannel pods running: $running_pods/$total_nodes"
          exit 0
        else
          echo "Flannel pods not all running: $running_pods/$total_nodes"
          kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-flannel get pods -o wide
          exit 1
        fi
      args:
        executable: /bin/bash
      retries: 15
      delay: 10
      register: flannel_pods
      until: flannel_pods.rc == 0

    - name: Verify CNI config exists on all nodes (critical for pod networking)
      ansible.builtin.shell: |
        # Check CNI config via kubectl exec on Flannel pods (safer than SSH)
        all_present=true
        for node in $(kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o jsonpath='{.items[*].metadata.name}'); do
          pod=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-flannel get pods \
            --field-selector spec.nodeName=$node -l app=flannel -o jsonpath='{.items[0].metadata.name}')
          
          if [ -n "$pod" ]; then
            # Check if CNI config was created by init container
            if ! kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-flannel exec $pod \
              -- test -f /etc/cni/net.d/10-flannel.conflist 2>/dev/null; then
              echo "CNI config missing on node $node (pod: $pod)"
              all_present=false
            else
              echo "✓ CNI config present on node $node"
            fi
          else
            echo "No Flannel pod found on node $node"
            all_present=false
          fi
        done
        
        if [ "$all_present" = "true" ]; then
          exit 0
        else
          echo "Not all nodes have CNI config"
          exit 1
        fi
      args:
        executable: /bin/bash
      retries: 10
      delay: 10
      register: cni_config_check
      until: cni_config_check.rc == 0

    - name: Restart kube-proxy pods to recover from CrashLoopBackOff (if any)
      ansible.builtin.shell: |
        # Delete any CrashLoopBackOff or Error kube-proxy pods to force clean restart
        crash_pods=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf \
          -n kube-system get pods -l k8s-app=kube-proxy \
          -o json | jq -r '.items[] | select(.status.containerStatuses[]? | (.state.waiting.reason? == "CrashLoopBackOff" or .state.waiting.reason? == "Error")) | .metadata.name')
        
        if [ -n "$crash_pods" ]; then
          echo "Restarting CrashLoopBackOff/Error kube-proxy pods:"
          echo "$crash_pods"
          echo "$crash_pods" | xargs -r kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-system delete pod --wait=false
          echo "Waiting 45s for kube-proxy to restart with CNI now available..."
          sleep 45
        else
          echo "No kube-proxy pods in CrashLoopBackOff/Error state"
        fi
      args:
        executable: /bin/bash
      changed_when: false
      ignore_errors: true

    - name: Wait for all kube-proxy pods to be Running
      ansible.builtin.shell: |
        total_nodes=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes --no-headers | wc -l)
        running_proxies=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf \
          -n kube-system get pods -l k8s-app=kube-proxy --field-selector=status.phase=Running --no-headers | wc -l)
        
        if [ "$running_proxies" -eq "$total_nodes" ]; then
          echo "All kube-proxy pods running: $running_proxies/$total_nodes"
          exit 0
        else
          echo "kube-proxy pods not all running: $running_proxies/$total_nodes"
          kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-system get pods -l k8s-app=kube-proxy -o wide
          exit 1
        fi
      args:
        executable: /bin/bash
      retries: 12
      delay: 10
      register: proxy_pods
      until: proxy_pods.rc == 0

# -----------------------------------------------------------------------------
# PHASE 6: Wait for All Nodes to be Ready
# Ensure all nodes are Ready before proceeding to application deployment
# -----------------------------------------------------------------------------
- name: Phase 6 - Wait for all nodes to be Ready
  hosts: monitoring_nodes
  become: true
  tasks:
    - name: Wait for all nodes to be Ready
      ansible.builtin.shell: |
        for i in {1..18}; do
          total_nodes=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes --no-headers | wc -l)
          ready_nodes=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf \
            get nodes --no-headers | awk '$2 == "Ready"' | wc -l)
          
          if [ "$ready_nodes" -eq "$total_nodes" ] && [ "$total_nodes" -gt 0 ]; then
            echo "All nodes Ready: $ready_nodes/$total_nodes"
            kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o wide
            exit 0
          fi
          
          echo "Waiting for nodes to be Ready: $ready_nodes/$total_nodes ($i/18)"
          sleep 10
        done
        
        echo "Not all nodes Ready after 3 minutes"
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o wide
        exit 1
      args:
        executable: /bin/bash

# -----------------------------------------------------------------------------
# PHASE 7: Node Configuration
# Remove taints and uncordon nodes for workload scheduling
# -----------------------------------------------------------------------------
- name: Phase 7 - Configure node scheduling
  hosts: monitoring_nodes
  become: true
  tasks:
    - name: Remove NoSchedule taint from control-plane (allow scheduling)
      ansible.builtin.shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf \
          taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule- || true
      args:
        executable: /bin/bash
      changed_when: false

    - name: Uncordon all nodes
      ansible.builtin.shell: |
        for node in $(kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o name); do
          kubectl --kubeconfig=/etc/kubernetes/admin.conf uncordon $node || true
        done
      args:
        executable: /bin/bash
      changed_when: false

# -----------------------------------------------------------------------------
# PHASE 8: Post-Deployment Validation
# Verify cluster health and component status
# -----------------------------------------------------------------------------
- name: Phase 8 - Post-deployment validation
  hosts: monitoring_nodes
  become: true
  tasks:
    - name: Verify kube-system pods are Running
      ansible.builtin.shell: |
        kubectl --kubeconfig=/etc/kubernetes/admin.conf \
          -n kube-system get pods -o wide
      args:
        executable: /bin/bash
      register: kube_system_pods
      changed_when: false

    - name: Display kube-system pod status
      ansible.builtin.debug:
        var: kube_system_pods.stdout_lines

    - name: Check for CrashLoopBackOff pods
      ansible.builtin.shell: |
        crash_pods=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf \
          get pods -A --field-selector=status.phase=Running \
          -o json | jq -r '.items[] | select(.status.containerStatuses[]? | .state.waiting.reason? == "CrashLoopBackOff") | .metadata.name')
        
        if [ -n "$crash_pods" ]; then
          echo "WARNING: CrashLoopBackOff pods detected:"
          echo "$crash_pods"
          exit 1
        else
          echo "No CrashLoopBackOff pods detected"
          exit 0
        fi
      args:
        executable: /bin/bash
      register: crash_check
      changed_when: false
      failed_when: false

    - name: Display crash check result
      ansible.builtin.debug:
        var: crash_check.stdout_lines

    - name: Verify CNI config exists on all nodes
      ansible.builtin.shell: |
        # Use IP addresses from inventory instead of hostnames
        {% for host in groups['all'] %}
        echo "Checking CNI config on {{ host }} ({{ hostvars[host]['ansible_host'] }})..."
        ssh -o StrictHostKeyChecking=no {{ hostvars[host]['ansible_host'] }} \
          'test -f /etc/cni/net.d/10-flannel.conflist && echo "✓ CNI config present" || echo "✗ CNI config missing"'
        {% endfor %}
      args:
        executable: /bin/bash
      register: cni_config_check
      changed_when: false
      failed_when: false

    - name: Display CNI config check results
      ansible.builtin.debug:
        var: cni_config_check.stdout_lines

    - name: Final cluster status summary
      ansible.builtin.shell: |
        echo "=== Cluster Deployment Complete ==="
        echo ""
        echo "Nodes:"
        kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes -o wide
        echo ""
        echo "Pods (kube-system):"
        kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-system get pods -o wide
        echo ""
        echo "Pods (kube-flannel):"
        kubectl --kubeconfig=/etc/kubernetes/admin.conf -n kube-flannel get pods -o wide
      args:
        executable: /bin/bash
      register: final_status
      changed_when: false

    - name: Display final cluster status
      ansible.builtin.debug:
        var: final_status.stdout_lines

# -----------------------------------------------------------------------------
# PHASE 9: Application Deployment
# Import higher-level application playbooks (monitoring stack, dashboards, Jellyfin)
# These are run after the cluster is healthy and ready for workloads.
# -----------------------------------------------------------------------------
- import_playbook: ../plays/deploy-apps.yaml

- import_playbook: ../plays/jellyfin.yml
