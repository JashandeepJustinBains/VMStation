=========================================
VMStation Security Audit
=========================================

[1/10] Checking for hardcoded secrets...
⚠️  WARNING: Hardcoded passwords - Found potential hardcoded passwords
  Review these files and use Kubernetes Secrets or ansible-vault

[2/10] Checking SSH key security...
✅ PASS: SSH directory permissions

[3/10] Checking for encrypted sensitive files...
⚠️  WARNING: Ansible vault encryption - secrets.yml not encrypted with ansible-vault

[4/10] Checking Kubernetes security configurations...
⚠️  WARNING: Privileged containers - Found privileged container configurations
  Consider using specific capabilities instead
⚠️  WARNING: Host network usage - Pods using host network detected
  Review necessity of hostNetwork configuration
⚠️  WARNING: Resource limits - Some deployments may be missing resource limits

[5/10] Checking RBAC configurations...
✅ PASS: RBAC ClusterRoles
✅ PASS: RBAC verb specificity

[6/10] Checking script file permissions...
✅ PASS: Script permissions

[7/10] Checking .gitignore for sensitive patterns...
✅ PASS: Gitignore coverage

[8/10] Checking monitoring security configuration...
✅ PASS: Grafana anonymous access
✅ PASS: Grafana anonymous role

[9/10] Checking network security...
✅ PASS: NodePort services
  For production: Consider using Ingress with TLS
✅ PASS: LoadBalancer services

[10/10] Checking container image configurations...
⚠️  WARNING: Container image tags - Using :latest tag detected
  Pin to specific versions for reproducibility
✅ PASS: Container images
  Verify these are from trusted sources:
  - docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1
  - docker.io/flannel/flannel:v0.24.2
  - ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
  - ghcr.io/flannel-io/flannel:v0.27.4
  - grafana/grafana:10.0.0
  - grafana/loki:2.9.2
  - grafana/promtail:2.9.2
  - jellyfin/jellyfin:latest
  - prometheuscommunity/ipmi-exporter:v1.6.1
  - prom/node-exporter:v1.6.1
  - prom/prometheus:v2.45.0
  - registry.k8s.io/coredns/coredns:v1.11.1
  - registry.k8s.io/kube-proxy:v1.29.15
  - registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.0

=========================================
Security Audit Summary
=========================================
Passed:   10
Warnings: 6
Errors:   0

⚠️  Security audit found warnings - review recommended
=========================================
VMStation Complete Validation Suite
=========================================

This test suite validates:
  1. Auto-sleep and wake configuration
  2. Monitoring exporters health
  3. Loki log aggregation
  4. Sleep/wake cycle (optional - requires confirmation)

Test order:
  - Non-destructive tests run first
  - Sleep/wake cycle test is optional (requires user confirmation)

Phase 1: Configuration Validation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Auto-Sleep/Wake Configuration
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Auto-Sleep/Wake Validation
Testing sleep/wake cycle and monitoring
=========================================

[1/10] Testing systemd timer on storagenodet3500...
❌ FAIL: Auto-sleep timer is not enabled on storagenodet3500

[2/10] Testing systemd timer on homelab (RHEL10)...
❌ FAIL: Auto-sleep timer is not enabled on homelab

[3/10] Testing auto-sleep script existence...
❌ FAIL: Auto-sleep monitor script missing on storagenodet3500
❌ FAIL: Sleep script missing on storagenodet3500

[4/10] Testing WoL configuration...
✅ PASS: WoL script exists and is executable
⚠️  WARN: WoL systemd service not found on masternode

[5/10] Testing kubectl access...
✅ PASS: kubectl access verified on masternode
ℹ️  INFO: Current cluster status:
  masternode         Ready   control-plane   7m33s   v1.29.15
  storagenodet3500   Ready   <none>          7m11s   v1.29.15

[6/10] Testing WoL tool availability...
✅ PASS: wakeonlan tool is available

[7/10] Testing node reachability...
✅ PASS: storagenodet3500 is reachable (192.168.4.61)
✅ PASS: homelab is reachable (192.168.4.62)

[8/10] Testing monitoring service configuration...
✅ PASS: Monitoring namespace exists
✅ PASS: Prometheus pods are running
✅ PASS: Grafana pods are running

[9/10] Testing log file configuration...
✅ PASS: VMStation state directory exists
✅ PASS: Auto-sleep log files exist

[10/10] Testing systemd timer schedules...
✅ PASS: Auto-sleep timer is scheduled
ℹ️  INFO: Timer schedule:
  Wed 2025-10-08 14:52:24 EDT 9min left Wed 2025-10-08 14:37:24 EDT 5min ago vmstation-autosleep.timer vmstation-autosleep.service

=========================================
Test Results Summary
=========================================
Passed:   11
Failed:   4
Warnings: 1

❌ Some tests failed. Review details above.

Common fixes:
  1. Deploy auto-sleep: ./deploy.sh setup
  2. Check systemd status: systemctl status vmstation-autosleep.timer
  3. Review logs: journalctl -u vmstation-autosleep -n 50

❌ SUITE FAILED: Auto-Sleep/Wake Configuration


Phase 2: Monitoring Health Validation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Monitoring Exporters Health
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Monitoring Exporters Health
Validating exporters, targets, and dashboards
=========================================

[1/8] Testing Prometheus targets...
✅ PASS: Prometheus targets API accessible
ℹ️  INFO: Targets UP: 14, DOWN: 4
❌ FAIL: 4 targets are DOWN
  - kubernetes-service-endpoints

[2/8] Testing node-exporter on all nodes...
curl http://192.168.4.63:9100/metrics success
✅ PASS: Node exporter healthy on masternode
curl http://192.168.4.61:9100/metrics success
✅ PASS: Node exporter healthy on storagenodet3500
❌ SUITE FAILED: Monitoring Exporters Health


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Loki Log Aggregation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Loki Log Aggregation Test
Validating Loki connectivity and log ingestion
=========================================

[1/6] Testing Loki pod status...
ℹ️  INFO: Loki pods found:
  loki-85ccd4898d-xq56h                 0/1     CrashLoopBackOff   6 (59s ago)   6m59s
❌ FAIL: Loki pods are not running

[2/6] Testing Loki service configuration...
✅ PASS: Loki service exists
ℹ️  INFO: Loki service details:
  loki                   NodePort    10.104.56.130    <none>        3100:31100/TCP      7m
✅ PASS: Loki service has endpoints

[3/6] Testing Loki API connectivity...
curl http://192.168.4.63:3100/ready error
⚠️  WARN: Loki ready endpoint not accessible (may be ClusterIP only)
ℹ️  INFO: Attempting to test via kubectl exec...
❌ FAIL: Loki is not ready

[4/6] Testing Promtail log shipper...
ℹ️  INFO: Promtail pods found:
  promtail-tn9jk                        1/1     Running            0             7m2s
  promtail-wv7mf                        1/1     Running            0             7m2s
✅ PASS: Promtail pods are running (2 instances)

[5/6] Testing Loki DNS resolution...
✅ PASS: Loki DNS resolution successful

[6/6] Testing Loki datasource in Grafana...
✅ PASS: Loki datasource is configured in Grafana
⚠️  WARN: Could not check Loki datasource health

=========================================
Test Results Summary
=========================================
Passed:   5
Failed:   2
Warnings: 2

❌ Loki log aggregation has issues.

Common fixes:
  1. Check Loki logs: kubectl logs -n monitoring -l app=loki
  2. Check Promtail logs: kubectl logs -n monitoring -l app=promtail
  3. Verify Loki service: kubectl get svc -n monitoring loki
  4. Check DNS: kubectl run -it --rm dns-test --image=busybox --restart=Never -- nslookup loki.monitoring

Connectivity errors:
  - DNS lookup failures: Check CoreDNS pods
  - 500 status: Check Loki logs for errors
  - Service unavailable: Verify Loki pods are running

❌ SUITE FAILED: Loki Log Aggregation


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Monitoring Access (Updated)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Monitoring Access Test
Testing anonymous access to endpoints
=========================================

[1/8] Testing Grafana Access...
Testing Grafana Web UI... ✅ PASS
  curl http://192.168.4.63:30300 success
Testing Grafana API (anonymous)... ✅ PASS
  curl http://192.168.4.63:30300/api/health success

[2/8] Testing Prometheus Access...
Testing Prometheus Web UI... ❌ FAIL (unexpected response)
  curl http://192.168.4.63:30090 failure
❌ SUITE FAILED: Monitoring Access (Updated)


Phase 3: Sleep/Wake Cycle Test (Optional)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

WARNING: This test will trigger cluster sleep and wake.
This is a destructive test that will:
  - Cordon and drain worker nodes
  - Scale down deployments
  - Send Wake-on-LAN packets
  - Measure wake time and validate service restoration

Run sleep/wake cycle test? [y/N]: y
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Sleep/Wake Cycle
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Sleep/Wake Cycle Test
Automated testing of full sleep/wake cycle
=========================================

⚠️  WARNING: This test will:
  1. Trigger cluster sleep (drain nodes, cordon)
  2. Send Wake-on-LAN packets to worker nodes
  3. Measure wake time and validate services

Continue with sleep/wake cycle test? [y/N]: y

[1/7] Recording initial cluster state...
✅ PASS: Cluster is accessible
ℹ️  INFO: Initial node status:
  masternode         Ready   control-plane   7m52s   v1.29.15
  storagenodet3500   Ready   <none>          7m30s   v1.29.15
ℹ️  INFO: Ready nodes: 2

[2/7] Triggering cluster sleep...
ℹ️  INFO: Running vmstation-sleep.sh on masternode...
[2025-10-08 14:42:43] ==========================================
[2025-10-08 14:42:43] Initiating cluster sleep sequence
[2025-10-08 14:42:43] ==========================================
[2025-10-08 14:42:43] Step 1: Cordoning and draining worker nodes...
[2025-10-08 14:42:43] Skipping control-plane node: masternode
[2025-10-08 14:42:43] Cordoning node: storagenodet3500
node/storagenodet3500 cordoned
[2025-10-08 14:42:43] Draining node: storagenodet3500
node/storagenodet3500 already cordoned
Warning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-w627d, kube-system/kube-proxy-jsnkw, monitoring/node-exporter-w9628, monitoring/promtail-tn9jk; deleting Pods that declare no controller: jellyfin/jellyfin
evicting pod jellyfin/jellyfin
pod/jellyfin evicted
node/storagenodet3500 drained
[2025-10-08 14:42:46] Step 2: Scaling down deployments...
[2025-10-08 14:42:46] No deployments found in default namespace
[2025-10-08 14:42:46] ==========================================
[2025-10-08 14:42:46] Cluster sleep sequence completed
[2025-10-08 14:42:46] ==========================================
✅ PASS: Sleep script executed

[3/7] Verifying node status after sleep...
ℹ️  INFO: Node status after sleep:
  masternode         Ready                      control-plane   8m5s    v1.29.15
  storagenodet3500   Ready,SchedulingDisabled   <none>          7m43s   v1.29.15
✅ PASS: Worker nodes are cordoned (1 nodes)

[4/7] Sending Wake-on-LAN packets...
ℹ️  INFO: Waking storagenodet3500 (b8:ac:6f:7e:6c:9d)...
Sending magic packet to 255.255.255.255:9 with b8:ac:6f:7e:6c:9d
✅ PASS: WoL packet sent to storagenodet3500 (wakeonlan)
ℹ️  INFO: Waking homelab (d0:94:66:30:d6:63)...
Sending magic packet to 255.255.255.255:9 with d0:94:66:30:d6:63
✅ PASS: WoL packet sent to homelab (wakeonlan)

[5/7] Measuring wake time...
ℹ️  INFO: Waiting for storagenodet3500 to respond (timeout: 120s)...
✅ PASS: storagenodet3500 responded after 0s
ℹ️  INFO: Waiting for homelab to respond (timeout: 120s)...
✅ PASS: homelab responded after 0s

[6/7] Validating service restoration...
ℹ️  INFO: Waiting 30s for services to stabilize...
✅ PASS: kubelet is active on storagenodet3500
❌ FAIL: node-exporter is not responding on storagenodet3500
✅ PASS: rke2 service is active on homelab
ℹ️  INFO: Checking cluster node status...
ℹ️  INFO: Node status after wake:
  masternode         Ready                      control-plane   8m37s   v1.29.15
  storagenodet3500   Ready,SchedulingDisabled   <none>          8m15s   v1.29.15
ℹ️  INFO: Note: Nodes remain cordoned until manually uncordoned with 'kubectl uncordon <node>'

[7/7] Validating monitoring stack...
curl http://192.168.4.63:30090/-/healthy error
❌ FAIL: Prometheus is not healthy after wake
curl http://192.168.4.63:30300/api/health ok
✅ PASS: Grafana is healthy after wake

=========================================
Sleep/Wake Cycle Test Results
=========================================
Passed: 10
Failed: 2

Wake Time Summary:
  storagenodet3500: 0s
  homelab:          0s

❌ Sleep/wake cycle test encountered failures.

Review details above for troubleshooting.
❌ SUITE FAILED: Sleep/Wake Cycle


=========================================
Complete Validation Summary
=========================================

Test Suites Run:    5
Suites Passed:      0
Suites Failed:      5

❌ Some test suites failed.

Review the output above for details.

Common next steps:
  1. Fix failed tests and re-run: ./tests/test-complete-validation.sh
  2. Deploy missing components: ./deploy.sh setup
  3. Check cluster health: kubectl get pods -A
  4. Review logs: journalctl -u vmstation-autosleep -n 50

=========================================
VMStation Auto-Sleep/Wake Validation
Testing sleep/wake cycle and monitoring
=========================================

[1/10] Testing systemd timer on storagenodet3500...
❌ FAIL: Auto-sleep timer is not enabled on storagenodet3500

[2/10] Testing systemd timer on homelab (RHEL10)...
❌ FAIL: Auto-sleep timer is not enabled on homelab

[3/10] Testing auto-sleep script existence...
❌ FAIL: Auto-sleep monitor script missing on storagenodet3500
❌ FAIL: Sleep script missing on storagenodet3500

[4/10] Testing WoL configuration...
✅ PASS: WoL script exists and is executable
⚠️  WARN: WoL systemd service not found on masternode

[5/10] Testing kubectl access...
✅ PASS: kubectl access verified on masternode
ℹ️  INFO: Current cluster status:
  masternode         Ready                      control-plane   8m39s   v1.29.15
  storagenodet3500   Ready,SchedulingDisabled   <none>          8m17s   v1.29.15

[6/10] Testing WoL tool availability...
✅ PASS: wakeonlan tool is available

[7/10] Testing node reachability...
✅ PASS: storagenodet3500 is reachable (192.168.4.61)
✅ PASS: homelab is reachable (192.168.4.62)

[8/10] Testing monitoring service configuration...
✅ PASS: Monitoring namespace exists
✅ PASS: Prometheus pods are running
✅ PASS: Grafana pods are running

[9/10] Testing log file configuration...
✅ PASS: VMStation state directory exists
✅ PASS: Auto-sleep log files exist

[10/10] Testing systemd timer schedules...
✅ PASS: Auto-sleep timer is scheduled
ℹ️  INFO: Timer schedule:
  Wed 2025-10-08 14:52:24 EDT 8min left Wed 2025-10-08 14:37:24 EDT 6min ago vmstation-autosleep.timer vmstation-autosleep.service

=========================================
Test Results Summary
=========================================
Passed:   11
Failed:   4
Warnings: 1

❌ Some tests failed. Review details above.

Common fixes:
  1. Deploy auto-sleep: ./deploy.sh setup
  2. Check systemd status: systemctl status vmstation-autosleep.timer
  3. Review logs: journalctl -u vmstation-autosleep -n 50

=========================================
VMStation Monitoring Exporters Health
Validating exporters, targets, and dashboards
=========================================

[1/8] Testing Prometheus targets...
✅ PASS: Prometheus targets API accessible
ℹ️  INFO: Targets UP: 14, DOWN: 4
❌ FAIL: 4 targets are DOWN
  - kubernetes-service-endpoints

[2/8] Testing node-exporter on all nodes...
curl http://192.168.4.63:9100/metrics success
✅ PASS: Node exporter healthy on masternode
curl http://192.168.4.61:9100/metrics success
✅ PASS: Node exporter healthy on storagenodet3500
=========================================
VMStation Loki Log Aggregation Test
Validating Loki connectivity and log ingestion
=========================================

[1/6] Testing Loki pod status...
ℹ️  INFO: Loki pods found:
  loki-85ccd4898d-xq56h                 0/1     CrashLoopBackOff   6 (2m6s ago)   8m6s
❌ FAIL: Loki pods are not running

[2/6] Testing Loki service configuration...
✅ PASS: Loki service exists
ℹ️  INFO: Loki service details:
  loki                   NodePort    10.104.56.130    <none>        3100:31100/TCP      8m6s
✅ PASS: Loki service has endpoints

[3/6] Testing Loki API connectivity...
curl http://192.168.4.63:3100/ready error
⚠️  WARN: Loki ready endpoint not accessible (may be ClusterIP only)
ℹ️  INFO: Attempting to test via kubectl exec...
❌ FAIL: Loki is not ready

[4/6] Testing Promtail log shipper...
ℹ️  INFO: Promtail pods found:
  promtail-tn9jk                        1/1     Running            0              8m8s
  promtail-wv7mf                        1/1     Running            0              8m8s
✅ PASS: Promtail pods are running (2 instances)

[5/6] Testing Loki DNS resolution...
⚠️  WARN: Could not verify Loki DNS resolution

[6/6] Testing Loki datasource in Grafana...
✅ PASS: Loki datasource is configured in Grafana
⚠️  WARN: Could not check Loki datasource health

=========================================
Test Results Summary
=========================================
Passed:   4
Failed:   2
Warnings: 3

❌ Loki log aggregation has issues.

Common fixes:
  1. Check Loki logs: kubectl logs -n monitoring -l app=loki
  2. Check Promtail logs: kubectl logs -n monitoring -l app=promtail
  3. Verify Loki service: kubectl get svc -n monitoring loki
  4. Check DNS: kubectl run -it --rm dns-test --image=busybox --restart=Never -- nslookup loki.monitoring

Connectivity errors:
  - DNS lookup failures: Check CoreDNS pods
  - 500 status: Check Loki logs for errors
  - Service unavailable: Verify Loki pods are running
