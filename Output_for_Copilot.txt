remote: Enumerating objects: 31, done.
remote: Counting objects: 100% (31/31), done.
remote: Compressing objects: 100% (20/20), done.
remote: Total 31 (delta 16), reused 25 (delta 11), pack-reused 0 (from 0)
Unpacking objects: 100% (31/31), 37.82 KiB | 1.58 MiB/s, done.
From https://github.com/JashandeepJustinBains/VMStation
   e92bafe..6f9d660  main                                         -> origin/main
 * [new branch]      copilot/diagnose-monitoring-stack-failures-2 -> origin/copilot/diagnose-monitoring-stack-failures-2
Updating e92bafe..6f9d660
Fast-forward
 AI_AGENT_IMPLEMENTATION_REPORT.md                    | 484 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 MONITORING_EMERGENCY_GUIDE.md                        | 176 ++++++++++++++++++++++++++++++++++++++++
 MONITORING_REMEDIATION_CHECKLIST.md                  | 385 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 MONITORING_STACK_FAILURE_RESOLUTION_SUMMARY.md       | 445 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 TODO.md                                              |   9 +-
 docs/MONITORING_STACK_DIAGNOSTICS_AND_REMEDIATION.md | 564 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 manifests/monitoring/loki.yaml                       |   9 +-
 manifests/monitoring/prometheus.yaml                 |   1 +
 scripts/ai-agent-issue-prompt.md                     |  59 ++++++++++++++
 scripts/diagnose-monitoring-stack.sh                 | 348 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 scripts/remediate-monitoring-stack.sh                | 426 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 scripts/validate-monitoring-stack.sh                 | 319 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 12 files changed, 3220 insertions(+), 5 deletions(-)
 create mode 100644 AI_AGENT_IMPLEMENTATION_REPORT.md
 create mode 100644 MONITORING_EMERGENCY_GUIDE.md
 create mode 100644 MONITORING_REMEDIATION_CHECKLIST.md
 create mode 100644 MONITORING_STACK_FAILURE_RESOLUTION_SUMMARY.md
 create mode 100644 docs/MONITORING_STACK_DIAGNOSTICS_AND_REMEDIATION.md
 create mode 100644 scripts/ai-agent-issue-prompt.md
 create mode 100755 scripts/diagnose-monitoring-stack.sh
 create mode 100755 scripts/remediate-monitoring-stack.sh
 create mode 100755 scripts/validate-monitoring-stack.sh
[2025-10-10 13:50:29] [INFO] ========================================
[2025-10-10 13:50:29] [INFO]  Comprehensive Cluster Reset
[2025-10-10 13:50:29] [INFO] ========================================
[2025-10-10 13:50:29] [INFO] This will reset:
[2025-10-10 13:50:29] [INFO]   - Debian Kubernetes cluster (kubeadm)
[2025-10-10 13:50:29] [INFO]   - RKE2 cluster on homelab
[2025-10-10 13:50:29] [INFO]   - All network interfaces and configs
[2025-10-10 13:50:29] [INFO]
[2025-10-10 13:50:29] [INFO] SSH keys and physical ethernet will be preserved
[2025-10-10 13:50:29] [INFO]
Proceed with comprehensive reset? [y/N]: y
[2025-10-10 13:50:32] [INFO]
[2025-10-10 13:50:32] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-10 13:50:32] [INFO]   PHASE 1: Resetting Debian Nodes
[2025-10-10 13:50:32] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY [Cluster Reset - Debian Nodes] ********************************************

TASK [Display reset banner] ****************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Resetting Kubernetes Cluster
    Target: masternode
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ok: [storagenodet3500] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Resetting Kubernetes Cluster
    Target: storagenodet3500
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Reset kubeadm (if initialized)] ******************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Stop kubelet service] ****************************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Stop containerd service] *************************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Kill any hanging kubeadm processes] **************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Remove Kubernetes configuration] *****************************************
changed: [masternode] => (item=/etc/kubernetes)
changed: [storagenodet3500] => (item=/etc/kubernetes)
changed: [masternode] => (item=/var/lib/kubelet)
changed: [storagenodet3500] => (item=/var/lib/kubelet)
changed: [masternode] => (item=/var/lib/etcd)
changed: [masternode] => (item=/etc/cni/net.d)
ok: [storagenodet3500] => (item=/var/lib/etcd)
changed: [masternode] => (item=/opt/cni/bin)
changed: [storagenodet3500] => (item=/etc/cni/net.d)
changed: [masternode] => (item=/root/.kube)
changed: [storagenodet3500] => (item=/opt/cni/bin)
ok: [storagenodet3500] => (item=/root/.kube)

TASK [Remove CNI network interfaces] *******************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Flush iptables rules] ****************************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Restart containerd service] **********************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Display reset complete message] ******************************************
ok: [masternode] =>
  msg: masternode reset complete
ok: [storagenodet3500] =>
  msg: storagenodet3500 reset complete

PLAY [Reset Complete Summary] **************************************************

TASK [Display summary] *********************************************************
ok: [localhost] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ✅ Cluster Reset Complete

    All Kubernetes configuration removed
    Cluster ready for fresh deployment

    Next steps:
    ./deploy.sh debian    # Redeploy Debian cluster
    ./deploy.sh all       # Redeploy both clusters
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY RECAP *********************************************************************
localhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
masternode                 : ok=10   changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
storagenodet3500           : ok=10   changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0

[2025-10-10 13:50:46] [INFO] ✓ Debian cluster reset completed
[2025-10-10 13:50:46] [INFO]
[2025-10-10 13:50:46] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-10 13:50:46] [INFO]   PHASE 2: Resetting RKE2 on Homelab
[2025-10-10 13:50:46] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-10 13:50:48] [INFO] No RKE2 installation found on homelab, skipping
[2025-10-10 13:50:48] [INFO]
[2025-10-10 13:50:48] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-10 13:50:48] [INFO]   Reset Complete!
[2025-10-10 13:50:48] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-10 13:50:48] [INFO]
[2025-10-10 13:50:48] [INFO] Cluster is ready for fresh deployment
[2025-10-10 13:50:48] [INFO]
[2025-10-10 13:50:48] [INFO] Logs:
[2025-10-10 13:50:48] [INFO]   - /srv/monitoring_data/VMStation/ansible/artifacts/reset-debian.log
[2025-10-10 13:50:48] [INFO]   - /srv/monitoring_data/VMStation/ansible/artifacts/uninstall-rke2.log
[2025-10-10 13:50:48] [INFO]
[2025-10-10 13:50:48] [INFO] Next steps:
[2025-10-10 13:50:48] [INFO]   ./deploy.sh all --with-rke2    # Full deployment
[2025-10-10 13:50:48] [INFO]   ./deploy.sh debian             # Debian only
[2025-10-10 13:50:48] [INFO]   ./deploy.sh rke2               # RKE2 only
[2025-10-10 13:50:48] [INFO]
[2025-10-10 13:50:48] [INFO] Setting up auto-sleep monitoring...

PLAY [Setup Auto-Sleep Monitoring] ********************************************************************************************************************************************************

TASK [Gathering Facts] ********************************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [Display auto-sleep setup banner] ****************************************************************************************************************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Auto-Sleep Monitoring Setup
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ok: [storagenodet3500] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Auto-Sleep Monitoring Setup
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ok: [homelab] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Auto-Sleep Monitoring Setup
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Confirm setup] **********************************************************************************************************************************************************************
[Confirm setup]
This will configure auto-sleep monitoring.
Cluster will sleep after 2 hours of inactivity.
Continue? (yes/no)
:
yes^Mok: [masternode]

TASK [Check confirmation] *****************************************************************************************************************************************************************
skipping: [masternode]
fatal: [storagenodet3500]: FAILED! => changed=false
  msg: Setup cancelled by user
fatal: [homelab]: FAILED! => changed=false
  msg: Setup cancelled by user

TASK [Create auto-sleep monitor script] ***************************************************************************************************************************************************
ok: [masternode]

TASK [Create cluster sleep script] ********************************************************************************************************************************************************
ok: [masternode]

TASK [Create systemd service for auto-sleep monitor] **************************************************************************************************************************************
ok: [masternode]

TASK [Create systemd timer for auto-sleep monitor] ****************************************************************************************************************************************
ok: [masternode]

TASK [Reload systemd] *********************************************************************************************************************************************************************
ok: [masternode]

TASK [Enable and start auto-sleep timer] **************************************************************************************************************************************************
ok: [masternode]

TASK [Display setup complete message] *****************************************************************************************************************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ✅ Auto-Sleep Monitoring Setup Complete

    Cluster will automatically sleep after 2 hours of inactivity

    Monitor status:
    systemctl status vmstation-autosleep.timer

    Disable auto-sleep:
    systemctl stop vmstation-autosleep.timer
    systemctl disable vmstation-autosleep.timer
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY RECAP ********************************************************************************************************************************************************************************
homelab                    : ok=2    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0
masternode                 : ok=10   changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0
storagenodet3500           : ok=2    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0

[2025-10-10 13:56:07] [INFO] ========================================
[2025-10-10 13:56:07] [INFO]  Deploying Kubernetes to Debian Nodes
[2025-10-10 13:56:07] [INFO] ========================================
[2025-10-10 13:56:07] [INFO] Target: monitoring_nodes + storage_nodes
[2025-10-10 13:56:07] [INFO] Playbook: /srv/monitoring_data/VMStation/ansible/playbooks/deploy-cluster.yaml
[2025-10-10 13:56:07] [INFO] Log: /srv/monitoring_data/VMStation/ansible/artifacts/deploy-debian.log
[2025-10-10 13:56:07] [INFO]
[2025-10-10 13:56:07] [INFO] Starting Debian deployment (this may take 10-15 minutes)...
[WARNING]: Collection kubernetes.core does not support Ansible version 2.14.18

PLAY [Phase 0: System Preparation - Install Kubernetes Binaries] ***************

TASK [Gathering Facts] *********************************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Set default flags] *******************************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Display Phase 0 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ok: [storagenodet3500] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Disable swap] ************************************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Remove swap from fstab] **************************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Load kernel modules for containerd] **************************************
ok: [masternode] => (item=overlay)
ok: [storagenodet3500] => (item=overlay)
ok: [masternode] => (item=br_netfilter)
ok: [storagenodet3500] => (item=br_netfilter)

TASK [Ensure modules load on boot] *********************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Configure sysctl for Kubernetes] *****************************************
ok: [masternode] => (item={'name': 'net.bridge.bridge-nf-call-iptables', 'value': '1'})
ok: [storagenodet3500] => (item={'name': 'net.bridge.bridge-nf-call-iptables', 'value': '1'})
ok: [masternode] => (item={'name': 'net.bridge.bridge-nf-call-ip6tables', 'value': '1'})
ok: [storagenodet3500] => (item={'name': 'net.bridge.bridge-nf-call-ip6tables', 'value': '1'})
ok: [masternode] => (item={'name': 'net.ipv4.ip_forward', 'value': '1'})
ok: [storagenodet3500] => (item={'name': 'net.ipv4.ip_forward', 'value': '1'})

TASK [Check if containerd is installed] ****************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Install containerd (try containerd.io first)] ****************************
skipping: [masternode]
skipping: [storagenodet3500]

TASK [Install containerd (fallback to containerd package)] *********************
skipping: [masternode]
skipping: [storagenodet3500]

TASK [Create containerd config directory] **************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Generate containerd default config] **************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Enable SystemdCgroup in containerd] **************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Restart containerd] ******************************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Configure crictl runtime endpoint] ***************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Add Kubernetes apt key] **************************************************
ok: [storagenodet3500]
ok: [masternode]

TASK [Add Kubernetes apt repository] *******************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Install Kubernetes binaries] *********************************************
ok: [storagenodet3500]
ok: [masternode]

TASK [Hold Kubernetes packages at current version] *****************************
ok: [masternode] => (item=kubelet)
ok: [storagenodet3500] => (item=kubelet)
ok: [masternode] => (item=kubeadm)
ok: [storagenodet3500] => (item=kubeadm)
ok: [masternode] => (item=kubectl)
ok: [storagenodet3500] => (item=kubectl)

TASK [Enable kubelet service] **************************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Create required Kubernetes directories] **********************************
changed: [masternode] => (item=/opt/cni/bin)
changed: [storagenodet3500] => (item=/opt/cni/bin)
changed: [masternode] => (item=/etc/cni/net.d)
changed: [storagenodet3500] => (item=/etc/cni/net.d)
changed: [masternode] => (item=/var/lib/kubelet)
changed: [storagenodet3500] => (item=/var/lib/kubelet)

TASK [Check if CNI plugins are installed] **************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Download CNI plugins if missing] *****************************************
ok: [storagenodet3500]
ok: [masternode]

TASK [Extract CNI plugins] *****************************************************
changed: [masternode]
changed: [storagenodet3500]

PLAY [Phase 1: Control Plane Initialization] ***********************************

TASK [Display Phase 1 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 1: Control Plane Initialization
    Target: masternode
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Check if cluster is already initialized] *********************************
ok: [masternode]

TASK [Initialize control plane (if not exists)] ********************************
changed: [masternode]

TASK [Regenerate admin.conf with kubeadm (fixes authentication issues)] ********
skipping: [masternode]

TASK [Create .kube directory for root] *****************************************
changed: [masternode]

TASK [Copy admin.conf to /root/.kube/config] ***********************************
changed: [masternode]

TASK [Set KUBECONFIG environment variable globally] ****************************
ok: [masternode]

TASK [Add KUBECONFIG to root's bash profile] ***********************************
ok: [masternode]

PLAY [Phase 2: Control Plane Validation] ***************************************

TASK [Display Phase 2 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 2: Control Plane Validation
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Wait for API server to be ready] *****************************************
ok: [masternode]

TASK [Verify control plane is responding] **************************************
changed: [masternode]

TASK [Display control plane status] ********************************************
ok: [masternode] =>
  msg:
  - "\e[0;32mKubernetes control plane\e[0m is running at \e[0;33mhttps://192.168.4.63:6443\e[0m"
  - "\e[0;32mCoreDNS\e[0m is running at \e[0;33mhttps://192.168.4.63:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\e[0m"
  - ''
  - To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

PLAY [Phase 3: Token Generation] ***********************************************

TASK [Display Phase 3 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 3: Token Generation
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Generate join token] *****************************************************
changed: [masternode]

TASK [Store join command] ******************************************************
ok: [masternode]

TASK [Display join command] ****************************************************
ok: [masternode] =>
  msg: 'Join command: kubeadm join 192.168.4.63:6443 --token ginps3.nboypd38r05qcehb --discovery-token-ca-cert-hash sha256:c24f7af41e7978b3377cc1fcda13c7be99adc1f03ee8def1eeb9be2940693da0 '

PLAY [Phase 4: CNI Deployment - Flannel] ***************************************

TASK [Display Phase 4 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 4: CNI Deployment (Flannel)
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Check if Flannel is already deployed] ************************************
ok: [masternode]

TASK [Deploy Flannel CNI] ******************************************************
changed: [masternode]

TASK [Wait for Flannel DaemonSet to be available] ******************************
changed: [masternode]

TASK [Display Flannel deployment status] ***************************************
ok: [masternode] =>
  msg: Flannel CNI deployed successfully

PLAY [Phase 5: Worker Node Join] ***********************************************

TASK [Display Phase 5 banner] **************************************************
ok: [storagenodet3500] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 5: Worker Node Join
    Target: storagenodet3500
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Check if node is already joined] *****************************************
ok: [storagenodet3500]

TASK [Join worker node to cluster] *********************************************
changed: [storagenodet3500]

TASK [Wait for kubelet to start] ***********************************************
ok: [storagenodet3500]

TASK [Display join status] *****************************************************
ok: [storagenodet3500] =>
  msg: storagenodet3500 successfully joined the cluster

PLAY [Phase 6: Cluster Validation] *********************************************

TASK [Display Phase 6 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 6: Cluster Validation
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Wait for nodes to be Ready] **********************************************
FAILED - RETRYING: [masternode]: Wait for nodes to be Ready (20 retries left).
changed: [masternode]

TASK [Ensure all nodes are schedulable (uncordon)] *****************************
changed: [masternode]

TASK [Get node status] *********************************************************
changed: [masternode]

TASK [Display node status] *****************************************************
ok: [masternode] =>
  msg:
  - NAME               STATUS   ROLES           AGE   VERSION    INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION   CONTAINER-RUNTIME
  - masternode         Ready    control-plane   35s   v1.29.15   192.168.4.63   <none>        Debian GNU/Linux 12 (bookworm)   6.1.0-32-amd64   containerd://1.6.20
  - storagenodet3500   Ready    <none>          13s   v1.29.15   192.168.4.61   <none>        Debian GNU/Linux 12 (bookworm)   6.1.0-34-amd64   containerd://1.6.20

TASK [Wait for CoreDNS to be Running] ******************************************
changed: [masternode]

TASK [Display validation success] **********************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ✅ Cluster Validation Successful
    - Ready Nodes: 2
    - CoreDNS Pods: 1
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY [Phase 7: Application Deployment - Monitoring Stack] **********************

TASK [Display Phase 7 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 7: Application Deployment (Monitoring)
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Create monitoring namespace] *********************************************
changed: [masternode]

TASK [Create monitoring data directories on control plane with proper permissions] ***
ok: [masternode] => (item={'path': '/srv/monitoring_data'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/grafana', 'owner': '472', 'group': '472'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/prometheus', 'owner': '65534', 'group': '65534'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/loki', 'owner': '10001', 'group': '10001'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/promtail', 'owner': '0', 'group': '0'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/syslog', 'owner': '0', 'group': '0'})

TASK [Apply monitoring PersistentVolumes and PVCs (prometheus/grafana/loki/promtail)] ***
changed: [masternode]

TASK [Fail if IPMI credentials required but not provided] **********************
skipping: [masternode]

TASK [Ensure ipmi credentials secret exists in monitoring namespace] ***********
skipping: [masternode]

TASK [Deploy Node Exporter (system metrics)] ***********************************
changed: [masternode]

TASK [Deploy Kube State Metrics (Kubernetes object state)] *********************
changed: [masternode]

TASK [Deploy Loki and Promtail (log aggregation and collection)] ***************
changed: [masternode]

TASK [Deploy IPMI Exporter] ****************************************************
changed: [masternode]

TASK [Scale up remote IPMI exporter if credentials are available] **************
skipping: [masternode]

TASK [Deploy Prometheus] *******************************************************
changed: [masternode]

TASK [Deploy Grafana] **********************************************************
changed: [masternode]

TASK [Wait for Node Exporter DaemonSet to be ready] ****************************
changed: [masternode]

TASK [Wait for Kube State Metrics to be ready] *********************************
changed: [masternode]

TASK [Wait for Loki to be ready] ***********************************************
FAILED - RETRYING: [masternode]: Wait for Loki to be ready (3 retries left).
FAILED - RETRYING: [masternode]: Wait for Loki to be ready (2 retries left).
FAILED - RETRYING: [masternode]: Wait for Loki to be ready (1 retries left).
changed: [masternode]

TASK [Wait for Promtail DaemonSet to be ready] *********************************
changed: [masternode]

TASK [Wait for Prometheus to be ready] *****************************************
FAILED - RETRYING: [masternode]: Wait for Prometheus to be ready (5 retries left).
FAILED - RETRYING: [masternode]: Wait for Prometheus to be ready (4 retries left).
FAILED - RETRYING: [masternode]: Wait for Prometheus to be ready (3 retries left).
FAILED - RETRYING: [masternode]: Wait for Prometheus to be ready (2 retries left).
FAILED - RETRYING: [masternode]: Wait for Prometheus to be ready (1 retries left).
changed: [masternode]

TASK [Wait for Grafana to be ready] ********************************************
changed: [masternode]

TASK [Wait for Blackbox Exporter to be ready] **********************************
changed: [masternode]

TASK [Deploy Jellyfin media server] ********************************************
changed: [masternode]

TASK [Wait for Jellyfin pod to be ready] ***************************************
changed: [masternode]

TASK [Display monitoring stack status] *****************************************
changed: [masternode]

TASK [Display Jellyfin status] *************************************************
changed: [masternode]

TASK [Display monitoring deployment result] ************************************
ok: [masternode] =>
  msg:
  - NAME                                      READY   STATUS             RESTARTS        AGE
  - pod/blackbox-exporter-5949885fb9-bwz6n    1/1     Running            0               5m57s
  - pod/grafana-5f879c7654-k45n2              1/1     Running            0               5m56s
  - pod/kube-state-metrics-5f6f5666cc-7tzkl   1/1     Running            0               6m
  - pod/loki-0                                1/1     Running            0               5m59s
  - pod/node-exporter-28dmd                   1/1     Running            0               6m1s
  - pod/node-exporter-ntznj                   1/1     Running            0               6m1s
  - pod/prometheus-0                          1/2     CrashLoopBackOff   5 (2m34s ago)   5m57s
  - pod/promtail-7gnwk                        1/1     Running            0               5m59s
  - pod/promtail-hlkgt                        1/1     Running            0               5m59s
  - ''
  - NAME                           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
  - service/blackbox-exporter      ClusterIP   10.97.133.25     <none>        9115/TCP            5m57s
  - service/grafana                NodePort    10.111.206.164   <none>        3000:30300/TCP      5m56s
  - service/ipmi-exporter          ClusterIP   10.111.183.162   <none>        9290/TCP            5m59s
  - service/ipmi-exporter-remote   ClusterIP   10.106.37.73     <none>        9291/TCP            5m59s
  - service/kube-state-metrics     ClusterIP   10.100.195.3     <none>        8080/TCP,8081/TCP   6m
  - service/loki                   ClusterIP   None             <none>        3100/TCP,9096/TCP   6m
  - service/loki-external          NodePort    10.104.129.102   <none>        3100:31100/TCP      5m59s
  - service/node-exporter          ClusterIP   None             <none>        9100/TCP            6m1s
  - service/prometheus             ClusterIP   None             <none>        9090/TCP            5m58s
  - service/prometheus-external    NodePort    10.98.188.118    <none>        9090:30090/TCP      5m57s
  - ''
  - NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                      AGE
  - daemonset.apps/ipmi-exporter   0         0         0       0            0           kubernetes.io/os=linux,vmstation.io/role=compute   5m59s
  - daemonset.apps/node-exporter   2         2         2       2            2           <none>                                             6m1s
  - daemonset.apps/promtail        2         2         2       2            2           <none>                                             5m59s
  - ''
  - NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
  - deployment.apps/blackbox-exporter      1/1     1            1           5m57s
  - deployment.apps/grafana                1/1     1            1           5m56s
  - deployment.apps/ipmi-exporter-remote   0/0     0            0           5m59s
  - deployment.apps/kube-state-metrics     1/1     1            1           6m
  - ''
  - NAME                                              DESIRED   CURRENT   READY   AGE
  - replicaset.apps/blackbox-exporter-5949885fb9      1         1         1       5m57s
  - replicaset.apps/grafana-5f879c7654                1         1         1       5m56s
  - replicaset.apps/ipmi-exporter-remote-869b4c8fd5   0         0         0       5m59s
  - replicaset.apps/kube-state-metrics-5f6f5666cc     1         1         1       6m
  - ''
  - NAME                          READY   AGE
  - statefulset.apps/loki         1/1     6m
  - statefulset.apps/prometheus   0/1     5m58s

TASK [Display Jellyfin deployment result] **************************************
ok: [masternode] =>
  msg:
  - NAME           READY   STATUS    RESTARTS   AGE     IP            NODE               NOMINATED NODE   READINESS GATES
  - pod/jellyfin   1/1     Running   0          4m26s   10.244.1.73   storagenodet3500   <none>           <none>
  - ''
  - NAME                       TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)                         AGE     SELECTOR
  - service/jellyfin-service   NodePort   10.98.22.93   <none>        8096:30096/TCP,8920:30920/TCP   4m26s   app=jellyfin,component=media-server

TASK [Verify Grafana endpoint is accessible] ***********************************
ok: [masternode]

TASK [Verify Prometheus endpoint is accessible] ********************************
ok: [masternode]

TASK [Display endpoint health status] ******************************************
ok: [masternode] =>
  msg: |-
    Monitoring Endpoints Health:
    - Grafana:    OK
    - Prometheus: OK

TASK [Display deployment complete message] *************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ✅ VMStation Kubernetes Cluster Deployment Complete!

    Cluster Status:
    - Control Plane: masternode
    - Worker Nodes: storagenodet3500
    - CNI: Flannel
    - Monitoring: Prometheus, Grafana
    - Media Server: Jellyfin (on storagenodet3500)

    Access URLs:
    - Grafana:    http://192.168.4.63:30300
    - Prometheus: http://192.168.4.63:30090
    - Jellyfin:   http://192.168.4.61:30096

    Next Steps:
    - Check cluster: kubectl get nodes
    - View pods: kubectl get pods -A
    - Test monitoring: curl http://192.168.4.63:30300
    - Test Jellyfin: curl http://192.168.4.61:30096/health
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY [Phase 8: Wake-on-LAN Validation] *****************************************

TASK [Skip WoL tests unless explicitly enabled] ********************************
skipping: [masternode]

TASK [Build wol_targets from inventory when not explicitly provided] ***********
skipping: [masternode]

TASK [Ensure wakeonlan is installed on control node (Debian family)] ***********
ok: [masternode]

TASK [Create sleep helper script on target nodes] ******************************
ok: [masternode]

TASK [Trigger sleep helper on each wol target (run as background)] *************
changed: [masternode -> localhost] => (item={'name': 'storagenodet3500', 'ip': '192.168.4.61', 'mac': 'b8:ac:6f:7e:6c:9d'})
changed: [masternode -> localhost] => (item={'name': 'homelab', 'ip': '192.168.4.62', 'mac': 'd0:94:66:30:d6:63'})

TASK [Send WoL magic packets from control-plane] *******************************
changed: [masternode -> localhost] => (item={'name': 'storagenodet3500', 'ip': '192.168.4.61', 'mac': 'b8:ac:6f:7e:6c:9d'})
changed: [masternode -> localhost] => (item={'name': 'homelab', 'ip': '192.168.4.62', 'mac': 'd0:94:66:30:d6:63'})

TASK [Wait for node SSH to return (timeout 120s)] ******************************
ok: [masternode -> localhost] => (item={'name': 'storagenodet3500', 'ip': '192.168.4.61', 'mac': 'b8:ac:6f:7e:6c:9d'})
ok: [masternode -> localhost] => (item={'name': 'homelab', 'ip': '192.168.4.62', 'mac': 'd0:94:66:30:d6:63'})

TASK [Collect wake results] ****************************************************
ok: [masternode] => (item={'name': 'storagenodet3500', 'ip': '192.168.4.61', 'mac': 'b8:ac:6f:7e:6c:9d'})
ok: [masternode] => (item={'name': 'homelab', 'ip': '192.168.4.62', 'mac': 'd0:94:66:30:d6:63'})

TASK [Show WoL report] *********************************************************
ok: [masternode] =>
  wol_report:
  - ip: 192.168.4.61
    mac: b8:ac:6f:7e:6c:9d
    name: storagenodet3500
    wol_out: Sending magic packet to 255.255.255.255:9 with b8:ac:6f:7e:6c:9d
  - ip: 192.168.4.62
    mac: d0:94:66:30:d6:63
    name: homelab
    wol_out: Sending magic packet to 255.255.255.255:9 with d0:94:66:30:d6:63

PLAY RECAP *********************************************************************
masternode                 : ok=84   changed=35   unreachable=0    failed=0    skipped=8    rescued=0    ignored=0
storagenodet3500           : ok=28   changed=4    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0

[2025-10-10 14:04:12] [INFO]
[2025-10-10 14:04:12] [INFO] ✓ Debian deployment completed successfully
[2025-10-10 14:04:12] [INFO]
[2025-10-10 14:04:12] [INFO] Running post-deployment verification...
[2025-10-10 14:04:17] [INFO] Verifying Debian cluster health...
[2025-10-10 14:04:17] [INFO] ✓ Debian cluster is healthy (2 Debian nodes Ready)
[2025-10-10 14:04:17] [INFO]
[2025-10-10 14:04:17] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-10 14:04:17] [INFO]   Debian Kubernetes Cluster Ready!
[2025-10-10 14:04:17] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-10 14:04:17] [INFO]
[2025-10-10 14:04:17] [INFO] Verification commands:
[2025-10-10 14:04:17] [INFO]   kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes
[2025-10-10 14:04:17] [INFO]   kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -A
[2025-10-10 14:04:17] [INFO]
[2025-10-10 14:04:17] [INFO] Log saved to: /srv/monitoring_data/VMStation/ansible/artifacts/deploy-debian.log
[2025-10-10 14:04:17] [INFO]
[2025-10-10 14:04:17] [INFO] ========================================
[2025-10-10 14:04:17] [INFO]  Deploy Monitoring Stack
[2025-10-10 14:04:17] [INFO] ========================================
[2025-10-10 14:04:17] [INFO] Target: monitoring_nodes
[2025-10-10 14:04:17] [INFO] Playbook: /srv/monitoring_data/VMStation/ansible/playbooks/deploy-monitoring-stack.yaml
[2025-10-10 14:04:17] [INFO] Log: /srv/monitoring_data/VMStation/ansible/artifacts/deploy-monitoring-stack.log
[2025-10-10 14:04:17] [INFO]
[2025-10-10 14:04:17] [INFO] Starting monitoring stack deployment...
[2025-10-10 14:04:17] [INFO] Components: Prometheus, Grafana, Loki, Promtail, Kube-state-metrics, Node-exporter, IPMI-exporter
[2025-10-10 14:04:17] [INFO]

PLAY [Deploy VMStation Enterprise Monitoring Stack] ****************************

TASK [Gathering Facts] *********************************************************
ok: [masternode]

TASK [Display deployment banner] ***********************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    VMStation Monitoring Stack Deployment
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Enterprise monitoring and observability platform

    Components:
    - Prometheus (metrics time-series database)
    - Grafana (dashboards and visualization)
    - Loki (log aggregation)
    - Promtail (log shipper)
    - Kube-state-metrics (K8s object metrics)
    - Node-exporter (system metrics)
    - Blackbox-exporter (probes)
    - IPMI-exporter (hardware monitoring)
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Verify Kubernetes cluster is accessible] *********************************
ok: [masternode]

TASK [Display cluster information] *********************************************
ok: [masternode] =>
  msg:
  - "\e[0;32mKubernetes control plane\e[0m is running at \e[0;33mhttps://192.168.4.63:6443\e[0m"
  - "\e[0;32mCoreDNS\e[0m is running at \e[0;33mhttps://192.168.4.63:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\e[0m"
  - ''
  - To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

TASK [Check available storage] *************************************************
ok: [masternode]

TASK [Display available storage] ***********************************************
ok: [masternode] =>
  msg: 'Available storage: 39G'

TASK [Check node resources] ****************************************************
ok: [masternode]

TASK [Display node resources] **************************************************
ok: [masternode] =>
  msg:
  - 'Total Memory: 7.7Gi'
  - 'Available Memory: 5.9Gi'
  - 'CPU Cores: 4'

TASK [Create monitoring namespace] *********************************************
ok: [masternode]

TASK [Label monitoring namespace] **********************************************
changed: [masternode]

TASK [Create monitoring data directories with proper permissions] **************
ok: [masternode] => (item={'path': '/srv/monitoring_data'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/grafana', 'owner': '472', 'group': '472'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/prometheus', 'owner': '65534', 'group': '65534'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/loki', 'owner': '10001', 'group': '10001'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/promtail', 'owner': '0', 'group': '0'})

TASK [Deploy Prometheus StatefulSet and services] ******************************
changed: [masternode]

TASK [Display Prometheus deployment result] ************************************
ok: [masternode] =>
  msg:
  - namespace/monitoring unchanged
  - serviceaccount/prometheus unchanged
  - clusterrole.rbac.authorization.k8s.io/prometheus unchanged
  - clusterrolebinding.rbac.authorization.k8s.io/prometheus unchanged
  - configmap/prometheus-config unchanged
  - configmap/prometheus-rules unchanged
  - statefulset.apps/prometheus configured
  - service/prometheus unchanged
  - service/prometheus-external unchanged
  - networkpolicy.networking.k8s.io/prometheus-netpol configured
  - serviceaccount/blackbox-exporter unchanged
  - deployment.apps/blackbox-exporter unchanged
  - configmap/blackbox-exporter-config unchanged
  - service/blackbox-exporter unchanged

TASK [Deploy Loki StatefulSet and services] ************************************
changed: [masternode]

TASK [Display Loki deployment result] ******************************************
ok: [masternode] =>
  msg:
  - namespace/monitoring configured
  - configmap/loki-config unchanged
  - statefulset.apps/loki configured
  - serviceaccount/loki unchanged
  - service/loki unchanged
  - service/loki-external unchanged
  - networkpolicy.networking.k8s.io/loki-netpol configured
  - configmap/promtail-config unchanged
  - daemonset.apps/promtail unchanged
  - serviceaccount/promtail unchanged
  - clusterrole.rbac.authorization.k8s.io/promtail unchanged
  - clusterrolebinding.rbac.authorization.k8s.io/promtail unchanged

TASK [Deploy Grafana] **********************************************************
ok: [masternode]

TASK [Display Grafana deployment result] ***************************************
ok: [masternode] =>
  msg:
  - configmap/grafana-datasources unchanged
  - configmap/grafana-dashboard-providers unchanged
  - configmap/grafana-dashboards unchanged
  - deployment.apps/grafana unchanged
  - service/grafana unchanged
  - persistentvolumeclaim/grafana-pvc unchanged

TASK [Deploy Kube-state-metrics] ***********************************************
ok: [masternode]

TASK [Deploy Node-exporter DaemonSet] ******************************************
ok: [masternode]

TASK [Deploy IPMI-exporter (optional - may fail without IPMI hardware)] ********
ok: [masternode]

TASK [Wait for Prometheus to be ready (120s)] **********************************
ok: [masternode]

TASK [Wait for Loki to be ready (120s)] ****************************************
ok: [masternode]

TASK [Wait for Grafana to be ready (60s)] **************************************
ok: [masternode]

TASK [Get all monitoring pods] *************************************************
ok: [masternode]

TASK [Display monitoring pods] *************************************************
ok: [masternode] =>
  msg:
  - NAME                                  READY   STATUS             RESTARTS       AGE     IP             NODE               NOMINATED NODE   READINESS GATES
  - blackbox-exporter-5949885fb9-bwz6n    1/1     Running            0              8m23s   10.244.0.18    masternode         <none>           <none>
  - grafana-5f879c7654-k45n2              1/1     Running            0              8m22s   10.244.0.19    masternode         <none>           <none>
  - kube-state-metrics-5f6f5666cc-7tzkl   1/1     Running            0              8m26s   10.244.0.15    masternode         <none>           <none>
  - loki-0                                1/1     Running            0              8m25s   10.244.0.17    masternode         <none>           <none>
  - node-exporter-28dmd                   1/1     Running            0              8m27s   192.168.4.63   masternode         <none>           <none>
  - node-exporter-ntznj                   1/1     Running            0              8m27s   192.168.4.61   storagenodet3500   <none>           <none>
  - prometheus-0                          1/2     CrashLoopBackOff   6 (2m8s ago)   8m23s   10.244.0.20    masternode         <none>           <none>
  - promtail-7gnwk                        1/1     Running            0              8m25s   10.244.0.16    masternode         <none>           <none>
  - promtail-hlkgt                        1/1     Running            0              8m25s   10.244.1.72    storagenodet3500   <none>           <none>

TASK [Get all monitoring services] *********************************************
ok: [masternode]

TASK [Display monitoring services] *********************************************
ok: [masternode] =>
  msg:
  - NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
  - blackbox-exporter      ClusterIP   10.97.133.25     <none>        9115/TCP            8m23s
  - grafana                NodePort    10.111.206.164   <none>        3000:30300/TCP      8m22s
  - ipmi-exporter          ClusterIP   10.111.183.162   <none>        9290/TCP            8m25s
  - ipmi-exporter-remote   ClusterIP   10.106.37.73     <none>        9291/TCP            8m25s
  - kube-state-metrics     ClusterIP   10.100.195.3     <none>        8080/TCP,8081/TCP   8m26s
  - loki                   ClusterIP   None             <none>        3100/TCP,9096/TCP   8m26s
  - loki-external          NodePort    10.104.129.102   <none>        3100:31100/TCP      8m25s
  - node-exporter          ClusterIP   None             <none>        9100/TCP            8m27s
  - prometheus             ClusterIP   None             <none>        9090/TCP            8m24s
  - prometheus-external    NodePort    10.98.188.118    <none>        9090:30090/TCP      8m23s

TASK [Get all monitoring PVCs] *************************************************
ok: [masternode]

TASK [Display monitoring PVCs] *************************************************
ok: [masternode] =>
  msg:
  - NAME                              STATUS   VOLUME          CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
  - grafana-pvc                       Bound    grafana-pv      2Gi        RWO                           <unset>                 8m29s
  - loki-data-loki-0                  Bound    loki-pv         20Gi       RWO                           <unset>                 8m27s
  - prometheus-storage-prometheus-0   Bound    prometheus-pv   10Gi       RWO                           <unset>                 8m24s
  - promtail-pvc                      Bound    promtail-pv     1Gi        RWO                           <unset>                 8m29s

TASK [Check Prometheus health] *************************************************
ok: [masternode]

TASK [Display Prometheus health status] ****************************************
ok: [masternode] =>
  msg: 'Prometheus health: FAILED'

TASK [Check Loki health] *******************************************************
ok: [masternode]

TASK [Display Loki health status] **********************************************
ok: [masternode] =>
  msg: 'Loki health: OK'

TASK [Check Grafana health] ****************************************************
ok: [masternode]

TASK [Display Grafana health status] *******************************************
ok: [masternode] =>
  msg: 'Grafana health: OK'

TASK [Display deployment summary] **********************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Monitoring Stack Deployment Complete
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    ✅ Prometheus: http://192.168.4.63:30090
    ✅ Grafana: http://192.168.4.63:30300
    ✅ Loki: http://192.168.4.63:31100

    Default Credentials:
    - Grafana: No login required (anonymous access enabled)

    Verification Commands:
    kubectl get pods -n monitoring
    kubectl get svc -n monitoring
    kubectl get pvc -n monitoring

    Next Steps:
    1. Access Grafana and verify dashboards load
    2. Check Prometheus targets: Status → Targets
    3. Query logs in Grafana: Explore → Loki
    4. Review alerts: Prometheus → Alerts
    5. Deploy infrastructure services (NTP, Syslog, Kerberos)

    Troubleshooting:
    - View logs: kubectl logs -n monitoring <pod-name>
    - Describe pod: kubectl describe pod -n monitoring <pod-name>
    - Check events: kubectl get events -n monitoring --sort-by='.lastTimestamp'

    Documentation:
    - docs/PROMETHEUS_ENTERPRISE_REWRITE.md
    - docs/LOKI_ENTERPRISE_REWRITE.md
    - docs/ENTERPRISE_MONITORING_ENHANCEMENT.md
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY RECAP *********************************************************************
masternode                 : ok=36   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0

[2025-10-10 14:06:34] [INFO]
[2025-10-10 14:06:34] [INFO] ✓ Monitoring stack deployment completed successfully
[2025-10-10 14:06:34] [INFO]
[2025-10-10 14:06:34] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-10 14:06:34] [INFO]   Monitoring Stack Ready!
[2025-10-10 14:06:34] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-10 14:06:34] [INFO]
[2025-10-10 14:06:34] [INFO] Access URLs (assuming masternode at 192.168.4.63):
[2025-10-10 14:06:34] [INFO]   - Prometheus: http://192.168.4.63:30090
[2025-10-10 14:06:34] [INFO]   - Grafana: http://192.168.4.63:30300
[2025-10-10 14:06:34] [INFO]   - Loki: http://192.168.4.63:31100
[2025-10-10 14:06:34] [INFO]
[2025-10-10 14:06:34] [INFO] Verification:
[2025-10-10 14:06:34] [INFO]   kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n monitoring
[2025-10-10 14:06:34] [INFO]   kubectl --kubeconfig=/etc/kubernetes/admin.conf get svc -n monitoring
[2025-10-10 14:06:34] [INFO]
[2025-10-10 14:06:34] [INFO] Log saved to: /srv/monitoring_data/VMStation/ansible/artifacts/deploy-monitoring-stack.log
[2025-10-10 14:06:34] [INFO]
[2025-10-10 14:06:34] [INFO] ========================================
[2025-10-10 14:06:34] [INFO]  Deploy Infrastructure Services
[2025-10-10 14:06:34] [INFO] ========================================
[2025-10-10 14:06:34] [INFO] Target: monitoring_nodes
[2025-10-10 14:06:34] [INFO] Playbook: /srv/monitoring_data/VMStation/ansible/playbooks/deploy-infrastructure-services.yaml
[2025-10-10 14:06:34] [INFO] Log: /srv/monitoring_data/VMStation/ansible/artifacts/deploy-infrastructure-services.log
[2025-10-10 14:06:34] [INFO]
[2025-10-10 14:06:34] [INFO] Starting infrastructure services deployment...
[2025-10-10 14:06:34] [INFO] Services: NTP/Chrony (time sync), Syslog Server, FreeIPA/Kerberos (optional)
[2025-10-10 14:06:34] [INFO]

PLAY [Deploy VMStation Infrastructure Services] ********************************

TASK [Gathering Facts] *********************************************************
ok: [masternode]

TASK [Display deployment banner] ***********************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    VMStation Infrastructure Services Deployment
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Core infrastructure for enterprise cluster operations

    Services:
    - NTP/Chrony (time synchronization)
    - Syslog Server (log aggregation)
    - FreeIPA/Kerberos (identity management)
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Verify Kubernetes cluster is accessible] *********************************
ok: [masternode]

TASK [Create infrastructure namespace] *****************************************
changed: [masternode]

TASK [Label infrastructure namespace] ******************************************
changed: [masternode]

PLAY [Deploy NTP/Chrony Time Synchronization Service] **************************

TASK [Display deployment banner] ***********************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    VMStation NTP Service Deployment
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Purpose: Enterprise time synchronization for cluster
    Component: Chrony NTP DaemonSet
    Target: All cluster nodes
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Check if Kubernetes cluster is accessible] *******************************
ok: [masternode]

TASK [Display cluster info] ****************************************************
ok: [masternode] =>
  msg:
  - "\e[0;32mKubernetes control plane\e[0m is running at \e[0;33mhttps://192.168.4.63:6443\e[0m"
  - "\e[0;32mCoreDNS\e[0m is running at \e[0;33mhttps://192.168.4.63:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\e[0m"
  - ''
  - To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

TASK [Check current time on control plane] *************************************
ok: [masternode]

TASK [Display control plane time] **********************************************
ok: [masternode] =>
  msg: 'Control plane current time: 2025-10-10 14:06:38 EDT'

TASK [Create infrastructure namespace] *****************************************
ok: [masternode]

TASK [Label infrastructure namespace] ******************************************
changed: [masternode]

TASK [Deploy Chrony NTP service manifest] **************************************
changed: [masternode]

TASK [Display NTP deployment result] *******************************************
ok: [masternode] =>
  msg:
  - namespace/infrastructure configured
  - configmap/chrony-config created
  - serviceaccount/chrony-ntp created
  - clusterrole.rbac.authorization.k8s.io/chrony-ntp-privileged created
  - clusterrolebinding.rbac.authorization.k8s.io/chrony-ntp-privileged created
  - daemonset.apps/chrony-ntp created
  - service/chrony-ntp created
  - networkpolicy.networking.k8s.io/chrony-ntp-netpol created

TASK [Wait for Chrony DaemonSet to be ready (60s)] *****************************
ok: [masternode]

TASK [Get Chrony DaemonSet status] *********************************************
ok: [masternode]

TASK [Display DaemonSet status] ************************************************
ok: [masternode] =>
  msg:
  - NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE   CONTAINERS               IMAGES                                                    SELECTOR
  - chrony-ntp   2         2         2       2            2           <none>          11s   chrony,chrony-exporter   cturra/ntp:latest,quay.io/superq/chrony-exporter:latest   app=chrony-ntp

TASK [Get Chrony pods status] **************************************************
ok: [masternode]

TASK [Display pods status] *****************************************************
ok: [masternode] =>
  msg:
  - NAME               READY   STATUS    RESTARTS   AGE   IP             NODE               NOMINATED NODE   READINESS GATES
  - chrony-ntp-5z5p7   2/2     Running   0          11s   192.168.4.61   storagenodet3500   <none>           <none>
  - chrony-ntp-9j5fn   2/2     Running   0          11s   192.168.4.63   masternode         <none>           <none>

TASK [Check Chrony service] ****************************************************
ok: [masternode]

TASK [Display service status] **************************************************
ok: [masternode] =>
  msg:
  - NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                    AGE
  - chrony-ntp   ClusterIP   None         <none>        123/UDP,123/TCP,9123/TCP   12s

TASK [Get Chrony pod name for testing] *****************************************
ok: [masternode]

TASK [Check Chrony sources (if pod is running)] ********************************
ok: [masternode]

TASK [Display Chrony sources] **************************************************
ok: [masternode] =>
  msg:
  - 'MS Name/IP address         Stratum Poll Reach LastRx Last sample               '
  - ===============================================================================
  - ^* time1.google.com              1   6    17     5    +68us[ +135us] +/-   17ms
  - ^- time2.google.com              1   6    17     3  -1069us[-1069us] +/-   17ms
  - ^- time3.google.com              1   6    17     4    +55us[  +55us] +/-   17ms
  - ^- time4.google.com              1   6    17     4   -949us[ -949us] +/-   18ms
  - ^- time.cloudflare.com           3   6    17     4    -89ms[  -89ms] +/-  108ms
  - ^- backoffice-1.incentre.net     3   6    17     4  -1987us[-1987us] +/-   43ms
  - ^- ntp.netlinkify.com            2   6    17     4  -1402us[-1402us] +/-   11ms
  - ^- 23.133.168.244                4   6    17     3  +2260us[+2260us] +/-   40ms
  - ^- ntp1.yyz.ca.hojmark.net       2   6    17    10  -4881us[-4881us] +/-   31ms

TASK [Check Chrony tracking (if pod is running)] *******************************
ok: [masternode]

TASK [Display Chrony tracking info] ********************************************
ok: [masternode] =>
  msg:
  - 'Reference ID    : D8EF2300 (time1.google.com)'
  - 'Stratum         : 2'
  - 'Ref time (UTC)  : Fri Oct 10 18:06:46 2025'
  - 'System time     : 0.000949399 seconds slow of NTP time'
  - 'Last offset     : +0.000067211 seconds'
  - 'RMS offset      : 0.000067211 seconds'
  - 'Frequency       : 0.285 ppm slow'
  - 'Residual freq   : -24.236 ppm'
  - 'Skew            : 0.665 ppm'
  - 'Root delay      : 0.033255246 seconds'
  - 'Root dispersion : 0.000499686 seconds'
  - 'Update interval : 0.2 seconds'
  - 'Leap status     : Normal'

TASK [Install chrony on control plane (if not present)] ************************
ok: [masternode]

TASK [Configure chrony client on control plane] ********************************
ok: [masternode]

TASK [Enable and start chrony service] *****************************************
ok: [masternode]

TASK [Wait for chrony to sync (10s)] *******************************************
ok: [masternode]

TASK [Verify system time sync status] ******************************************
ok: [masternode]

TASK [Display system time sync status] *****************************************
ok: [masternode] =>
  msg:
  - 'Reference ID    : D8EF2308 (time3.google.com)'
  - 'Stratum         : 2'
  - 'Ref time (UTC)  : Fri Oct 10 18:07:08 2025'
  - 'System time     : 0.000117697 seconds fast of NTP time'
  - 'Last offset     : +0.000210010 seconds'
  - 'RMS offset      : 0.000218715 seconds'
  - 'Frequency       : 17.218 ppm fast'
  - 'Residual freq   : -0.000 ppm'
  - 'Skew            : 0.023 ppm'
  - 'Root delay      : 0.032768857 seconds'
  - 'Root dispersion : 0.000340920 seconds'
  - 'Update interval : 1041.0 seconds'
  - 'Leap status     : Normal'

TASK [Display deployment summary] **********************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    NTP Service Deployment Complete
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    ✅ Chrony DaemonSet deployed to infrastructure namespace
    ✅ NTP service running on all cluster nodes
    ✅ Time synchronization active

    Verification:
    - Check pods: kubectl get pods -n infrastructure -l app=chrony-ntp
    - Check sync: kubectl exec -n infrastructure <pod-name> -c chrony -- chronyc tracking
    - View logs: kubectl logs -n infrastructure -l app=chrony-ntp -c chrony

    Monitoring:
    - Chrony metrics exposed on port 9123
    - Add to Prometheus scrape config for monitoring

    Next Steps:
    1. Verify time sync across all worker nodes
    2. Update Prometheus to scrape chrony-exporter metrics
    3. Add NTP monitoring dashboard to Grafana
    4. Test log timestamp consistency
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY [Configure Time Sync on Debian Worker Nodes] ******************************

TASK [Gathering Facts] *********************************************************
ok: [storagenodet3500]

TASK [Install chrony on worker node] *******************************************
ok: [storagenodet3500]

TASK [Configure chrony client on worker node] **********************************
ok: [storagenodet3500]

TASK [Enable and start chrony on worker] ***************************************
ok: [storagenodet3500]

TASK [Wait for worker chrony to sync (10s)] ************************************
ok: [storagenodet3500]

TASK [Verify worker time sync] *************************************************
ok: [storagenodet3500]

TASK [Display worker time sync status] *****************************************
ok: [storagenodet3500] =>
  msg:
  - 'Reference ID    : C0A8043F (masternode)'
  - 'Stratum         : 11'
  - 'Ref time (UTC)  : Fri Oct 10 18:06:41 2025'
  - 'System time     : 0.000042903 seconds slow of NTP time'
  - 'Last offset     : -0.000046647 seconds'
  - 'RMS offset      : 0.000043151 seconds'
  - 'Frequency       : 8.885 ppm fast'
  - 'Residual freq   : -0.001 ppm'
  - 'Skew            : 0.109 ppm'
  - 'Root delay      : 0.000194123 seconds'
  - 'Root dispersion : 0.000070888 seconds'
  - 'Update interval : 973.7 seconds'
  - 'Leap status     : Normal'

PLAY [Configure Time Sync on RHEL Homelab Node] ********************************

TASK [Gathering Facts] *********************************************************
ok: [homelab]

TASK [Install chrony on homelab (RHEL)] ****************************************
ok: [homelab]

TASK [Configure chrony client on homelab] **************************************
ok: [homelab]

TASK [Enable and start chrony on homelab] **************************************
ok: [homelab]

TASK [Wait for homelab chrony to sync (10s)] ***********************************
ok: [homelab]

TASK [Verify homelab time sync] ************************************************
ok: [homelab]

TASK [Display homelab time sync status] ****************************************
ok: [homelab] =>
  msg:
  - 'Reference ID    : C0A8043F (homelab)'
  - 'Stratum         : 3'
  - 'Ref time (UTC)  : Fri Oct 10 17:50:15 2025'
  - 'System time     : 0.000000009 seconds fast of NTP time'
  - 'Last offset     : -0.000006279 seconds'
  - 'RMS offset      : 0.000038825 seconds'
  - 'Frequency       : 1.413 ppm fast'
  - 'Residual freq   : -0.002 ppm'
  - 'Skew            : 0.098 ppm'
  - 'Root delay      : 0.033264093 seconds'
  - 'Root dispersion : 0.002381216 seconds'
  - 'Update interval : 64.8 seconds'
  - 'Leap status     : Normal'

PLAY [Deploy Syslog Ingestor Service] ******************************************

TASK [Display deployment banner] ***********************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    VMStation Syslog Ingestor Deployment
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Purpose: Centralized syslog collection and forwarding
    Component: Syslog-NG StatefulSet
    Integration: Loki log aggregation
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Check if Kubernetes cluster is accessible] *******************************
ok: [masternode]

TASK [Check if Loki is deployed and ready] *************************************
ok: [masternode]

TASK [Display Loki status] *****************************************************
ok: [masternode] =>
  msg: 'Loki pod status: Running'

TASK [Warn if Loki is not ready] ***********************************************
skipping: [masternode]

TASK [Create syslog data directory on host] ************************************
ok: [masternode]

TASK [Apply syslog PersistentVolume] *******************************************
changed: [masternode]

TASK [Display syslog PV deployment result] *************************************
ok: [masternode] =>
  msg:
  - persistentvolume/syslog-pv created

TASK [Ensure infrastructure namespace exists] **********************************
ok: [masternode]

TASK [Deploy syslog server manifest] *******************************************
changed: [masternode]

TASK [Display syslog deployment result] ****************************************
ok: [masternode] =>
  msg:
  - namespace/infrastructure configured
  - configmap/syslog-ng-config created
  - serviceaccount/syslog-server created
  - statefulset.apps/syslog-server created
  - configmap/syslog-exporter-config created
  - service/syslog-server created
  - networkpolicy.networking.k8s.io/syslog-server-netpol created

TASK [Wait for syslog StatefulSet to be ready (60s)] ***************************
ok: [masternode]

TASK [Get syslog StatefulSet status] *******************************************
ok: [masternode]

TASK [Display StatefulSet status] **********************************************
ok: [masternode] =>
  msg:
  - NAME            READY   AGE   CONTAINERS                  IMAGES
  - syslog-server   1/1     12s   syslog-ng,syslog-exporter   balabit/syslog-ng:latest,prom/statsd-exporter:latest

TASK [Get syslog pods status] **************************************************
ok: [masternode]

TASK [Display pods status] *****************************************************
ok: [masternode] =>
  msg:
  - NAME              READY   STATUS    RESTARTS   AGE   IP            NODE         NOMINATED NODE   READINESS GATES
  - syslog-server-0   2/2     Running   0          12s   10.244.0.21   masternode   <none>           <none>

TASK [Check syslog service] ****************************************************
ok: [masternode]

TASK [Display service status] **************************************************
ok: [masternode] =>
  msg:
  - NAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                    AGE
  - syslog-server   NodePort   10.110.63.48   <none>        514:30514/UDP,514:30515/TCP,601:30601/TCP,9102:31252/TCP   13s

TASK [Get syslog pod name] *****************************************************
ok: [masternode]

TASK [Send test log message] ***************************************************
ok: [masternode]

TASK [Check syslog-ng logs] ****************************************************
ok: [masternode]

TASK [Display syslog logs] *****************************************************
ok: [masternode] =>
  msg:
  - SYSLOG_SERVER_PORT_9102_TCP_ADDR=10.110.63.48
  - SYSLOG_SERVER_PORT_9102_TCP_PORT=9102
  - SYSLOG_SERVER_PORT_9102_TCP_PROTO=tcp
  - SYSLOG_SERVER_SERVICE_HOST=10.110.63.48
  - SYSLOG_SERVER_SERVICE_PORT=514
  - SYSLOG_SERVER_SERVICE_PORT_METRICS=9102
  - SYSLOG_SERVER_SERVICE_PORT_SYSLOG_TCP=514
  - SYSLOG_SERVER_SERVICE_PORT_SYSLOG_TLS=601
  - SYSLOG_SERVER_SERVICE_PORT_SYSLOG_UDP=514
  - TERM=dumb
  - UID=0
  - _=echo
  - ''
  - 'Starting syslog-ng with params: '
  - 'syslog-ng: Error setting capabilities, capability management disabled; error=''Operation not permitted'''
  - '[2025-10-10T18:07:50.768064] WARNING: Configuration file format is too old, syslog-ng is running in compatibility mode. Please update it to use the syslog-ng 4.10 format at your time of convenience. To upgrade the configuration, please review the warnings about incompatible changes printed by syslog-ng, and once completed change the @version header at the top of the configuration file; config-version=''4.0'''
  - '[2025-10-10T18:07:50.768064] WARNING: Your configuration file uses an obsoleted keyword, please update your configuration; keyword=''stats_freq'', change=''Use the stats() block. E.g. stats(freq(1));'', location=''/etc/syslog-ng/syslog-ng.conf:19:3'''
  - '[2025-10-10T18:07:50.768064] WARNING: Your configuration file uses an obsoleted keyword, please update your configuration; keyword=''stats_level'', change=''Use the stats() block. E.g. stats(level(1));'', location=''/etc/syslog-ng/syslog-ng.conf:20:3'''
  - '[2025-10-10T18:07:50.768064] WARNING: window sizing for tcp sources were changed in syslog-ng 3.3, the configuration value was divided by the value of max-connections(). The result was too small, increasing to a reasonable minimum value; orig_log_iw_size=''10'', new_log_iw_size=''100'', min_iw_size_per_reader=''100'', min_log_fifo_size=''100000'''
  - '[2025-10-10T18:07:50.768064] WARNING: window sizing for tcp sources were changed in syslog-ng 3.3, the configuration value was divided by the value of max-connections(). The result was too small, increasing to a reasonable minimum value; orig_log_iw_size=''10'', new_log_iw_size=''100'', min_iw_size_per_reader=''100'', min_log_fifo_size=''100000'''

TASK [Install rsyslog on control plane (if not present)] ***********************
ok: [masternode]

TASK [Configure rsyslog to forward to cluster syslog server] *******************
ok: [masternode]

TASK [Restart rsyslog service] *************************************************
changed: [masternode]

TASK [Test rsyslog forwarding] *************************************************
ok: [masternode]

TASK [Wait for log propagation (5s)] *******************************************
ok: [masternode]

TASK [Display deployment summary] **********************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Syslog Ingestor Deployment Complete
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    ✅ Syslog-NG StatefulSet deployed to infrastructure namespace
    ✅ Syslog service listening on:
       - UDP: NodePort 30514 (traditional)
       - TCP: NodePort 30515 (reliable)
       - TLS: NodePort 30601 (RFC5424)
    ✅ Logs forwarding to Loki in monitoring namespace
    ✅ Local backup logs stored in /var/log/syslog-ng (in pod)

    Verification:
    - Check pods: kubectl get pods -n infrastructure -l app=syslog-server
    - View logs: kubectl logs -n infrastructure -l app=syslog-server -c syslog-ng
    - Test: logger -t test "Test message" (on any node)
    - Query in Grafana: {job="syslog"}

    External Syslog Sources:
    - Configure devices to send syslog to: 192.168.4.63:30514 (UDP)
    - Or: 192.168.4.63:30515 (TCP, recommended)

    Monitoring:
    - Syslog metrics exposed on port 9102
    - Add to Prometheus for monitoring

    Next Steps:
    1. Configure worker nodes to forward syslog
    2. Configure external devices (routers, switches, etc.)
    3. Add syslog monitoring dashboard to Grafana
    4. Test log search in Loki/Grafana
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY [Configure Syslog Forwarding on Debian Worker Nodes] **********************

TASK [Install rsyslog on worker] ***********************************************
ok: [storagenodet3500]

TASK [Configure rsyslog forwarding on worker] **********************************
ok: [storagenodet3500]

TASK [Restart rsyslog on worker] ***********************************************
changed: [storagenodet3500]

TASK [Test worker syslog forwarding] *******************************************
ok: [storagenodet3500]

PLAY [Configure Syslog Forwarding on RHEL Homelab Node] ************************

TASK [Install rsyslog on homelab (RHEL)] ***************************************
ok: [homelab]

TASK [Configure rsyslog forwarding on homelab] *********************************
ok: [homelab]

TASK [Restart rsyslog on homelab] **********************************************
changed: [homelab]

TASK [Test homelab syslog forwarding] ******************************************
ok: [homelab]

PLAY [Deploy FreeIPA/Kerberos Identity Management Service] *********************

TASK [Display deployment banner] ***********************************************
skipping: [masternode]

TASK [Check if Kubernetes cluster is accessible] *******************************
skipping: [masternode]

TASK [Check if NTP service is running] *****************************************
skipping: [masternode]

TASK [Warn if NTP is not running] **********************************************
skipping: [masternode]

TASK [Check system time synchronization] ***************************************
skipping: [masternode]

TASK [Display time sync status] ************************************************
skipping: [masternode]

TASK [Check available resources] ***********************************************
skipping: [masternode]

TASK [Display available resources] *********************************************
skipping: [masternode]

TASK [Create FreeIPA data directory on host] ***********************************
skipping: [masternode]

TASK [Create FreeIPA subdirectories] *******************************************
skipping: [masternode] => (item=/srv/monitoring_data/freeipa/var/lib/ipa)
skipping: [masternode] => (item=/srv/monitoring_data/freeipa/var/log)
skipping: [masternode] => (item=/srv/monitoring_data/freeipa/etc/dirsrv)
skipping: [masternode] => (item=/srv/monitoring_data/freeipa/etc/krb5kdc)
skipping: [masternode]

TASK [Prompt for secure passwords] *********************************************
skipping: [masternode]

TASK [Pause for security review (15 seconds)] **********************************
skipping: [masternode]

TASK [Ensure infrastructure namespace exists] **********************************
skipping: [masternode]

TASK [Check if FreeIPA is already deployed] ************************************
skipping: [masternode]

TASK [Display existing FreeIPA status] *****************************************
skipping: [masternode]

TASK [Deploy FreeIPA manifest] *************************************************
skipping: [masternode]

TASK [Display FreeIPA deployment result] ***************************************
skipping: [masternode]

TASK [Display installation progress message] ***********************************
skipping: [masternode]

TASK [Wait for FreeIPA pod to be created] **************************************
skipping: [masternode]

TASK [Wait for FreeIPA to be ready (up to 15 minutes)] *************************
skipping: [masternode]

TASK [Check if FreeIPA is ready] ***********************************************
skipping: [masternode]

TASK [Get FreeIPA StatefulSet status] ******************************************
skipping: [masternode]

TASK [Display StatefulSet status] **********************************************
skipping: [masternode]

TASK [Get FreeIPA pod status] **************************************************
skipping: [masternode]

TASK [Display pod status] ******************************************************
skipping: [masternode]

TASK [Check FreeIPA services] **************************************************
skipping: [masternode]

TASK [Display service status] **************************************************
skipping: [masternode]

TASK [Check FreeIPA installation status (if pod is running)] *******************
skipping: [masternode]

TASK [Display IPA status] ******************************************************
skipping: [masternode]

TASK [Add FreeIPA to /etc/hosts] ***********************************************
skipping: [masternode]

TASK [Test DNS resolution] *****************************************************
skipping: [masternode]

TASK [Display DNS test results] ************************************************
skipping: [masternode]

TASK [Display deployment summary] **********************************************
skipping: [masternode]

PLAY [Infrastructure Services Deployment Summary] ******************************

TASK [Get all infrastructure pods] *********************************************
ok: [masternode]

TASK [Display infrastructure pods] *********************************************
ok: [masternode] =>
  msg:
  - NAME               READY   STATUS    RESTARTS   AGE    IP             NODE               NOMINATED NODE   READINESS GATES
  - chrony-ntp-5z5p7   2/2     Running   0          102s   192.168.4.61   storagenodet3500   <none>           <none>
  - chrony-ntp-9j5fn   2/2     Running   0          102s   192.168.4.63   masternode         <none>           <none>
  - syslog-server-0    2/2     Running   0          33s    10.244.0.21    masternode         <none>           <none>

TASK [Get all infrastructure services] *****************************************
ok: [masternode]

TASK [Display infrastructure services] *****************************************
ok: [masternode] =>
  msg:
  - NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                    AGE
  - chrony-ntp      ClusterIP   None           <none>        123/UDP,123/TCP,9123/TCP                                   103s
  - syslog-server   NodePort    10.110.63.48   <none>        514:30514/UDP,514:30515/TCP,601:30601/TCP,9102:31252/TCP   34s

TASK [Display final summary] ***************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Infrastructure Services Deployment Complete
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    Deployed Services:
    ✅ NTP/Chrony: Time synchronization active
       - DaemonSet running on all nodes
       - Metrics: http://192.168.4.63:9123/metrics

    ✅ Syslog Server: Centralized logging active
       - UDP: 192.168.4.63:30514
       - TCP: 192.168.4.63:30515
       - Forwarding to Loki


    Time Sync Status:
    - All cluster nodes syncing to Chrony NTP
    - Log timestamps will be consistent
    - Kerberos requires accurate time sync

    Verification:
    kubectl get pods -n infrastructure
    kubectl get svc -n infrastructure

    Time Sync Check:
    kubectl exec -n infrastructure <chrony-pod> -c chrony -- chronyc tracking

    Syslog Test:
    logger -t test "Test message from $(hostname)"

    Next Steps:
    1. Verify time sync across all nodes
    2. Configure devices to send syslog to masternode
    3. Set up Kerberos clients (if deployed)
    4. Add infrastructure monitoring dashboards
    5. Test log correlation with synchronized timestamps

    Documentation:
    - ansible/playbooks/deploy-ntp-service.yaml
    - ansible/playbooks/deploy-syslog-service.yaml
    - ansible/playbooks/deploy-kerberos-service.yaml
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY RECAP *********************************************************************
homelab                    : ok=11   changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
masternode                 : ok=65   changed=7    unreachable=0    failed=0    skipped=34   rescued=0    ignored=0
storagenodet3500           : ok=11   changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0

[2025-10-10 14:08:22] [INFO]
[2025-10-10 14:08:22] [INFO] ✓ Infrastructure services deployment completed successfully
[2025-10-10 14:08:22] [INFO]
[2025-10-10 14:08:22] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-10 14:08:22] [INFO]   Infrastructure Services Ready!
[2025-10-10 14:08:22] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-10 14:08:22] [INFO]
[2025-10-10 14:08:22] [INFO] Deployed services:
[2025-10-10 14:08:22] [INFO]   - NTP/Chrony: Cluster-wide time synchronization
[2025-10-10 14:08:22] [INFO]   - Syslog Server: Centralized log aggregation
[2025-10-10 14:08:22] [INFO]   - FreeIPA/Kerberos: Identity management (if enabled)
[2025-10-10 14:08:22] [INFO]
[2025-10-10 14:08:22] [INFO] Verification:
[2025-10-10 14:08:22] [INFO]   kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n infrastructure
[2025-10-10 14:08:22] [INFO]   kubectl --kubeconfig=/etc/kubernetes/admin.conf get svc -n infrastructure
[2025-10-10 14:08:22] [INFO]   ./tests/validate-time-sync.sh
[2025-10-10 14:08:22] [INFO]
[2025-10-10 14:08:22] [INFO] Log saved to: /srv/monitoring_data/VMStation/ansible/artifacts/deploy-infrastructure-services.log
[2025-10-10 14:08:22] [INFO]
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  VMStation Monitoring Stack - Diagnostic Report
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Output directory: /tmp/monitoring-diagnostics-20251010-140822


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. Cluster and Pod Status
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
► Get all monitoring pods
  Saved to: 01-pods-status.txt
► Get PVC and PV status
  Saved to: 02-pvc-pv-status.txt
► Get services status
  Saved to: 03-services-status.txt
► Get endpoints status
  Saved to: 04-endpoints-status.txt

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
2. Prometheus Diagnostics
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
► Describe prometheus-0 pod
  Saved to: 05-prometheus-describe.txt
► Get prometheus-0 logs (last 500 lines)
  Saved to: 06-prometheus-logs.txt
► Get prometheus-0 init container logs
  Saved to: 07-prometheus-init-logs.txt
► Get prometheus endpoints (YAML)
  Saved to: 08-prometheus-endpoints.yaml

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
3. Loki Diagnostics
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
► Describe loki-0 pod
  Saved to: 09-loki-describe.txt
► Get loki-0 logs (last 500 lines)
  Saved to: 10-loki-logs.txt
► Get loki-0 init container logs
  Saved to: 11-loki-init-logs.txt
► Get loki endpoints (YAML)
  Saved to: 12-loki-endpoints.yaml

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
4. StatefulSets and ConfigMaps
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
► Get prometheus StatefulSet (YAML)
  Saved to: 13-prometheus-statefulset.yaml
► Get loki StatefulSet (YAML)
  Saved to: 14-loki-statefulset.yaml
► Get prometheus ConfigMap (YAML)
  Saved to: 15-prometheus-configmap.yaml
► Get loki ConfigMap (YAML)
  Saved to: 16-loki-configmap.yaml

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
5. Events and Logs
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
► Get recent events (last 100)
  Saved to: 17-recent-events.txt

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
6. Host Directory Permissions (if running on masternode)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
► Checking /srv/monitoring_data permissions
  Saved to: 18-host-permissions.txt

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
7. Readiness Probe Tests
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
► Testing Prometheus readiness endpoint (from within pod)
error: unable to upgrade connection: container not found ("prometheus")
  Saved to: 19-prometheus-readiness-test.txt
► Testing Loki readiness endpoint (from within pod)
Defaulted container "loki" out of: loki, init-loki-data (init)
Connecting to localhost:3100 ([::1]:3100)
writing to stdout
-                    100% |********************************|     6  0:00:00 ETA
written to stdout
  Saved to: 20-loki-readiness-test.txt

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
8. Analysis and Recommendations
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
✓ Analysis saved to: 00-ANALYSIS-AND-RECOMMENDATIONS.txt

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Diagnostic Report Complete
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Diagnostic report generated successfully!
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Output directory: /tmp/monitoring-diagnostics-20251010-140822

Key files to review:
  - 00-ANALYSIS-AND-RECOMMENDATIONS.txt (START HERE)
  - 06-prometheus-logs.txt (Prometheus errors)
  - 10-loki-logs.txt (Loki errors)
  - 18-host-permissions.txt (Directory permissions)

Next steps:
  1. Review the analysis and recommendations
  2. Apply the recommended fixes
  3. Run validation tests

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  VMStation Monitoring Stack - Automated Remediation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Step 1: Safety Checks and Backups
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Creating backup directory: /tmp/monitoring-backups-20251010-140823
Backing up current monitoring namespace state...
✓ Backups saved to: /tmp/monitoring-backups-20251010-140823

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Step 2: Fix Prometheus Permission Issue
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

This will add explicit SecurityContext to Prometheus StatefulSet:
  - fsGroup: 65534 (nobody group)
  - runAsUser: 65534 (nobody user)
  - runAsGroup: 65534 (nobody group)
  - runAsNonRoot: true

Apply Prometheus SecurityContext fix? [y/N] y
Patching Prometheus StatefulSet...
statefulset.apps/prometheus patched
✓ Prometheus StatefulSet patched
Deleting prometheus-0 pod to apply changes...
pod "prometheus-0" deleted
✓ Prometheus pod deletion initiated

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Step 3: Fix Loki Frontend Worker Issue
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

This will disable the frontend_worker in Loki configuration:
  - Removes frontend_worker.frontend_address connection
  - Safe for single-instance deployments
  - Loki will continue to function normally

Apply Loki frontend_worker fix? [y/N] y
Getting current Loki ConfigMap...
Applying Loki ConfigMap patch...
configmap/loki-config configured
✓ Loki ConfigMap updated
Deleting loki-0 pod to apply changes...
pod "loki-0" deleted
✓ Loki pod deletion initiated

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Step 4: Verify Host Directory Permissions (Optional)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Checking if we're running on the masternode...
✓ Found /srv/monitoring_data directory

Current permissions:
total 8
drwxr-xr-x  9 root   root     128 Oct  9 16:26 .
drwxr-xr-x 12 root   root    4096 Aug 28 15:29 ..
drwxr-xr-x  9    472     472  126 Oct 10 14:08 grafana
drwxr-xr-x  2 root   root       6 Sep  2 15:57 local-path-provisioner
drwxr-xr-x  9  10001   10001  134 Oct  9 17:32 loki
drwxr-xr-x 10 nobody nogroup  240 Oct 10 14:08 prometheus
drwxr-xr-x  3 root   root      18 Sep  2 15:57 promtail
drwxr-xr-x  2 root   root      55 Oct  9 17:44 syslog
drwxr-xr-x 11 root   root    4096 Oct 10 13:50 VMStation

Fix host directory permissions? [y/N] y
Setting correct ownership...
✓ Permissions fixed

Updated permissions:
total 8
drwxr-xr-x  9 root   root     128 Oct  9 16:26 .
drwxr-xr-x 12 root   root    4096 Aug 28 15:29 ..
drwxr-xr-x  9    472     472  126 Oct 10 14:08 grafana
drwxr-xr-x  2 root   root       6 Sep  2 15:57 local-path-provisioner
drwxr-xr-x  9  10001   10001  134 Oct  9 17:32 loki
drwxr-xr-x 10 nobody nogroup  252 Oct 10 14:08 prometheus
drwxr-xr-x  3 root   root      18 Sep  2 15:57 promtail
drwxr-xr-x  2 root   root      55 Oct  9 17:44 syslog
drwxr-xr-x 11 root   root    4096 Oct 10 13:50 VMStation

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Step 5: Wait for Pods to Restart
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Waiting for pods to restart (max 120 seconds)...

Waiting for prometheus-0...
error: timed out waiting for the condition on pods/prometheus-0
Timeout waiting for prometheus-0 (check manually)
Waiting for loki-0...
pod/loki-0 condition met

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Step 6: Validation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Pod Status:
NAME                                  READY   STATUS             RESTARTS      AGE
blackbox-exporter-5949885fb9-bwz6n    1/1     Running            0             12m
grafana-5f879c7654-k45n2              1/1     Running            0             12m
kube-state-metrics-5f6f5666cc-7tzkl   1/1     Running            0             12m
loki-0                                1/1     Running            0             119s
node-exporter-28dmd                   1/1     Running            0             12m
node-exporter-ntznj                   1/1     Running            0             12m
prometheus-0                          1/2     CrashLoopBackOff   4 (12s ago)   2m8s
promtail-7gnwk                        1/1     Running            0             12m
promtail-hlkgt                        1/1     Running            0             12m

Endpoints Status:
NAME         ENDPOINTS                           AGE
prometheus                                       12m
loki         10.244.0.23:3100,10.244.0.23:9096   12m

Recent Events:
119s        Normal    Scheduled                 pod/loki-0                                 Successfully assigned monitoring/loki-0 to masternode
118s        Normal    Started                   pod/loki-0                                 Started container loki
118s        Normal    Created                   pod/loki-0                                 Created container: loki
118s        Normal    Pulled                    pod/loki-0                                 Container image "grafana/loki:2.9.4" already present on machine
99s         Warning   Unhealthy                 pod/loki-0                                 Startup probe failed: HTTP probe failed with statuscode: 503
69s         Normal    Pulled                    pod/prometheus-0                           Container image "prom/prometheus:v2.48.0" already present on machine
69s         Normal    Started                   pod/prometheus-0                           Started container prometheus
69s         Normal    Created                   pod/prometheus-0                           Created container: prometheus
68s         Warning   Unhealthy                 pod/prometheus-0                           Startup probe failed: HTTP probe failed with statuscode: 503
62s         Warning   BackOff                   pod/prometheus-0                           Back-off restarting failed container prometheus in pod prometheus-0_monitoring(1373e632-0536-4e00-8284-1a17ecd9ab55)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Remediation Complete
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Remediation steps completed!
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Backups saved to: /tmp/monitoring-backups-20251010-140823

Next steps:
  1. Monitor pod status: kubectl get pods -n monitoring -w
  2. Check logs if issues persist:
     - kubectl logs -n monitoring prometheus-0
     - kubectl logs -n monitoring loki-0
  3. Validate Grafana connectivity:
     - Access Grafana: http://<masternode-ip>:30300
     - Check datasources under Configuration
  4. Run validation script: ./scripts/validate-monitoring-stack.sh

If issues persist:
  - Review logs in /tmp/monitoring-backups-20251010-140823
  - Check the diagnostic output
  - Restore from backup if needed: kubectl apply -f /tmp/monitoring-backups-20251010-140823/

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  VMStation Monitoring Stack - Validation Report
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test 1: Pod Status
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

[TEST] Checking Prometheus pod status
  ⚠ WARN: Prometheus pod is Running but not Ready
=========================================
VMStation Complete Validation Suite
=========================================

This test suite validates:
  1. Auto-sleep and wake configuration
  2. Monitoring exporters health
  3. Loki log aggregation
  4. Loki ConfigMap drift prevention
  5. Headless service endpoints validation
  6. Sleep/wake cycle (optional - requires confirmation)

Test order:
  - Non-destructive tests run first
  - Sleep/wake cycle test is optional (requires user confirmation)

Phase 1: Configuration Validation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Auto-Sleep/Wake Configuration
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Auto-Sleep/Wake Validation
Testing sleep/wake cycle and monitoring
=========================================

[1/10] Testing systemd timer on storagenodet3500...
✅ PASS: Auto-sleep timer is enabled on storagenodet3500
✅ PASS: Auto-sleep timer is active on storagenodet3500

[2/10] Testing systemd timer on homelab (RHEL10)...
✅ PASS: Auto-sleep timer is enabled on homelab
⚠️  WARN: Auto-sleep timer is not active on homelab

[3/10] Testing auto-sleep script existence...
✅ PASS: Auto-sleep monitor script exists on storagenodet3500
✅ PASS: Sleep script exists on storagenodet3500

[4/10] Testing WoL configuration...
✅ PASS: WoL script exists and is executable
⚠️  WARN: WoL systemd service not found on masternode

[5/10] Testing kubectl access...
✅ PASS: kubectl access verified on masternode
ℹ️  INFO: Current cluster status:
  masternode         Ready   control-plane   13m   v1.29.15
  storagenodet3500   Ready   <none>          13m   v1.29.15

[6/10] Testing WoL tool availability...
✅ PASS: wakeonlan tool is available

[7/10] Testing node reachability...
✅ PASS: storagenodet3500 is reachable (192.168.4.61)
✅ PASS: homelab is reachable (192.168.4.62)

[8/10] Testing monitoring service configuration...
✅ PASS: Monitoring namespace exists
⚠️  WARN: Prometheus pods may not be running
✅ PASS: Grafana pods are running

[9/10] Testing log file configuration...
✅ PASS: VMStation state directory exists
✅ PASS: Auto-sleep log files exist

[10/10] Testing systemd timer schedules...
✅ PASS: Auto-sleep timer is scheduled
ℹ️  INFO: Timer schedule:
  Fri 2025-10-10 14:16:54 EDT 6min left Fri 2025-10-10 14:01:54 EDT 8min ago vmstation-autosleep.timer vmstation-autosleep.service

=========================================
Test Results Summary
=========================================
Passed:   15
Failed:   0
Warnings: 3

✅ All critical tests passed!

Auto-Sleep/Wake Configuration:
  - Systemd timers configured on both nodes
  - Scripts deployed and executable
  - Monitoring services available

Manual Testing:
  1. Trigger sleep: ssh root@192.168.4.63 'sudo /usr/local/bin/vmstation-sleep.sh'
  2. Check node status: ssh root@192.168.4.63 'kubectl get nodes'
  3. Send WoL: wakeonlan b8:ac:6f:7e:6c:9d
  4. Monitor wake time and verify services

✅ SUITE PASSED: Auto-Sleep/Wake Configuration


Phase 2: Monitoring Health Validation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Monitoring Exporters Health
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Monitoring Exporters Health
Validating exporters, targets, and dashboards
=========================================

[1/8] Testing Prometheus targets...
❌ FAIL: Cannot access Prometheus targets API

[2/8] Testing node-exporter on all nodes...
curl http://192.168.4.63:9100/metrics success
✅ PASS: Node exporter healthy on masternode
curl http://192.168.4.61:9100/metrics success
✅ PASS: Node exporter healthy on storagenodet3500
❌ SUITE FAILED: Monitoring Exporters Health


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Loki Log Aggregation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Loki Log Aggregation Test
Validating Loki connectivity and log ingestion
=========================================

[1/6] Testing Loki pod status...
ℹ️  INFO: Loki pods found:
  loki-0                                1/1     Running            0             2m6s
✅ PASS: Loki pods are running

[2/6] Testing Loki service configuration...
✅ PASS: Loki service exists
ℹ️  INFO: Loki service details:
  loki                   ClusterIP   None             <none>        3100/TCP,9096/TCP   12m
  loki-external          NodePort    10.104.129.102   <none>        3100:31100/TCP      12m
✅ PASS: Loki service has endpoints

[3/6] Testing Loki API connectivity...
curl http://192.168.4.63:31100/ready ok
✅ PASS: Loki is ready

[4/6] Testing Promtail log shipper...
ℹ️  INFO: Promtail pods found:
  promtail-7gnwk                        1/1     Running            0             12m
  promtail-hlkgt                        1/1     Running            0             12m
✅ PASS: Promtail pods are running (2 instances)

[5/6] Testing Loki DNS resolution...
✅ PASS: Loki DNS resolution successful

[6/6] Testing Loki datasource in Grafana...
✅ PASS: Loki datasource is configured in Grafana
⚠️  WARN: Could not check Loki datasource health

=========================================
Test Results Summary
=========================================
Passed:   7
Failed:   0
Warnings: 1

✅ Loki log aggregation is healthy!

Loki Configuration:
  - Loki pods: Running
  - Promtail: Collecting logs
  - Grafana datasource: Configured

Query logs via Grafana:
  URL: http://192.168.4.63:30300/explore
  Select 'Loki' datasource and run queries

✅ SUITE PASSED: Loki Log Aggregation


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Loki ConfigMap Drift Prevention
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Loki ConfigMap Drift Test
Validating in-cluster config matches repo
=========================================

[1/5] Checking Loki manifest in repository...
✅ PASS: Loki manifest exists in repository

[2/5] Extracting ConfigMap from repository...
❌ FAIL: Failed to extract ConfigMap from repository
❌ SUITE FAILED: Loki ConfigMap Drift Prevention


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Monitoring Access (Updated)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Monitoring Access Test
Testing anonymous access to endpoints
=========================================

[1/8] Testing Grafana Access...
Testing Grafana Web UI... ✅ PASS
  curl http://192.168.4.63:30300 success
Testing Grafana API (anonymous)... ✅ PASS
  curl http://192.168.4.63:30300/api/health success

[2/8] Testing Prometheus Access...
Testing Prometheus Web UI... ❌ FAIL (not accessible)
  curl http://192.168.4.63:30090 error
Testing Prometheus Health... ❌ FAIL (not accessible)
  curl http://192.168.4.63:30090/-/healthy error
❌ SUITE FAILED: Monitoring Access (Updated)


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Headless Service Endpoints
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

======================================================================
VMStation Headless Service Endpoints Diagnostic
======================================================================

[1/10] Checking monitoring namespace...
✓ Monitoring namespace exists

[2/10] Checking pod status in monitoring namespace...
NAME                                  READY   STATUS             RESTARTS      AGE     IP             NODE               NOMINATED NODE   READINESS GATES
blackbox-exporter-5949885fb9-bwz6n    1/1     Running            0             12m     10.244.0.18    masternode         <none>           <none>
grafana-5f879c7654-k45n2              1/1     Running            0             12m     10.244.0.19    masternode         <none>           <none>
kube-state-metrics-5f6f5666cc-7tzkl   1/1     Running            0             12m     10.244.0.15    masternode         <none>           <none>
loki-0                                1/1     Running            0             2m14s   10.244.0.23    masternode         <none>           <none>
node-exporter-28dmd                   1/1     Running            0             12m     192.168.4.63   masternode         <none>           <none>
node-exporter-ntznj                   1/1     Running            0             12m     192.168.4.61   storagenodet3500   <none>           <none>
prometheus-0                          1/2     CrashLoopBackOff   4 (27s ago)   2m23s   10.244.0.22    masternode         <none>           <none>
promtail-7gnwk                        1/1     Running            0             12m     10.244.0.16    masternode         <none>           <none>
promtail-hlkgt                        1/1     Running            0             12m     10.244.1.72    storagenodet3500   <none>           <none>

✓ Found 1 Prometheus pod(s)
✗ Prometheus pod(s) are NOT Ready
NAME           READY   STATUS             RESTARTS      AGE     IP            NODE         NOMINATED NODE   READINESS GATES
prometheus-0   1/2     CrashLoopBackOff   4 (27s ago)   2m23s   10.244.0.22   masternode   <none>           <none>
✓ Found 1 Loki pod(s)
✓ Loki pod(s) are Ready

[3/10] Checking StatefulSets and Deployments...
NAME         READY   AGE
loki         1/1     12m
prometheus   0/1     12m
✓ Prometheus StatefulSet exists
✗ Prometheus StatefulSet: /1 replicas ready
✓ Loki StatefulSet exists
✓ Loki StatefulSet: 1/1 replicas ready

[4/10] Checking service selectors...
ℹ Prometheus service:
  Selector: {"app.kubernetes.io/component":"monitoring","app.kubernetes.io/name":"prometheus"}
✓ Prometheus service selector found
ℹ Loki service:
  Selector: {"app.kubernetes.io/component":"logging","app.kubernetes.io/name":"loki"}
✓ Loki service selector found

[5/10] Checking pod labels match service selectors...
ℹ Prometheus pod labels:
  {"app":"prometheus","app.kubernetes.io/component":"monitoring","app.kubernetes.io/name":"prometheus","apps.kubernetes.io/pod-index":"0","controller-revision-hash":"prometheus-f77559857","statefulset.kubernetes.io/pod-name":"prometheus-0"}
✓ Prometheus pod has correct app.kubernetes.io/name label
✓ Prometheus pod has correct app.kubernetes.io/component label
ℹ Loki pod labels:
  {"app":"loki","app.kubernetes.io/component":"logging","app.kubernetes.io/name":"loki","apps.kubernetes.io/pod-index":"0","controller-revision-hash":"loki-5c9f84dfc6","statefulset.kubernetes.io/pod-name":"loki-0"}
✓ Loki pod has correct app.kubernetes.io/name label
✓ Loki pod has correct app.kubernetes.io/component label

[6/10] Checking service endpoints...
✗ Prometheus service has NO endpoints (empty)
ℹ This means pods are not matching the service selector or pods are not ready
✓ Loki endpoints: 10.244.0.23

[7/10] Checking PersistentVolumeClaims...
NAME                              STATUS   VOLUME          CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
grafana-pvc                       Bound    grafana-pv      2Gi        RWO                           <unset>                 12m
loki-data-loki-0                  Bound    loki-pv         20Gi       RWO                           <unset>                 12m
prometheus-storage-prometheus-0   Bound    prometheus-pv   10Gi       RWO                           <unset>                 12m
promtail-pvc                      Bound    promtail-pv     1Gi        RWO                           <unset>                 12m
✓ No PVCs in Pending state
✓ 4 PVC(s) successfully bound

[8/10] Checking for pod failures...
✓ No pods in error states

[9/10] Checking headless service configuration...
✓ Prometheus service is headless (ClusterIP: None)
ℹ For headless services, use FQDN: prometheus.monitoring.svc.cluster.local
✓ Loki service is headless (ClusterIP: None)
ℹ For headless services, use FQDN: loki.monitoring.svc.cluster.local

[10/10] Testing DNS resolution for headless services...
ℹ Testing DNS resolution from within cluster...
✓ DNS resolution works for prometheus.monitoring.svc.cluster.local
✓ DNS resolution works for loki.monitoring.svc.cluster.local

======================================================================
Diagnostic Summary
======================================================================

ℹ Common root causes for empty endpoints:
  A) Service selector and pod labels don't match
  B) Pods not running or not ready (CrashLoopBackOff, Pending, etc.)
  C) PVCs stuck in Pending state
  D) Permission errors on PersistentVolumes

ℹ Recommended fixes:
  1. Check pod status: kubectl get pods -n monitoring -o wide
  2. Check pod logs: kubectl logs -n monitoring <pod-name> --tail=100
  3. Describe pod: kubectl describe pod -n monitoring <pod-name>
  4. Check PVC status: kubectl get pvc -n monitoring
  5. Verify service selector matches pod labels
  6. Use FQDNs for headless services in Grafana datasources

ℹ For detailed troubleshooting, see:
  docs/HEADLESS_SERVICE_ENDPOINTS_TROUBLESHOOTING.md

✅ SUITE PASSED: Headless Service Endpoints


Phase 3: Sleep/Wake Cycle Test (Optional)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

WARNING: This test will trigger cluster sleep and wake.
This is a destructive test that will:
  - Cordon and drain worker nodes
  - Scale down deployments
  - Send Wake-on-LAN packets
  - Measure wake time and validate service restoration

Run sleep/wake cycle test? [y/N]: N
⚠️  Sleep/wake cycle test skipped by user

=========================================
Complete Validation Summary
=========================================

Test Suites Run:    6
Suites Passed:      3
Suites Failed:      3

❌ Some test suites failed.

Review the output above for details.

Common next steps:
  1. Fix failed tests and re-run: ./tests/test-complete-validation.sh
  2. Deploy missing components: ./deploy.sh setup
  3. Check cluster health: kubectl get pods -A
  4. Review logs: journalctl -u vmstation-autosleep -n 50



root@masternode:/srv/monitoring_data/VMStation# kubectl get pods -o wide -A
NAMESPACE        NAME                                  READY   STATUS             RESTARTS        AGE     IP             NODE               NOMINATED NODE   READINESS GATES
infrastructure   chrony-ntp-5z5p7                      2/2     Running            0               11m     192.168.4.61   storagenodet3500   <none>           <none>
infrastructure   chrony-ntp-9j5fn                      2/2     Running            0               11m     192.168.4.63   masternode         <none>           <none>
infrastructure   syslog-server-0                       2/2     Running            0               9m53s   10.244.0.21    masternode         <none>           <none>
jellyfin         jellyfin                              1/1     Running            0               18m     10.244.1.73    storagenodet3500   <none>           <none>
kube-flannel     kube-flannel-ds-67lkp                 1/1     Running            0               19m     192.168.4.63   masternode         <none>           <none>
kube-flannel     kube-flannel-ds-rfx44                 1/1     Running            0               19m     192.168.4.61   storagenodet3500   <none>           <none>
kube-system      coredns-76f75df574-drmtx              1/1     Running            0               19m     10.244.0.14    masternode         <none>           <none>
kube-system      coredns-76f75df574-nr22v              1/1     Running            0               19m     10.244.0.13    masternode         <none>           <none>
kube-system      etcd-masternode                       1/1     Running            47              20m     192.168.4.63   masternode         <none>           <none>
kube-system      kube-apiserver-masternode             1/1     Running            47              20m     192.168.4.63   masternode         <none>           <none>
kube-system      kube-controller-manager-masternode    1/1     Running            47              20m     192.168.4.63   masternode         <none>           <none>
kube-system      kube-proxy-9fssv                      1/1     Running            0               19m     192.168.4.63   masternode         <none>           <none>
kube-system      kube-proxy-vz86t                      1/1     Running            0               19m     192.168.4.61   storagenodet3500   <none>           <none>
kube-system      kube-scheduler-masternode             1/1     Running            47              20m     192.168.4.63   masternode         <none>           <none>
monitoring       blackbox-exporter-5949885fb9-bwz6n    1/1     Running            0               19m     10.244.0.18    masternode         <none>           <none>
monitoring       grafana-5f879c7654-k45n2              1/1     Running            0               19m     10.244.0.19    masternode         <none>           <none>
monitoring       kube-state-metrics-5f6f5666cc-7tzkl   1/1     Running            0               19m     10.244.0.15    masternode         <none>           <none>
monitoring       loki-0                                1/1     Running            0               8m53s   10.244.0.23    masternode         <none>           <none>
monitoring       node-exporter-28dmd                   1/1     Running            0               19m     192.168.4.63   masternode         <none>           <none>
monitoring       node-exporter-ntznj                   1/1     Running            0               19m     192.168.4.61   storagenodet3500   <none>           <none>
monitoring       prometheus-0                          1/2     CrashLoopBackOff   6 (2m41s ago)   9m2s    10.244.0.22    masternode         <none>           <none>
monitoring       promtail-7gnwk                        1/1     Running            0               19m     10.244.0.16    masternode         <none>           <none>
monitoring       promtail-hlkgt                        1/1     Running            0               19m     10.244.1.72    storagenodet3500   <none>           <none>
root@masternode:/srv/monitoring_data/VMStation# kubectl describe pod -n monitoring prometheeus-0
Error from server (NotFound): pods "prometheeus-0" not found
root@masternode:/srv/monitoring_data/VMStation# kubectl describe pod -n monitoring prometheus-0
Name:                 prometheus-0
Namespace:            monitoring
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Service Account:      prometheus
Node:                 masternode/192.168.4.63
Start Time:           Fri, 10 Oct 2025 14:08:39 -0400
Labels:               app=prometheus
                      app.kubernetes.io/component=monitoring
                      app.kubernetes.io/name=prometheus
                      apps.kubernetes.io/pod-index=0
                      controller-revision-hash=prometheus-f77559857
                      statefulset.kubernetes.io/pod-name=prometheus-0
Annotations:          checksum/config: {{ .Values.config | sha256sum }}
                      prometheus.io/path: /metrics
                      prometheus.io/port: 9090
                      prometheus.io/scrape: true
Status:               Running
IP:                   10.244.0.22
IPs:
  IP:           10.244.0.22
Controlled By:  StatefulSet/prometheus
Init Containers:
  init-chown-data:
    Container ID:  containerd://44f9ce016f927eda8245db558c33de4c8a81c3b550c0bd50086efa64ff1cd621
    Image:         busybox:latest
    Image ID:      docker.io/library/busybox@sha256:d82f458899c9696cb26a7c02d5568f81c8c8223f8661bb2a7988b269c8b9051e
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      chown -R 65534:65534 /prometheus
      chmod -R 755 /prometheus

    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 10 Oct 2025 14:08:40 -0400
      Finished:     Fri, 10 Oct 2025 14:08:40 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /prometheus from prometheus-storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8vq47 (ro)
Containers:
  prometheus:
    Container ID:  containerd://c154169c651de8db1b093f50952e8b86b123fa602e29cb580f88c7f127e54cc9
    Image:         prom/prometheus:v2.48.0
    Image ID:      docker.io/prom/prometheus@sha256:b440bc0e8aa5bab44a782952c09516b6a50f9d7b2325c1ffafac7bc833298e2e
    Port:          9090/TCP
    Host Port:     0/TCP
    Args:
      --config.file=/etc/prometheus/prometheus.yml
      --storage.tsdb.path=/prometheus
      --storage.tsdb.retention.time=30d
      --storage.tsdb.retention.size=4GB
      --storage.tsdb.wal-compression
      --query.timeout=2m
      --query.max-concurrency=20
      --query.max-samples=50000000
      --web.console.libraries=/etc/prometheus/console_libraries
      --web.console.templates=/etc/prometheus/consoles
      --web.route-prefix=/
      --web.external-url=http://prometheus.monitoring.svc.cluster.local:9090
      --web.enable-lifecycle
      --web.enable-remote-write-receiver
      --web.cors.origin=.*
      --web.enable-admin-api
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Fri, 10 Oct 2025 14:14:56 -0400
      Finished:     Fri, 10 Oct 2025 14:15:00 -0400
    Ready:          False
    Restart Count:  6
    Limits:
      cpu:     2
      memory:  4Gi
    Requests:
      cpu:        500m
      memory:     1Gi
    Liveness:     http-get http://:web/-/healthy delay=30s timeout=10s period=30s #success=1 #failure=3
    Readiness:    http-get http://:web/-/ready delay=30s timeout=5s period=10s #success=1 #failure=3
    Startup:      http-get http://:web/-/ready delay=0s timeout=5s period=15s #success=1 #failure=20
    Environment:  <none>
    Mounts:
      /etc/prometheus from prometheus-config (ro)
      /etc/prometheus/rules from prometheus-rules (ro)
      /prometheus from prometheus-storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8vq47 (ro)
  config-reloader:
    Container ID:  containerd://508cdee8999b028f1dc30fece7db517624a04c4125fae22f1733758a46f91158
    Image:         quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0
    Image ID:      quay.io/prometheus-operator/prometheus-config-reloader@sha256:e20576b76ffd85d2a28d62809092f47b339737320e80646ec6d0e7ac0f4c8e43
    Port:          8080/TCP
    Host Port:     0/TCP
    Args:
      --listen-address=:8080
      --reload-url=http://localhost:9090/-/reload
      --watched-dir=/etc/prometheus
    State:          Running
      Started:      Fri, 10 Oct 2025 14:08:40 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     50m
      memory:  64Mi
    Requests:
      cpu:        10m
      memory:     16Mi
    Environment:  <none>
    Mounts:
      /etc/prometheus from prometheus-config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8vq47 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  prometheus-storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  prometheus-storage-prometheus-0
    ReadOnly:   false
  prometheus-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-config
    Optional:  false
  prometheus-rules:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-rules
    Optional:  false
  kube-api-access-8vq47:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              node-role.kubernetes.io/control-plane=
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                     From               Message
  ----     ------     ----                    ----               -------
  Normal   Scheduled  9m31s                   default-scheduler  Successfully assigned monitoring/prometheus-0 to masternode
  Normal   Pulled     9m30s                   kubelet            Container image "busybox:latest" already present on machine
  Normal   Created    9m30s                   kubelet            Created container: init-chown-data
  Normal   Started    9m30s                   kubelet            Started container init-chown-data
  Normal   Pulled     9m30s                   kubelet            Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
  Normal   Created    9m30s                   kubelet            Created container: config-reloader
  Normal   Started    9m30s                   kubelet            Started container config-reloader
  Normal   Pulled     8m32s (x4 over 9m30s)   kubelet            Container image "prom/prometheus:v2.48.0" already present on machine
  Normal   Created    8m32s (x4 over 9m30s)   kubelet            Created container: prometheus
  Normal   Started    8m32s (x4 over 9m30s)   kubelet            Started container prometheus
  Warning  Unhealthy  8m31s                   kubelet            Startup probe failed: HTTP probe failed with statuscode: 503
  Warning  BackOff    4m29s (x26 over 9m17s)  kubelet            Back-off restarting failed container prometheus in pod prometheus-0_monitoring(1373e632-0536-4e00-8284-1a17ecd9ab55)
root@masternode:/srv/monitoring_data/VMStation# kubectl logs -n monitoring prometheus-0
Defaulted container "prometheus" out of: prometheus, config-reloader, init-chown-data (init)
ts=2025-10-10T18:14:56.558Z caller=main.go:583 level=info msg="Starting Prometheus Server" mode=server version="(version=2.48.0, branch=HEAD, revision=6d80b30990bc297d95b5c844e118c4011fad8054)"
ts=2025-10-10T18:14:56.558Z caller=main.go:588 level=info build_context="(go=go1.21.4, platform=linux/amd64, user=root@26117804242c, date=20231116-04:35:21, tags=netgo,builtinassets,stringlabels)"
ts=2025-10-10T18:14:56.559Z caller=main.go:589 level=info host_details="(Linux 6.1.0-32-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.129-1 (2025-03-06) x86_64 prometheus-0 (none))"
ts=2025-10-10T18:14:56.559Z caller=main.go:590 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2025-10-10T18:14:56.559Z caller=main.go:591 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2025-10-10T18:14:56.561Z caller=web.go:566 level=info component=web msg="Start listening for connections" address=0.0.0.0:9090
ts=2025-10-10T18:14:56.561Z caller=main.go:1024 level=info msg="Starting TSDB ..."
ts=2025-10-10T18:14:56.561Z caller=repair.go:56 level=info component=tsdb msg="Found healthy block" mint=1759964317177 maxt=1759968000000 ulid=01K73B2599T4AQZSHDS79QV2XZ
ts=2025-10-10T18:14:56.561Z caller=repair.go:56 level=info component=tsdb msg="Found healthy block" mint=1759989600336 maxt=1759996800000 ulid=01K7435QAC72RY72Z7V8Q4QQ45
ts=2025-10-10T18:14:56.562Z caller=repair.go:56 level=info component=tsdb msg="Found healthy block" mint=1759968000000 maxt=1759989600000 ulid=01K7435V9KGM8RMJ130R8ZVH3M
ts=2025-10-10T18:14:56.562Z caller=repair.go:56 level=info component=tsdb msg="Found healthy block" mint=1759996800221 maxt=1760004000000 ulid=01K74A1EA3W6BPHA9CJSBBHM20
ts=2025-10-10T18:14:56.562Z caller=repair.go:56 level=info component=tsdb msg="Found healthy block" mint=1760004000000 maxt=1760011200000 ulid=01K74GX61KCGJ91FNG3GCB66FY
ts=2025-10-10T18:14:56.563Z caller=tls_config.go:274 level=info component=web msg="Listening on" address=[::]:9090
ts=2025-10-10T18:14:56.563Z caller=tls_config.go:277 level=info component=web msg="TLS is disabled." http2=false address=[::]:9090
ts=2025-10-10T18:14:56.573Z caller=head.go:601 level=info component=tsdb msg="Replaying on-disk memory mappable chunks if any"
ts=2025-10-10T18:14:56.652Z caller=head.go:682 level=info component=tsdb msg="On-disk memory mappable chunks replay completed" duration=79.516067ms
ts=2025-10-10T18:14:56.652Z caller=head.go:690 level=info component=tsdb msg="Replaying WAL, this may take a while"
ts=2025-10-10T18:14:56.802Z caller=head.go:726 level=info component=tsdb msg="WAL checkpoint loaded"
ts=2025-10-10T18:14:56.858Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=14 maxSegment=284
ts=2025-10-10T18:14:57.319Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=15 maxSegment=284
ts=2025-10-10T18:14:57.999Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=16 maxSegment=284
ts=2025-10-10T18:14:58.314Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=17 maxSegment=284
ts=2025-10-10T18:15:00.312Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=18 maxSegment=284
ts=2025-10-10T18:15:00.312Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=19 maxSegment=284
ts=2025-10-10T18:15:00.312Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=20 maxSegment=284
ts=2025-10-10T18:15:00.313Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=21 maxSegment=284
ts=2025-10-10T18:15:00.313Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=22 maxSegment=284
ts=2025-10-10T18:15:00.313Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=23 maxSegment=284
ts=2025-10-10T18:15:00.313Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=24 maxSegment=284
ts=2025-10-10T18:15:00.313Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=25 maxSegment=284
ts=2025-10-10T18:15:00.314Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=26 maxSegment=284
ts=2025-10-10T18:15:00.314Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=27 maxSegment=284
ts=2025-10-10T18:15:00.314Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=28 maxSegment=284
ts=2025-10-10T18:15:00.314Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=29 maxSegment=284
ts=2025-10-10T18:15:00.314Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=30 maxSegment=284
ts=2025-10-10T18:15:00.314Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=31 maxSegment=284
ts=2025-10-10T18:15:00.315Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=32 maxSegment=284
ts=2025-10-10T18:15:00.315Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=33 maxSegment=284
ts=2025-10-10T18:15:00.315Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=34 maxSegment=284
ts=2025-10-10T18:15:00.315Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=35 maxSegment=284
ts=2025-10-10T18:15:00.315Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=36 maxSegment=284
ts=2025-10-10T18:15:00.315Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=37 maxSegment=284
ts=2025-10-10T18:15:00.316Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=38 maxSegment=284
ts=2025-10-10T18:15:00.316Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=39 maxSegment=284
ts=2025-10-10T18:15:00.316Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=40 maxSegment=284
ts=2025-10-10T18:15:00.316Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=41 maxSegment=284
ts=2025-10-10T18:15:00.316Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=42 maxSegment=284
ts=2025-10-10T18:15:00.316Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=43 maxSegment=284
ts=2025-10-10T18:15:00.317Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=44 maxSegment=284
ts=2025-10-10T18:15:00.317Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=45 maxSegment=284
ts=2025-10-10T18:15:00.317Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=46 maxSegment=284
ts=2025-10-10T18:15:00.317Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=47 maxSegment=284
ts=2025-10-10T18:15:00.317Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=48 maxSegment=284
ts=2025-10-10T18:15:00.317Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=49 maxSegment=284
ts=2025-10-10T18:15:00.318Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=50 maxSegment=284
ts=2025-10-10T18:15:00.318Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=51 maxSegment=284
ts=2025-10-10T18:15:00.318Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=52 maxSegment=284
ts=2025-10-10T18:15:00.318Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=53 maxSegment=284
ts=2025-10-10T18:15:00.318Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=54 maxSegment=284
ts=2025-10-10T18:15:00.318Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=55 maxSegment=284
ts=2025-10-10T18:15:00.318Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=56 maxSegment=284
ts=2025-10-10T18:15:00.319Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=57 maxSegment=284
ts=2025-10-10T18:15:00.319Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=58 maxSegment=284
ts=2025-10-10T18:15:00.319Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=59 maxSegment=284
ts=2025-10-10T18:15:00.319Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=60 maxSegment=284
ts=2025-10-10T18:15:00.319Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=61 maxSegment=284
ts=2025-10-10T18:15:00.319Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=62 maxSegment=284
ts=2025-10-10T18:15:00.319Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=63 maxSegment=284
ts=2025-10-10T18:15:00.319Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=64 maxSegment=284
ts=2025-10-10T18:15:00.320Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=65 maxSegment=284
ts=2025-10-10T18:15:00.320Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=66 maxSegment=284
ts=2025-10-10T18:15:00.320Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=67 maxSegment=284
ts=2025-10-10T18:15:00.322Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=68 maxSegment=284
ts=2025-10-10T18:15:00.322Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=69 maxSegment=284
ts=2025-10-10T18:15:00.322Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=70 maxSegment=284
ts=2025-10-10T18:15:00.367Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=71 maxSegment=284
ts=2025-10-10T18:15:00.367Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=72 maxSegment=284
ts=2025-10-10T18:15:00.367Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=73 maxSegment=284
ts=2025-10-10T18:15:00.367Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=74 maxSegment=284
ts=2025-10-10T18:15:00.368Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=75 maxSegment=284
ts=2025-10-10T18:15:00.368Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=76 maxSegment=284
ts=2025-10-10T18:15:00.368Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=77 maxSegment=284
ts=2025-10-10T18:15:00.371Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=78 maxSegment=284
ts=2025-10-10T18:15:00.371Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=79 maxSegment=284
ts=2025-10-10T18:15:00.372Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=80 maxSegment=284
ts=2025-10-10T18:15:00.373Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=81 maxSegment=284
ts=2025-10-10T18:15:00.376Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=82 maxSegment=284
ts=2025-10-10T18:15:00.376Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=83 maxSegment=284
ts=2025-10-10T18:15:00.377Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=84 maxSegment=284
ts=2025-10-10T18:15:00.379Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=85 maxSegment=284
ts=2025-10-10T18:15:00.380Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=86 maxSegment=284
ts=2025-10-10T18:15:00.382Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=87 maxSegment=284
ts=2025-10-10T18:15:00.383Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=88 maxSegment=284
ts=2025-10-10T18:15:00.384Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=89 maxSegment=284
ts=2025-10-10T18:15:00.386Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=90 maxSegment=284
ts=2025-10-10T18:15:00.392Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=91 maxSegment=284
ts=2025-10-10T18:15:00.393Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=92 maxSegment=284
ts=2025-10-10T18:15:00.393Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=93 maxSegment=284
ts=2025-10-10T18:15:00.394Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=94 maxSegment=284
ts=2025-10-10T18:15:00.394Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=95 maxSegment=284
ts=2025-10-10T18:15:00.395Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=96 maxSegment=284
ts=2025-10-10T18:15:00.396Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=97 maxSegment=284
ts=2025-10-10T18:15:00.398Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=98 maxSegment=284
ts=2025-10-10T18:15:00.399Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=99 maxSegment=284
ts=2025-10-10T18:15:00.399Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=100 maxSegment=284
ts=2025-10-10T18:15:00.400Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=101 maxSegment=284
ts=2025-10-10T18:15:00.400Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=102 maxSegment=284
ts=2025-10-10T18:15:00.400Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=103 maxSegment=284
ts=2025-10-10T18:15:00.400Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=104 maxSegment=284
ts=2025-10-10T18:15:00.401Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=105 maxSegment=284
ts=2025-10-10T18:15:00.401Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=106 maxSegment=284
ts=2025-10-10T18:15:00.401Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=107 maxSegment=284
ts=2025-10-10T18:15:00.401Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=108 maxSegment=284
ts=2025-10-10T18:15:00.401Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=109 maxSegment=284
ts=2025-10-10T18:15:00.401Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=110 maxSegment=284
ts=2025-10-10T18:15:00.401Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=111 maxSegment=284
ts=2025-10-10T18:15:00.402Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=112 maxSegment=284
ts=2025-10-10T18:15:00.402Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=113 maxSegment=284
ts=2025-10-10T18:15:00.402Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=114 maxSegment=284
ts=2025-10-10T18:15:00.402Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=115 maxSegment=284
ts=2025-10-10T18:15:00.402Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=116 maxSegment=284
ts=2025-10-10T18:15:00.402Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=117 maxSegment=284
ts=2025-10-10T18:15:00.402Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=118 maxSegment=284
ts=2025-10-10T18:15:00.403Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=119 maxSegment=284
ts=2025-10-10T18:15:00.403Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=120 maxSegment=284
ts=2025-10-10T18:15:00.403Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=121 maxSegment=284
ts=2025-10-10T18:15:00.403Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=122 maxSegment=284
ts=2025-10-10T18:15:00.403Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=123 maxSegment=284
ts=2025-10-10T18:15:00.403Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=124 maxSegment=284
ts=2025-10-10T18:15:00.403Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=125 maxSegment=284
ts=2025-10-10T18:15:00.404Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=126 maxSegment=284
ts=2025-10-10T18:15:00.404Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=127 maxSegment=284
ts=2025-10-10T18:15:00.404Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=128 maxSegment=284
ts=2025-10-10T18:15:00.404Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=129 maxSegment=284
ts=2025-10-10T18:15:00.404Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=130 maxSegment=284
ts=2025-10-10T18:15:00.404Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=131 maxSegment=284
ts=2025-10-10T18:15:00.404Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=132 maxSegment=284
ts=2025-10-10T18:15:00.404Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=133 maxSegment=284
ts=2025-10-10T18:15:00.405Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=134 maxSegment=284
ts=2025-10-10T18:15:00.405Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=135 maxSegment=284
ts=2025-10-10T18:15:00.405Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=136 maxSegment=284
ts=2025-10-10T18:15:00.405Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=137 maxSegment=284
ts=2025-10-10T18:15:00.405Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=138 maxSegment=284
ts=2025-10-10T18:15:00.405Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=139 maxSegment=284
ts=2025-10-10T18:15:00.405Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=140 maxSegment=284
ts=2025-10-10T18:15:00.406Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=141 maxSegment=284
ts=2025-10-10T18:15:00.406Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=142 maxSegment=284
ts=2025-10-10T18:15:00.406Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=143 maxSegment=284
ts=2025-10-10T18:15:00.406Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=144 maxSegment=284
ts=2025-10-10T18:15:00.406Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=145 maxSegment=284
ts=2025-10-10T18:15:00.406Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=146 maxSegment=284
ts=2025-10-10T18:15:00.406Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=147 maxSegment=284
ts=2025-10-10T18:15:00.407Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=148 maxSegment=284
ts=2025-10-10T18:15:00.407Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=149 maxSegment=284
ts=2025-10-10T18:15:00.407Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=150 maxSegment=284
ts=2025-10-10T18:15:00.407Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=151 maxSegment=284
ts=2025-10-10T18:15:00.407Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=152 maxSegment=284
ts=2025-10-10T18:15:00.407Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=153 maxSegment=284
ts=2025-10-10T18:15:00.408Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=154 maxSegment=284
ts=2025-10-10T18:15:00.408Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=155 maxSegment=284
ts=2025-10-10T18:15:00.408Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=156 maxSegment=284
ts=2025-10-10T18:15:00.408Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=157 maxSegment=284
ts=2025-10-10T18:15:00.408Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=158 maxSegment=284
ts=2025-10-10T18:15:00.408Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=159 maxSegment=284
ts=2025-10-10T18:15:00.408Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=160 maxSegment=284
ts=2025-10-10T18:15:00.409Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=161 maxSegment=284
ts=2025-10-10T18:15:00.409Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=162 maxSegment=284
ts=2025-10-10T18:15:00.409Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=163 maxSegment=284
ts=2025-10-10T18:15:00.409Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=164 maxSegment=284
ts=2025-10-10T18:15:00.409Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=165 maxSegment=284
ts=2025-10-10T18:15:00.409Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=166 maxSegment=284
ts=2025-10-10T18:15:00.409Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=167 maxSegment=284
ts=2025-10-10T18:15:00.410Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=168 maxSegment=284
ts=2025-10-10T18:15:00.410Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=169 maxSegment=284
ts=2025-10-10T18:15:00.410Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=170 maxSegment=284
ts=2025-10-10T18:15:00.410Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=171 maxSegment=284
ts=2025-10-10T18:15:00.411Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=172 maxSegment=284
ts=2025-10-10T18:15:00.411Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=173 maxSegment=284
ts=2025-10-10T18:15:00.411Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=174 maxSegment=284
ts=2025-10-10T18:15:00.411Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=175 maxSegment=284
ts=2025-10-10T18:15:00.411Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=176 maxSegment=284
ts=2025-10-10T18:15:00.412Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=177 maxSegment=284
ts=2025-10-10T18:15:00.412Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=178 maxSegment=284
ts=2025-10-10T18:15:00.412Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=179 maxSegment=284
ts=2025-10-10T18:15:00.412Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=180 maxSegment=284
ts=2025-10-10T18:15:00.413Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=181 maxSegment=284
ts=2025-10-10T18:15:00.413Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=182 maxSegment=284
ts=2025-10-10T18:15:00.413Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=183 maxSegment=284
ts=2025-10-10T18:15:00.414Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=184 maxSegment=284
ts=2025-10-10T18:15:00.414Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=185 maxSegment=284
ts=2025-10-10T18:15:00.414Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=186 maxSegment=284
ts=2025-10-10T18:15:00.414Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=187 maxSegment=284
ts=2025-10-10T18:15:00.415Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=188 maxSegment=284
ts=2025-10-10T18:15:00.415Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=189 maxSegment=284
ts=2025-10-10T18:15:00.415Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=190 maxSegment=284
ts=2025-10-10T18:15:00.415Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=191 maxSegment=284
ts=2025-10-10T18:15:00.416Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=192 maxSegment=284
ts=2025-10-10T18:15:00.416Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=193 maxSegment=284
ts=2025-10-10T18:15:00.416Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=194 maxSegment=284
ts=2025-10-10T18:15:00.416Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=195 maxSegment=284
ts=2025-10-10T18:15:00.417Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=196 maxSegment=284
ts=2025-10-10T18:15:00.417Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=197 maxSegment=284
ts=2025-10-10T18:15:00.417Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=198 maxSegment=284
ts=2025-10-10T18:15:00.417Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=199 maxSegment=284
ts=2025-10-10T18:15:00.417Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=200 maxSegment=284
ts=2025-10-10T18:15:00.417Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=201 maxSegment=284
ts=2025-10-10T18:15:00.417Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=202 maxSegment=284
ts=2025-10-10T18:15:00.418Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=203 maxSegment=284
ts=2025-10-10T18:15:00.418Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=204 maxSegment=284
ts=2025-10-10T18:15:00.418Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=205 maxSegment=284
ts=2025-10-10T18:15:00.418Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=206 maxSegment=284
ts=2025-10-10T18:15:00.418Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=207 maxSegment=284
ts=2025-10-10T18:15:00.418Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=208 maxSegment=284
ts=2025-10-10T18:15:00.418Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=209 maxSegment=284
ts=2025-10-10T18:15:00.418Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=210 maxSegment=284
ts=2025-10-10T18:15:00.419Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=211 maxSegment=284
ts=2025-10-10T18:15:00.419Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=212 maxSegment=284
ts=2025-10-10T18:15:00.419Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=213 maxSegment=284
ts=2025-10-10T18:15:00.419Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=214 maxSegment=284
ts=2025-10-10T18:15:00.419Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=215 maxSegment=284
ts=2025-10-10T18:15:00.419Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=216 maxSegment=284
ts=2025-10-10T18:15:00.419Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=217 maxSegment=284
ts=2025-10-10T18:15:00.420Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=218 maxSegment=284
ts=2025-10-10T18:15:00.420Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=219 maxSegment=284
ts=2025-10-10T18:15:00.420Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=220 maxSegment=284
ts=2025-10-10T18:15:00.420Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=221 maxSegment=284
ts=2025-10-10T18:15:00.421Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=222 maxSegment=284
ts=2025-10-10T18:15:00.421Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=223 maxSegment=284
ts=2025-10-10T18:15:00.421Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=224 maxSegment=284
ts=2025-10-10T18:15:00.422Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=225 maxSegment=284
ts=2025-10-10T18:15:00.422Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=226 maxSegment=284
ts=2025-10-10T18:15:00.423Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=227 maxSegment=284
ts=2025-10-10T18:15:00.423Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=228 maxSegment=284
ts=2025-10-10T18:15:00.424Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=229 maxSegment=284
ts=2025-10-10T18:15:00.424Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=230 maxSegment=284
ts=2025-10-10T18:15:00.424Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=231 maxSegment=284
ts=2025-10-10T18:15:00.425Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=232 maxSegment=284
ts=2025-10-10T18:15:00.426Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=233 maxSegment=284
ts=2025-10-10T18:15:00.427Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=234 maxSegment=284
ts=2025-10-10T18:15:00.428Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=235 maxSegment=284
ts=2025-10-10T18:15:00.428Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=236 maxSegment=284
ts=2025-10-10T18:15:00.429Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=237 maxSegment=284
ts=2025-10-10T18:15:00.430Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=238 maxSegment=284
ts=2025-10-10T18:15:00.431Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=239 maxSegment=284
ts=2025-10-10T18:15:00.432Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=240 maxSegment=284
ts=2025-10-10T18:15:00.432Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=241 maxSegment=284
ts=2025-10-10T18:15:00.433Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=242 maxSegment=284
ts=2025-10-10T18:15:00.467Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=243 maxSegment=284
ts=2025-10-10T18:15:00.467Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=244 maxSegment=284
ts=2025-10-10T18:15:00.468Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=245 maxSegment=284
ts=2025-10-10T18:15:00.468Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=246 maxSegment=284
ts=2025-10-10T18:15:00.468Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=247 maxSegment=284
ts=2025-10-10T18:15:00.468Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=248 maxSegment=284
ts=2025-10-10T18:15:00.469Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=249 maxSegment=284
ts=2025-10-10T18:15:00.469Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=250 maxSegment=284
ts=2025-10-10T18:15:00.469Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=251 maxSegment=284
ts=2025-10-10T18:15:00.470Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=252 maxSegment=284
ts=2025-10-10T18:15:00.470Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=253 maxSegment=284
ts=2025-10-10T18:15:00.471Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=254 maxSegment=284
ts=2025-10-10T18:15:00.472Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=255 maxSegment=284
ts=2025-10-10T18:15:00.472Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=256 maxSegment=284
ts=2025-10-10T18:15:00.473Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=257 maxSegment=284
ts=2025-10-10T18:15:00.474Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=258 maxSegment=284
ts=2025-10-10T18:15:00.474Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=259 maxSegment=284
ts=2025-10-10T18:15:00.475Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=260 maxSegment=284
ts=2025-10-10T18:15:00.475Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=261 maxSegment=284
ts=2025-10-10T18:15:00.476Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=262 maxSegment=284
ts=2025-10-10T18:15:00.477Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=263 maxSegment=284
ts=2025-10-10T18:15:00.478Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=264 maxSegment=284
ts=2025-10-10T18:15:00.478Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=265 maxSegment=284
ts=2025-10-10T18:15:00.479Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=266 maxSegment=284
ts=2025-10-10T18:15:00.479Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=267 maxSegment=284
ts=2025-10-10T18:15:00.480Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=268 maxSegment=284
ts=2025-10-10T18:15:00.481Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=269 maxSegment=284
ts=2025-10-10T18:15:00.481Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=270 maxSegment=284
ts=2025-10-10T18:15:00.482Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=271 maxSegment=284
ts=2025-10-10T18:15:00.482Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=272 maxSegment=284
ts=2025-10-10T18:15:00.482Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=273 maxSegment=284
ts=2025-10-10T18:15:00.482Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=274 maxSegment=284
ts=2025-10-10T18:15:00.482Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=275 maxSegment=284
ts=2025-10-10T18:15:00.483Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=276 maxSegment=284
ts=2025-10-10T18:15:00.483Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=277 maxSegment=284
ts=2025-10-10T18:15:00.483Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=278 maxSegment=284
ts=2025-10-10T18:15:00.484Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=279 maxSegment=284
ts=2025-10-10T18:15:00.484Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=280 maxSegment=284
ts=2025-10-10T18:15:00.484Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=281 maxSegment=284
ts=2025-10-10T18:15:00.484Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=282 maxSegment=284
ts=2025-10-10T18:15:00.485Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=283 maxSegment=284
ts=2025-10-10T18:15:00.485Z caller=head.go:761 level=info component=tsdb msg="WAL segment loaded" segment=284 maxSegment=284
ts=2025-10-10T18:15:00.485Z caller=head.go:798 level=info component=tsdb msg="WAL replay completed" checkpoint_replay_duration=149.836172ms wal_replay_duration=3.68271636s wbl_replay_duration=149ns total_replay_duration=3.912131684s
ts=2025-10-10T18:15:00.542Z caller=main.go:1045 level=info fs_type=XFS_SUPER_MAGIC
ts=2025-10-10T18:15:00.542Z caller=main.go:1048 level=info msg="TSDB started"
ts=2025-10-10T18:15:00.542Z caller=main.go:1229 level=info msg="Loading configuration file" filename=/etc/prometheus/prometheus.yml
ts=2025-10-10T18:15:00.544Z caller=kubernetes.go:329 level=info component="discovery manager scrape" discovery=kubernetes config=syslog-server msg="Using pod service account via in-cluster config"
ts=2025-10-10T18:15:00.545Z caller=kubernetes.go:329 level=info component="discovery manager scrape" discovery=kubernetes config=kubernetes-nodes msg="Using pod service account via in-cluster config"
ts=2025-10-10T18:15:00.545Z caller=kubernetes.go:329 level=info component="discovery manager scrape" discovery=kubernetes config=kube-state-metrics msg="Using pod service account via in-cluster config"
ts=2025-10-10T18:15:00.545Z caller=kubernetes.go:329 level=info component="discovery manager scrape" discovery=kubernetes config=kubernetes-apiservers msg="Using pod service account via in-cluster config"
ts=2025-10-10T18:15:00.545Z caller=kubernetes.go:329 level=info component="discovery manager scrape" discovery=kubernetes config=chrony-ntp msg="Using pod service account via in-cluster config"
ts=2025-10-10T18:15:00.547Z caller=manager.go:1049 level=error component="rule manager" msg="loading groups failed" err="/etc/prometheus/rules/alerts.yml: yaml: line 95: mapping values are not allowed in this context"
ts=2025-10-10T18:15:00.547Z caller=manager.go:1049 level=error component="rule manager" msg="loading groups failed" err="/etc/prometheus/rules/alerts.yml: yaml: line 95: mapping values are not allowed in this context"
ts=2025-10-10T18:15:00.547Z caller=main.go:1255 level=error msg="Failed to apply configuration" err="error loading rules, previous rule set restored"
ts=2025-10-10T18:15:00.547Z caller=main.go:883 level=info msg="Stopping scrape discovery manager..."
ts=2025-10-10T18:15:00.547Z caller=main.go:897 level=info msg="Stopping notify discovery manager..."
ts=2025-10-10T18:15:00.547Z caller=manager.go:1026 level=info component="rule manager" msg="Stopping rule manager..."
ts=2025-10-10T18:15:00.547Z caller=manager.go:1036 level=info component="rule manager" msg="Rule manager stopped"
ts=2025-10-10T18:15:00.547Z caller=main.go:934 level=info msg="Stopping scrape manager..."
ts=2025-10-10T18:15:00.547Z caller=main.go:879 level=info msg="Scrape discovery manager stopped"
ts=2025-10-10T18:15:00.547Z caller=main.go:1229 level=info msg="Loading configuration file" filename=/etc/prometheus/prometheus.yml
ts=2025-10-10T18:15:00.548Z caller=main.go:893 level=info msg="Notify discovery manager stopped"
ts=2025-10-10T18:15:00.548Z caller=main.go:926 level=info msg="Scrape manager stopped"
ts=2025-10-10T18:15:00.548Z caller=manager.go:1012 level=info component="rule manager" msg="Starting rule manager..."
ts=2025-10-10T18:15:00.550Z caller=kubernetes.go:329 level=info component="discovery manager scrape" discovery=kubernetes config=kubernetes-nodes msg="Using pod service account via in-cluster config"
ts=2025-10-10T18:15:00.550Z caller=kubernetes.go:329 level=info component="discovery manager scrape" discovery=kubernetes config=kubernetes-service-endpoints msg="Using pod service account via in-cluster config"
ts=2025-10-10T18:15:00.551Z caller=kubernetes.go:329 level=info component="discovery manager scrape" discovery=kubernetes config=syslog-server msg="Using pod service account via in-cluster config"
ts=2025-10-10T18:15:00.551Z caller=kubernetes.go:329 level=info component="discovery manager scrape" discovery=kubernetes config=kube-state-metrics msg="Using pod service account via in-cluster config"
ts=2025-10-10T18:15:00.551Z caller=kubernetes.go:329 level=info component="discovery manager scrape" discovery=kubernetes config=chrony-ntp msg="Using pod service account via in-cluster config"
ts=2025-10-10T18:15:00.553Z caller=manager.go:1049 level=error component="rule manager" msg="loading groups failed" err="/etc/prometheus/rules/alerts.yml: yaml: line 95: mapping values are not allowed in this context"
ts=2025-10-10T18:15:00.553Z caller=manager.go:1049 level=error component="rule manager" msg="loading groups failed" err="/etc/prometheus/rules/alerts.yml: yaml: line 95: mapping values are not allowed in this context"
ts=2025-10-10T18:15:00.553Z caller=main.go:1255 level=error msg="Failed to apply configuration" err="error loading rules, previous rule set restored"
ts=2025-10-10T18:15:00.553Z caller=main.go:972 level=error msg="Error reloading config" err="one or more errors occurred while applying the new configuration (--config.file=\"/etc/prometheus/prometheus.yml\")"
ts=2025-10-10T18:15:00.701Z caller=notifier.go:604 level=info component=notifier msg="Stopping notification manager..."
ts=2025-10-10T18:15:00.701Z caller=main.go:1155 level=info msg="Notifier manager stopped"
ts=2025-10-10T18:15:00.701Z caller=main.go:1164 level=error err="error loading config from \"/etc/prometheus/prometheus.yml\": one or more errors occurred while applying the new configuration (--config.file=\"/etc/prometheus/prometheus.yml\")"
root@masternode:/srv/monitoring_data/VMStation# kubectl events -n monitoring prometheus-0
LAST SEEN                TYPE      REASON                    OBJECT                                     MESSAGE
20m                      Normal    Scheduled                 Pod/node-exporter-ntznj                    Successfully assigned monitoring/node-exporter-ntznj to storagenodet3500
20m                      Normal    SuccessfulCreate          DaemonSet/node-exporter                    Created pod: node-exporter-ntznj
20m                      Normal    SuccessfulCreate          DaemonSet/node-exporter                    Created pod: node-exporter-28dmd
20m                      Normal    Scheduled                 Pod/node-exporter-28dmd                    Successfully assigned monitoring/node-exporter-28dmd to masternode
20m                      Normal    Pulled                    Pod/node-exporter-28dmd                    Container image "prom/node-exporter:v1.6.1" already present on machine
20m                      Normal    Created                   Pod/node-exporter-28dmd                    Created container: node-exporter
20m                      Normal    Created                   Pod/node-exporter-ntznj                    Created container: node-exporter
20m                      Normal    Started                   Pod/node-exporter-28dmd                    Started container node-exporter
20m                      Normal    Pulled                    Pod/node-exporter-ntznj                    Container image "prom/node-exporter:v1.6.1" already present on machine
20m                      Normal    Scheduled                 Pod/kube-state-metrics-5f6f5666cc-7tzkl    Successfully assigned monitoring/kube-state-metrics-5f6f5666cc-7tzkl to masternode
20m                      Normal    Started                   Pod/node-exporter-ntznj                    Started container node-exporter
20m                      Normal    ScalingReplicaSet         Deployment/kube-state-metrics              Scaled up replica set kube-state-metrics-5f6f5666cc to 1
20m                      Normal    SuccessfulCreate          ReplicaSet/kube-state-metrics-5f6f5666cc   Created pod: kube-state-metrics-5f6f5666cc-7tzkl
20m                      Normal    Started                   Pod/kube-state-metrics-5f6f5666cc-7tzkl    Started container kube-state-metrics
20m                      Normal    Created                   Pod/kube-state-metrics-5f6f5666cc-7tzkl    Created container: kube-state-metrics
20m                      Normal    Pulled                    Pod/kube-state-metrics-5f6f5666cc-7tzkl    Container image "registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.0" already present on machine
20m                      Warning   FailedScheduling          Pod/loki-0                                 0/2 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.
20m                      Normal    SuccessfulCreate          DaemonSet/promtail                         Created pod: promtail-7gnwk
20m                      Normal    SuccessfulCreate          StatefulSet/loki                           create Claim loki-data-loki-0 Pod loki-0 in StatefulSet loki success
20m                      Normal    Scheduled                 Pod/promtail-7gnwk                         Successfully assigned monitoring/promtail-7gnwk to masternode
20m                      Normal    Pulled                    Pod/promtail-7gnwk                         Container image "grafana/promtail:2.9.2" already present on machine
20m                      Normal    Created                   Pod/promtail-7gnwk                         Created container: promtail
20m                      Normal    Started                   Pod/promtail-7gnwk                         Started container promtail
20m                      Normal    SuccessfulCreate          DaemonSet/promtail                         Created pod: promtail-hlkgt
20m                      Normal    Scheduled                 Pod/promtail-hlkgt                         Successfully assigned monitoring/promtail-hlkgt to storagenodet3500
20m                      Normal    Scheduled                 Pod/loki-0                                 Successfully assigned monitoring/loki-0 to masternode
20m                      Normal    Pulled                    Pod/promtail-hlkgt                         Container image "grafana/promtail:2.9.2" already present on machine
20m                      Normal    Created                   Pod/promtail-hlkgt                         Created container: promtail
20m                      Normal    Started                   Pod/promtail-hlkgt                         Started container promtail
20m                      Normal    Pulled                    Pod/loki-0                                 Container image "grafana/loki:2.9.4" already present on machine
20m                      Normal    SuccessfulCreate          StatefulSet/prometheus                     create Claim prometheus-storage-prometheus-0 Pod prometheus-0 in StatefulSet prometheus success
20m                      Normal    Started                   Pod/loki-0                                 Started container loki
20m                      Warning   FailedScheduling          Pod/prometheus-0                           0/2 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.
20m                      Normal    Pulled                    Pod/blackbox-exporter-5949885fb9-bwz6n     Container image "prom/blackbox-exporter:v0.25.0" already present on machine
20m                      Normal    Started                   Pod/loki-0                                 Started container init-loki-data
20m                      Normal    Created                   Pod/loki-0                                 Created container: init-loki-data
20m                      Normal    Pulled                    Pod/loki-0                                 Container image "busybox:latest" already present on machine
20m                      Normal    Scheduled                 Pod/blackbox-exporter-5949885fb9-bwz6n     Successfully assigned monitoring/blackbox-exporter-5949885fb9-bwz6n to masternode
20m                      Normal    Created                   Pod/blackbox-exporter-5949885fb9-bwz6n     Created container: blackbox-exporter
20m                      Normal    Created                   Pod/loki-0                                 Created container: loki
20m                      Normal    Started                   Pod/blackbox-exporter-5949885fb9-bwz6n     Started container blackbox-exporter
20m                      Normal    SuccessfulCreate          ReplicaSet/blackbox-exporter-5949885fb9    Created pod: blackbox-exporter-5949885fb9-bwz6n
20m                      Normal    ScalingReplicaSet         Deployment/blackbox-exporter               Scaled up replica set blackbox-exporter-5949885fb9 to 1
20m                      Normal    Pulled                    Pod/grafana-5f879c7654-k45n2               Container image "grafana/grafana:10.0.0" already present on machine
20m                      Normal    SuccessfulCreate          ReplicaSet/grafana-5f879c7654              Created pod: grafana-5f879c7654-k45n2
20m                      Normal    Scheduled                 Pod/prometheus-0                           Successfully assigned monitoring/prometheus-0 to masternode
20m                      Normal    Scheduled                 Pod/grafana-5f879c7654-k45n2               Successfully assigned monitoring/grafana-5f879c7654-k45n2 to masternode
20m                      Normal    Created                   Pod/grafana-5f879c7654-k45n2               Created container: grafana
20m                      Normal    ScalingReplicaSet         Deployment/grafana                         Scaled up replica set grafana-5f879c7654 to 1
20m                      Normal    Started                   Pod/grafana-5f879c7654-k45n2               Started container grafana
20m                      Normal    Pulled                    Pod/prometheus-0                           Container image "busybox:latest" already present on machine
20m                      Normal    Created                   Pod/prometheus-0                           Created container: config-reloader
20m                      Normal    Pulled                    Pod/prometheus-0                           Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
20m                      Normal    Created                   Pod/prometheus-0                           Created container: init-chown-data
20m                      Normal    Started                   Pod/prometheus-0                           Started container init-chown-data
20m                      Normal    Started                   Pod/prometheus-0                           Started container config-reloader
19m (x2 over 20m)        Warning   Unhealthy                 Pod/loki-0                                 Startup probe failed: HTTP probe failed with statuscode: 503
19m (x4 over 20m)        Normal    Pulled                    Pod/prometheus-0                           Container image "prom/prometheus:v2.48.0" already present on machine
19m (x4 over 20m)        Normal    Started                   Pod/prometheus-0                           Started container prometheus
19m (x4 over 20m)        Normal    Created                   Pod/prometheus-0                           Created container: prometheus
19m (x2 over 19m)        Warning   Unhealthy                 Pod/prometheus-0                           Startup probe failed: HTTP probe failed with statuscode: 503
9m58s (x51 over 19m)     Warning   BackOff                   Pod/prometheus-0                           Back-off restarting failed container prometheus in pod prometheus-0_monitoring(70fb75c9-2036-4610-b935-9412e73fb6ae)
9m39s (x2 over 20m)      Normal    SuccessfulCreate          StatefulSet/prometheus                     create Pod prometheus-0 in StatefulSet prometheus successful
9m39s (x8 over 9m40s)    Normal    SuccessfulDelete          StatefulSet/prometheus                     delete Pod prometheus-0 in StatefulSet prometheus successful
9m39s (x9 over 9m40s)    Warning   RecreatingFailedPod       StatefulSet/prometheus                     StatefulSet monitoring/prometheus is recreating failed Pod prometheus-0
9m39s                    Warning   FailedDelete              StatefulSet/prometheus                     delete Pod prometheus-0 in StatefulSet prometheus failed error: pods "prometheus-0" not found
9m39s                    Normal    Scheduled                 Pod/prometheus-0                           Successfully assigned monitoring/prometheus-0 to masternode
9m38s                    Normal    Created                   Pod/prometheus-0                           Created container: config-reloader
9m38s                    Normal    Pulled                    Pod/prometheus-0                           Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
9m38s                    Normal    Pulled                    Pod/prometheus-0                           Container image "busybox:latest" already present on machine
9m38s                    Normal    Created                   Pod/prometheus-0                           Created container: init-chown-data
9m38s                    Normal    Started                   Pod/prometheus-0                           Started container init-chown-data
9m38s                    Normal    Started                   Pod/prometheus-0                           Started container config-reloader
9m33s                    Normal    Killing                   Pod/loki-0                                 Stopping container loki
9m31s (x7 over 9m31s)    Normal    RecreatingTerminatedPod   StatefulSet/loki                           StatefulSet monitoring/loki is recreating terminated Pod loki-0
9m31s (x6 over 9m31s)    Normal    SuccessfulDelete          StatefulSet/loki                           delete Pod loki-0 in StatefulSet loki successful
9m31s                    Warning   FailedDelete              StatefulSet/loki                           delete Pod loki-0 in StatefulSet loki failed error: pods "loki-0" not found
9m30s                    Normal    Started                   Pod/loki-0                                 Started container init-loki-data
9m30s (x2 over 20m)      Normal    SuccessfulCreate          StatefulSet/loki                           create Pod loki-0 in StatefulSet loki successful
9m30s                    Normal    Created                   Pod/loki-0                                 Created container: init-loki-data
9m30s                    Normal    Pulled                    Pod/loki-0                                 Container image "busybox:latest" already present on machine
9m30s                    Normal    Scheduled                 Pod/loki-0                                 Successfully assigned monitoring/loki-0 to masternode
9m29s                    Normal    Pulled                    Pod/loki-0                                 Container image "grafana/loki:2.9.4" already present on machine
9m29s                    Normal    Created                   Pod/loki-0                                 Created container: loki
9m29s                    Normal    Started                   Pod/loki-0                                 Started container loki
9m10s (x2 over 9m20s)    Warning   Unhealthy                 Pod/loki-0                                 Startup probe failed: HTTP probe failed with statuscode: 503
8m40s (x4 over 9m38s)    Normal    Created                   Pod/prometheus-0                           Created container: prometheus
8m40s (x4 over 9m38s)    Normal    Started                   Pod/prometheus-0                           Started container prometheus
8m40s (x4 over 9m38s)    Normal    Pulled                    Pod/prometheus-0                           Container image "prom/prometheus:v2.48.0" already present on machine
8m39s                    Warning   Unhealthy                 Pod/prometheus-0                           Startup probe failed: HTTP probe failed with statuscode: 503
7m15s                    Normal    Scheduled                 Pod/dns-test                               Successfully assigned monitoring/dns-test to storagenodet3500
7m14s                    Normal    Pulling                   Pod/dns-test                               Pulling image "busybox:latest"
7m13s                    Normal    Started                   Pod/dns-test                               Started container dns-test
7m13s                    Normal    Created                   Pod/dns-test                               Created container: dns-test
7m13s                    Normal    Pulled                    Pod/dns-test                               Successfully pulled image "busybox:latest" in 1.003s (1.003s including waiting)
7m9s                     Normal    Killing                   Pod/dns-test                               Stopping container dns-test
4m37s (x26 over 9m25s)   Warning   BackOff                   Pod/prometheus-0                           Back-off restarting failed container prometheus in pod prometheus-0_monitoring(1373e632-0536-4e00-8284-1a17ecd9ab55)


***1) Dashboard copy and paste
Error on all dashboard entries 
"Status: 500. Message: Get "http://prometheus.monitoring.svc.cluster.local:9090/api/v1/query_range?end=1760121810&query=sum%28kube_pod_info%29&start=1760118210&step=15": dial tcp: lookup prometheus.monitoring.svc.cluster.local: no such host"
in the current pre configured dashboards this is what they look like

1. IPMI Hardware Monitoring - RHEL 10 Enterprise Server


Last 6 hours



1m


Server Temperature Sensors
No data

Fan Speeds
No data

Power Consumption
No data

Voltage Sensors
No data

Current Temperature Status
No data

BMC Status
No data

Current Power Draw
No data

Sensor Status Table

No data




2. Dashboards
Loki Logs & Aggregation


Last 1 hour



30s

Log Volume by Namespace

Mean	Max
{}
11.3 logs/sec	177 logs/sec
Application Logs
2025-10-10 14:32:24.857	
system-logs
[2025-10-10 14:32:24] Auto-sleep check completed
2025-10-10 14:32:24.857	
system-logs
[2025-10-10 14:32:24] Activity detected - updated last activity timestamp
2025-10-10 14:32:24.857	
system-logs
[2025-10-10 14:32:24] Active non-system pods: 4
2025-10-10 14:32:24.606	
system-logs
[2025-10-10 14:32:24] Checking cluster activity...
2025-10-10 14:27:28.775	
system-logs
[2025-10-10 14:27:28] INFO: kubeconfig not found at /etc/kubernetes/admin.conf - skipping (not a control plane node)
2025-10-10 14:17:24.831	
system-logs
[2025-10-10 14:17:24] Auto-sleep check completed
2025-10-10 14:17:24.831	
system-logs
[2025-10-10 14:17:24] Activity detected - updated last activity timestamp
2025-10-10 14:17:24.831	
system-logs
[2025-10-10 14:17:24] Active non-system pods: 4
2025-10-10 14:17:24.580	
system-logs
[2025-10-10 14:17:24] Checking cluster activity...
2025-10-10 14:17:01.282	
system-logs
2025-10-10T14:17:01.216589-04:00 masternode CRON[1370593]: (root) CMD (cd / && run-parts --report /etc/cron.hourly)
2025-10-10 14:17:01.263	
system-logs
2025-10-10T14:17:01.218533-04:00 masternode CRON[1370592]: pam_unix(cron:session): session closed for user root
2025-10-10 14:17:01.263	
system-logs
2025-10-10T14:17:01.216234-04:00 masternode CRON[1370592]: pam_unix(cron:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:17:01.244	
system-logs
2025-10-10T14:17:01.058993-04:00 storagenodet3500 CRON[2325642]: (root) CMD (cd / && run-parts --report /etc/cron.hourly)
2025-10-10 14:17:01.244	
system-logs
2025-10-10T14:17:01.062088-04:00 storagenodet3500 CRON[2325641]: pam_unix(cron:session): session closed for user root
2025-10-10 14:17:01.244	
system-logs
2025-10-10T14:17:01.058400-04:00 storagenodet3500 CRON[2325641]: pam_unix(cron:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:16:31.179	
system-logs
2025-10-10T14:16:31.174801-04:00 masternode kernel: [331559.878015] brcmsmac bcma0:1: brcms_ops_config: change power-save mode: false (implement)
2025-10-10 14:16:31.179	
system-logs
2025-10-10T14:16:31.174771-04:00 masternode kernel: [331559.877985] brcmsmac bcma0:1: brcms_ops_bss_info_changed: qos enabled: false (implement)
2025-10-10 14:12:25.434	
system-logs
[2025-10-10 14:12:25] INFO: kubeconfig not found at /etc/kubernetes/admin.conf - skipping (not a control plane node)
2025-10-10 14:11:40.394	
system-logs
2025-10-10T14:11:40.268881-04:00 storagenodet3500 kernel: [768481.888209] cni0: port 3(vethe8c83dc7) entered disabled state
2025-10-10 14:11:40.394	
system-logs
2025-10-10T14:11:40.268877-04:00 storagenodet3500 kernel: [768481.888204] device vethe8c83dc7 left promiscuous mode
2025-10-10 14:11:40.394	
system-logs
2025-10-10T14:11:40.268855-04:00 storagenodet3500 kernel: [768481.887613] cni0: port 3(vethe8c83dc7) entered disabled state
2025-10-10 14:11:04.295	
system-logs
2025-10-10T14:11:04.156909-04:00 storagenodet3500 kernel: [768445.774325] cni0: port 3(vethe8c83dc7) entered forwarding state
2025-10-10 14:11:04.295	
system-logs
2025-10-10T14:11:04.156907-04:00 storagenodet3500 kernel: [768445.774322] cni0: port 3(vethe8c83dc7) entered blocking state
2025-10-10 14:11:04.295	
system-logs
2025-10-10T14:11:04.156905-04:00 storagenodet3500 kernel: [768445.774284] IPv6: ADDRCONF(NETDEV_CHANGE): vethe8c83dc7: link becomes ready
2025-10-10 14:11:04.295	
system-logs
2025-10-10T14:11:04.156892-04:00 storagenodet3500 kernel: [768445.774220] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 14:11:04.295	
system-logs
2025-10-10T14:11:04.148872-04:00 storagenodet3500 kernel: [768445.769531] device vethe8c83dc7 entered promiscuous mode
2025-10-10 14:11:04.295	
system-logs
2025-10-10T14:11:04.148869-04:00 storagenodet3500 kernel: [768445.769440] cni0: port 3(vethe8c83dc7) entered disabled state
2025-10-10 14:11:04.295	
system-logs
2025-10-10T14:11:04.148851-04:00 storagenodet3500 kernel: [768445.769431] cni0: port 3(vethe8c83dc7) entered blocking state
2025-10-10 14:11:01.789	
system-logs
2025-10-10T14:11:01.592862-04:00 storagenodet3500 kernel: [768443.212485] cni0: port 3(vethee629b76) entered disabled state
2025-10-10 14:11:01.789	
system-logs
2025-10-10T14:11:01.592861-04:00 storagenodet3500 kernel: [768443.212481] device vethee629b76 left promiscuous mode
2025-10-10 14:11:01.789	
system-logs
2025-10-10T14:11:01.592845-04:00 storagenodet3500 kernel: [768443.211665] cni0: port 3(vethee629b76) entered disabled state
2025-10-10 14:11:01.726	
system-logs
2025-10-10T14:11:01.686867-04:00 masternode systemd-logind[611]: Removed session 526.
2025-10-10 14:11:01.726	
system-logs
2025-10-10T14:11:01.685894-04:00 masternode systemd-logind[611]: Session 526 logged out. Waiting for processes to exit.
2025-10-10 14:11:01.726	
system-logs
2025-10-10T14:11:01.682425-04:00 masternode sshd[1367985]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:11:01.726	
system-logs
2025-10-10T14:11:01.682329-04:00 masternode sshd[1367985]: Disconnected from user root 192.168.4.63 port 35624
2025-10-10 14:11:01.726	
system-logs
2025-10-10T14:11:01.682131-04:00 masternode sshd[1367985]: Received disconnect from 192.168.4.63 port 35624:11: disconnected by user
2025-10-10 14:10:57.029	
system-logs
2025-10-10T14:10:56.820873-04:00 storagenodet3500 kernel: [768438.441129] cni0: port 3(vethee629b76) entered forwarding state
2025-10-10 14:10:57.029	
system-logs
2025-10-10T14:10:56.820871-04:00 storagenodet3500 kernel: [768438.441123] cni0: port 3(vethee629b76) entered blocking state
2025-10-10 14:10:57.029	
system-logs
2025-10-10T14:10:56.820869-04:00 storagenodet3500 kernel: [768438.441067] IPv6: ADDRCONF(NETDEV_CHANGE): vethee629b76: link becomes ready
2025-10-10 14:10:57.029	
system-logs
2025-10-10T14:10:56.820852-04:00 storagenodet3500 kernel: [768438.440995] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 14:10:57.029	
system-logs
2025-10-10T14:10:56.816893-04:00 storagenodet3500 kernel: [768438.435493] device vethee629b76 entered promiscuous mode
2025-10-10 14:10:57.029	
system-logs
2025-10-10T14:10:56.816889-04:00 storagenodet3500 kernel: [768438.435428] cni0: port 3(vethee629b76) entered disabled state
2025-10-10 14:10:57.029	
system-logs
2025-10-10T14:10:56.816858-04:00 storagenodet3500 kernel: [768438.435421] cni0: port 3(vethee629b76) entered blocking state
2025-10-10 14:10:56.212	
system-logs
2025-10-10T14:10:56.068170-04:00 masternode sshd[1367985]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:56.212	
system-logs
2025-10-10T14:10:56.037807-04:00 masternode systemd-logind[611]: New session 526 of user root.
2025-10-10 14:10:56.212	
system-logs
2025-10-10T14:10:56.033566-04:00 masternode sshd[1367985]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:56.212	
system-logs
2025-10-10T14:10:56.032232-04:00 masternode sshd[1367985]: Accepted publickey for root from 192.168.4.63 port 35624 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:55.961	
system-logs
2025-10-10T14:10:55.746445-04:00 masternode systemd-logind[611]: Removed session 525.
2025-10-10 14:10:55.961	
system-logs
2025-10-10T14:10:55.745311-04:00 masternode systemd-logind[611]: Session 525 logged out. Waiting for processes to exit.
2025-10-10 14:10:55.961	
system-logs
2025-10-10T14:10:55.741607-04:00 masternode sshd[1367964]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:55.961	
system-logs
2025-10-10T14:10:55.741497-04:00 masternode sshd[1367964]: Disconnected from user root 192.168.4.63 port 35616
2025-10-10 14:10:55.961	
system-logs
2025-10-10T14:10:55.741331-04:00 masternode sshd[1367964]: Received disconnect from 192.168.4.63 port 35616:11: disconnected by user
2025-10-10 14:10:55.711	
system-logs
2025-10-10T14:10:55.624229-04:00 masternode sshd[1367964]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:55.711	
system-logs
2025-10-10T14:10:55.593121-04:00 masternode systemd-logind[611]: New session 525 of user root.
2025-10-10 14:10:55.711	
system-logs
2025-10-10T14:10:55.589718-04:00 masternode sshd[1367964]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:55.710	
system-logs
2025-10-10T14:10:55.588509-04:00 masternode sshd[1367964]: Accepted publickey for root from 192.168.4.63 port 35616 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:55.460	
system-logs
2025-10-10T14:10:55.293103-04:00 masternode systemd-logind[611]: Removed session 524.
2025-10-10 14:10:55.460	
system-logs
2025-10-10T14:10:55.291906-04:00 masternode systemd-logind[611]: Session 524 logged out. Waiting for processes to exit.
2025-10-10 14:10:55.460	
system-logs
2025-10-10T14:10:55.288263-04:00 masternode sshd[1367926]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:55.459	
system-logs
2025-10-10T14:10:55.288092-04:00 masternode sshd[1367926]: Disconnected from user root 192.168.4.63 port 35604
2025-10-10 14:10:55.459	
system-logs
2025-10-10T14:10:55.287901-04:00 masternode sshd[1367926]: Received disconnect from 192.168.4.63 port 35604:11: disconnected by user
2025-10-10 14:10:55.209	
system-logs
2025-10-10T14:10:55.174577-04:00 masternode sshd[1367926]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:55.209	
system-logs
2025-10-10T14:10:55.140992-04:00 masternode systemd-logind[611]: New session 524 of user root.
2025-10-10 14:10:55.209	
system-logs
2025-10-10T14:10:55.137836-04:00 masternode sshd[1367926]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:55.209	
system-logs
2025-10-10T14:10:55.136371-04:00 masternode sshd[1367926]: Accepted publickey for root from 192.168.4.63 port 35604 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:54.959	
system-logs
2025-10-10T14:10:54.857107-04:00 masternode systemd-logind[611]: Removed session 523.
2025-10-10 14:10:54.959	
system-logs
2025-10-10T14:10:54.856061-04:00 masternode systemd-logind[611]: Session 523 logged out. Waiting for processes to exit.
2025-10-10 14:10:54.959	
system-logs
2025-10-10T14:10:54.852482-04:00 masternode sshd[1367899]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:54.959	
system-logs
2025-10-10T14:10:54.852348-04:00 masternode sshd[1367899]: Disconnected from user root 192.168.4.63 port 35592
2025-10-10 14:10:54.959	
system-logs
2025-10-10T14:10:54.852157-04:00 masternode sshd[1367899]: Received disconnect from 192.168.4.63 port 35592:11: disconnected by user
2025-10-10 14:10:54.959	
system-logs
2025-10-10T14:10:54.745709-04:00 masternode sshd[1367899]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:54.959	
system-logs
2025-10-10T14:10:54.735769-04:00 masternode systemd-logind[611]: New session 523 of user root.
2025-10-10 14:10:54.959	
system-logs
2025-10-10T14:10:54.709719-04:00 masternode sshd[1367899]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:54.708	
system-logs
2025-10-10T14:10:54.708196-04:00 masternode sshd[1367899]: Accepted publickey for root from 192.168.4.63 port 35592 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:54.458	
system-logs
2025-10-10T14:10:54.427719-04:00 masternode systemd-logind[611]: Removed session 522.
2025-10-10 14:10:54.458	
system-logs
2025-10-10T14:10:54.426880-04:00 masternode systemd-logind[611]: Session 522 logged out. Waiting for processes to exit.
2025-10-10 14:10:54.458	
system-logs
2025-10-10T14:10:54.423026-04:00 masternode sshd[1367878]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:54.458	
system-logs
2025-10-10T14:10:54.422956-04:00 masternode sshd[1367878]: Disconnected from user root 192.168.4.63 port 35588
2025-10-10 14:10:54.458	
system-logs
2025-10-10T14:10:54.422796-04:00 masternode sshd[1367878]: Received disconnect from 192.168.4.63 port 35588:11: disconnected by user
2025-10-10 14:10:54.458	
system-logs
2025-10-10T14:10:54.303721-04:00 masternode sshd[1367878]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:54.458	
system-logs
2025-10-10T14:10:54.277339-04:00 masternode systemd-logind[611]: New session 522 of user root.
2025-10-10 14:10:54.458	
system-logs
2025-10-10T14:10:54.273691-04:00 masternode sshd[1367878]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:54.458	
system-logs
2025-10-10T14:10:54.272194-04:00 masternode sshd[1367878]: Accepted publickey for root from 192.168.4.63 port 35588 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:53.455	
system-logs
2025-10-10T14:10:53.382071-04:00 masternode systemd-logind[611]: Removed session 521.
2025-10-10 14:10:53.455	
system-logs
2025-10-10T14:10:53.381134-04:00 masternode systemd-logind[611]: Session 521 logged out. Waiting for processes to exit.
2025-10-10 14:10:53.455	
system-logs
2025-10-10T14:10:53.377129-04:00 masternode sshd[1367844]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:53.455	
system-logs
2025-10-10T14:10:53.377006-04:00 masternode sshd[1367844]: Disconnected from user root 192.168.4.63 port 35584
2025-10-10 14:10:53.455	
system-logs
2025-10-10T14:10:53.376763-04:00 masternode sshd[1367844]: Received disconnect from 192.168.4.63 port 35584:11: disconnected by user
2025-10-10 14:10:53.455	
system-logs
2025-10-10T14:10:53.323939-04:00 masternode sshd[1367844]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:53.455	
system-logs
2025-10-10T14:10:53.293541-04:00 masternode systemd-logind[611]: New session 521 of user root.
2025-10-10 14:10:53.455	
system-logs
2025-10-10T14:10:53.290151-04:00 masternode sshd[1367844]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:53.455	
system-logs
2025-10-10T14:10:53.288694-04:00 masternode sshd[1367844]: Accepted publickey for root from 192.168.4.63 port 35584 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:53.205	
system-logs
2025-10-10T14:10:53.015362-04:00 masternode systemd-logind[611]: Removed session 520.
2025-10-10 14:10:53.205	
system-logs
2025-10-10T14:10:53.014314-04:00 masternode systemd-logind[611]: Session 520 logged out. Waiting for processes to exit.
2025-10-10 14:10:53.205	
system-logs
2025-10-10T14:10:53.010934-04:00 masternode sshd[1367835]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:53.205	
system-logs
2025-10-10T14:10:53.010770-04:00 masternode sshd[1367835]: Disconnected from user root 192.168.4.63 port 35576
2025-10-10 14:10:53.205	
system-logs
2025-10-10T14:10:53.010561-04:00 masternode sshd[1367835]: Received disconnect from 192.168.4.63 port 35576:11: disconnected by user
2025-10-10 14:10:53.205	
system-logs
2025-10-10T14:10:52.959888-04:00 masternode sshd[1367835]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:52.954	
system-logs
2025-10-10T14:10:52.933383-04:00 masternode systemd-logind[611]: New session 520 of user root.
2025-10-10 14:10:52.954	
system-logs
2025-10-10T14:10:52.929414-04:00 masternode sshd[1367835]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:52.954	
system-logs
2025-10-10T14:10:52.928046-04:00 masternode sshd[1367835]: Accepted publickey for root from 192.168.4.63 port 35576 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:52.703	
system-logs
2025-10-10T14:10:52.651853-04:00 masternode systemd-logind[611]: Removed session 519.
2025-10-10 14:10:52.703	
system-logs
2025-10-10T14:10:52.650916-04:00 masternode systemd-logind[611]: Session 519 logged out. Waiting for processes to exit.
2025-10-10 14:10:52.703	
system-logs
2025-10-10T14:10:52.646911-04:00 masternode sshd[1367827]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:52.703	
system-logs
2025-10-10T14:10:52.646776-04:00 masternode sshd[1367827]: Disconnected from user root 192.168.4.63 port 35570
2025-10-10 14:10:52.703	
system-logs
2025-10-10T14:10:52.646540-04:00 masternode sshd[1367827]: Received disconnect from 192.168.4.63 port 35570:11: disconnected by user
2025-10-10 14:10:52.703	
system-logs
2025-10-10T14:10:52.599198-04:00 masternode sshd[1367827]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:52.703	
system-logs
2025-10-10T14:10:52.568963-04:00 masternode systemd-logind[611]: New session 519 of user root.
2025-10-10 14:10:52.703	
system-logs
2025-10-10T14:10:52.565525-04:00 masternode sshd[1367827]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:52.703	
system-logs
2025-10-10T14:10:52.564108-04:00 masternode sshd[1367827]: Accepted publickey for root from 192.168.4.63 port 35570 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:52.452	
system-logs
2025-10-10T14:10:52.281885-04:00 masternode systemd-logind[611]: Removed session 518.
2025-10-10 14:10:52.452	
system-logs
2025-10-10T14:10:52.281065-04:00 masternode systemd-logind[611]: Session 518 logged out. Waiting for processes to exit.
2025-10-10 14:10:52.452	
system-logs
2025-10-10T14:10:52.276745-04:00 masternode sshd[1367810]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:52.452	
system-logs
2025-10-10T14:10:52.276657-04:00 masternode sshd[1367810]: Disconnected from user root 192.168.4.63 port 35558
2025-10-10 14:10:52.452	
system-logs
2025-10-10T14:10:52.276429-04:00 masternode sshd[1367810]: Received disconnect from 192.168.4.63 port 35558:11: disconnected by user
2025-10-10 14:10:52.202	
system-logs
2025-10-10T14:10:52.163488-04:00 masternode sshd[1367810]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:52.202	
system-logs
2025-10-10T14:10:52.124718-04:00 masternode systemd-logind[611]: New session 518 of user root.
2025-10-10 14:10:52.202	
system-logs
2025-10-10T14:10:52.121194-04:00 masternode sshd[1367810]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:52.202	
system-logs
2025-10-10T14:10:52.119692-04:00 masternode sshd[1367810]: Accepted publickey for root from 192.168.4.63 port 35558 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:51.951	
system-logs
2025-10-10T14:10:51.837846-04:00 masternode systemd-logind[611]: Removed session 517.
2025-10-10 14:10:51.951	
system-logs
2025-10-10T14:10:51.836819-04:00 masternode systemd-logind[611]: Session 517 logged out. Waiting for processes to exit.
2025-10-10 14:10:51.951	
system-logs
2025-10-10T14:10:51.833425-04:00 masternode sshd[1367793]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:51.951	
system-logs
2025-10-10T14:10:51.833182-04:00 masternode sshd[1367793]: Disconnected from user root 192.168.4.63 port 35544
2025-10-10 14:10:51.951	
system-logs
2025-10-10T14:10:51.832963-04:00 masternode sshd[1367793]: Received disconnect from 192.168.4.63 port 35544:11: disconnected by user
2025-10-10 14:10:51.951	
system-logs
2025-10-10T14:10:51.720319-04:00 masternode sshd[1367793]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:51.700	
system-logs
2025-10-10T14:10:51.681822-04:00 masternode systemd-logind[611]: New session 517 of user root.
2025-10-10 14:10:51.700	
system-logs
2025-10-10T14:10:51.677814-04:00 masternode sshd[1367793]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:51.700	
system-logs
2025-10-10T14:10:51.676279-04:00 masternode sshd[1367793]: Accepted publickey for root from 192.168.4.63 port 35544 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:51.450	
system-logs
2025-10-10T14:10:51.389092-04:00 masternode systemd-logind[611]: Removed session 516.
2025-10-10 14:10:51.450	
system-logs
2025-10-10T14:10:51.387901-04:00 masternode systemd-logind[611]: Session 516 logged out. Waiting for processes to exit.
2025-10-10 14:10:51.450	
system-logs
2025-10-10T14:10:51.383898-04:00 masternode sshd[1367778]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:51.450	
system-logs
2025-10-10T14:10:51.383733-04:00 masternode sshd[1367778]: Disconnected from user root 192.168.4.63 port 35532
2025-10-10 14:10:51.450	
system-logs
2025-10-10T14:10:51.383554-04:00 masternode sshd[1367778]: Received disconnect from 192.168.4.63 port 35532:11: disconnected by user
2025-10-10 14:10:51.450	
system-logs
2025-10-10T14:10:51.271336-04:00 masternode sshd[1367778]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:51.450	
system-logs
2025-10-10T14:10:51.241249-04:00 masternode systemd-logind[611]: New session 516 of user root.
2025-10-10 14:10:51.450	
system-logs
2025-10-10T14:10:51.237365-04:00 masternode sshd[1367778]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:51.450	
system-logs
2025-10-10T14:10:51.235885-04:00 masternode sshd[1367778]: Accepted publickey for root from 192.168.4.63 port 35532 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:51.199	
system-logs
2025-10-10T14:10:50.950267-04:00 masternode systemd-logind[611]: Removed session 515.
2025-10-10 14:10:50.949	
system-logs
2025-10-10T14:10:50.948995-04:00 masternode systemd-logind[611]: Session 515 logged out. Waiting for processes to exit.
2025-10-10 14:10:50.949	
system-logs
2025-10-10T14:10:50.945061-04:00 masternode sshd[1367760]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:50.949	
system-logs
2025-10-10T14:10:50.944933-04:00 masternode sshd[1367760]: Disconnected from user root 192.168.4.63 port 35518
2025-10-10 14:10:50.949	
system-logs
2025-10-10T14:10:50.944685-04:00 masternode sshd[1367760]: Received disconnect from 192.168.4.63 port 35518:11: disconnected by user
2025-10-10 14:10:50.949	
system-logs
2025-10-10T14:10:50.835646-04:00 masternode sshd[1367760]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:50.949	
system-logs
2025-10-10T14:10:50.801651-04:00 masternode systemd-logind[611]: New session 515 of user root.
2025-10-10 14:10:50.949	
system-logs
2025-10-10T14:10:50.797876-04:00 masternode sshd[1367760]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:50.949	
system-logs
2025-10-10T14:10:50.796341-04:00 masternode sshd[1367760]: Accepted publickey for root from 192.168.4.63 port 35518 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:50.698	
system-logs
2025-10-10T14:10:50.517298-04:00 masternode systemd-logind[611]: Removed session 514.
2025-10-10 14:10:50.698	
system-logs
2025-10-10T14:10:50.516334-04:00 masternode systemd-logind[611]: Session 514 logged out. Waiting for processes to exit.
2025-10-10 14:10:50.698	
system-logs
2025-10-10T14:10:50.512403-04:00 masternode sshd[1367690]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:50.698	
system-logs
2025-10-10T14:10:50.512229-04:00 masternode sshd[1367690]: Disconnected from user root 192.168.4.63 port 35514
2025-10-10 14:10:50.698	
system-logs
2025-10-10T14:10:50.512041-04:00 masternode sshd[1367690]: Received disconnect from 192.168.4.63 port 35514:11: disconnected by user
2025-10-10 14:10:50.448	
system-logs
2025-10-10T14:10:50.402583-04:00 masternode sshd[1367690]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:50.448	
system-logs
2025-10-10T14:10:50.390125-04:00 masternode systemd-logind[611]: New session 514 of user root.
2025-10-10 14:10:50.448	
system-logs
2025-10-10T14:10:50.386221-04:00 masternode sshd[1367690]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:50.448	
system-logs
2025-10-10T14:10:50.384370-04:00 masternode sshd[1367690]: Accepted publickey for root from 192.168.4.63 port 35514 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:50.197	
system-logs
2025-10-10T14:10:50.108157-04:00 masternode systemd-logind[611]: Removed session 513.
2025-10-10 14:10:50.197	
system-logs
2025-10-10T14:10:50.107104-04:00 masternode systemd-logind[611]: Session 513 logged out. Waiting for processes to exit.
2025-10-10 14:10:50.197	
system-logs
2025-10-10T14:10:50.103154-04:00 masternode sshd[1367682]: pam_unix(sshd:session): session closed for user root
2025-10-10 14:10:50.197	
system-logs
2025-10-10T14:10:50.103065-04:00 masternode sshd[1367682]: Disconnected from user root 192.168.4.63 port 35506
2025-10-10 14:10:50.197	
system-logs
2025-10-10T14:10:50.102800-04:00 masternode sshd[1367682]: Received disconnect from 192.168.4.63 port 35506:11: disconnected by user
2025-10-10 14:10:50.197	
system-logs
2025-10-10T14:10:50.055801-04:00 masternode sshd[1367682]: pam_env(sshd:session): deprecated reading of user environment enabled
2025-10-10 14:10:50.197	
system-logs
2025-10-10T14:10:50.025173-04:00 masternode systemd-logind[611]: New session 513 of user root.
2025-10-10 14:10:50.197	
system-logs
2025-10-10T14:10:50.021126-04:00 masternode sshd[1367682]: pam_unix(sshd:session): session opened for user root(uid=0) by (uid=0)
2025-10-10 14:10:50.197	
system-logs
2025-10-10T14:10:50.019785-04:00 masternode sshd[1367682]: Accepted publickey for root from 192.168.4.63 port 35506 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:49.761	
system-logs
2025-10-10T14:10:49.740578-04:00 storagenodet3500 sshd[2323436]: Disconnected from user root 192.168.4.63 port 54228
2025-10-10 14:10:49.761	
system-logs
2025-10-10T14:10:49.740444-04:00 storagenodet3500 sshd[2323436]: Received disconnect from 192.168.4.63 port 54228:11: disconnected by user
2025-10-10 14:10:49.761	
system-logs
2025-10-10T14:10:49.685219-04:00 storagenodet3500 sshd[2323436]: Accepted publickey for root from 192.168.4.63 port 54228 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:49.510	
system-logs
2025-10-10T14:10:49.400259-04:00 storagenodet3500 sshd[2323433]: Disconnected from user root 192.168.4.63 port 54222
2025-10-10 14:10:49.510	
system-logs
2025-10-10T14:10:49.400109-04:00 storagenodet3500 sshd[2323433]: Received disconnect from 192.168.4.63 port 54222:11: disconnected by user
2025-10-10 14:10:49.510	
system-logs
2025-10-10T14:10:49.346700-04:00 storagenodet3500 sshd[2323433]: Accepted publickey for root from 192.168.4.63 port 54222 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:48.258	
system-logs
2025-10-10T14:10:48.162778-04:00 storagenodet3500 sshd[2323427]: Disconnected from user root 192.168.4.63 port 47898
2025-10-10 14:10:48.258	
system-logs
2025-10-10T14:10:48.162635-04:00 storagenodet3500 sshd[2323427]: Received disconnect from 192.168.4.63 port 47898:11: disconnected by user
2025-10-10 14:10:48.258	
system-logs
2025-10-10T14:10:48.096626-04:00 storagenodet3500 sshd[2323427]: Accepted publickey for root from 192.168.4.63 port 47898 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:10:48.007	
system-logs
2025-10-10T14:10:47.812663-04:00 storagenodet3500 sshd[2323423]: Disconnected from user root 192.168.4.63 port 47882
2025-10-10 14:10:48.007	
system-logs
2025-10-10T14:10:47.812513-04:00 storagenodet3500 sshd[2323423]: Received disconnect from 192.168.4.63 port 47882:11: disconnected by user
2025-10-10 14:10:47.756	
system-logs
2025-10-10T14:10:47.744257-04:00 storagenodet3500 sshd[2323423]: Accepted publickey for root from 192.168.4.63 port 47882 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:09:35.226	
system-logs
2025-10-10T14:09:35.150822-04:00 masternode kernel: [331143.848525] brcmsmac bcma0:1: brcms_ops_config: change power-save mode: false (implement)
2025-10-10 14:09:35.226	
system-logs
2025-10-10T14:09:35.150784-04:00 masternode kernel: [331143.848495] brcmsmac bcma0:1: brcms_ops_bss_info_changed: qos enabled: false (implement)
2025-10-10 14:09:14.272	
system-logs
2025-10-10T14:09:14.125268-04:00 storagenodet3500 sshd[2321186]: Disconnected from user root 192.168.4.63 port 37580
2025-10-10 14:09:14.272	
system-logs
2025-10-10T14:09:14.125075-04:00 storagenodet3500 sshd[2321186]: Received disconnect from 192.168.4.63 port 37580:11: disconnected by user
2025-10-10 14:08:48.574	
system-logs
2025-10-10T14:08:48.446864-04:00 masternode kernel: [331097.143488] IPv6: ADDRCONF(NETDEV_CHANGE): vethc29965bf: link becomes ready
2025-10-10 14:08:48.574	
system-logs
2025-10-10T14:08:48.442736-04:00 masternode kernel: [331097.137993] cni0: port 5(vethc29965bf) entered forwarding state
2025-10-10 14:08:48.574	
system-logs
2025-10-10T14:08:48.442734-04:00 masternode kernel: [331097.137988] cni0: port 5(vethc29965bf) entered blocking state
2025-10-10 14:08:48.574	
system-logs
2025-10-10T14:08:48.442717-04:00 masternode kernel: [331097.137898] device vethc29965bf entered promiscuous mode
2025-10-10 14:08:48.574	
system-logs
2025-10-10T14:08:48.438696-04:00 masternode kernel: [331097.134462] cni0: port 5(vethc29965bf) entered disabled state
2025-10-10 14:08:48.574	
system-logs
2025-10-10T14:08:48.438679-04:00 masternode kernel: [331097.134455] cni0: port 5(vethc29965bf) entered blocking state
2025-10-10 14:08:47.565	
system-logs
2025-10-10T14:08:47.534728-04:00 masternode kernel: [331096.231669] cni0: port 5(veth9c32b7e2) entered disabled state
2025-10-10 14:08:47.565	
system-logs
2025-10-10T14:08:47.534726-04:00 masternode kernel: [331096.231664] device veth9c32b7e2 left promiscuous mode
2025-10-10 14:08:47.565	
system-logs
2025-10-10T14:08:47.534699-04:00 masternode kernel: [331096.230837] cni0: port 5(veth9c32b7e2) entered disabled state
2025-10-10 14:08:40.038	
system-logs
2025-10-10T14:08:40.034791-04:00 masternode kernel: [331088.729602] cni0: port 8(veth84fd5207) entered forwarding state
2025-10-10 14:08:40.038	
system-logs
2025-10-10T14:08:40.034790-04:00 masternode kernel: [331088.729598] cni0: port 8(veth84fd5207) entered blocking state
2025-10-10 14:08:40.038	
system-logs
2025-10-10T14:08:40.034788-04:00 masternode kernel: [331088.729550] IPv6: ADDRCONF(NETDEV_CHANGE): veth84fd5207: link becomes ready
2025-10-10 14:08:40.038	
system-logs
2025-10-10T14:08:40.034777-04:00 masternode kernel: [331088.729494] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 14:08:40.038	
system-logs
2025-10-10T14:08:40.027272-04:00 masternode kernel: [331088.724346] device veth84fd5207 entered promiscuous mode
2025-10-10 14:08:40.038	
system-logs
2025-10-10T14:08:40.027270-04:00 masternode kernel: [331088.724209] cni0: port 8(veth84fd5207) entered disabled state
2025-10-10 14:08:40.038	
system-logs
2025-10-10T14:08:40.027254-04:00 masternode kernel: [331088.724202] cni0: port 8(veth84fd5207) entered blocking state
2025-10-10 14:08:38.784	
system-logs
2025-10-10T14:08:38.590752-04:00 masternode kernel: [331087.287479] cni0: port 8(veth0f84b9ca) entered disabled state
2025-10-10 14:08:38.784	
system-logs
2025-10-10T14:08:38.590749-04:00 masternode kernel: [331087.287473] device veth0f84b9ca left promiscuous mode
2025-10-10 14:08:38.784	
system-logs
2025-10-10T14:08:38.590717-04:00 masternode kernel: [331087.286864] cni0: port 8(veth0f84b9ca) entered disabled state
2025-10-10 14:08:22.058	
system-logs
2025-10-10T14:08:22.052486-04:00 masternode python3[1365501]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get svc -n infrastructure#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:08:21.808	
system-logs
2025-10-10T14:08:21.730245-04:00 masternode python3[1365467]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n infrastructure -o wide#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:08:15.115	
system-logs
2025-10-10T14:08:15.026594-04:00 storagenodet3500 vmstation-worker: Test message from storagenodet3500 at Fri Oct 10 14:08:15 EDT 2025
2025-10-10 14:08:15.115	
system-logs
2025-10-10T14:08:15.018908-04:00 storagenodet3500 python3[2322777]: ansible-ansible.legacy.command Invoked with _raw_params=logger -t vmstation-worker "Test message from storagenodet3500 at $(date)"#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:08:14.865	
system-logs
2025-10-10T14:08:14.629276-04:00 storagenodet3500 python3[2322735]: ansible-systemd Invoked with name=rsyslog state=restarted enabled=True daemon_reload=False daemon_reexec=False scope=system no_block=False force=None masked=None
2025-10-10 14:08:14.364	
system-logs
2025-10-10T14:08:14.133642-04:00 storagenodet3500 python3[2322728]: ansible-ansible.legacy.file Invoked with mode=0644 dest=/etc/rsyslog.d/50-vmstation-syslog.conf _original_basename=tmp9fvvxcib recurse=False state=file path=/etc/rsyslog.d/50-vmstation-syslog.conf force=False follow=True modification_time_format=%Y%m%d%H%M.%S access_time_format=%Y%m%d%H%M.%S unsafe_writes=False _diff_peek=None src=None modification_time=None access_time=None owner=None group=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:08:13.863	
system-logs
2025-10-10T14:08:13.846169-04:00 storagenodet3500 python3[2322722]: ansible-ansible.legacy.stat Invoked with path=/etc/rsyslog.d/50-vmstation-syslog.conf follow=False get_checksum=True checksum_algorithm=sha1 get_md5=False get_mime=True get_attributes=True
2025-10-10 14:08:11.356	
system-logs
2025-10-10T14:08:11.116474-04:00 storagenodet3500 python3[2322140]: ansible-apt Invoked with name=rsyslog state=present update_cache=True package=['rsyslog'] update_cache_retries=5 update_cache_retry_max_delay=12 cache_valid_time=0 purge=False force=False upgrade=no dpkg_options=force-confdef,force-confold autoremove=False autoclean=False fail_on_autoremove=False only_upgrade=False force_apt_get=False clean=False allow_unauthenticated=False allow_downgrade=False allow_change_held_packages=False lock_timeout=60 deb=None default_release=None install_recommends=None policy_rc_d=None
2025-10-10 14:08:05.766	
system-logs
2025-10-10T14:08:05.617899-04:00 masternode python3[1365203]: ansible-wait_for Invoked with timeout=5 host=127.0.0.1 connect_timeout=5 delay=0 active_connection_states=['ESTABLISHED', 'FIN_WAIT1', 'FIN_WAIT2', 'SYN_RECV', 'SYN_SENT', 'TIME_WAIT'] state=started sleep=1 port=None path=None search_regex=None exclude_hosts=None msg=None
2025-10-10 14:08:05.515	
system-logs
2025-10-10T14:08:05.376560-04:00 masternode vmstation-control-plane: Test message from control plane at Fri 10 Oct 2025 02:08:05 PM EDT
2025-10-10 14:08:05.515	
system-logs
2025-10-10T14:08:05.371413-04:00 masternode python3[1365174]: ansible-ansible.legacy.command Invoked with _raw_params=logger -t vmstation-control-plane "Test message from control plane at $(date)"#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:08:05.264	
system-logs
2025-10-10T14:08:05.046266-04:00 masternode python3[1365132]: ansible-systemd Invoked with name=rsyslog state=restarted enabled=True daemon_reload=False daemon_reexec=False scope=system no_block=False force=None masked=None
2025-10-10 14:08:04.763	
system-logs
2025-10-10T14:08:04.673696-04:00 masternode python3[1365106]: ansible-ansible.legacy.file Invoked with mode=0644 dest=/etc/rsyslog.d/50-vmstation-syslog.conf _original_basename=tmpaqes6hyn recurse=False state=file path=/etc/rsyslog.d/50-vmstation-syslog.conf force=False follow=True modification_time_format=%Y%m%d%H%M.%S access_time_format=%Y%m%d%H%M.%S unsafe_writes=False _diff_peek=None src=None modification_time=None access_time=None owner=None group=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:08:04.512	
system-logs
2025-10-10T14:08:04.458284-04:00 masternode python3[1365096]: ansible-ansible.legacy.stat Invoked with path=/etc/rsyslog.d/50-vmstation-syslog.conf follow=False get_checksum=True checksum_algorithm=sha1 get_md5=False get_mime=True get_attributes=True
2025-10-10 14:08:02.758	
system-logs
2025-10-10T14:08:02.703588-04:00 masternode python3[1364605]: ansible-apt Invoked with name=rsyslog state=present update_cache=True package=['rsyslog'] update_cache_retries=5 update_cache_retry_max_delay=12 cache_valid_time=0 purge=False force=False upgrade=no dpkg_options=force-confdef,force-confold autoremove=False autoclean=False fail_on_autoremove=False only_upgrade=False force_apt_get=False clean=False allow_unauthenticated=False allow_downgrade=False allow_change_held_packages=False lock_timeout=60 deb=None default_release=None install_recommends=None policy_rc_d=None
2025-10-10 14:08:02.257	
system-logs
2025-10-10T14:08:02.252369-04:00 masternode python3[1364568]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf logs -n infrastructure syslog-server-0  -c syslog-ng --tail=20#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:08:02.007	
system-logs
2025-10-10T14:08:01.871757-04:00 masternode python3[1364518]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf exec -n infrastructure syslog-server-0  -c syslog-ng -- logger -t vmstation-test "Syslog service deployment test message at $(date)"#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:08:01.756	
system-logs
2025-10-10T14:08:01.566085-04:00 masternode python3[1364484]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n infrastructure  -l app=syslog-server -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "none"#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:08:01.255	
system-logs
2025-10-10T14:08:01.236839-04:00 masternode python3[1364450]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get svc -n infrastructure syslog-server#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:08:01.004	
system-logs
2025-10-10T14:08:00.912998-04:00 masternode python3[1364416]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n infrastructure -l app=syslog-server -o wide#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:08:00.753	
system-logs
2025-10-10T14:08:00.591051-04:00 masternode python3[1364382]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get statefulset -n infrastructure syslog-server -o wide#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:07:50.406	
system-logs
2025-10-10T14:07:50.390751-04:00 masternode kernel: [331039.087350] cni0: port 9(veth578f247b) entered forwarding state
2025-10-10 14:07:50.406	
system-logs
2025-10-10T14:07:50.390751-04:00 masternode kernel: [331039.087346] cni0: port 9(veth578f247b) entered blocking state
2025-10-10 14:07:50.406	
system-logs
2025-10-10T14:07:50.390749-04:00 masternode kernel: [331039.087296] IPv6: ADDRCONF(NETDEV_CHANGE): veth578f247b: link becomes ready
2025-10-10 14:07:50.406	
system-logs
2025-10-10T14:07:50.390733-04:00 masternode kernel: [331039.087254] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 14:07:50.406	
system-logs
2025-10-10T14:07:50.386709-04:00 masternode kernel: [331039.081676] device veth578f247b entered promiscuous mode
2025-10-10 14:07:50.406	
system-logs
2025-10-10T14:07:50.386707-04:00 masternode kernel: [331039.081367] cni0: port 9(veth578f247b) entered disabled state
2025-10-10 14:07:50.406	
system-logs
2025-10-10T14:07:50.386692-04:00 masternode kernel: [331039.081360] cni0: port 9(veth578f247b) entered blocking state
2025-10-10 14:07:50.000	
syslog
syslog-server-0
syslog
syslog-ng starting up; version='4.10.1'
2025-10-10 14:07:48.722	
system-logs
2025-10-10T14:07:48.520990-04:00 masternode python3[1364125]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=ready pod  -l app=syslog-server -n infrastructure --timeout=60s#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:07:47.720	
system-logs
2025-10-10T14:07:47.709823-04:00 masternode python3[1364085]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /srv/monitoring_data/VMStation/manifests/infrastructure/syslog-server.yaml#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:07:47.469	
system-logs
2025-10-10T14:07:47.414475-04:00 masternode python3[1364054]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf create namespace infrastructure#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:07:47.218	
system-logs
2025-10-10T14:07:47.015378-04:00 masternode python3[1364019]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /srv/monitoring_data/VMStation/manifests/infrastructure/syslog-pv.yaml#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:07:46.968	
system-logs
2025-10-10T14:07:46.787163-04:00 masternode python3[1363993]: ansible-file Invoked with path=/srv/monitoring_data/syslog state=directory mode=0755 owner=root group=root recurse=False force=False follow=True modification_time_format=%Y%m%d%H%M.%S access_time_format=%Y%m%d%H%M.%S unsafe_writes=False _original_basename=None _diff_peek=None src=None modification_time=None access_time=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:07:46.467	
system-logs
2025-10-10T14:07:46.410790-04:00 masternode python3[1363955]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n monitoring -l app=loki -o jsonpath='{.items[*].status.phase}'#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:07:46.216	
system-logs
2025-10-10T14:07:46.111511-04:00 masternode python3[1363905]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf cluster-info _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:07:28.993	
system-logs
2025-10-10T14:07:28.842929-04:00 storagenodet3500 python3[2321929]: ansible-ansible.legacy.command Invoked with _raw_params=chronyc tracking _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:07:18.720	
system-logs
2025-10-10T14:07:18.559729-04:00 storagenodet3500 python3[2321890]: ansible-wait_for Invoked with timeout=10 host=127.0.0.1 connect_timeout=5 delay=0 active_connection_states=['ESTABLISHED', 'FIN_WAIT1', 'FIN_WAIT2', 'SYN_RECV', 'SYN_SENT', 'TIME_WAIT'] state=started sleep=1 port=None path=None search_regex=None exclude_hosts=None msg=None
2025-10-10 14:07:18.219	
system-logs
2025-10-10T14:07:18.217911-04:00 storagenodet3500 python3[2321884]: ansible-systemd Invoked with name=chrony state=started enabled=True daemon_reload=False daemon_reexec=False scope=system no_block=False force=None masked=None
2025-10-10 14:07:17.968	
system-logs
2025-10-10T14:07:17.737305-04:00 storagenodet3500 python3[2321877]: ansible-ansible.legacy.file Invoked with mode=0644 dest=/etc/chrony/chrony.conf _original_basename=tmp1e2z_ms1 recurse=False state=file path=/etc/chrony/chrony.conf force=False follow=True modification_time_format=%Y%m%d%H%M.%S access_time_format=%Y%m%d%H%M.%S unsafe_writes=False _diff_peek=None src=None modification_time=None access_time=None owner=None group=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:07:17.467	
system-logs
2025-10-10T14:07:17.450097-04:00 storagenodet3500 python3[2321871]: ansible-ansible.legacy.stat Invoked with path=/etc/chrony/chrony.conf follow=False get_checksum=True checksum_algorithm=sha1 get_md5=False get_mime=True get_attributes=True
2025-10-10 14:07:14.709	
system-logs
2025-10-10T14:07:14.529894-04:00 storagenodet3500 python3[2321286]: ansible-apt Invoked with name=chrony state=present update_cache=True package=['chrony'] update_cache_retries=5 update_cache_retry_max_delay=12 cache_valid_time=0 purge=False force=False upgrade=no dpkg_options=force-confdef,force-confold autoremove=False autoclean=False fail_on_autoremove=False only_upgrade=False force_apt_get=False clean=False allow_unauthenticated=False allow_downgrade=False allow_change_held_packages=False lock_timeout=60 deb=None default_release=None install_recommends=None policy_rc_d=None
2025-10-10 14:07:13.205	
system-logs
2025-10-10T14:07:13.197529-04:00 storagenodet3500 python3[2321195]: ansible-ansible.legacy.setup Invoked with gather_subset=['all'] gather_timeout=10 filter=[] fact_path=/etc/ansible/facts.d
2025-10-10 14:07:12.704	
system-logs
2025-10-10T14:07:12.660314-04:00 storagenodet3500 sshd[2321186]: Accepted publickey for root from 192.168.4.63 port 37580 ssh2: RSA SHA256:fWiiKUf2zJjczXqPl4lAmxLZiYWi1dfitYBNcsqqemE
2025-10-10 14:07:12.381	
system-logs
2025-10-10T14:07:12.234782-04:00 masternode python3[1363683]: ansible-ansible.legacy.command Invoked with _raw_params=chronyc tracking _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:07:02.108	
system-logs
2025-10-10T14:07:02.006202-04:00 masternode python3[1363595]: ansible-wait_for Invoked with timeout=10 host=127.0.0.1 connect_timeout=5 delay=0 active_connection_states=['ESTABLISHED', 'FIN_WAIT1', 'FIN_WAIT2', 'SYN_RECV', 'SYN_SENT', 'TIME_WAIT'] state=started sleep=1 port=None path=None search_regex=None exclude_hosts=None msg=None
2025-10-10 14:07:01.858	
system-logs
2025-10-10T14:07:01.627605-04:00 masternode python3[1363567]: ansible-systemd Invoked with name=chrony state=started enabled=True daemon_reload=False daemon_reexec=False scope=system no_block=False force=None masked=None
2025-10-10 14:07:01.106	
system-logs
2025-10-10T14:07:00.940781-04:00 masternode python3[1363541]: ansible-ansible.legacy.file Invoked with mode=0644 dest=/etc/chrony/chrony.conf _original_basename=tmpka0wjvvb recurse=False state=file path=/etc/chrony/chrony.conf force=False follow=True modification_time_format=%Y%m%d%H%M.%S access_time_format=%Y%m%d%H%M.%S unsafe_writes=False _diff_peek=None src=None modification_time=None access_time=None owner=None group=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:07:00.605	
system-logs
2025-10-10T14:07:00.574671-04:00 masternode python3[1363531]: ansible-ansible.legacy.stat Invoked with path=/etc/chrony/chrony.conf follow=False get_checksum=True checksum_algorithm=sha1 get_md5=False get_mime=True get_attributes=True
2025-10-10 14:06:58.348	
system-logs
2025-10-10T14:06:58.225641-04:00 masternode python3[1363027]: ansible-apt Invoked with name=chrony state=present update_cache=True package=['chrony'] update_cache_retries=5 update_cache_retry_max_delay=12 cache_valid_time=0 purge=False force=False upgrade=no dpkg_options=force-confdef,force-confold autoremove=False autoclean=False fail_on_autoremove=False only_upgrade=False force_apt_get=False clean=False allow_unauthenticated=False allow_downgrade=False allow_change_held_packages=False lock_timeout=60 deb=None default_release=None install_recommends=None policy_rc_d=None
2025-10-10 14:06:57.596	
system-logs
2025-10-10T14:06:57.535494-04:00 masternode python3[1362993]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf exec -n infrastructure chrony-ntp-5z5p7  -c chrony -- chronyc tracking#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:52.083	
system-logs
2025-10-10T14:06:51.891896-04:00 masternode python3[1362920]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf exec -n infrastructure chrony-ntp-5z5p7  -c chrony -- chronyc sources#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:51.582	
system-logs
2025-10-10T14:06:51.582508-04:00 masternode python3[1362887]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n infrastructure  -l app=chrony-ntp -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "none"#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:51.332	
system-logs
2025-10-10T14:06:51.260083-04:00 masternode python3[1362853]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get svc -n infrastructure chrony-ntp#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:51.081	
system-logs
2025-10-10T14:06:50.935698-04:00 masternode python3[1362819]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n infrastructure -l app=chrony-ntp -o wide#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:50.830	
system-logs
2025-10-10T14:06:50.612517-04:00 masternode python3[1362784]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get daemonset -n infrastructure chrony-ntp -o wide#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:40.302	
system-logs
2025-10-10T14:06:40.153748-04:00 masternode python3[1362582]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=ready pod  -l app=chrony-ntp -n infrastructure --timeout=60s#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:39.550	
system-logs
2025-10-10T14:06:39.395023-04:00 masternode python3[1362547]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /srv/monitoring_data/VMStation/manifests/infrastructure/chrony-ntp.yaml _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:39.299	
system-logs
2025-10-10T14:06:39.099663-04:00 masternode python3[1362514]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf label namespace infrastructure  name=infrastructure vmstation.io/component=infrastructure --overwrite#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:39.048	
system-logs
2025-10-10T14:06:38.804856-04:00 masternode python3[1362483]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf create namespace infrastructure#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:38.547	
system-logs
2025-10-10T14:06:38.543723-04:00 masternode python3[1362454]: ansible-ansible.legacy.command Invoked with _raw_params=date '+%Y-%m-%d %H:%M:%S %Z' _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:38.296	
system-logs
2025-10-10T14:06:38.219346-04:00 masternode python3[1362420]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf cluster-info _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:38.046	
system-logs
2025-10-10T14:06:37.844415-04:00 masternode python3[1362383]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf label namespace infrastructure  name=infrastructure vmstation.io/component=infrastructure --overwrite#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:37.544	
system-logs
2025-10-10T14:06:37.533295-04:00 masternode python3[1362351]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf create namespace infrastructure#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:37.293	
system-logs
2025-10-10T14:06:37.235508-04:00 masternode python3[1362319]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf cluster-info _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:36.291	
system-logs
2025-10-10T14:06:36.253105-04:00 masternode python3[1362166]: ansible-ansible.legacy.setup Invoked with gather_subset=['all'] gather_timeout=10 filter=[] fact_path=/etc/ansible/facts.d
2025-10-10 14:06:34.787	
system-logs
2025-10-10T14:06:34.621903-04:00 masternode python3[1362088]: ansible-ansible.legacy.uri Invoked with url=http://127.0.0.1:30300/api/health method=GET status_code=[200] force=False http_agent=ansible-httpget use_proxy=True validate_certs=True force_basic_auth=False use_gssapi=False body_format=raw return_content=False follow_redirects=safe timeout=30 headers={} remote_src=False unredirected_headers=[] decompress=True use_netrc=True unsafe_writes=False url_username=None url_password=NOT_LOGGING_PARAMETER client_cert=None client_key=None dest=None body=None src=None creates=None removes=None unix_socket=None ca_path=None ciphers=None mode=None owner=None group=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:06:34.286	
system-logs
2025-10-10T14:06:34.209019-04:00 masternode python3[1362060]: ansible-ansible.legacy.uri Invoked with url=http://127.0.0.1:31100/ready method=GET status_code=[200] force=False http_agent=ansible-httpget use_proxy=True validate_certs=True force_basic_auth=False use_gssapi=False body_format=raw return_content=False follow_redirects=safe timeout=30 headers={} remote_src=False unredirected_headers=[] decompress=True use_netrc=True unsafe_writes=False url_username=None url_password=NOT_LOGGING_PARAMETER client_cert=None client_key=None dest=None body=None src=None creates=None removes=None unix_socket=None ca_path=None ciphers=None mode=None owner=None group=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:06:34.036	
system-logs
2025-10-10T14:06:33.810735-04:00 masternode python3[1362033]: ansible-ansible.legacy.uri Invoked with url=http://127.0.0.1:30090/-/healthy method=GET status_code=[200] force=False http_agent=ansible-httpget use_proxy=True validate_certs=True force_basic_auth=False use_gssapi=False body_format=raw return_content=False follow_redirects=safe timeout=30 headers={} remote_src=False unredirected_headers=[] decompress=True use_netrc=True unsafe_writes=False url_username=None url_password=NOT_LOGGING_PARAMETER client_cert=None client_key=None dest=None body=None src=None creates=None removes=None unix_socket=None ca_path=None ciphers=None mode=None owner=None group=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:06:33.283	
system-logs
2025-10-10T14:06:33.233605-04:00 masternode python3[1361999]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get pvc -n monitoring#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:33.033	
system-logs
2025-10-10T14:06:32.908922-04:00 masternode python3[1361965]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get svc -n monitoring#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:32.782	
system-logs
2025-10-10T14:06:32.582096-04:00 masternode python3[1361931]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -n monitoring -o wide#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:32.280	
system-logs
2025-10-10T14:06:32.183667-04:00 masternode python3[1361897]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=ready pod  -l app=grafana -n monitoring --timeout=60s#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:06:32.029	
system-logs
2025-10-10T14:06:31.779722-04:00 masternode python3[1361862]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=ready pod  -l app.kubernetes.io/name=loki -n monitoring --timeout=120s#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:31.700	
system-logs
2025-10-10T14:04:31.495603-04:00 masternode python3[1361570]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=ready pod  -l app.kubernetes.io/name=prometheus -n monitoring --timeout=120s#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:30.698	
system-logs
2025-10-10T14:04:30.549917-04:00 masternode python3[1361535]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /srv/monitoring_data/VMStation/manifests/monitoring/ipmi-exporter.yaml#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:29.946	
system-logs
2025-10-10T14:04:29.916725-04:00 masternode python3[1361502]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /srv/monitoring_data/VMStation/manifests/monitoring/node-exporter.yaml#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:29.445	
system-logs
2025-10-10T14:04:29.234842-04:00 masternode python3[1361469]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /srv/monitoring_data/VMStation/manifests/monitoring/kube-state-metrics.yaml#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:28.192	
system-logs
2025-10-10T14:04:28.189454-04:00 masternode python3[1361434]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /srv/monitoring_data/VMStation/manifests/monitoring/grafana.yaml _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:26.939	
system-logs
2025-10-10T14:04:26.741461-04:00 masternode python3[1361399]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /srv/monitoring_data/VMStation/manifests/monitoring/loki.yaml _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:25.185	
system-logs
2025-10-10T14:04:25.013077-04:00 masternode python3[1361321]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /srv/monitoring_data/VMStation/manifests/monitoring/prometheus.yaml _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:24.934	
system-logs
2025-10-10T14:04:24.754227-04:00 masternode python3[1361296]: ansible-file Invoked with path=/srv/monitoring_data/promtail state=directory mode=0755 owner=0 group=0 recurse=False force=False follow=True modification_time_format=%Y%m%d%H%M.%S access_time_format=%Y%m%d%H%M.%S unsafe_writes=False _original_basename=None _diff_peek=None src=None modification_time=None access_time=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:04:24.433	
system-logs
2025-10-10T14:04:24.423910-04:00 masternode python3[1361272]: ansible-file Invoked with path=/srv/monitoring_data/loki state=directory mode=0755 owner=10001 group=10001 recurse=False force=False follow=True modification_time_format=%Y%m%d%H%M.%S access_time_format=%Y%m%d%H%M.%S unsafe_writes=False _original_basename=None _diff_peek=None src=None modification_time=None access_time=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:04:24.183	
system-logs
2025-10-10T14:04:24.068701-04:00 masternode python3[1361248]: ansible-file Invoked with path=/srv/monitoring_data/prometheus state=directory mode=0755 owner=65534 group=65534 recurse=False force=False follow=True modification_time_format=%Y%m%d%H%M.%S access_time_format=%Y%m%d%H%M.%S unsafe_writes=False _original_basename=None _diff_peek=None src=None modification_time=None access_time=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:04:23.931	
system-logs
2025-10-10T14:04:23.733210-04:00 masternode python3[1361224]: ansible-file Invoked with path=/srv/monitoring_data/grafana state=directory mode=0755 owner=472 group=472 recurse=False force=False follow=True modification_time_format=%Y%m%d%H%M.%S access_time_format=%Y%m%d%H%M.%S unsafe_writes=False _original_basename=None _diff_peek=None src=None modification_time=None access_time=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:04:23.426	
system-logs
2025-10-10T14:04:23.397083-04:00 masternode python3[1361199]: ansible-file Invoked with path=/srv/monitoring_data state=directory mode=0755 owner=root group=root recurse=False force=False follow=True modification_time_format=%Y%m%d%H%M.%S access_time_format=%Y%m%d%H%M.%S unsafe_writes=False _original_basename=None _diff_peek=None src=None modification_time=None access_time=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:04:22.922	
system-logs
2025-10-10T14:04:22.670294-04:00 masternode python3[1361167]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf label namespace monitoring  name=monitoring vmstation.io/component=monitoring --overwrite#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:22.418	
system-logs
2025-10-10T14:04:22.201762-04:00 masternode python3[1361136]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf create namespace monitoring#012 _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:21.916	
system-logs
2025-10-10T14:04:21.796836-04:00 masternode python3[1361098]: ansible-ansible.legacy.command Invoked with _raw_params=echo "Total Memory: $(free -h | grep Mem | awk '{print $2}')"#012echo "Available Memory: $(free -h | grep Mem | awk '{print $7}')"#012echo "CPU Cores: $(nproc)"#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:21.665	
system-logs
2025-10-10T14:04:21.437740-04:00 masternode python3[1361066]: ansible-ansible.legacy.command Invoked with _raw_params=df -h /srv | tail -1 | awk '{print $4}' _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:20.913	
system-logs
2025-10-10T14:04:20.895689-04:00 masternode python3[1361032]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf cluster-info _uses_shell=False stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:19.642	
system-logs
2025-10-10T14:04:19.534546-04:00 masternode python3[1360898]: ansible-ansible.legacy.setup Invoked with gather_subset=['all'] gather_timeout=10 filter=[] fact_path=/etc/ansible/facts.d
2025-10-10 14:04:12.368	
system-logs
2025-10-10T14:04:12.118067-04:00 masternode python3[1360738]: ansible-wait_for Invoked with host=192.168.4.62 port=22 timeout=120 connect_timeout=5 delay=0 active_connection_states=['ESTABLISHED', 'FIN_WAIT1', 'FIN_WAIT2', 'SYN_RECV', 'SYN_SENT', 'TIME_WAIT'] state=started sleep=1 path=None search_regex=None exclude_hosts=None msg=None
2025-10-10 14:04:12.117	
system-logs
2025-10-10T14:04:11.906347-04:00 masternode python3[1360713]: ansible-wait_for Invoked with host=192.168.4.61 port=22 timeout=120 connect_timeout=5 delay=0 active_connection_states=['ESTABLISHED', 'FIN_WAIT1', 'FIN_WAIT2', 'SYN_RECV', 'SYN_SENT', 'TIME_WAIT'] state=started sleep=1 path=None search_regex=None exclude_hosts=None msg=None
2025-10-10 14:04:11.997	
system-logs
2025-10-10T14:04:11.934338-04:00 storagenodet3500 sshd[2320491]: Connection closed by 192.168.4.63 port 41466
2025-10-10 14:04:11.997	
system-logs
2025-10-10T14:04:11.934138-04:00 storagenodet3500 sshd[2320491]: error: kex_exchange_identification: Connection closed by remote host
2025-10-10 14:04:11.866	
system-logs
2025-10-10T14:04:11.624188-04:00 masternode python3[1360686]: ansible-ansible.legacy.command Invoked with _raw_params=wakeonlan d0:94:66:30:d6:63 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:11.615	
system-logs
2025-10-10T14:04:11.390536-04:00 masternode python3[1360659]: ansible-ansible.legacy.command Invoked with _raw_params=wakeonlan b8:ac:6f:7e:6c:9d _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:10.612	
system-logs
2025-10-10T14:04:10.511649-04:00 masternode python3[1360632]: ansible-ansible.legacy.command Invoked with _raw_params=ssh jashandeepjustinbains@192.168.4.62 'nohup /usr/local/bin/vmstation-sleep-ansible.sh >/tmp/vmstation-sleep.log 2>&1 &' _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:10.492	
system-logs
2025-10-10T14:04:10.288241-04:00 storagenodet3500 sshd[2320486]: Disconnected from user root 192.168.4.63 port 41454
2025-10-10 14:04:10.492	
system-logs
2025-10-10T14:04:10.288027-04:00 storagenodet3500 sshd[2320486]: Received disconnect from 192.168.4.63 port 41454:11: disconnected by user
2025-10-10 14:04:10.242	
system-logs
2025-10-10T14:04:10.231065-04:00 storagenodet3500 sshd[2320486]: Accepted publickey for root from 192.168.4.63 port 41454 ssh2: RSA SHA256:x1+4zYqsx83nkRUF1MJypQp4ujLAtvATxu7/KYztXWg
2025-10-10 14:04:10.111	
system-logs
2025-10-10T14:04:09.947344-04:00 masternode python3[1360605]: ansible-ansible.legacy.command Invoked with _raw_params=ssh root@192.168.4.61 'nohup /usr/local/bin/vmstation-sleep-ansible.sh >/tmp/vmstation-sleep.log 2>&1 &' _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:09.860	
system-logs
2025-10-10T14:04:09.680086-04:00 masternode python3[1360579]: ansible-ansible.legacy.file Invoked with mode=0755 dest=/usr/local/bin/vmstation-sleep-ansible.sh _original_basename=tmpsf9wwydz recurse=False state=file path=/usr/local/bin/vmstation-sleep-ansible.sh force=False follow=True modification_time_format=%Y%m%d%H%M.%S access_time_format=%Y%m%d%H%M.%S unsafe_writes=False _diff_peek=None src=None modification_time=None access_time=None owner=None group=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:04:09.610	
system-logs
2025-10-10T14:04:09.464818-04:00 masternode python3[1360569]: ansible-ansible.legacy.stat Invoked with path=/usr/local/bin/vmstation-sleep-ansible.sh follow=False get_checksum=True checksum_algorithm=sha1 get_md5=False get_mime=True get_attributes=True
2025-10-10 14:04:08.857	
system-logs
2025-10-10T14:04:08.743147-04:00 masternode python3[1360537]: ansible-apt Invoked with name=wakeonlan state=present package=['wakeonlan'] update_cache_retries=5 update_cache_retry_max_delay=12 cache_valid_time=0 purge=False force=False upgrade=no dpkg_options=force-confdef,force-confold autoremove=False autoclean=False fail_on_autoremove=False only_upgrade=False force_apt_get=False clean=False allow_unauthenticated=False allow_downgrade=False allow_change_held_packages=False lock_timeout=60 update_cache=None deb=None default_release=None install_recommends=None policy_rc_d=None
2025-10-10 14:04:08.105	
system-logs
2025-10-10T14:04:08.067547-04:00 masternode python3[1360503]: ansible-ansible.legacy.uri Invoked with url=http://192.168.4.63:30090/-/healthy method=GET status_code=[200] timeout=10 force=False http_agent=ansible-httpget use_proxy=True validate_certs=True force_basic_auth=False use_gssapi=False body_format=raw return_content=False follow_redirects=safe headers={} remote_src=False unredirected_headers=[] decompress=True use_netrc=True unsafe_writes=False url_username=None url_password=NOT_LOGGING_PARAMETER client_cert=None client_key=None dest=None body=None src=None creates=None removes=None unix_socket=None ca_path=None ciphers=None mode=None owner=None group=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:04:07.854	
system-logs
2025-10-10T14:04:07.689078-04:00 masternode python3[1360477]: ansible-ansible.legacy.uri Invoked with url=http://192.168.4.63:30300/api/health method=GET status_code=[200] timeout=10 force=False http_agent=ansible-httpget use_proxy=True validate_certs=True force_basic_auth=False use_gssapi=False body_format=raw return_content=False follow_redirects=safe headers={} remote_src=False unredirected_headers=[] decompress=True use_netrc=True unsafe_writes=False url_username=None url_password=NOT_LOGGING_PARAMETER client_cert=None client_key=None dest=None body=None src=None creates=None removes=None unix_socket=None ca_path=None ciphers=None mode=None owner=None group=None seuser=None serole=None selevel=None setype=None attributes=None
2025-10-10 14:04:07.104	
system-logs
2025-10-10T14:04:07.061192-04:00 masternode python3[1360440]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get all -n jellyfin -o wide _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:04:06.853	
system-logs
2025-10-10T14:04:06.712354-04:00 masternode python3[1360406]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf get all -n monitoring _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 14:02:39.321	
system-logs
2025-10-10T14:02:39.214813-04:00 masternode kernel: [330727.903224] brcmsmac bcma0:1: brcms_ops_config: change power-save mode: false (implement)
2025-10-10 14:02:39.321	
system-logs
2025-10-10T14:02:39.214782-04:00 masternode kernel: [330727.903194] brcmsmac bcma0:1: brcms_ops_bss_info_changed: qos enabled: false (implement)
2025-10-10 14:01:54.199	
system-logs
[2025-10-10 14:01:54] Auto-sleep check completed
2025-10-10 14:01:54.199	
system-logs
[2025-10-10 14:01:54] Activity detected - updated last activity timestamp
2025-10-10 14:01:54.199	
system-logs
[2025-10-10 14:01:54] Active non-system pods: 1
2025-10-10 14:01:54.199	
system-logs
[2025-10-10 14:01:54] Checking cluster activity...
2025-10-10 13:59:42.034	
system-logs
2025-10-10T13:59:41.828910-04:00 storagenodet3500 kernel: [767763.444075] cni0: port 2(vethffbe2d88) entered forwarding state
2025-10-10 13:59:42.034	
system-logs
2025-10-10T13:59:41.828909-04:00 storagenodet3500 kernel: [767763.444071] cni0: port 2(vethffbe2d88) entered blocking state
2025-10-10 13:59:42.034	
system-logs
2025-10-10T13:59:41.828905-04:00 storagenodet3500 kernel: [767763.444035] IPv6: ADDRCONF(NETDEV_CHANGE): vethffbe2d88: link becomes ready
2025-10-10 13:59:42.034	
system-logs
2025-10-10T13:59:41.828887-04:00 storagenodet3500 kernel: [767763.443957] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:59:42.034	
system-logs
2025-10-10T13:59:41.824940-04:00 storagenodet3500 kernel: [767763.438364] device vethffbe2d88 entered promiscuous mode
2025-10-10 13:59:42.034	
system-logs
2025-10-10T13:59:41.824937-04:00 storagenodet3500 kernel: [767763.438298] cni0: port 2(vethffbe2d88) entered disabled state
2025-10-10 13:59:42.034	
system-logs
2025-10-10T13:59:41.824914-04:00 storagenodet3500 kernel: [767763.438291] cni0: port 2(vethffbe2d88) entered blocking state
2025-10-10 13:59:41.632	
system-logs
2025-10-10T13:59:41.412939-04:00 masternode python3[1359628]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=ready pod/jellyfin  -n jellyfin  --timeout=300s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:59:41.130	
system-logs
2025-10-10T13:59:40.900755-04:00 masternode python3[1359591]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f /srv/monitoring_data/VMStation/ansible/playbooks/../../manifests/jellyfin/jellyfin.yaml _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:59:40.630	
system-logs
2025-10-10T13:59:40.500729-04:00 masternode python3[1359555]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/blackbox-exporter  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:59:40.128	
system-logs
2025-10-10T13:59:40.102967-04:00 masternode python3[1359520]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/grafana  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:59:39.878	
system-logs
2025-10-10T13:59:39.806231-04:00 masternode python3[1359489]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/prometheus  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:59:29.601	
system-logs
2025-10-10T13:59:29.534494-04:00 masternode python3[1359437]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/prometheus  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:59:19.327	
system-logs
2025-10-10T13:59:19.260298-04:00 masternode python3[1359388]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/prometheus  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:59:09.050	
system-logs
2025-10-10T13:59:08.941410-04:00 masternode python3[1359303]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/prometheus  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:58:58.775	
system-logs
2025-10-10T13:58:58.670988-04:00 masternode python3[1359213]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/prometheus  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:58:50.166	
system-logs
2025-10-10T13:58:50.123814-04:00 storagenodet3500 sshd[2317409]: Disconnected from user root 192.168.4.63 port 37510
2025-10-10 13:58:50.166	
system-logs
2025-10-10T13:58:50.123521-04:00 storagenodet3500 sshd[2317409]: Received disconnect from 192.168.4.63 port 37510:11: disconnected by user
2025-10-10 13:58:48.498	
system-logs
2025-10-10T13:58:48.393558-04:00 masternode python3[1359161]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/prometheus  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:58:48.247	
system-logs
2025-10-10T13:58:48.081032-04:00 masternode python3[1359125]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf rollout status daemonset/promtail  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:58:47.997	
system-logs
2025-10-10T13:58:47.786443-04:00 masternode python3[1359094]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/loki  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:58:37.695	
system-logs
2025-10-10T13:58:37.516129-04:00 masternode python3[1358963]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/loki  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:58:27.423	
system-logs
2025-10-10T13:58:27.238302-04:00 masternode python3[1358901]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/loki  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:58:16.890	
system-logs
2025-10-10T13:58:16.869857-04:00 masternode python3[1358755]: ansible-ansible.legacy.command Invoked with _raw_params=kubectl --kubeconfig=/etc/kubernetes/admin.conf wait --for=condition=available deployment/loki  -n monitoring  --timeout=120s#012 _uses_shell=True stdin_add_newline=True strip_empty_ends=True argv=None chdir=None executable=None creates=None removes=None stdin=None
2025-10-10 13:58:14.579	
system-logs
2025-10-10T13:58:07.865001-04:00 storagenodet3500 kernel: [767669.479312] cni0: port 1(veth6769dc0f) entered forwarding state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T13:58:07.865000-04:00 storagenodet3500 kernel: [767669.479308] cni0: port 1(veth6769dc0f) entered blocking state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T13:58:07.864997-04:00 storagenodet3500 kernel: [767669.479263] IPv6: ADDRCONF(NETDEV_CHANGE): veth6769dc0f: link becomes ready
2025-10-10 13:58:14.579	
system-logs
2025-10-10T13:58:07.864975-04:00 storagenodet3500 kernel: [767669.479199] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.579	
system-logs
2025-10-10T13:58:07.860932-04:00 storagenodet3500 kernel: [767669.473798] device veth6769dc0f entered promiscuous mode
2025-10-10 13:58:14.579	
system-logs
2025-10-10T13:58:07.860930-04:00 storagenodet3500 kernel: [767669.473729] cni0: port 1(veth6769dc0f) entered disabled state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T13:58:07.860906-04:00 storagenodet3500 kernel: [767669.473721] cni0: port 1(veth6769dc0f) entered blocking state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T13:50:37.421033-04:00 storagenodet3500 kernel: [767219.029679] cni0: port 1(vetha58f48fc) entered disabled state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T13:50:37.421031-04:00 storagenodet3500 kernel: [767219.029675] device vetha58f48fc left promiscuous mode
2025-10-10 13:58:14.579	
system-logs
2025-10-10T13:50:37.421012-04:00 storagenodet3500 kernel: [767219.028847] cni0: port 1(vetha58f48fc) entered disabled state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T11:05:24.960906-04:00 storagenodet3500 kernel: [757306.483402] cni0: port 2(veth37a5e31d) entered disabled state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T11:05:24.960901-04:00 storagenodet3500 kernel: [757306.483397] device veth37a5e31d left promiscuous mode
2025-10-10 13:58:14.579	
system-logs
2025-10-10T11:05:24.960875-04:00 storagenodet3500 kernel: [757306.482635] cni0: port 2(veth37a5e31d) entered disabled state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:55.084879-04:00 storagenodet3500 kernel: [756976.606641] cni0: port 3(veth7fa9f529) entered disabled state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:55.084877-04:00 storagenodet3500 kernel: [756976.606637] device veth7fa9f529 left promiscuous mode
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:55.084858-04:00 storagenodet3500 kernel: [756976.605746] cni0: port 3(veth7fa9f529) entered disabled state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:19.160935-04:00 storagenodet3500 kernel: [756940.680430] cni0: port 3(veth7fa9f529) entered forwarding state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:19.160933-04:00 storagenodet3500 kernel: [756940.680426] cni0: port 3(veth7fa9f529) entered blocking state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:19.160930-04:00 storagenodet3500 kernel: [756940.680385] IPv6: ADDRCONF(NETDEV_CHANGE): veth7fa9f529: link becomes ready
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:19.160905-04:00 storagenodet3500 kernel: [756940.680321] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:19.156911-04:00 storagenodet3500 kernel: [756940.675377] device veth7fa9f529 entered promiscuous mode
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:19.156908-04:00 storagenodet3500 kernel: [756940.675290] cni0: port 3(veth7fa9f529) entered disabled state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:19.156889-04:00 storagenodet3500 kernel: [756940.675282] cni0: port 3(veth7fa9f529) entered blocking state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:16.156948-04:00 storagenodet3500 kernel: [756937.675230] cni0: port 3(vethd72d8a63) entered disabled state
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:16.156945-04:00 storagenodet3500 kernel: [756937.675226] device vethd72d8a63 left promiscuous mode
2025-10-10 13:58:14.579	
system-logs
2025-10-10T10:59:16.156923-04:00 storagenodet3500 kernel: [756937.674415] cni0: port 3(vethd72d8a63) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:59:11.600911-04:00 storagenodet3500 kernel: [756933.120846] cni0: port 3(vethd72d8a63) entered forwarding state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:59:11.600910-04:00 storagenodet3500 kernel: [756933.120842] cni0: port 3(vethd72d8a63) entered blocking state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:59:11.600908-04:00 storagenodet3500 kernel: [756933.120802] IPv6: ADDRCONF(NETDEV_CHANGE): vethd72d8a63: link becomes ready
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:59:11.600892-04:00 storagenodet3500 kernel: [756933.120731] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:59:11.596922-04:00 storagenodet3500 kernel: [756933.115657] device vethd72d8a63 entered promiscuous mode
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:59:11.596919-04:00 storagenodet3500 kernel: [756933.115589] cni0: port 3(vethd72d8a63) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:59:11.596897-04:00 storagenodet3500 kernel: [756933.115582] cni0: port 3(vethd72d8a63) entered blocking state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:59:00.980873-04:00 storagenodet3500 kernel: [756922.502222] cni0: port 3(vetha542b4ba) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:59:00.980871-04:00 storagenodet3500 kernel: [756922.502217] device vetha542b4ba left promiscuous mode
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:59:00.980851-04:00 storagenodet3500 kernel: [756922.501438] cni0: port 3(vetha542b4ba) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:58:24.932858-04:00 storagenodet3500 kernel: [756886.451790] cni0: port 3(vetha542b4ba) entered forwarding state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:58:24.932856-04:00 storagenodet3500 kernel: [756886.451785] cni0: port 3(vetha542b4ba) entered blocking state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:58:24.932854-04:00 storagenodet3500 kernel: [756886.451738] IPv6: ADDRCONF(NETDEV_CHANGE): vetha542b4ba: link becomes ready
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:58:24.932846-04:00 storagenodet3500 kernel: [756886.451663] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:58:24.928933-04:00 storagenodet3500 kernel: [756886.446364] device vetha542b4ba entered promiscuous mode
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:58:24.928929-04:00 storagenodet3500 kernel: [756886.446296] cni0: port 3(vetha542b4ba) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:58:24.928895-04:00 storagenodet3500 kernel: [756886.446289] cni0: port 3(vetha542b4ba) entered blocking state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:47:57.796866-04:00 storagenodet3500 kernel: [756259.309653] cni0: port 2(veth37a5e31d) entered forwarding state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:47:57.796864-04:00 storagenodet3500 kernel: [756259.309648] cni0: port 2(veth37a5e31d) entered blocking state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:47:57.796861-04:00 storagenodet3500 kernel: [756259.309604] IPv6: ADDRCONF(NETDEV_CHANGE): veth37a5e31d: link becomes ready
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:47:57.796851-04:00 storagenodet3500 kernel: [756259.309530] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:47:57.790046-04:00 storagenodet3500 kernel: [756259.303830] device veth37a5e31d entered promiscuous mode
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:47:57.790043-04:00 storagenodet3500 kernel: [756259.303734] cni0: port 2(veth37a5e31d) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:47:57.790017-04:00 storagenodet3500 kernel: [756259.303727] cni0: port 2(veth37a5e31d) entered blocking state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:46:23.972875-04:00 storagenodet3500 kernel: [756165.486744] cni0: port 1(vetha58f48fc) entered forwarding state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:46:23.972873-04:00 storagenodet3500 kernel: [756165.486738] cni0: port 1(vetha58f48fc) entered blocking state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:46:23.972871-04:00 storagenodet3500 kernel: [756165.486689] IPv6: ADDRCONF(NETDEV_CHANGE): vetha58f48fc: link becomes ready
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:46:23.972856-04:00 storagenodet3500 kernel: [756165.486612] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:46:23.968879-04:00 storagenodet3500 kernel: [756165.480953] device vetha58f48fc entered promiscuous mode
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:46:23.968876-04:00 storagenodet3500 kernel: [756165.480858] cni0: port 1(vetha58f48fc) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:46:23.968852-04:00 storagenodet3500 kernel: [756165.480850] cni0: port 1(vetha58f48fc) entered blocking state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:43:54.693041-04:00 storagenodet3500 kernel: [756016.203199] cni0: port 1(veth21ce3064) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:43:54.693034-04:00 storagenodet3500 kernel: [756016.203195] device veth21ce3064 left promiscuous mode
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:43:54.688853-04:00 storagenodet3500 kernel: [756016.202360] cni0: port 1(veth21ce3064) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:43:53.212871-04:00 storagenodet3500 kernel: [756014.724322] cni0: port 2(vethcedcc170) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:43:53.212870-04:00 storagenodet3500 kernel: [756014.724317] device vethcedcc170 left promiscuous mode
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:43:53.212854-04:00 storagenodet3500 kernel: [756014.723499] cni0: port 2(vethcedcc170) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:42:17.020931-04:00 storagenodet3500 kernel: [755918.532725] cni0: port 3(veth13d1051b) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:42:17.020923-04:00 storagenodet3500 kernel: [755918.532718] device veth13d1051b left promiscuous mode
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:42:17.020901-04:00 storagenodet3500 kernel: [755918.531834] cni0: port 3(veth13d1051b) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:41.112874-04:00 storagenodet3500 kernel: [755882.624097] cni0: port 3(veth13d1051b) entered forwarding state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:41.112872-04:00 storagenodet3500 kernel: [755882.624094] cni0: port 3(veth13d1051b) entered blocking state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:41.112870-04:00 storagenodet3500 kernel: [755882.624059] IPv6: ADDRCONF(NETDEV_CHANGE): veth13d1051b: link becomes ready
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:41.112855-04:00 storagenodet3500 kernel: [755882.623996] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:41.108900-04:00 storagenodet3500 kernel: [755882.619066] device veth13d1051b entered promiscuous mode
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:41.108899-04:00 storagenodet3500 kernel: [755882.618991] cni0: port 3(veth13d1051b) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:41.108879-04:00 storagenodet3500 kernel: [755882.618984] cni0: port 3(veth13d1051b) entered blocking state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:38.308992-04:00 storagenodet3500 kernel: [755879.820114] cni0: port 3(vethe9a5edfe) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:38.308989-04:00 storagenodet3500 kernel: [755879.820110] device vethe9a5edfe left promiscuous mode
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:38.308965-04:00 storagenodet3500 kernel: [755879.819348] cni0: port 3(vethe9a5edfe) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:33.876913-04:00 storagenodet3500 kernel: [755875.387952] cni0: port 3(vethe9a5edfe) entered forwarding state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:33.876912-04:00 storagenodet3500 kernel: [755875.387948] cni0: port 3(vethe9a5edfe) entered blocking state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:33.876909-04:00 storagenodet3500 kernel: [755875.387913] IPv6: ADDRCONF(NETDEV_CHANGE): vethe9a5edfe: link becomes ready
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:33.876895-04:00 storagenodet3500 kernel: [755875.387849] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:33.872889-04:00 storagenodet3500 kernel: [755875.382583] device vethe9a5edfe entered promiscuous mode
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:33.872886-04:00 storagenodet3500 kernel: [755875.382497] cni0: port 3(vethe9a5edfe) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:33.872863-04:00 storagenodet3500 kernel: [755875.382489] cni0: port 3(vethe9a5edfe) entered blocking state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:22.988982-04:00 storagenodet3500 kernel: [755864.500317] cni0: port 3(veth438965f9) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:22.988979-04:00 storagenodet3500 kernel: [755864.500312] device veth438965f9 left promiscuous mode
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:41:22.988954-04:00 storagenodet3500 kernel: [755864.499471] cni0: port 3(veth438965f9) entered disabled state
2025-10-10 13:58:14.578	
system-logs
2025-10-10T10:40:47.008901-04:00 storagenodet3500 kernel: [755828.517222] cni0: port 3(veth438965f9) entered forwarding state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:40:47.008899-04:00 storagenodet3500 kernel: [755828.517218] cni0: port 3(veth438965f9) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:40:47.008897-04:00 storagenodet3500 kernel: [755828.517175] IPv6: ADDRCONF(NETDEV_CHANGE): veth438965f9: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:40:47.008884-04:00 storagenodet3500 kernel: [755828.517116] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:40:47.000885-04:00 storagenodet3500 kernel: [755828.511676] device veth438965f9 entered promiscuous mode
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:40:47.000881-04:00 storagenodet3500 kernel: [755828.511602] cni0: port 3(veth438965f9) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:40:47.000856-04:00 storagenodet3500 kernel: [755828.511595] cni0: port 3(veth438965f9) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:29:55.472914-04:00 storagenodet3500 kernel: [755176.976689] cni0: port 2(vethcedcc170) entered forwarding state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:29:55.472912-04:00 storagenodet3500 kernel: [755176.976685] cni0: port 2(vethcedcc170) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:29:55.472910-04:00 storagenodet3500 kernel: [755176.976652] IPv6: ADDRCONF(NETDEV_CHANGE): vethcedcc170: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:29:55.472895-04:00 storagenodet3500 kernel: [755176.976589] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:29:55.468867-04:00 storagenodet3500 kernel: [755176.971266] device vethcedcc170 entered promiscuous mode
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:29:55.468865-04:00 storagenodet3500 kernel: [755176.971195] cni0: port 2(vethcedcc170) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:29:55.468849-04:00 storagenodet3500 kernel: [755176.971188] cni0: port 2(vethcedcc170) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:28:21.648918-04:00 storagenodet3500 kernel: [755083.153248] cni0: port 1(veth21ce3064) entered forwarding state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:28:21.648917-04:00 storagenodet3500 kernel: [755083.153244] cni0: port 1(veth21ce3064) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:28:21.648914-04:00 storagenodet3500 kernel: [755083.153211] IPv6: ADDRCONF(NETDEV_CHANGE): veth21ce3064: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:28:21.648894-04:00 storagenodet3500 kernel: [755083.153151] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:28:21.644877-04:00 storagenodet3500 kernel: [755083.147912] device veth21ce3064 entered promiscuous mode
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:28:21.644874-04:00 storagenodet3500 kernel: [755083.147819] cni0: port 1(veth21ce3064) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:28:21.644854-04:00 storagenodet3500 kernel: [755083.147810] cni0: port 1(veth21ce3064) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:26:10.132985-04:00 storagenodet3500 kernel: [754951.634284] cni0: port 1(vethc4e152bf) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:26:10.132981-04:00 storagenodet3500 kernel: [754951.634280] device vethc4e152bf left promiscuous mode
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:26:10.132961-04:00 storagenodet3500 kernel: [754951.633490] cni0: port 1(vethc4e152bf) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:26:08.712962-04:00 storagenodet3500 kernel: [754950.215519] cni0: port 2(vethb41cb019) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:26:08.712950-04:00 storagenodet3500 kernel: [754950.215514] device vethb41cb019 left promiscuous mode
2025-10-10 13:58:14.577	
system-logs
2025-10-10T10:26:08.712921-04:00 storagenodet3500 kernel: [754950.214710] cni0: port 2(vethb41cb019) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:17:48.672927-04:00 storagenodet3500 kernel: [704049.736527] cni0: port 2(vethb41cb019) entered forwarding state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:17:48.672926-04:00 storagenodet3500 kernel: [704049.736523] cni0: port 2(vethb41cb019) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:17:48.672922-04:00 storagenodet3500 kernel: [704049.736489] IPv6: ADDRCONF(NETDEV_CHANGE): vethb41cb019: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:17:48.672908-04:00 storagenodet3500 kernel: [704049.736424] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:17:48.664893-04:00 storagenodet3500 kernel: [704049.731051] device vethb41cb019 entered promiscuous mode
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:17:48.664889-04:00 storagenodet3500 kernel: [704049.730982] cni0: port 2(vethb41cb019) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:17:48.664863-04:00 storagenodet3500 kernel: [704049.730975] cni0: port 2(vethb41cb019) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:16:14.696890-04:00 storagenodet3500 kernel: [703955.761425] cni0: port 1(vethc4e152bf) entered forwarding state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:16:14.696888-04:00 storagenodet3500 kernel: [703955.761422] cni0: port 1(vethc4e152bf) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:16:14.696886-04:00 storagenodet3500 kernel: [703955.761384] IPv6: ADDRCONF(NETDEV_CHANGE): vethc4e152bf: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:16:14.696860-04:00 storagenodet3500 kernel: [703955.761323] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:16:14.692890-04:00 storagenodet3500 kernel: [703955.755310] device vethc4e152bf entered promiscuous mode
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:16:14.692887-04:00 storagenodet3500 kernel: [703955.755203] cni0: port 1(vethc4e152bf) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:16:14.692863-04:00 storagenodet3500 kernel: [703955.755195] cni0: port 1(vethc4e152bf) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:14:05.349084-04:00 storagenodet3500 kernel: [703826.411613] cni0: port 1(veth75ab7e85) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:14:05.349081-04:00 storagenodet3500 kernel: [703826.411609] device veth75ab7e85 left promiscuous mode
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:14:05.349060-04:00 storagenodet3500 kernel: [703826.410739] cni0: port 1(veth75ab7e85) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:14:04.100917-04:00 storagenodet3500 kernel: [703825.163532] cni0: port 2(vetha460a52c) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:14:04.100913-04:00 storagenodet3500 kernel: [703825.163527] device vetha460a52c left promiscuous mode
2025-10-10 13:58:14.577	
system-logs
2025-10-09T20:14:04.100886-04:00 storagenodet3500 kernel: [703825.162736] cni0: port 2(vetha460a52c) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:25:46.552887-04:00 storagenodet3500 kernel: [697327.556889] cni0: port 2(vetha460a52c) entered forwarding state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:25:46.552886-04:00 storagenodet3500 kernel: [697327.556884] cni0: port 2(vetha460a52c) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:25:46.552884-04:00 storagenodet3500 kernel: [697327.556841] IPv6: ADDRCONF(NETDEV_CHANGE): vetha460a52c: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:25:46.552872-04:00 storagenodet3500 kernel: [697327.556768] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:25:46.544891-04:00 storagenodet3500 kernel: [697327.551587] device vetha460a52c entered promiscuous mode
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:25:46.544888-04:00 storagenodet3500 kernel: [697327.551517] cni0: port 2(vetha460a52c) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:25:46.544865-04:00 storagenodet3500 kernel: [697327.551510] cni0: port 2(vetha460a52c) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:24:12.140890-04:00 storagenodet3500 kernel: [697233.145464] cni0: port 1(veth75ab7e85) entered forwarding state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:24:12.140889-04:00 storagenodet3500 kernel: [697233.145458] cni0: port 1(veth75ab7e85) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:24:12.140887-04:00 storagenodet3500 kernel: [697233.145410] IPv6: ADDRCONF(NETDEV_CHANGE): veth75ab7e85: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:24:12.140866-04:00 storagenodet3500 kernel: [697233.145333] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:24:12.136876-04:00 storagenodet3500 kernel: [697233.140091] device veth75ab7e85 entered promiscuous mode
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:24:12.136873-04:00 storagenodet3500 kernel: [697233.139967] cni0: port 1(veth75ab7e85) entered disabled state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:24:12.136850-04:00 storagenodet3500 kernel: [697233.139959] cni0: port 1(veth75ab7e85) entered blocking state
2025-10-10 13:58:14.577	
system-logs
2025-10-09T18:21:59.748991-04:00 storagenodet3500 kernel: [697100.753615] cni0: port 1(vethcb9d8b9e) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T18:21:59.748988-04:00 storagenodet3500 kernel: [697100.753610] device vethcb9d8b9e left promiscuous mode
2025-10-10 13:58:14.576	
system-logs
2025-10-09T18:21:59.748969-04:00 storagenodet3500 kernel: [697100.752917] cni0: port 1(vethcb9d8b9e) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T18:21:58.225078-04:00 storagenodet3500 kernel: [697099.229222] cni0: port 2(vethbfa6eb8f) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T18:21:58.225066-04:00 storagenodet3500 kernel: [697099.229218] device vethbfa6eb8f left promiscuous mode
2025-10-10 13:58:14.576	
system-logs
2025-10-09T18:21:58.225042-04:00 storagenodet3500 kernel: [697099.228454] cni0: port 2(vethbfa6eb8f) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:34:06.368947-04:00 storagenodet3500 kernel: [694227.347599] cni0: port 2(vethbfa6eb8f) entered forwarding state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:34:06.368945-04:00 storagenodet3500 kernel: [694227.347594] cni0: port 2(vethbfa6eb8f) entered blocking state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:34:06.368942-04:00 storagenodet3500 kernel: [694227.347549] IPv6: ADDRCONF(NETDEV_CHANGE): vethbfa6eb8f: link becomes ready
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:34:06.368918-04:00 storagenodet3500 kernel: [694227.347471] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:34:06.364866-04:00 storagenodet3500 kernel: [694227.342177] device vethbfa6eb8f entered promiscuous mode
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:34:06.364864-04:00 storagenodet3500 kernel: [694227.342048] cni0: port 2(vethbfa6eb8f) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:34:06.364849-04:00 storagenodet3500 kernel: [694227.342040] cni0: port 2(vethbfa6eb8f) entered blocking state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:32:32.092881-04:00 storagenodet3500 kernel: [694133.072863] cni0: port 1(vethcb9d8b9e) entered forwarding state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:32:32.092880-04:00 storagenodet3500 kernel: [694133.072859] cni0: port 1(vethcb9d8b9e) entered blocking state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:32:32.092877-04:00 storagenodet3500 kernel: [694133.072816] IPv6: ADDRCONF(NETDEV_CHANGE): vethcb9d8b9e: link becomes ready
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:32:32.092855-04:00 storagenodet3500 kernel: [694133.072745] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:32:32.088883-04:00 storagenodet3500 kernel: [694133.067753] device vethcb9d8b9e entered promiscuous mode
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:32:32.088881-04:00 storagenodet3500 kernel: [694133.067686] cni0: port 1(vethcb9d8b9e) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:32:32.088858-04:00 storagenodet3500 kernel: [694133.067679] cni0: port 1(vethcb9d8b9e) entered blocking state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:30:19.152880-04:00 storagenodet3500 kernel: [694000.132034] cni0: port 1(vethe64559dc) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:30:19.152877-04:00 storagenodet3500 kernel: [694000.132029] device vethe64559dc left promiscuous mode
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:30:19.152850-04:00 storagenodet3500 kernel: [694000.131269] cni0: port 1(vethe64559dc) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:30:17.957007-04:00 storagenodet3500 kernel: [693998.934677] cni0: port 2(veth8c88dfd6) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:30:17.957005-04:00 storagenodet3500 kernel: [693998.934673] device veth8c88dfd6 left promiscuous mode
2025-10-10 13:58:14.576	
system-logs
2025-10-09T17:30:17.956982-04:00 storagenodet3500 kernel: [693998.933960] cni0: port 2(veth8c88dfd6) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677656-04:00 storagenodet3500 kernel: [690137.721089] audit: type=1400 audit(1760041556.775:13): apparmor="STATUS" operation="profile_load" profile="unconfined" name="/usr/sbin/chronyd" pid=1971801 comm="apparmor_parser"
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677655-04:00 storagenodet3500 kernel: [689528.362196] cni0: port 2(veth8c88dfd6) entered forwarding state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677655-04:00 storagenodet3500 kernel: [689528.362190] cni0: port 2(veth8c88dfd6) entered blocking state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677654-04:00 storagenodet3500 kernel: [689528.362146] IPv6: ADDRCONF(NETDEV_CHANGE): veth8c88dfd6: link becomes ready
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677653-04:00 storagenodet3500 kernel: [689528.362057] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677652-04:00 storagenodet3500 kernel: [689528.357187] device veth8c88dfd6 entered promiscuous mode
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677651-04:00 storagenodet3500 kernel: [689528.357102] cni0: port 2(veth8c88dfd6) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677641-04:00 storagenodet3500 kernel: [689528.357093] cni0: port 2(veth8c88dfd6) entered blocking state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677640-04:00 storagenodet3500 kernel: [689434.783132] cni0: port 1(vethe64559dc) entered forwarding state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677638-04:00 storagenodet3500 kernel: [689434.783129] cni0: port 1(vethe64559dc) entered blocking state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677637-04:00 storagenodet3500 kernel: [689434.783093] IPv6: ADDRCONF(NETDEV_CHANGE): vethe64559dc: link becomes ready
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677636-04:00 storagenodet3500 kernel: [689434.783025] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677634-04:00 storagenodet3500 kernel: [689434.777720] device vethe64559dc entered promiscuous mode
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677633-04:00 storagenodet3500 kernel: [689434.777652] cni0: port 1(vethe64559dc) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677632-04:00 storagenodet3500 kernel: [689434.777645] cni0: port 1(vethe64559dc) entered blocking state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677631-04:00 storagenodet3500 kernel: [689303.430697] cni0: port 1(veth4c29d80e) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677629-04:00 storagenodet3500 kernel: [689303.430693] device veth4c29d80e left promiscuous mode
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677628-04:00 storagenodet3500 kernel: [689303.430144] cni0: port 1(veth4c29d80e) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677626-04:00 storagenodet3500 kernel: [689302.236623] cni0: port 2(veth3751ead8) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677625-04:00 storagenodet3500 kernel: [689302.236618] device veth3751ead8 left promiscuous mode
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677624-04:00 storagenodet3500 kernel: [689302.235861] cni0: port 2(veth3751ead8) entered disabled state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677622-04:00 storagenodet3500 kernel: [680584.014913] cni0: port 2(veth3751ead8) entered forwarding state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677590-04:00 storagenodet3500 kernel: [680584.014908] cni0: port 2(veth3751ead8) entered blocking state
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677590-04:00 storagenodet3500 kernel: [680584.014860] IPv6: ADDRCONF(NETDEV_CHANGE): veth3751ead8: link becomes ready
2025-10-10 13:58:14.576	
system-logs
2025-10-09T16:27:58.677589-04:00 storagenodet3500 kernel: [680584.014779] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677587-04:00 storagenodet3500 kernel: [680584.008710] device veth3751ead8 entered promiscuous mode
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677581-04:00 storagenodet3500 kernel: [680584.008636] cni0: port 2(veth3751ead8) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677580-04:00 storagenodet3500 kernel: [680584.008628] cni0: port 2(veth3751ead8) entered blocking state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677580-04:00 storagenodet3500 kernel: [680490.402095] cni0: port 1(veth4c29d80e) entered forwarding state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677579-04:00 storagenodet3500 kernel: [680490.402092] cni0: port 1(veth4c29d80e) entered blocking state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677578-04:00 storagenodet3500 kernel: [680490.402057] IPv6: ADDRCONF(NETDEV_CHANGE): veth4c29d80e: link becomes ready
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677576-04:00 storagenodet3500 kernel: [680490.401994] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677570-04:00 storagenodet3500 kernel: [680490.396823] device veth4c29d80e entered promiscuous mode
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677569-04:00 storagenodet3500 kernel: [680490.396730] cni0: port 1(veth4c29d80e) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677569-04:00 storagenodet3500 kernel: [680490.396722] cni0: port 1(veth4c29d80e) entered blocking state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677568-04:00 storagenodet3500 kernel: [680359.730311] cni0: port 1(vethd9823845) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677568-04:00 storagenodet3500 kernel: [680359.730307] device vethd9823845 left promiscuous mode
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677567-04:00 storagenodet3500 kernel: [680359.729540] cni0: port 1(vethd9823845) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677565-04:00 storagenodet3500 kernel: [680358.622580] cni0: port 2(veth48ee1110) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677555-04:00 storagenodet3500 kernel: [680358.622575] device veth48ee1110 left promiscuous mode
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677554-04:00 storagenodet3500 kernel: [680358.621789] cni0: port 2(veth48ee1110) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677553-04:00 storagenodet3500 kernel: [679675.589578] cni0: port 2(veth48ee1110) entered forwarding state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677552-04:00 storagenodet3500 kernel: [679675.589573] cni0: port 2(veth48ee1110) entered blocking state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677551-04:00 storagenodet3500 kernel: [679675.589528] IPv6: ADDRCONF(NETDEV_CHANGE): veth48ee1110: link becomes ready
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677548-04:00 storagenodet3500 kernel: [679675.589467] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677540-04:00 storagenodet3500 kernel: [679675.584201] device veth48ee1110 entered promiscuous mode
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677539-04:00 storagenodet3500 kernel: [679675.584058] cni0: port 2(veth48ee1110) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677539-04:00 storagenodet3500 kernel: [679675.584049] cni0: port 2(veth48ee1110) entered blocking state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677538-04:00 storagenodet3500 kernel: [679581.976489] cni0: port 1(vethd9823845) entered forwarding state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677538-04:00 storagenodet3500 kernel: [679581.976484] cni0: port 1(vethd9823845) entered blocking state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677537-04:00 storagenodet3500 kernel: [679581.976431] IPv6: ADDRCONF(NETDEV_CHANGE): vethd9823845: link becomes ready
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677535-04:00 storagenodet3500 kernel: [679581.976346] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677529-04:00 storagenodet3500 kernel: [679581.970723] device vethd9823845 entered promiscuous mode
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677528-04:00 storagenodet3500 kernel: [679581.970619] cni0: port 1(vethd9823845) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677528-04:00 storagenodet3500 kernel: [679581.970612] cni0: port 1(vethd9823845) entered blocking state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677527-04:00 storagenodet3500 kernel: [679451.054663] cni0: port 1(vethfb917e9f) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677527-04:00 storagenodet3500 kernel: [679451.054658] device vethfb917e9f left promiscuous mode
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677525-04:00 storagenodet3500 kernel: [679451.053982] cni0: port 1(vethfb917e9f) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677518-04:00 storagenodet3500 kernel: [679449.939217] cni0: port 2(veth9ecd8f67) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677518-04:00 storagenodet3500 kernel: [679449.939212] device veth9ecd8f67 left promiscuous mode
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677517-04:00 storagenodet3500 kernel: [679449.938432] cni0: port 2(veth9ecd8f67) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677517-04:00 storagenodet3500 kernel: [675508.441426] cni0: port 2(veth9ecd8f67) entered forwarding state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677516-04:00 storagenodet3500 kernel: [675508.441421] cni0: port 2(veth9ecd8f67) entered blocking state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677514-04:00 storagenodet3500 kernel: [675508.441379] IPv6: ADDRCONF(NETDEV_CHANGE): veth9ecd8f67: link becomes ready
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677508-04:00 storagenodet3500 kernel: [675508.441308] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677507-04:00 storagenodet3500 kernel: [675508.436203] device veth9ecd8f67 entered promiscuous mode
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677507-04:00 storagenodet3500 kernel: [675508.436137] cni0: port 2(veth9ecd8f67) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677506-04:00 storagenodet3500 kernel: [675508.436130] cni0: port 2(veth9ecd8f67) entered blocking state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677506-04:00 storagenodet3500 kernel: [675414.828449] cni0: port 1(vethfb917e9f) entered forwarding state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677505-04:00 storagenodet3500 kernel: [675414.828443] cni0: port 1(vethfb917e9f) entered blocking state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677503-04:00 storagenodet3500 kernel: [675414.828392] IPv6: ADDRCONF(NETDEV_CHANGE): vethfb917e9f: link becomes ready
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677497-04:00 storagenodet3500 kernel: [675414.828313] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677496-04:00 storagenodet3500 kernel: [675414.822746] device vethfb917e9f entered promiscuous mode
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677496-04:00 storagenodet3500 kernel: [675414.822657] cni0: port 1(vethfb917e9f) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677495-04:00 storagenodet3500 kernel: [675414.822649] cni0: port 1(vethfb917e9f) entered blocking state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677495-04:00 storagenodet3500 kernel: [675285.261357] cni0: port 1(vethae204673) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677493-04:00 storagenodet3500 kernel: [675285.261352] device vethae204673 left promiscuous mode
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677485-04:00 storagenodet3500 kernel: [675285.260675] cni0: port 1(vethae204673) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677484-04:00 storagenodet3500 kernel: [675284.113353] cni0: port 2(veth4ab27d42) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677484-04:00 storagenodet3500 kernel: [675284.113349] device veth4ab27d42 left promiscuous mode
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677483-04:00 storagenodet3500 kernel: [675284.112641] cni0: port 2(veth4ab27d42) entered disabled state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677483-04:00 storagenodet3500 kernel: [674320.531746] cni0: port 2(veth4ab27d42) entered forwarding state
2025-10-10 13:58:14.575	
system-logs
2025-10-09T16:27:58.677482-04:00 storagenodet3500 kernel: [674320.531743] cni0: port 2(veth4ab27d42) entered blocking state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677479-04:00 storagenodet3500 kernel: [674320.531710] IPv6: ADDRCONF(NETDEV_CHANGE): veth4ab27d42: link becomes ready
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677465-04:00 storagenodet3500 kernel: [674320.531644] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677464-04:00 storagenodet3500 kernel: [674320.526418] device veth4ab27d42 entered promiscuous mode
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677463-04:00 storagenodet3500 kernel: [674320.526350] cni0: port 2(veth4ab27d42) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677462-04:00 storagenodet3500 kernel: [674320.526342] cni0: port 2(veth4ab27d42) entered blocking state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677461-04:00 storagenodet3500 kernel: [674226.877333] cni0: port 1(vethae204673) entered forwarding state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677460-04:00 storagenodet3500 kernel: [674226.877330] cni0: port 1(vethae204673) entered blocking state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677459-04:00 storagenodet3500 kernel: [674226.877296] IPv6: ADDRCONF(NETDEV_CHANGE): vethae204673: link becomes ready
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677458-04:00 storagenodet3500 kernel: [674226.877231] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677458-04:00 storagenodet3500 kernel: [674226.871726] device vethae204673 entered promiscuous mode
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677457-04:00 storagenodet3500 kernel: [674226.871600] cni0: port 1(vethae204673) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677457-04:00 storagenodet3500 kernel: [674226.871592] cni0: port 1(vethae204673) entered blocking state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677456-04:00 storagenodet3500 kernel: [674098.511093] cni0: port 1(vethb00ba8f8) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677455-04:00 storagenodet3500 kernel: [674098.511088] device vethb00ba8f8 left promiscuous mode
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677455-04:00 storagenodet3500 kernel: [674098.510714] cni0: port 1(vethb00ba8f8) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677454-04:00 storagenodet3500 kernel: [674097.327359] cni0: port 2(vethab9d742f) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677454-04:00 storagenodet3500 kernel: [674097.327354] device vethab9d742f left promiscuous mode
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677453-04:00 storagenodet3500 kernel: [674097.326665] cni0: port 2(vethab9d742f) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677453-04:00 storagenodet3500 kernel: [670360.092032] cni0: port 2(vethab9d742f) entered forwarding state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677452-04:00 storagenodet3500 kernel: [670360.092028] cni0: port 2(vethab9d742f) entered blocking state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677451-04:00 storagenodet3500 kernel: [670360.091983] IPv6: ADDRCONF(NETDEV_CHANGE): vethab9d742f: link becomes ready
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677451-04:00 storagenodet3500 kernel: [670360.091893] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677450-04:00 storagenodet3500 kernel: [670360.086523] device vethab9d742f entered promiscuous mode
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677449-04:00 storagenodet3500 kernel: [670360.086428] cni0: port 2(vethab9d742f) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677449-04:00 storagenodet3500 kernel: [670360.086420] cni0: port 2(vethab9d742f) entered blocking state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677448-04:00 storagenodet3500 kernel: [670266.412444] cni0: port 1(vethb00ba8f8) entered forwarding state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677447-04:00 storagenodet3500 kernel: [670266.412439] cni0: port 1(vethb00ba8f8) entered blocking state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677447-04:00 storagenodet3500 kernel: [670266.412393] IPv6: ADDRCONF(NETDEV_CHANGE): vethb00ba8f8: link becomes ready
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677446-04:00 storagenodet3500 kernel: [670266.412322] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677446-04:00 storagenodet3500 kernel: [670266.407074] device vethb00ba8f8 entered promiscuous mode
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677445-04:00 storagenodet3500 kernel: [670266.406989] cni0: port 1(vethb00ba8f8) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677444-04:00 storagenodet3500 kernel: [670266.406981] cni0: port 1(vethb00ba8f8) entered blocking state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677444-04:00 storagenodet3500 kernel: [670135.900539] cni0: port 1(veth0afba758) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677443-04:00 storagenodet3500 kernel: [670135.900535] device veth0afba758 left promiscuous mode
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677443-04:00 storagenodet3500 kernel: [670135.899771] cni0: port 1(veth0afba758) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677442-04:00 storagenodet3500 kernel: [670134.896164] cni0: port 2(veth6c6cf25e) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677441-04:00 storagenodet3500 kernel: [670134.896160] device veth6c6cf25e left promiscuous mode
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677441-04:00 storagenodet3500 kernel: [670134.895182] cni0: port 2(veth6c6cf25e) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677439-04:00 storagenodet3500 kernel: [664414.199888] cni0: port 2(veth6c6cf25e) entered forwarding state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677362-04:00 storagenodet3500 kernel: [664414.199884] cni0: port 2(veth6c6cf25e) entered blocking state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677361-04:00 storagenodet3500 kernel: [664414.199839] IPv6: ADDRCONF(NETDEV_CHANGE): veth6c6cf25e: link becomes ready
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677361-04:00 storagenodet3500 kernel: [664414.199769] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677360-04:00 storagenodet3500 kernel: [664414.194719] device veth6c6cf25e entered promiscuous mode
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677359-04:00 storagenodet3500 kernel: [664414.194622] cni0: port 2(veth6c6cf25e) entered disabled state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677359-04:00 storagenodet3500 kernel: [664414.194614] cni0: port 2(veth6c6cf25e) entered blocking state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677358-04:00 storagenodet3500 kernel: [664321.984938] cni0: port 1(veth0afba758) entered forwarding state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677357-04:00 storagenodet3500 kernel: [664321.984933] cni0: port 1(veth0afba758) entered blocking state
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677357-04:00 storagenodet3500 kernel: [664321.984819] IPv6: ADDRCONF(NETDEV_CHANGE): veth0afba758: link becomes ready
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677356-04:00 storagenodet3500 kernel: [664321.984739] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677356-04:00 storagenodet3500 kernel: [664321.979245] device veth0afba758 entered promiscuous mode
2025-10-10 13:58:14.574	
system-logs
2025-10-09T16:27:58.677355-04:00 storagenodet3500 kernel: [664321.979156] cni0: port 1(veth0afba758) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677354-04:00 storagenodet3500 kernel: [664321.979148] cni0: port 1(veth0afba758) entered blocking state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677354-04:00 storagenodet3500 kernel: [664193.117832] cni0: port 1(veth19419f8f) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677353-04:00 storagenodet3500 kernel: [664193.117828] device veth19419f8f left promiscuous mode
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677352-04:00 storagenodet3500 kernel: [664193.117105] cni0: port 1(veth19419f8f) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677352-04:00 storagenodet3500 kernel: [664192.149007] cni0: port 2(veth485d35b2) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677351-04:00 storagenodet3500 kernel: [664192.149002] device veth485d35b2 left promiscuous mode
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677351-04:00 storagenodet3500 kernel: [664192.148224] cni0: port 2(veth485d35b2) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677350-04:00 storagenodet3500 kernel: [661263.283859] cni0: port 2(veth485d35b2) entered forwarding state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677349-04:00 storagenodet3500 kernel: [661263.283855] cni0: port 2(veth485d35b2) entered blocking state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677348-04:00 storagenodet3500 kernel: [661263.283809] IPv6: ADDRCONF(NETDEV_CHANGE): veth485d35b2: link becomes ready
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677344-04:00 storagenodet3500 kernel: [661263.283718] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677338-04:00 storagenodet3500 kernel: [661263.278572] device veth485d35b2 entered promiscuous mode
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677338-04:00 storagenodet3500 kernel: [661263.278506] cni0: port 2(veth485d35b2) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677337-04:00 storagenodet3500 kernel: [661263.278499] cni0: port 2(veth485d35b2) entered blocking state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677336-04:00 storagenodet3500 kernel: [661170.736335] cni0: port 1(veth19419f8f) entered forwarding state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677336-04:00 storagenodet3500 kernel: [661170.736331] cni0: port 1(veth19419f8f) entered blocking state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677335-04:00 storagenodet3500 kernel: [661170.736299] IPv6: ADDRCONF(NETDEV_CHANGE): veth19419f8f: link becomes ready
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677335-04:00 storagenodet3500 kernel: [661170.736239] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677334-04:00 storagenodet3500 kernel: [661170.730793] device veth19419f8f entered promiscuous mode
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677333-04:00 storagenodet3500 kernel: [661170.730725] cni0: port 1(veth19419f8f) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677333-04:00 storagenodet3500 kernel: [661170.730718] cni0: port 1(veth19419f8f) entered blocking state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677332-04:00 storagenodet3500 kernel: [660675.192467] cni0: port 1(veth692221fd) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677331-04:00 storagenodet3500 kernel: [660675.192462] device veth692221fd left promiscuous mode
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677331-04:00 storagenodet3500 kernel: [660675.191372] cni0: port 1(veth692221fd) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677330-04:00 storagenodet3500 kernel: [660673.722179] cni0: port 2(veth5e0a2116) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677330-04:00 storagenodet3500 kernel: [660673.722175] device veth5e0a2116 left promiscuous mode
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677329-04:00 storagenodet3500 kernel: [660673.721394] cni0: port 2(veth5e0a2116) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677328-04:00 storagenodet3500 kernel: [658990.581371] cni0: port 2(veth5e0a2116) entered forwarding state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677327-04:00 storagenodet3500 kernel: [658990.581366] cni0: port 2(veth5e0a2116) entered blocking state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677327-04:00 storagenodet3500 kernel: [658990.581325] IPv6: ADDRCONF(NETDEV_CHANGE): veth5e0a2116: link becomes ready
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677326-04:00 storagenodet3500 kernel: [658990.581233] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677326-04:00 storagenodet3500 kernel: [658990.575911] device veth5e0a2116 entered promiscuous mode
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677324-04:00 storagenodet3500 kernel: [658990.575841] cni0: port 2(veth5e0a2116) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677322-04:00 storagenodet3500 kernel: [658990.575834] cni0: port 2(veth5e0a2116) entered blocking state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677321-04:00 storagenodet3500 kernel: [658468.108022] cni0: port 1(veth692221fd) entered forwarding state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677320-04:00 storagenodet3500 kernel: [658468.108018] cni0: port 1(veth692221fd) entered blocking state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677318-04:00 storagenodet3500 kernel: [658468.107977] IPv6: ADDRCONF(NETDEV_CHANGE): veth692221fd: link becomes ready
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677317-04:00 storagenodet3500 kernel: [658468.107916] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677316-04:00 storagenodet3500 kernel: [658468.103001] device veth692221fd entered promiscuous mode
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677314-04:00 storagenodet3500 kernel: [658468.102872] cni0: port 1(veth692221fd) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677313-04:00 storagenodet3500 kernel: [658468.102864] cni0: port 1(veth692221fd) entered blocking state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677312-04:00 storagenodet3500 kernel: [658337.787009] cni0: port 1(vethcc5f3018) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677311-04:00 storagenodet3500 kernel: [658337.787004] device vethcc5f3018 left promiscuous mode
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677310-04:00 storagenodet3500 kernel: [658337.786293] cni0: port 1(vethcc5f3018) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677309-04:00 storagenodet3500 kernel: [657933.892230] cni0: port 1(vethcc5f3018) entered forwarding state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677308-04:00 storagenodet3500 kernel: [657933.892226] cni0: port 1(vethcc5f3018) entered blocking state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677306-04:00 storagenodet3500 kernel: [657933.892188] IPv6: ADDRCONF(NETDEV_CHANGE): vethcc5f3018: link becomes ready
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677305-04:00 storagenodet3500 kernel: [657933.892121] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677304-04:00 storagenodet3500 kernel: [657933.886768] device vethcc5f3018 entered promiscuous mode
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677303-04:00 storagenodet3500 kernel: [657933.886706] cni0: port 1(vethcc5f3018) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677302-04:00 storagenodet3500 kernel: [657933.886699] cni0: port 1(vethcc5f3018) entered blocking state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677301-04:00 storagenodet3500 kernel: [657801.979734] cni0: port 1(veth2e3caf65) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677300-04:00 storagenodet3500 kernel: [657801.979730] device veth2e3caf65 left promiscuous mode
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677300-04:00 storagenodet3500 kernel: [657801.979100] cni0: port 1(veth2e3caf65) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677299-04:00 storagenodet3500 kernel: [657800.614683] cni0: port 2(vethd76ffe16) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677298-04:00 storagenodet3500 kernel: [657800.614678] device vethd76ffe16 left promiscuous mode
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677298-04:00 storagenodet3500 kernel: [657800.614046] cni0: port 2(vethd76ffe16) entered disabled state
2025-10-10 13:58:14.573	
system-logs
2025-10-09T16:27:58.677297-04:00 storagenodet3500 kernel: [629297.432438] cni0: port 2(vethd76ffe16) entered forwarding state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677296-04:00 storagenodet3500 kernel: [629297.432434] cni0: port 2(vethd76ffe16) entered blocking state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677295-04:00 storagenodet3500 kernel: [629297.432407] IPv6: ADDRCONF(NETDEV_CHANGE): vethd76ffe16: link becomes ready
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677294-04:00 storagenodet3500 kernel: [629297.432332] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677294-04:00 storagenodet3500 kernel: [629297.426715] device vethd76ffe16 entered promiscuous mode
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677293-04:00 storagenodet3500 kernel: [629297.426589] cni0: port 2(vethd76ffe16) entered disabled state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677292-04:00 storagenodet3500 kernel: [629297.426579] cni0: port 2(vethd76ffe16) entered blocking state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677291-04:00 storagenodet3500 kernel: [628774.932687] cni0: port 1(veth2e3caf65) entered forwarding state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677291-04:00 storagenodet3500 kernel: [628774.932683] cni0: port 1(veth2e3caf65) entered blocking state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677290-04:00 storagenodet3500 kernel: [628774.932648] IPv6: ADDRCONF(NETDEV_CHANGE): veth2e3caf65: link becomes ready
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677289-04:00 storagenodet3500 kernel: [628774.932577] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677289-04:00 storagenodet3500 kernel: [628774.927459] device veth2e3caf65 entered promiscuous mode
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677288-04:00 storagenodet3500 kernel: [628774.927370] cni0: port 1(veth2e3caf65) entered disabled state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677286-04:00 storagenodet3500 kernel: [628774.927361] cni0: port 1(veth2e3caf65) entered blocking state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677254-04:00 storagenodet3500 kernel: [628645.690471] cni0: port 1(veth3247d409) entered disabled state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677245-04:00 storagenodet3500 kernel: [628645.690466] device veth3247d409 left promiscuous mode
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677245-04:00 storagenodet3500 kernel: [628645.689818] cni0: port 1(veth3247d409) entered disabled state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677244-04:00 storagenodet3500 kernel: [628644.339622] cni0: port 2(veth8810a15d) entered disabled state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677243-04:00 storagenodet3500 kernel: [628644.339617] device veth8810a15d left promiscuous mode
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677243-04:00 storagenodet3500 kernel: [628644.338898] cni0: port 2(veth8810a15d) entered disabled state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677240-04:00 storagenodet3500 kernel: [628199.302315] cni0: port 2(veth8810a15d) entered forwarding state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677229-04:00 storagenodet3500 kernel: [628199.302311] cni0: port 2(veth8810a15d) entered blocking state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677228-04:00 storagenodet3500 kernel: [628199.302277] IPv6: ADDRCONF(NETDEV_CHANGE): veth8810a15d: link becomes ready
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677227-04:00 storagenodet3500 kernel: [628199.302203] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677226-04:00 storagenodet3500 kernel: [628199.296609] device veth8810a15d entered promiscuous mode
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677225-04:00 storagenodet3500 kernel: [628199.296531] cni0: port 2(veth8810a15d) entered disabled state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677224-04:00 storagenodet3500 kernel: [628199.296524] cni0: port 2(veth8810a15d) entered blocking state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677221-04:00 storagenodet3500 kernel: [627676.792221] cni0: port 1(veth3247d409) entered forwarding state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677210-04:00 storagenodet3500 kernel: [627676.792215] cni0: port 1(veth3247d409) entered blocking state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677209-04:00 storagenodet3500 kernel: [627676.792167] IPv6: ADDRCONF(NETDEV_CHANGE): veth3247d409: link becomes ready
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677208-04:00 storagenodet3500 kernel: [627676.792090] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677207-04:00 storagenodet3500 kernel: [627676.786013] device veth3247d409 entered promiscuous mode
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677206-04:00 storagenodet3500 kernel: [627676.785909] cni0: port 1(veth3247d409) entered disabled state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677203-04:00 storagenodet3500 kernel: [627676.785900] cni0: port 1(veth3247d409) entered blocking state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677194-04:00 storagenodet3500 kernel: [627511.652129] cni0: port 1(veth57ffc9a3) entered disabled state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677192-04:00 storagenodet3500 kernel: [627511.652124] device veth57ffc9a3 left promiscuous mode
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677191-04:00 storagenodet3500 kernel: [627511.651409] cni0: port 1(veth57ffc9a3) entered disabled state
2025-10-10 13:58:14.524	
system-logs
2025-10-09T16:27:58.677191-04:00 storagenodet3500 kernel: [627270.359005] cni0: port 1(veth57ffc9a3) entered forwarding state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677190-04:00 storagenodet3500 kernel: [627270.359001] cni0: port 1(veth57ffc9a3) entered blocking state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677189-04:00 storagenodet3500 kernel: [627270.358968] IPv6: ADDRCONF(NETDEV_CHANGE): veth57ffc9a3: link becomes ready
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677187-04:00 storagenodet3500 kernel: [627270.358904] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677179-04:00 storagenodet3500 kernel: [627270.353889] device veth57ffc9a3 entered promiscuous mode
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677178-04:00 storagenodet3500 kernel: [627270.353818] cni0: port 1(veth57ffc9a3) entered disabled state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677178-04:00 storagenodet3500 kernel: [627270.353809] cni0: port 1(veth57ffc9a3) entered blocking state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677177-04:00 storagenodet3500 kernel: [627137.963682] cni0: port 1(vethfa955c44) entered disabled state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677176-04:00 storagenodet3500 kernel: [627137.963678] device vethfa955c44 left promiscuous mode
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677098-04:00 storagenodet3500 kernel: [627137.962941] cni0: port 1(vethfa955c44) entered disabled state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677084-04:00 storagenodet3500 kernel: [627136.621456] cni0: port 2(veth592fb8c0) entered disabled state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677083-04:00 storagenodet3500 kernel: [627136.621451] device veth592fb8c0 left promiscuous mode
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677083-04:00 storagenodet3500 kernel: [627136.620738] cni0: port 2(veth592fb8c0) entered disabled state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677082-04:00 storagenodet3500 kernel: [626723.654054] cni0: port 3(veth6f038a08) entered disabled state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677081-04:00 storagenodet3500 kernel: [626723.654049] device veth6f038a08 left promiscuous mode
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677080-04:00 storagenodet3500 kernel: [626723.653145] cni0: port 3(veth6f038a08) entered disabled state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677080-04:00 storagenodet3500 kernel: [626719.140592] cni0: port 3(veth6f038a08) entered forwarding state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677079-04:00 storagenodet3500 kernel: [626719.140589] cni0: port 3(veth6f038a08) entered blocking state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677079-04:00 storagenodet3500 kernel: [626719.140558] IPv6: ADDRCONF(NETDEV_CHANGE): veth6f038a08: link becomes ready
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677078-04:00 storagenodet3500 kernel: [626719.140493] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677077-04:00 storagenodet3500 kernel: [626719.135370] device veth6f038a08 entered promiscuous mode
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677077-04:00 storagenodet3500 kernel: [626719.135241] cni0: port 3(veth6f038a08) entered disabled state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677076-04:00 storagenodet3500 kernel: [626719.135232] cni0: port 3(veth6f038a08) entered blocking state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677075-04:00 storagenodet3500 kernel: [626128.844433] cni0: port 2(veth592fb8c0) entered forwarding state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677075-04:00 storagenodet3500 kernel: [626128.844430] cni0: port 2(veth592fb8c0) entered blocking state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677074-04:00 storagenodet3500 kernel: [626128.844402] IPv6: ADDRCONF(NETDEV_CHANGE): veth592fb8c0: link becomes ready
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677073-04:00 storagenodet3500 kernel: [626128.844344] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677073-04:00 storagenodet3500 kernel: [626128.838502] device veth592fb8c0 entered promiscuous mode
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677072-04:00 storagenodet3500 kernel: [626128.838414] cni0: port 2(veth592fb8c0) entered disabled state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677072-04:00 storagenodet3500 kernel: [626128.838406] cni0: port 2(veth592fb8c0) entered blocking state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677071-04:00 storagenodet3500 kernel: [625606.323670] cni0: port 1(vethfa955c44) entered forwarding state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677070-04:00 storagenodet3500 kernel: [625606.323666] cni0: port 1(vethfa955c44) entered blocking state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677070-04:00 storagenodet3500 kernel: [625606.323626] IPv6: ADDRCONF(NETDEV_CHANGE): vethfa955c44: link becomes ready
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677069-04:00 storagenodet3500 kernel: [625606.323565] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677069-04:00 storagenodet3500 kernel: [625606.318309] device vethfa955c44 entered promiscuous mode
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677068-04:00 storagenodet3500 kernel: [625606.318215] cni0: port 1(vethfa955c44) entered disabled state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677067-04:00 storagenodet3500 kernel: [625606.318207] cni0: port 1(vethfa955c44) entered blocking state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677067-04:00 storagenodet3500 kernel: [625475.166069] cni0: port 1(vethde04a31b) entered disabled state
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677066-04:00 storagenodet3500 kernel: [625475.166064] device vethde04a31b left promiscuous mode
2025-10-10 13:58:14.523	
system-logs
2025-10-09T16:27:58.677065-04:00 storagenodet3500 kernel: [625475.165280] cni0: port 1(vethde04a31b) entered disabled state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677065-04:00 storagenodet3500 kernel: [625473.816474] cni0: port 2(vethe4439e85) entered disabled state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677064-04:00 storagenodet3500 kernel: [625473.816470] device vethe4439e85 left promiscuous mode
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677063-04:00 storagenodet3500 kernel: [625473.815760] cni0: port 2(vethe4439e85) entered disabled state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677063-04:00 storagenodet3500 kernel: [623983.745238] cni0: port 3(vethcc583c96) entered disabled state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677062-04:00 storagenodet3500 kernel: [623983.745234] device vethcc583c96 left promiscuous mode
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677061-04:00 storagenodet3500 kernel: [623983.744444] cni0: port 3(vethcc583c96) entered disabled state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677061-04:00 storagenodet3500 kernel: [623978.792846] cni0: port 3(vethcc583c96) entered forwarding state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677060-04:00 storagenodet3500 kernel: [623978.792842] cni0: port 3(vethcc583c96) entered blocking state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677059-04:00 storagenodet3500 kernel: [623978.792812] IPv6: ADDRCONF(NETDEV_CHANGE): vethcc583c96: link becomes ready
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677059-04:00 storagenodet3500 kernel: [623978.792748] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677058-04:00 storagenodet3500 kernel: [623978.787572] device vethcc583c96 entered promiscuous mode
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677058-04:00 storagenodet3500 kernel: [623978.787499] cni0: port 3(vethcc583c96) entered disabled state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677057-04:00 storagenodet3500 kernel: [623978.787492] cni0: port 3(vethcc583c96) entered blocking state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677056-04:00 storagenodet3500 kernel: [623957.647189] cni0: port 3(vethd4d00a73) entered disabled state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677056-04:00 storagenodet3500 kernel: [623957.647185] device vethd4d00a73 left promiscuous mode
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677055-04:00 storagenodet3500 kernel: [623957.646455] cni0: port 3(vethd4d00a73) entered disabled state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677055-04:00 storagenodet3500 kernel: [623953.376632] cni0: port 3(vethd4d00a73) entered forwarding state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677054-04:00 storagenodet3500 kernel: [623953.376628] cni0: port 3(vethd4d00a73) entered blocking state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677053-04:00 storagenodet3500 kernel: [623953.376592] IPv6: ADDRCONF(NETDEV_CHANGE): vethd4d00a73: link becomes ready
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677052-04:00 storagenodet3500 kernel: [623953.376518] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677052-04:00 storagenodet3500 kernel: [623953.371194] device vethd4d00a73 entered promiscuous mode
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677051-04:00 storagenodet3500 kernel: [623953.371095] cni0: port 3(vethd4d00a73) entered disabled state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677051-04:00 storagenodet3500 kernel: [623953.371087] cni0: port 3(vethd4d00a73) entered blocking state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677050-04:00 storagenodet3500 kernel: [623585.759243] cni0: port 2(vethe4439e85) entered forwarding state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677049-04:00 storagenodet3500 kernel: [623585.759239] cni0: port 2(vethe4439e85) entered blocking state
2025-10-10 13:58:14.522	
system-logs
2025-10-09T16:27:58.677049-04:00 storagenodet3500 kernel: [623585.759198] IPv6: ADDRCONF(NETDEV_CHANGE): vethe4439e85: link becomes ready
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677048-04:00 storagenodet3500 kernel: [623585.759132] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677048-04:00 storagenodet3500 kernel: [623585.753576] device vethe4439e85 entered promiscuous mode
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677047-04:00 storagenodet3500 kernel: [623585.753467] cni0: port 2(vethe4439e85) entered disabled state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677046-04:00 storagenodet3500 kernel: [623585.753458] cni0: port 2(vethe4439e85) entered blocking state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677046-04:00 storagenodet3500 kernel: [623568.162481] cni0: port 1(vethde04a31b) entered forwarding state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677045-04:00 storagenodet3500 kernel: [623568.162477] cni0: port 1(vethde04a31b) entered blocking state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677044-04:00 storagenodet3500 kernel: [623568.162443] IPv6: ADDRCONF(NETDEV_CHANGE): vethde04a31b: link becomes ready
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677044-04:00 storagenodet3500 kernel: [623568.162373] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677043-04:00 storagenodet3500 kernel: [623568.156690] device vethde04a31b entered promiscuous mode
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677042-04:00 storagenodet3500 kernel: [623568.156593] cni0: port 1(vethde04a31b) entered disabled state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677042-04:00 storagenodet3500 kernel: [623568.156585] cni0: port 1(vethde04a31b) entered blocking state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677041-04:00 storagenodet3500 kernel: [623436.198734] cni0: port 1(veth1669f4b8) entered disabled state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677041-04:00 storagenodet3500 kernel: [623436.198729] device veth1669f4b8 left promiscuous mode
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677040-04:00 storagenodet3500 kernel: [623436.198212] cni0: port 1(veth1669f4b8) entered disabled state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677039-04:00 storagenodet3500 kernel: [618950.330342] cni0: port 2(veth2fef9975) entered disabled state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677039-04:00 storagenodet3500 kernel: [618950.330337] device veth2fef9975 left promiscuous mode
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677038-04:00 storagenodet3500 kernel: [618950.329473] cni0: port 2(veth2fef9975) entered disabled state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677037-04:00 storagenodet3500 kernel: [618936.535644] cni0: port 3(vethfaae6501) entered disabled state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677037-04:00 storagenodet3500 kernel: [618936.535635] device vethfaae6501 left promiscuous mode
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677036-04:00 storagenodet3500 kernel: [618936.534837] cni0: port 3(vethfaae6501) entered disabled state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677035-04:00 storagenodet3500 kernel: [618931.136824] cni0: port 3(vethfaae6501) entered forwarding state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677035-04:00 storagenodet3500 kernel: [618931.136819] cni0: port 3(vethfaae6501) entered blocking state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677034-04:00 storagenodet3500 kernel: [618931.136778] IPv6: ADDRCONF(NETDEV_CHANGE): vethfaae6501: link becomes ready
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677033-04:00 storagenodet3500 kernel: [618931.136695] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677032-04:00 storagenodet3500 kernel: [618931.131984] device vethfaae6501 entered promiscuous mode
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677032-04:00 storagenodet3500 kernel: [618931.131895] cni0: port 3(vethfaae6501) entered disabled state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677031-04:00 storagenodet3500 kernel: [618931.131888] cni0: port 3(vethfaae6501) entered blocking state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677030-04:00 storagenodet3500 kernel: [618451.740367] cni0: port 2(veth2fef9975) entered forwarding state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677030-04:00 storagenodet3500 kernel: [618451.740364] cni0: port 2(veth2fef9975) entered blocking state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677029-04:00 storagenodet3500 kernel: [618451.740328] IPv6: ADDRCONF(NETDEV_CHANGE): veth2fef9975: link becomes ready
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677028-04:00 storagenodet3500 kernel: [618451.740260] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677028-04:00 storagenodet3500 kernel: [618451.735287] device veth2fef9975 entered promiscuous mode
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677027-04:00 storagenodet3500 kernel: [618451.735220] cni0: port 2(veth2fef9975) entered disabled state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677027-04:00 storagenodet3500 kernel: [618451.735213] cni0: port 2(veth2fef9975) entered blocking state
2025-10-10 13:58:14.480	
system-logs
2025-10-09T16:27:58.677026-04:00 storagenodet3500 kernel: [618432.475068] cni0: port 1(veth1669f4b8) entered forwarding state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677025-04:00 storagenodet3500 kernel: [618432.475063] cni0: port 1(veth1669f4b8) entered blocking state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677025-04:00 storagenodet3500 kernel: [618432.475011] IPv6: ADDRCONF(NETDEV_CHANGE): veth1669f4b8: link becomes ready
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677024-04:00 storagenodet3500 kernel: [618432.474888] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677023-04:00 storagenodet3500 kernel: [618432.469345] device veth1669f4b8 entered promiscuous mode
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677023-04:00 storagenodet3500 kernel: [618432.469243] cni0: port 1(veth1669f4b8) entered disabled state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677022-04:00 storagenodet3500 kernel: [618432.469235] cni0: port 1(veth1669f4b8) entered blocking state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677021-04:00 storagenodet3500 kernel: [618302.964654] cni0: port 1(veth621ff4ac) entered disabled state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677020-04:00 storagenodet3500 kernel: [618302.964648] device veth621ff4ac left promiscuous mode
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677020-04:00 storagenodet3500 kernel: [618302.963892] cni0: port 1(veth621ff4ac) entered disabled state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677019-04:00 storagenodet3500 kernel: [618020.311296] perf: interrupt took too long (50326 > 9815), lowering kernel.perf_event_max_sample_rate to 3750
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677018-04:00 storagenodet3500 kernel: [618020.311275] INFO: NMI handler (perf_event_nmi_handler) took too long to run: 5.585 msecs
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677017-04:00 storagenodet3500 kernel: [613274.449871] cni0: port 1(veth621ff4ac) entered forwarding state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677017-04:00 storagenodet3500 kernel: [613274.449868] cni0: port 1(veth621ff4ac) entered blocking state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677016-04:00 storagenodet3500 kernel: [613274.449834] IPv6: ADDRCONF(NETDEV_CHANGE): veth621ff4ac: link becomes ready
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677015-04:00 storagenodet3500 kernel: [613274.449776] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677015-04:00 storagenodet3500 kernel: [613274.444879] device veth621ff4ac entered promiscuous mode
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677014-04:00 storagenodet3500 kernel: [613274.444811] cni0: port 1(veth621ff4ac) entered disabled state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677013-04:00 storagenodet3500 kernel: [613274.444804] cni0: port 1(veth621ff4ac) entered blocking state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677013-04:00 storagenodet3500 kernel: [613142.526825] cni0: port 1(veth428b7a1a) entered disabled state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677012-04:00 storagenodet3500 kernel: [613142.526820] device veth428b7a1a left promiscuous mode
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.677010-04:00 storagenodet3500 kernel: [613142.526113] cni0: port 1(veth428b7a1a) entered disabled state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676980-04:00 storagenodet3500 kernel: [612897.696079] cni0: port 1(veth428b7a1a) entered forwarding state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676904-04:00 storagenodet3500 kernel: [612897.696075] cni0: port 1(veth428b7a1a) entered blocking state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676903-04:00 storagenodet3500 kernel: [612897.696036] IPv6: ADDRCONF(NETDEV_CHANGE): veth428b7a1a: link becomes ready
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676903-04:00 storagenodet3500 kernel: [612897.695966] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676902-04:00 storagenodet3500 kernel: [612897.690319] device veth428b7a1a entered promiscuous mode
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676902-04:00 storagenodet3500 kernel: [612897.690231] cni0: port 1(veth428b7a1a) entered disabled state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676901-04:00 storagenodet3500 kernel: [612897.690223] cni0: port 1(veth428b7a1a) entered blocking state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676901-04:00 storagenodet3500 kernel: [602629.060918] cni0: port 1(veth37dc049d) entered disabled state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676900-04:00 storagenodet3500 kernel: [602629.060913] device veth37dc049d left promiscuous mode
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676899-04:00 storagenodet3500 kernel: [602629.060206] cni0: port 1(veth37dc049d) entered disabled state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676899-04:00 storagenodet3500 kernel: [602624.490256] cni0: port 1(veth37dc049d) entered forwarding state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676898-04:00 storagenodet3500 kernel: [602624.490251] cni0: port 1(veth37dc049d) entered blocking state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676898-04:00 storagenodet3500 kernel: [602624.490205] IPv6: ADDRCONF(NETDEV_CHANGE): veth37dc049d: link becomes ready
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676897-04:00 storagenodet3500 kernel: [602624.490128] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676896-04:00 storagenodet3500 kernel: [602624.485212] device veth37dc049d entered promiscuous mode
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676896-04:00 storagenodet3500 kernel: [602624.485146] cni0: port 1(veth37dc049d) entered disabled state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676895-04:00 storagenodet3500 kernel: [602624.485139] cni0: port 1(veth37dc049d) entered blocking state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676895-04:00 storagenodet3500 kernel: [602608.904920] cni0: port 1(veth0fc956a1) entered disabled state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676894-04:00 storagenodet3500 kernel: [602608.904915] device veth0fc956a1 left promiscuous mode
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676893-04:00 storagenodet3500 kernel: [602608.904153] cni0: port 1(veth0fc956a1) entered disabled state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676892-04:00 storagenodet3500 kernel: [602604.007228] cni0: port 1(veth0fc956a1) entered forwarding state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676892-04:00 storagenodet3500 kernel: [602604.007224] cni0: port 1(veth0fc956a1) entered blocking state
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676891-04:00 storagenodet3500 kernel: [602604.007188] IPv6: ADDRCONF(NETDEV_CHANGE): veth0fc956a1: link becomes ready
2025-10-10 13:58:14.479	
system-logs
2025-10-09T16:27:58.676891-04:00 storagenodet3500 kernel: [602604.007124] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676890-04:00 storagenodet3500 kernel: [602604.001715] device veth0fc956a1 entered promiscuous mode
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676889-04:00 storagenodet3500 kernel: [602604.001643] cni0: port 1(veth0fc956a1) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676889-04:00 storagenodet3500 kernel: [602604.001635] cni0: port 1(veth0fc956a1) entered blocking state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676888-04:00 storagenodet3500 kernel: [602448.802044] cni0: port 1(veth5bad89a3) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676887-04:00 storagenodet3500 kernel: [602448.802038] device veth5bad89a3 left promiscuous mode
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676887-04:00 storagenodet3500 kernel: [602448.800981] cni0: port 1(veth5bad89a3) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676886-04:00 storagenodet3500 kernel: [601895.640157] cni0: port 2(veth62a83bf5) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676886-04:00 storagenodet3500 kernel: [601895.640152] device veth62a83bf5 left promiscuous mode
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676885-04:00 storagenodet3500 kernel: [601895.639424] cni0: port 2(veth62a83bf5) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676884-04:00 storagenodet3500 kernel: [601891.910529] cni0: port 2(veth62a83bf5) entered forwarding state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676883-04:00 storagenodet3500 kernel: [601891.910525] cni0: port 2(veth62a83bf5) entered blocking state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676883-04:00 storagenodet3500 kernel: [601891.910492] IPv6: ADDRCONF(NETDEV_CHANGE): veth62a83bf5: link becomes ready
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676882-04:00 storagenodet3500 kernel: [601891.910428] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676882-04:00 storagenodet3500 kernel: [601891.905564] device veth62a83bf5 entered promiscuous mode
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676881-04:00 storagenodet3500 kernel: [601891.905498] cni0: port 2(veth62a83bf5) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676880-04:00 storagenodet3500 kernel: [601891.905491] cni0: port 2(veth62a83bf5) entered blocking state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676880-04:00 storagenodet3500 kernel: [601859.761831] cni0: port 1(veth5bad89a3) entered forwarding state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676879-04:00 storagenodet3500 kernel: [601859.761828] cni0: port 1(veth5bad89a3) entered blocking state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676879-04:00 storagenodet3500 kernel: [601859.761796] IPv6: ADDRCONF(NETDEV_CHANGE): veth5bad89a3: link becomes ready
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676878-04:00 storagenodet3500 kernel: [601859.761736] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676877-04:00 storagenodet3500 kernel: [601859.756550] device veth5bad89a3 entered promiscuous mode
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676877-04:00 storagenodet3500 kernel: [601859.756437] cni0: port 1(veth5bad89a3) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676876-04:00 storagenodet3500 kernel: [601859.756429] cni0: port 1(veth5bad89a3) entered blocking state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676875-04:00 storagenodet3500 kernel: [601728.238402] cni0: port 1(veth992b243d) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676875-04:00 storagenodet3500 kernel: [601728.238397] device veth992b243d left promiscuous mode
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676874-04:00 storagenodet3500 kernel: [601728.237632] cni0: port 1(veth992b243d) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676874-04:00 storagenodet3500 kernel: [601726.871096] cni0: port 2(veth300b6555) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676873-04:00 storagenodet3500 kernel: [601726.871090] device veth300b6555 left promiscuous mode
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676872-04:00 storagenodet3500 kernel: [601726.870515] cni0: port 2(veth300b6555) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676872-04:00 storagenodet3500 kernel: [599657.898226] cni0: port 2(veth300b6555) entered forwarding state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676871-04:00 storagenodet3500 kernel: [599657.898223] cni0: port 2(veth300b6555) entered blocking state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676871-04:00 storagenodet3500 kernel: [599657.898185] IPv6: ADDRCONF(NETDEV_CHANGE): veth300b6555: link becomes ready
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676870-04:00 storagenodet3500 kernel: [599657.898111] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676869-04:00 storagenodet3500 kernel: [599657.892714] device veth300b6555 entered promiscuous mode
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676869-04:00 storagenodet3500 kernel: [599657.892649] cni0: port 2(veth300b6555) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676868-04:00 storagenodet3500 kernel: [599657.892642] cni0: port 2(veth300b6555) entered blocking state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676867-04:00 storagenodet3500 kernel: [599655.744994] cni0: port 1(veth992b243d) entered forwarding state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676867-04:00 storagenodet3500 kernel: [599655.744990] cni0: port 1(veth992b243d) entered blocking state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676866-04:00 storagenodet3500 kernel: [599655.744948] IPv6: ADDRCONF(NETDEV_CHANGE): veth992b243d: link becomes ready
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676865-04:00 storagenodet3500 kernel: [599655.744867] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676865-04:00 storagenodet3500 kernel: [599655.739188] device veth992b243d entered promiscuous mode
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676864-04:00 storagenodet3500 kernel: [599655.739080] cni0: port 1(veth992b243d) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676863-04:00 storagenodet3500 kernel: [599655.739072] cni0: port 1(veth992b243d) entered blocking state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676863-04:00 storagenodet3500 kernel: [599526.676570] cni0: port 1(veth1e9a69a9) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676862-04:00 storagenodet3500 kernel: [599526.676565] device veth1e9a69a9 left promiscuous mode
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676862-04:00 storagenodet3500 kernel: [599526.675853] cni0: port 1(veth1e9a69a9) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676861-04:00 storagenodet3500 kernel: [597545.517135] cni0: port 2(veth63d75b67) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676860-04:00 storagenodet3500 kernel: [597545.517130] device veth63d75b67 left promiscuous mode
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676859-04:00 storagenodet3500 kernel: [597545.516445] cni0: port 2(veth63d75b67) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676859-04:00 storagenodet3500 kernel: [597536.536464] cni0: port 3(veth64c13a33) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676858-04:00 storagenodet3500 kernel: [597536.536459] device veth64c13a33 left promiscuous mode
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676858-04:00 storagenodet3500 kernel: [597536.535345] cni0: port 3(veth64c13a33) entered disabled state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676857-04:00 storagenodet3500 kernel: [597531.387703] cni0: port 3(veth64c13a33) entered forwarding state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676856-04:00 storagenodet3500 kernel: [597531.387699] cni0: port 3(veth64c13a33) entered blocking state
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676856-04:00 storagenodet3500 kernel: [597531.387664] IPv6: ADDRCONF(NETDEV_CHANGE): veth64c13a33: link becomes ready
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676855-04:00 storagenodet3500 kernel: [597531.387596] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.478	
system-logs
2025-10-09T16:27:58.676855-04:00 storagenodet3500 kernel: [597531.382324] device veth64c13a33 entered promiscuous mode
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676854-04:00 storagenodet3500 kernel: [597531.382215] cni0: port 3(veth64c13a33) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676853-04:00 storagenodet3500 kernel: [597531.382207] cni0: port 3(veth64c13a33) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676852-04:00 storagenodet3500 kernel: [597128.914677] cni0: port 2(veth63d75b67) entered forwarding state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676852-04:00 storagenodet3500 kernel: [597128.914671] cni0: port 2(veth63d75b67) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676851-04:00 storagenodet3500 kernel: [597128.914616] IPv6: ADDRCONF(NETDEV_CHANGE): veth63d75b67: link becomes ready
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676850-04:00 storagenodet3500 kernel: [597128.914544] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676850-04:00 storagenodet3500 kernel: [597128.891464] device veth63d75b67 entered promiscuous mode
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676849-04:00 storagenodet3500 kernel: [597128.891365] cni0: port 2(veth63d75b67) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676848-04:00 storagenodet3500 kernel: [597128.891358] cni0: port 2(veth63d75b67) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676848-04:00 storagenodet3500 kernel: [597109.057529] cni0: port 1(veth1e9a69a9) entered forwarding state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676847-04:00 storagenodet3500 kernel: [597109.057524] cni0: port 1(veth1e9a69a9) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676846-04:00 storagenodet3500 kernel: [597109.057473] IPv6: ADDRCONF(NETDEV_CHANGE): veth1e9a69a9: link becomes ready
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676846-04:00 storagenodet3500 kernel: [597109.057411] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676845-04:00 storagenodet3500 kernel: [597109.052316] device veth1e9a69a9 entered promiscuous mode
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676844-04:00 storagenodet3500 kernel: [597109.052227] cni0: port 1(veth1e9a69a9) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676844-04:00 storagenodet3500 kernel: [597109.052219] cni0: port 1(veth1e9a69a9) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676843-04:00 storagenodet3500 kernel: [596973.617502] cni0: port 1(veth2f9ca239) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676842-04:00 storagenodet3500 kernel: [596973.617497] device veth2f9ca239 left promiscuous mode
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676841-04:00 storagenodet3500 kernel: [596973.616678] cni0: port 1(veth2f9ca239) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676840-04:00 storagenodet3500 kernel: [596972.395046] cni0: port 2(veth5d1601b5) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676839-04:00 storagenodet3500 kernel: [596972.395041] device veth5d1601b5 left promiscuous mode
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676836-04:00 storagenodet3500 kernel: [596972.394265] cni0: port 2(veth5d1601b5) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676835-04:00 storagenodet3500 kernel: [595322.127652] cni0: port 3(vetha4f02360) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676834-04:00 storagenodet3500 kernel: [595322.127647] device vetha4f02360 left promiscuous mode
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676833-04:00 storagenodet3500 kernel: [595322.127249] cni0: port 3(vetha4f02360) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676832-04:00 storagenodet3500 kernel: [595317.447882] cni0: port 3(vetha4f02360) entered forwarding state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676830-04:00 storagenodet3500 kernel: [595317.447878] cni0: port 3(vetha4f02360) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676816-04:00 storagenodet3500 kernel: [595317.447845] IPv6: ADDRCONF(NETDEV_CHANGE): vetha4f02360: link becomes ready
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676816-04:00 storagenodet3500 kernel: [595317.447775] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676815-04:00 storagenodet3500 kernel: [595317.442215] device vetha4f02360 entered promiscuous mode
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676815-04:00 storagenodet3500 kernel: [595317.442135] cni0: port 3(vetha4f02360) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676814-04:00 storagenodet3500 kernel: [595317.442126] cni0: port 3(vetha4f02360) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676813-04:00 storagenodet3500 kernel: [595299.019608] cni0: port 3(veth696fd02e) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676813-04:00 storagenodet3500 kernel: [595299.019601] device veth696fd02e left promiscuous mode
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676811-04:00 storagenodet3500 kernel: [595299.018718] cni0: port 3(veth696fd02e) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676792-04:00 storagenodet3500 kernel: [595291.343978] cni0: port 3(veth696fd02e) entered forwarding state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676715-04:00 storagenodet3500 kernel: [595291.343975] cni0: port 3(veth696fd02e) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676715-04:00 storagenodet3500 kernel: [595291.343946] IPv6: ADDRCONF(NETDEV_CHANGE): veth696fd02e: link becomes ready
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676714-04:00 storagenodet3500 kernel: [595291.343884] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676714-04:00 storagenodet3500 kernel: [595291.339012] device veth696fd02e entered promiscuous mode
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676713-04:00 storagenodet3500 kernel: [595291.338919] cni0: port 3(veth696fd02e) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676713-04:00 storagenodet3500 kernel: [595291.338912] cni0: port 3(veth696fd02e) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676712-04:00 storagenodet3500 kernel: [594921.418068] cni0: port 2(veth5d1601b5) entered forwarding state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676711-04:00 storagenodet3500 kernel: [594921.418064] cni0: port 2(veth5d1601b5) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676711-04:00 storagenodet3500 kernel: [594921.418020] IPv6: ADDRCONF(NETDEV_CHANGE): veth5d1601b5: link becomes ready
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676710-04:00 storagenodet3500 kernel: [594921.417946] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676710-04:00 storagenodet3500 kernel: [594921.412445] device veth5d1601b5 entered promiscuous mode
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676709-04:00 storagenodet3500 kernel: [594921.412360] cni0: port 2(veth5d1601b5) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676709-04:00 storagenodet3500 kernel: [594921.412353] cni0: port 2(veth5d1601b5) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676708-04:00 storagenodet3500 kernel: [594903.654724] cni0: port 1(veth2f9ca239) entered forwarding state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676707-04:00 storagenodet3500 kernel: [594903.654719] cni0: port 1(veth2f9ca239) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676707-04:00 storagenodet3500 kernel: [594903.654670] IPv6: ADDRCONF(NETDEV_CHANGE): veth2f9ca239: link becomes ready
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676706-04:00 storagenodet3500 kernel: [594903.654594] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676705-04:00 storagenodet3500 kernel: [594903.648730] device veth2f9ca239 entered promiscuous mode
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676705-04:00 storagenodet3500 kernel: [594903.648587] cni0: port 1(veth2f9ca239) entered disabled state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676704-04:00 storagenodet3500 kernel: [594903.648578] cni0: port 1(veth2f9ca239) entered blocking state
2025-10-10 13:58:14.477	
system-logs
2025-10-09T16:27:58.676703-04:00 storagenodet3500 kernel: [594770.702245] cni0: port 1(veth31e08b89) entered disabled state
System Logs (kube-system)
No data
Monitoring Stack Logs
No data
Error Log Rate


3. Network & DNS Performance


Last 1 hour



30s


HTTP Probe Success
No data

HTTP Latency (Seconds)
No data

DNS Query Time
No data

DNS Error Rate
No data

CoreDNS Pod Restarts
No data

Blackbox Probe Failures (Last Hour)
No data

Probe Status Table

No data

4. Node Metrics - Detailed System Monitoring


Last 1 hour



30s


CPU Usage by Node
No data

Memory Usage by Node
No data

Disk Usage by Node
No data

Network Traffic by Node
No data

System Load Average (5m)
No data

OS Distribution by Node

No data

5.Prometheus Metrics & Health


Last 1 hour



30s


Prometheus Targets Up
No data

Total Scrape Targets
No data

Query Duration
No data

Samples Ingested Rate

No data

Scrape Target Status
No data

TSDB Head Series
No data

Storage Size
No data

6. VMStation Kubernetes Cluster Overview


Last 1 hour



30s


Total Nodes
No data

Running Pods
No data

Failed Pods
No data

Node CPU Usage
No data

Node Memory Usage
No data

Node Status

No data