1.
TASK [Display CNI plugin verification results] ************************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "=== Verifying CNI Plugin Installation ===\n✓ bridge plugin installed and executable\n✓ host-local plugin installed and executable\n✓ loopback plugin installed and executable\n✓ flannel plugin installed and executable\n✅ All required CNI plugins verified successfully\n\nAll installed CNI plugins:\ntotal 79484\ndrwxr-xr-x 2 root root     4096 Sep 11 14:40 .\ndrwxr-xr-x 3 root root     4096 Aug 25 22:18 ..\n-rwxr-xr-x 1 root root  4016001 Sep 11  2023 bandwidth\n-rwxr-xr-x 1 root root  4531309 Sep 11  2023 bridge\n-rwxr-xr-x 1 root root 10816051 Sep 11  2023 dhcp\n-rwxr-xr-x 1 root root  4171248 Sep 11  2023 dummy\n-rwxr-xr-x 1 root root  4649749 Sep 11  2023 firewall\n-rwxr-xr-x 1 root root  2907995 Sep 11 14:40 flannel\n-rwxr-xr-x 1 root root  4059321 Sep 11  2023 host-device\n-rwxr-xr-x 1 root root  3444776 Sep 11  2023 host-local\n-rwxr-xr-x 1 root root  4193323 Sep 11  2023 ipvlan\n-rwxr-xr-x 1 root root  3514598 Sep 11  2023 loopback\n-rwxr-xr-x 1 root root  4227193 Sep 11  2023 macvlan\n-rwxr-xr-x 1 root root  3955775 Sep 11  2023 portmap\n-rwxr-xr-x 1 root root  4348835 Sep 11  2023 ptp\n-rwxr-xr-x 1 root root  3716095 Sep 11  2023 sbr\n-rwxr-xr-x 1 root root  2984504 Sep 11  2023 static\n-rwxr-xr-x 1 root root  4258344 Sep 11  2023 tap\n-rwxr-xr-x 1 root root  3603365 Sep 11  2023 tuning\n-rwxr-xr-x 1 root root  4187498 Sep 11  2023 vlan\n-rwxr-xr-x 1 root root  3754911 Sep 11  2023 vrf"
}
ok: [192.168.4.61] => {
    "msg": "=== Verifying CNI Plugin Installation ===\n✓ bridge plugin installed and executable\n✓ host-local plugin installed and executable\n✓ loopback plugin installed and executable\n✓ flannel plugin installed and executable\n✅ All required CNI plugins verified successfully\n\nAll installed CNI plugins:\ntotal 79484\ndrwxr-xr-x 2 root root     4096 Sep 11 19:03 .\ndrwxr-xr-x 3 root root     4096 Sep 11 14:40 ..\n-rwxr-xr-x 1 root root  4016001 May  9  2023 bandwidth\n-rwxr-xr-x 1 root root  4531309 May  9  2023 bridge\n-rwxr-xr-x 1 root root 10816051 May  9  2023 dhcp\n-rwxr-xr-x 1 root root  4171248 May  9  2023 dummy\n-rwxr-xr-x 1 root root  4649749 May  9  2023 firewall\n-rwxr-xr-x 1 root root  2907995 Sep 11 19:03 flannel\n-rwxr-xr-x 1 root root  4059321 May  9  2023 host-device\n-rwxr-xr-x 1 root root  3444776 May  9  2023 host-local\n-rwxr-xr-x 1 root root  4193323 May  9  2023 ipvlan\n-rwxr-xr-x 1 root root  3514598 May  9  2023 loopback\n-rwxr-xr-x 1 root root  4227193 May  9  2023 macvlan\n-rwxr-xr-x 1 root root  3955775 May  9  2023 portmap\n-rwxr-xr-x 1 root root  4348835 May  9  2023 ptp\n-rwxr-xr-x 1 root root  3716095 May  9  2023 sbr\n-rwxr-xr-x 1 root root  2984504 May  9  2023 static\n-rwxr-xr-x 1 root root  4258344 May  9  2023 tap\n-rwxr-xr-x 1 root root  3603365 May  9  2023 tuning\n-rwxr-xr-x 1 root root  4187498 May  9  2023 vlan\n-rwxr-xr-x 1 root root  3754911 May  9  2023 vrf"
}
ok: [192.168.4.62] => {
    "msg": "=== Verifying CNI Plugin Installation ===\n✓ bridge plugin installed and executable\n✓ host-local plugin installed and executable\n✓ loopback plugin installed and executable\n✓ flannel plugin installed and executable\n✅ All required CNI plugins verified successfully\n\nAll installed CNI plugins:\ntotal 57800\ndrwxr-xr-x. 2 root root    4096 Sep 11 21:53 .\ndrwxr-xr-x. 3 root root      17 Sep 11 14:40 ..\n-rwxr-xr-x. 1 root root 2868856 Sep 11  2023 bandwidth\n-rwxr-xr-x. 1 root root 3248936 Sep 11  2023 bridge\n-rwxr-xr-x. 1 root root 8021392 Sep 11  2023 dhcp\n-rwxr-xr-x. 1 root root 2971752 Sep 11  2023 dummy\n-rwxr-xr-x. 1 root root 3333560 Sep 11  2023 firewall\n-rwxr-xr-x. 1 root root 2907995 Sep 11 21:53 flannel\n-rwxr-xr-x. 1 root root 2888056 Sep 11  2023 host-device\n-rwxr-xr-x. 1 root root 2426584 Sep 11  2023 host-local\n-rwxr-xr-x. 1 root root 2989072 Sep 11  2023 ipvlan\n-rwxr-xr-x. 1 root root 2492304 Sep 11  2023 loopback\n-rwxr-xr-x. 1 root root 3015104 Sep 11  2023 macvlan\n-rwxr-xr-x. 1 root root 2820904 Sep 11  2023 portmap\n-rwxr-xr-x. 1 root root 3112536 Sep 11  2023 ptp\n-rwxr-xr-x. 1 root root 2642160 Sep 11  2023 sbr\n-rwxr-xr-x. 1 root root 2158744 Sep 11  2023 static\n-rwxr-xr-x. 1 root root 3035352 Sep 11  2023 tap\n-rwxr-xr-x. 1 root root 2556368 Sep 11  2023 tuning\n-rwxr-xr-x. 1 root root 2984672 Sep 11  2023 vlan\n-rwxr-xr-x. 1 root root 2665880 Sep 11  2023 vrf"
}

PLAY [Comprehensive Worker Node Installation Verification] ************************************************************************************************************************************

TASK [Gathering Facts] ************************************************************************************************************************************************************************
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Check all required Kubernetes services and packages] ************************************************************************************************************************************
changed: [192.168.4.62]
changed: [192.168.4.61]

TASK [Display worker node verification results] ***********************************************************************************************************************************************
ok: [192.168.4.61] => {
    "msg": "=== Worker Node Installation Verification ===\nNode: storagenodeT3500 (192.168.4.61)\nTimestamp: Thu Sep 11 21:57:56 EDT 2025\n\n=== Package Verification ===\n✓ kubelet: Kubernetes v1.29.15\n✓ kubeadm: \n✓ kubectl: \n✓ containerd: containerd github.com/containerd/containerd 1.6.20~ds1 1.6.20~ds1-1+deb12u1\n\n=== Service Unit Verification ===\n✓ kubelet.service unit exists (enabled: enabled)\n✓ containerd.service unit exists (enabled: enabled)\n\n=== CNI Plugin Verification ===\n✓ CNI plugin: bridge\n✓ CNI plugin: host-local\n✓ CNI plugin: loopback\n✓ CNI plugin: flannel\n\n=== Directory Structure Verification ===\n✓ Directory exists: /opt/cni/bin\n✓ Directory exists: /etc/cni/net.d\n✓ Directory exists: /var/lib/kubelet\n✓ Directory exists: /etc/systemd/system/kubelet.service.d\n\n=== Containerd Configuration Verification ===\n✓ containerd config exists\n✓ containerd cgroup driver configured\n\n=== Overall Status ===\n✅ Worker node installation verification PASSED\nNode is ready for cluster join"
}
ok: [192.168.4.62] => {
    "msg": "=== Worker Node Installation Verification ===\nNode: homelab (192.168.4.62)\nTimestamp: Thu 11 Sep 2025 09:57:56 PM EDT\n\n=== Package Verification ===\n✓ kubelet: Kubernetes v1.29.15\n✓ kubeadm: \n✓ kubectl: \n✓ containerd: containerd containerd.io 1.7.27 05044ec0a9a75232cad458027ca83437aae3f4da\n\n=== Service Unit Verification ===\n✓ kubelet.service unit exists (enabled: enabled)\n✓ containerd.service unit exists (enabled: enabled)\n\n=== CNI Plugin Verification ===\n✓ CNI plugin: bridge\n✓ CNI plugin: host-local\n✓ CNI plugin: loopback\n✓ CNI plugin: flannel\n\n=== Directory Structure Verification ===\n✓ Directory exists: /opt/cni/bin\n✓ Directory exists: /etc/cni/net.d\n✓ Directory exists: /var/lib/kubelet\n✓ Directory exists: /etc/systemd/system/kubelet.service.d\n\n=== Containerd Configuration Verification ===\n✓ containerd config exists\n⚠ containerd cgroup driver may not be properly configured\n\n=== Overall Status ===\n✅ Worker node installation verification PASSED\nNode is ready for cluster join"
}

PLAY [Initialize Kubernetes Control Plane] ****************************************************************************************************************************************************

TASK [Gathering Facts] ************************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Check if cluster exists] ****************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Initialize cluster with secure authorization mode] **************************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Initialize cluster with AlwaysAllow fallback (if secure mode failed)] *******************************************************************************************************************
skipping: [192.168.4.63]

TASK [Display authorization mode warning if fallback was used] ********************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "WARNING: Cluster initialized with --authorization-mode=AlwaysAllow\nThis is less secure and should only be used for troubleshooting.\nConsider investigating why Node,RBAC mode failed.\n"
}

TASK [Create .kube directory] *****************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Copy admin.conf] ************************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Open firewall ports for Kubernetes] *****************************************************************************************************************************************************
skipping: [192.168.4.63] => (item=6443/tcp)
skipping: [192.168.4.63] => (item=10250/tcp)
skipping: [192.168.4.63] => (item=10251/tcp)
skipping: [192.168.4.63] => (item=10252/tcp)
skipping: [192.168.4.63] => (item=8472/udp)
skipping: [192.168.4.63]

TASK [Check if Flannel CNI is already installed] **********************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Install Flannel CNI] ********************************************************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Wait for Flannel DaemonSet to be created] ***********************************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Ensure CoreDNS has correct replica count and proper scheduling] *************************************************************************************************************************
changed: [192.168.4.63]

TASK [Check Flannel namespace and resources] **************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Check CNI plugins availability] *********************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Check containerd CNI configuration] *****************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Analyze CNI runtime status] *************************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Reapply Flannel when CNI shows only loopback interface] *********************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Check Flannel DaemonSet status] *********************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Display comprehensive CNI readiness status] *********************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "=== CNI Readiness Status ===\n\nControl Plane Flannel Status:\nChecking Flannel deployment status:\nNAME           STATUS   AGE\nkube-flannel   Active   29h\nNAME                        READY   STATUS             RESTARTS         AGE\npod/kube-flannel-ds-bjwls   1/1     Running            0                175m\npod/kube-flannel-ds-n2cln   1/1     Running            1 (7h17m ago)    29h\npod/kube-flannel-ds-x74hm   0/1     CrashLoopBackOff   10 (4m40s ago)   54m\n\nNAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/kube-flannel-ds   3         3         2       3            2           <none>          29h\nNode status:\nNAME               STATUS   ROLES           AGE    VERSION\nhomelab            Ready    <none>          173m   v1.29.15\nmasternode         Ready    control-plane   29h    v1.29.15\nstoragenodet3500   Ready    <none>          175m   v1.29.15\n\nCNI Plugins Status:\n=== CNI Plugins Status ===\n-rwxr-xr-x 1 root root  4531309 Sep 11  2023 bridge\n-rwxr-xr-x 1 root root  2907995 Sep 11 14:40 flannel\n-rwxr-xr-x 1 root root  3444776 Sep 11  2023 host-local\n-rwxr-xr-x 1 root root  3514598 Sep 11  2023 loopback\n\n=== CNI Configuration Directory ===\ntotal 16\ndrwxr-xr-x 2 root root 4096 Sep 11 14:40 .\ndrwxr-xr-x 3 root root 4096 Sep  9 14:27 ..\n-rw-r--r-- 1 root root  268 Sep 10 19:32 00-placeholder.conflist\n-rw-r--r-- 1 root root  292 Sep 11 14:40 10-flannel.conflist\n\nContainerd CNI Configuration:\n=== Containerd CNI Configuration ===\n    stream_server_port = \"0\"\n    systemd_cgroup = false\n    tolerate_missing_hugetlb_controller = true\n    unset_seccomp_profile = \"\"\n\n    [plugins.\"io.containerd.grpc.v1.cri\".cni]\n      bin_dir = \"/opt/cni/bin\"\n      conf_dir = \"/etc/cni/net.d\"\n      conf_template = \"\"\n      ip_pref = \"\"\n      max_conf_num = 1\n\n    [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n--\n      no_pivot = false\n      snapshotter = \"overlayfs\"\n\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.default_runtime]\n        base_runtime_spec = \"\"\n        cni_conf_dir = \"\"\n        cni_max_conf_num = 0\n        container_annotations = []\n        pod_annotations = []\n        privileged_without_host_devices = false\n        runtime_engine = \"\"\n        runtime_path = \"\"\n--\n\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes]\n\n        [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n          base_runtime_spec = \"\"\n          cni_conf_dir = \"\"\n          cni_max_conf_num = 0\n          container_annotations = []\n          pod_annotations = []\n          privileged_without_host_devices = false\n          runtime_engine = \"\"\n          runtime_path = \"\"\n--\n            ShimCgroup = \"\"\n            SystemdCgroup = true\n\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.untrusted_workload_runtime]\n        base_runtime_spec = \"\"\n        cni_conf_dir = \"\"\n        cni_max_conf_num = 0\n        container_annotations = []\n        pod_annotations = []\n        privileged_without_host_devices = false\n        runtime_engine = \"\"\n        runtime_path = \"\"\n\nCNI Runtime Analysis:\ncni_has_real_network=true\n\nFlannel DaemonSet Status:\n=== Flannel DaemonSet and Pod Status ===\nNAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE   CONTAINERS     IMAGES                               SELECTOR\ndaemonset.apps/kube-flannel-ds   3         3         2       3            2           <none>          29h   kube-flannel   ghcr.io/flannel-io/flannel:v0.27.3   app=flannel,k8s-app=flannel\n\nNAME                        READY   STATUS             RESTARTS         AGE    IP             NODE               NOMINATED NODE   READINESS GATES\npod/kube-flannel-ds-bjwls   1/1     Running            0                175m   192.168.4.61   storagenodet3500   <none>           <none>\npod/kube-flannel-ds-n2cln   1/1     Running            1 (7h17m ago)    29h    192.168.4.63   masternode         <none>           <none>\npod/kube-flannel-ds-x74hm   0/1     CrashLoopBackOff   10 (4m41s ago)   54m    192.168.4.62   homelab            <none>           <none>\n\n=== Flannel Readiness Check ===\n2\n\n\nNote: Flannel pods may show CrashLoopBackOff until worker nodes join.\nThis is expected behavior in a single-node control plane setup.\n\nCNI Configuration Ready: Worker nodes can now join the cluster.\n"
}

TASK [Wait for CoreDNS to stabilize after Flannel setup] **************************************************************************************************************************************
Pausing for 30 seconds
(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
ok: [192.168.4.63]

TASK [Check CoreDNS pod status and fix if needed] *********************************************************************************************************************************************

HANGS HERE

2.
root@masternode:~# kubectl get pods --all-namespaces -o wide
NAMESPACE              NAME                                         READY   STATUS              RESTARTS         AGE    IP             NODE               NOMINATED NODE   READINESS GATES
jellyfin               jellyfin                                     0/1     Running             6 (3m27s ago)    25m    10.244.0.2     storagenodet3500   <none>           <none>
kube-flannel           kube-flannel-ds-bjwls                        1/1     Running             0                176m   192.168.4.61   storagenodet3500   <none>           <none>
kube-flannel           kube-flannel-ds-n2cln                        1/1     Running             1 (7h18m ago)    29h    192.168.4.63   masternode         <none>           <none>
kube-flannel           kube-flannel-ds-x74hm                        1/1     Running             11 (5m39s ago)   55m    192.168.4.62   homelab            <none>           <none>
kube-system            coredns-68444cf7cd-vq79r                     0/1     ContainerCreating   0                28s    <none>         masternode         <none>           <none>
kube-system            etcd-masternode                              1/1     Running             4 (7h18m ago)    29h    192.168.4.63   masternode         <none>           <none>
kube-system            kube-apiserver-masternode                    1/1     Running             11 (7h18m ago)   29h    192.168.4.63   masternode         <none>           <none>
kube-system            kube-controller-manager-masternode           1/1     Running             32 (7h18m ago)   29h    192.168.4.63   masternode         <none>           <none>
kube-system            kube-proxy-2zfsw                             1/1     Running             1 (7h18m ago)    29h    192.168.4.63   masternode         <none>           <none>
kube-system            kube-proxy-nh8vb                             0/1     CrashLoopBackOff    32 (36s ago)     174m   192.168.4.62   homelab            <none>           <none>
kube-system            kube-proxy-r8tlw                             1/1     Running             0                176m   192.168.4.61   storagenodet3500   <none>           <none>
kube-system            kube-scheduler-masternode                    1/1     Running             32 (7h18m ago)   29h    192.168.4.63   masternode         <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-597744f4cf-lmvlj   0/1     ContainerCreating   0                8h     <none>         masternode         <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-fbd9c767-8pzxd     0/1     ContainerCreating   0                9h     <none>         masternode         <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-547749b7d9-zcs4w        0/1     ContainerCreating   0                9h     <none>         masternode         <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-7fc5595d85-cgh9p        0/1     ContainerCreating   0                8h     <none>         masternode         <none>           <none>
monitoring             grafana-554bf5687f-l8snn                     0/1     ContainerCreating   0                9h     <none>         masternode         <none>           <none>
monitoring             grafana-79db5b584f-jdbgf                     0/1     ContainerCreating   0                84m    <none>         masternode         <none>           <none>
monitoring             loki-564bd8dfb7-wfjzk                        0/1     ContainerCreating   0                9h     <none>         masternode         <none>           <none>
monitoring             loki-85d467fb56-xt466                        0/1     ContainerCreating   0                84m    <none>         masternode         <none>           <none>
monitoring             prometheus-54d6cfcf7d-rs7vj                  0/1     ContainerCreating   0                9h     <none>         masternode         <none>           <none>
monitoring             prometheus-74887c8bb6-rqdl5                  0/1     ContainerCreating   0                84m    <none>         masternode         <none>           <none>
root@masternode:~# kubectl describe pod coredns-68444cf7cd-vq79r -n kube-system
Name:                 coredns-68444cf7cd-vq79r
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Service Account:      coredns
Node:                 masternode/192.168.4.63
Start Time:           Thu, 11 Sep 2025 21:58:33 -0400
Labels:               k8s-app=kube-dns
                      pod-template-hash=68444cf7cd
Annotations:          <none>
Status:               Pending
IP:
IPs:                  <none>
Controlled By:        ReplicaSet/coredns-68444cf7cd
Containers:
  coredns:
    Container ID:
    Image:         registry.k8s.io/coredns/coredns:v1.11.1
    Image ID:
    Ports:         53/UDP (dns), 53/TCP (dns-tcp), 9153/TCP (metrics)
    Host Ports:    0/UDP (dns), 0/TCP (dns-tcp), 0/TCP (metrics)
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6q8xk (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-6q8xk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason                  Age                From               Message
  ----     ------                  ----               ----               -------
  Normal   Scheduled               59s                default-scheduler  Successfully assigned kube-system/coredns-68444cf7cd-vq79r to masternode
  Warning  FailedCreatePodSandBox  59s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "23876f3217b3512647b3ba3b6d1667d4dbba339bbce6fde0d2d8df6feb3e5dee": plugin type="bridge" failed (add): failed to set bridge addr: "cni0" already has an IP address different from 10.244.0.1/16
  Warning  FailedCreatePodSandBox  58s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "809d4801a440ea859f79317215423b7b10de8368f17503ff427196b681b3b9cf": plugin type="bridge" failed (add): failed to set bridge addr: "cni0" already has an IP address different from 10.244.0.1/16
  Warning  FailedCreatePodSandBox  57s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "64210ba2477479c4669127c0898ec617817014e2984760b276b7fad735597ea0": plugin type="bridge" failed (add): failed to set bridge addr: "cni0" already has an IP address different from 10.244.0.1/16
  Warning  FailedCreatePodSandBox  46s (x2 over 56s)  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to reserve sandbox name "coredns-68444cf7cd-vq79r_kube-system_c002f635-edb4-4087-8d49-097dc080494f_1": name "coredns-68444cf7cd-vq79r_kube-system_c002f635-edb4-4087-8d49-097dc080494f_1" is reserved for "809d4801a440ea859f79317215423b7b10de8368f17503ff427196b681b3b9cf"
  Warning  FailedCreatePodSandBox  31s                kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "bf6ca3e732f901f0ee0a23652201c41859344395b4372b4ffe5481abc12019d5": plugin type="bridge" failed (add): failed to set bridge addr: "cni0" already has an IP address different from 10.244.0.1/16
  Normal   SandboxChanged          1s (x8 over 58s)   kubelet            Pod sandbox changed, it will be killed and re-created.
  Warning  FailedCreatePodSandBox  1s (x3 over 30s)   kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to reserve sandbox name "coredns-68444cf7cd-vq79r_kube-system_c002f635-edb4-4087-8d49-097dc080494f_1": name "coredns-68444cf7cd-vq79r_kube-system_c002f635-edb4-4087-8d49-097dc080494f_1" is reserved for "bf6ca3e732f901f0ee0a23652201c41859344395b4372b4ffe5481abc12019d5"


root@masternode:~# kubectl describe pod kube-proxy-nh8vb -n kube-system
Name:                 kube-proxy-nh8vb
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      kube-proxy
Node:                 homelab/192.168.4.62
Start Time:           Thu, 11 Sep 2025 19:04:38 -0400
Labels:               controller-revision-hash=5787cd6d6c
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   192.168.4.62
IPs:
  IP:           192.168.4.62
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://0d1974a350a02db3683c37197791d3e473f22091223e0360e1d60e2b6ab06c7e
    Image:         registry.k8s.io/kube-proxy:v1.29.15
    Image ID:      registry.k8s.io/kube-proxy@sha256:243026cfce3209b89d9f883789108276ffec87d98190ac2a77776edd4e0e6015
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    2
      Started:      Thu, 11 Sep 2025 21:57:24 -0400
      Finished:     Thu, 11 Sep 2025 21:58:25 -0400
    Ready:          False
    Restart Count:  32
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qjdt2 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:
  kube-api-access-qjdt2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason   Age                   From     Message
  ----     ------   ----                  ----     -------
  Normal   Killing  14m (x31 over 175m)   kubelet  Stopping container kube-proxy
  Warning  BackOff  40s (x682 over 175m)  kubelet  Back-off restarting failed container kube-proxy in pod kube-proxy-nh8vb_kube-system(5458a476-3646-4c15-a73b-dc5348a7c271)


root@masternode:~# kubectl describe pod kube-flannel-ds-x74hm  -n kube-flannel
Name:                 kube-flannel-ds-x74hm
Namespace:            kube-flannel
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      flannel
Node:                 homelab/192.168.4.62
Start Time:           Thu, 11 Sep 2025 21:03:11 -0400
Labels:               app=flannel
                      controller-revision-hash=8f698f8cd
                      k8s-app=flannel
                      pod-template-generation=1
                      tier=node
Annotations:          <none>
Status:               Running
IP:                   192.168.4.62
IPs:
  IP:           192.168.4.62
Controlled By:  DaemonSet/kube-flannel-ds
Init Containers:
  install-cni-plugin:
    Container ID:  containerd://81fe9e990ba59c20c7492910346c4a40c3aa4e620f84ebb960791c274228c031
    Image:         ghcr.io/flannel-io/flannel-cni-plugin:v1.7.1-flannel1
    Image ID:      ghcr.io/flannel-io/flannel-cni-plugin@sha256:cb3176a2c9eae5fa0acd7f45397e706eacb4577dac33cad89f93b775ff5611df
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /flannel
      /opt/cni/bin/flannel
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 11 Sep 2025 21:59:53 -0400
      Finished:     Thu, 11 Sep 2025 21:59:53 -0400
    Ready:          True
    Restart Count:  12
    Environment:    <none>
    Mounts:
      /opt/cni/bin from cni-plugin (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ssqkl (ro)
  install-cni:
    Container ID:  containerd://c5039c683d312d1c2872f129b41f578fac81bfbbbc77d9e434badee9981f2c3d
    Image:         ghcr.io/flannel-io/flannel:v0.27.3
    Image ID:      ghcr.io/flannel-io/flannel@sha256:8cc0cf9e94df48e98be84bce3e61984bbd46c3c44ad35707ec7ef40e96b009d1
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 11 Sep 2025 21:59:54 -0400
      Finished:     Thu, 11 Sep 2025 21:59:54 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ssqkl (ro)
Containers:
  kube-flannel:
    Container ID:  containerd://d935e65d252986f9e12770f160c54f10f7cb95371132c06c8231f423fea514b3
    Image:         ghcr.io/flannel-io/flannel:v0.27.3
    Image ID:      ghcr.io/flannel-io/flannel@sha256:8cc0cf9e94df48e98be84bce3e61984bbd46c3c44ad35707ec7ef40e96b009d1
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 11 Sep 2025 21:58:31 -0400
      Finished:     Thu, 11 Sep 2025 21:59:53 -0400
    Ready:          False
    Restart Count:  11
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:                   kube-flannel-ds-x74hm (v1:metadata.name)
      POD_NAMESPACE:              kube-flannel (v1:metadata.namespace)
      EVENT_QUEUE_DEPTH:          5000
      CONT_WHEN_CACHE_NOT_READY:  false
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ssqkl (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:
  cni-plugin:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  kube-api-access-ssqkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 :NoSchedule op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason          Age                    From               Message
  ----     ------          ----                   ----               -------
  Normal   Scheduled       57m                    default-scheduler  Successfully assigned kube-flannel/kube-flannel-ds-x74hm to homelab
  Normal   Created         56m (x2 over 57m)      kubelet            Created container: install-cni
  Normal   Started         56m (x2 over 57m)      kubelet            Started container install-cni
  Normal   Started         56m (x2 over 57m)      kubelet            Started container kube-flannel
  Normal   Pulled          56m (x2 over 57m)      kubelet            Container image "ghcr.io/flannel-io/flannel:v0.27.3" already present on machine
  Normal   Created         56m (x2 over 57m)      kubelet            Created container: kube-flannel
  Normal   Started         52m (x3 over 57m)      kubelet            Started container install-cni-plugin
  Normal   Created         52m (x3 over 57m)      kubelet            Created container: install-cni-plugin
  Normal   Pulled          52m (x3 over 57m)      kubelet            Container image "ghcr.io/flannel-io/flannel-cni-plugin:v1.7.1-flannel1" already present on machine
  Normal   SandboxChanged  52m (x2 over 56m)      kubelet            Pod sandbox changed, it will be killed and re-created.
  Normal   Pulled          52m (x3 over 57m)      kubelet            Container image "ghcr.io/flannel-io/flannel:v0.27.3" already present on machine
  Normal   Killing         7m36s (x11 over 56m)   kubelet            Stopping container kube-flannel
  Warning  BackOff         2m39s (x145 over 52m)  kubelet            Back-off restarting failed container kube-flannel in pod kube-flannel-ds-x74hm_kube-flannel(b0702f00-3fef-4d83-8afb-43a2fc88e203)


It appears that the jellyfin node is simply not able to communicate with the masternode which should be fixed after the coredns pod is fixed
root@masternode:~# kubectl describe pod jellyfin -n jellyfin
Name:             jellyfin
Namespace:        jellyfin
Priority:         0
Service Account:  default
Node:             storagenodet3500/192.168.4.61
Start Time:       Thu, 11 Sep 2025 21:34:00 -0400
Labels:           app=jellyfin
                  component=media-server
Annotations:      <none>
Status:           Running
IP:               10.244.0.2
IPs:
  IP:  10.244.0.2
Containers:
  jellyfin:
    Container ID:   containerd://3af3472e996f136044b2ed2664b527e2ca06be1676ab22c5f413bacb0a430271
    Image:          jellyfin/jellyfin:latest
    Image ID:       docker.io/jellyfin/jellyfin@sha256:7ae36aab93ef9b6aaff02b37f8bb23df84bb2d7a3f6054ec8fc466072a648ce2
    Ports:          8096/TCP (http), 8920/TCP (https)
    Host Ports:     0/TCP (http), 0/TCP (https)
    State:          Running
      Started:      Thu, 11 Sep 2025 21:59:07 -0400
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 11 Sep 2025 21:55:34 -0400
      Finished:     Thu, 11 Sep 2025 21:59:07 -0400
    Ready:          False
    Restart Count:  7
    Limits:
      cpu:     1
      memory:  2Gi
    Requests:
      cpu:      200m
      memory:   512Mi
    Liveness:   http-get http://:8096/health delay=120s timeout=10s period=30s #success=1 #failure=3
    Readiness:  http-get http://:8096/health delay=60s timeout=5s period=10s #success=1 #failure=3
    Environment:
      JELLYFIN_PublishedServerUrl:  http://192.168.4.61:30096
    Mounts:
      /config from config (rw)
      /media from media (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qzdn5 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  media:
    Type:          HostPath (bare host directory volume)
    Path:          /srv/media
    HostPathType:  Directory
  config:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/jellyfin
    HostPathType:  DirectoryOrCreate
  kube-api-access-qzdn5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/hostname=storagenodet3500
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  27m                   default-scheduler  Successfully assigned jellyfin/jellyfin to storagenodet3500
  Normal   Pulling    27m                   kubelet            Pulling image "jellyfin/jellyfin:latest"
  Normal   Pulled     26m                   kubelet            Successfully pulled image "jellyfin/jellyfin:latest" in 51.614s (51.614s including waiting)
  Normal   Started    26m                   kubelet            Started container jellyfin
  Warning  Unhealthy  23m (x3 over 24m)     kubelet            Liveness probe failed: Get "http://10.244.0.2:8096/health": dial tcp 10.244.0.2:8096: connect: no route to host
  Normal   Killing    23m                   kubelet            Container jellyfin failed liveness probe, will be restarted
  Normal   Pulled     23m                   kubelet            Container image "jellyfin/jellyfin:latest" already present on machine
  Normal   Created    23m (x2 over 26m)     kubelet            Created container: jellyfin
  Warning  Unhealthy  7m46s (x85 over 25m)  kubelet            Readiness probe failed: Get "http://10.244.0.2:8096/health": dial tcp 10.244.0.2:8096: connect: no route to host
  Warning  Unhealthy  2m44s (x14 over 24m)  kubelet            Readiness probe failed: Get "http://10.244.0.2:8096/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
