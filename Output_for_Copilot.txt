remote: Enumerating objects: 29, done.
remote: Counting objects: 100% (29/29), done.
remote: Compressing objects: 100% (16/16), done.
remote: Total 29 (delta 22), reused 19 (delta 13), pack-reused 0 (from 0)
Unpacking objects: 100% (29/29), 17.90 KiB | 1.63 MiB/s, done.
From https://github.com/JashandeepJustinBains/VMStation
   a785ef4..62a2e5c  main                          -> origin/main
 * [new branch]      copilot/fix-deployment-issues -> origin/copilot/fix-deployment-issues
Updating a785ef4..62a2e5c
Fast-forward
 DEPLOYMENT_FIXES_SUMMARY.md               | 309 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 GRAFANA_FIX_SUMMARY.md                    | 167 ---------------------------------------------------
 IMPLEMENTATION_SUMMARY.md                 |   2 +-
 MONITORING_FIX_README.md                  | 150 ---------------------------------------------
 Output_for_Copilot.txt                    | 454 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++--------------------
 QUICK_FIX_SUMMARY.md                      | 138 ------------------------------------------
 QUICK_START_FIXES.md                      | 160 ++++++++++++++++++++++++++++++++++++++++++++++++
 TODO.md                                   |  50 ++++++++++++++-
 VISUAL_FIX_SUMMARY.md                     | 245 --------------------------------------------------------------------------
 VISUAL_MONITORING_FIXES_SUMMARY.md        | 293 ----------------------------------------------------------------------------------------
 manifests/monitoring/loki.yaml            |  16 +++++
 tests/test-deployment-fixes.sh            | 145 ++++++++++++++++++++++++++++++++++++++++++++
 tests/test-loki-validation.sh             |   2 +-
 tests/test-monitoring-access.sh           |  19 +++++-
 tests/test-monitoring-exporters-health.sh |  34 +++++++++--
 15 files changed, 1115 insertions(+), 1069 deletions(-)
 create mode 100644 DEPLOYMENT_FIXES_SUMMARY.md
 delete mode 100644 GRAFANA_FIX_SUMMARY.md
 delete mode 100644 MONITORING_FIX_README.md
 delete mode 100644 QUICK_FIX_SUMMARY.md
 create mode 100644 QUICK_START_FIXES.md
 delete mode 100644 VISUAL_FIX_SUMMARY.md
 delete mode 100644 VISUAL_MONITORING_FIXES_SUMMARY.md
 create mode 100755 tests/test-deployment-fixes.sh
==============================================
VMStation Pre-Deployment Checklist
==============================================

1. Checking repository structure...
✅ PASS: Repository structure is valid

2. Checking required files exist...
✅ PASS: ansible/playbooks/deploy-cluster.yaml exists
✅ PASS: ansible/inventory/hosts.yml exists
✅ PASS: deploy.sh exists
✅ PASS: manifests/monitoring/prometheus.yaml exists
✅ PASS: manifests/monitoring/grafana.yaml exists
✅ PASS: manifests/monitoring/loki.yaml exists

3. Checking Ansible installation...
✅ PASS: Ansible installed: ansible-playbook [core 2.14.18]

4. Validating playbook syntax...
✅ PASS: Deploy playbook syntax valid

5. Checking inventory file...
✅ PASS: Inventory file valid
⚠️  WARN: Group 'monitoring_nodes' not found in inventory
⚠️  WARN: Group 'storage_nodes' not found in inventory
⚠️  WARN: Group 'compute_nodes' not found in inventory

6. Verifying playbook phase order...
❌ FAIL: Playbook phase order incorrect: 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3 4 4 4 4 5 5 5 5 6 6 6 6 7 7 7 7 8 8 (expected: 0 1 2 3 4 5 6 7 8)

7. Checking monitoring manifests...
✅ PASS: Found 10 monitoring manifests

8. Checking for critical fixes...
✅ PASS: Blackbox-exporter has nodeSelector configured
✅ PASS: Phase 0 has sufficient tasks (29)
✅ PASS: Phase 8 is positioned after Phase 7 (line 715 > 458)

9. Checking deploy.sh script...
✅ PASS: deploy.sh is executable
✅ PASS: deploy.sh syntax is valid

10. Checking SSH connectivity (if possible)...
✅ PASS: SSH key ~/.ssh/id_k3s exists

==============================================
Summary
==============================================
Passed:  17
Warnings: 3
Failed:  1

❌ NOT READY FOR DEPLOYMENT

Please fix the 1 failed checks before deploying.

[2025-10-08 22:27:59] [INFO] ========================================
[2025-10-08 22:27:59] [INFO]  Comprehensive Cluster Reset
[2025-10-08 22:27:59] [INFO] ========================================
[2025-10-08 22:27:59] [INFO] This will reset:
[2025-10-08 22:27:59] [INFO]   - Debian Kubernetes cluster (kubeadm)
[2025-10-08 22:27:59] [INFO]   - RKE2 cluster on homelab
[2025-10-08 22:27:59] [INFO]   - All network interfaces and configs
[2025-10-08 22:27:59] [INFO]
[2025-10-08 22:27:59] [INFO] SSH keys and physical ethernet will be preserved
[2025-10-08 22:27:59] [INFO]
Proceed with comprehensive reset? [y/N]: y
[2025-10-08 22:28:10] [INFO]
[2025-10-08 22:28:10] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:28:10] [INFO]   PHASE 1: Resetting Debian Nodes
[2025-10-08 22:28:10] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY [Cluster Reset - Debian Nodes] ********************************************

TASK [Display reset banner] ****************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Resetting Kubernetes Cluster
    Target: masternode
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ok: [storagenodet3500] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Resetting Kubernetes Cluster
    Target: storagenodet3500
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Reset kubeadm (if initialized)] ******************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Stop kubelet service] ****************************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Stop containerd service] *************************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Kill any hanging kubeadm processes] **************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Remove Kubernetes configuration] *****************************************
changed: [masternode] => (item=/etc/kubernetes)
changed: [storagenodet3500] => (item=/etc/kubernetes)
changed: [masternode] => (item=/var/lib/kubelet)
changed: [storagenodet3500] => (item=/var/lib/kubelet)
changed: [masternode] => (item=/var/lib/etcd)
ok: [storagenodet3500] => (item=/var/lib/etcd)
changed: [masternode] => (item=/etc/cni/net.d)
changed: [masternode] => (item=/opt/cni/bin)
changed: [storagenodet3500] => (item=/etc/cni/net.d)
changed: [masternode] => (item=/root/.kube)
changed: [storagenodet3500] => (item=/opt/cni/bin)
ok: [storagenodet3500] => (item=/root/.kube)

TASK [Remove CNI network interfaces] *******************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Flush iptables rules] ****************************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Restart containerd service] **********************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Display reset complete message] ******************************************
ok: [masternode] =>
  msg: masternode reset complete
ok: [storagenodet3500] =>
  msg: storagenodet3500 reset complete

PLAY [Reset Complete Summary] **************************************************

TASK [Display summary] *********************************************************
ok: [localhost] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ✅ Cluster Reset Complete

    All Kubernetes configuration removed
    Cluster ready for fresh deployment

    Next steps:
    ./deploy.sh debian    # Redeploy Debian cluster
    ./deploy.sh all       # Redeploy both clusters
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY RECAP *********************************************************************
localhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
masternode                 : ok=10   changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
storagenodet3500           : ok=10   changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0

[2025-10-08 22:28:24] [INFO] ✓ Debian cluster reset completed
[2025-10-08 22:28:24] [INFO]
[2025-10-08 22:28:24] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:28:24] [INFO]   PHASE 2: Resetting RKE2 on Homelab
[2025-10-08 22:28:24] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:28:25] [INFO] No RKE2 installation found on homelab, skipping
[2025-10-08 22:28:25] [INFO]
[2025-10-08 22:28:25] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:28:25] [INFO]   Reset Complete!
[2025-10-08 22:28:25] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:28:25] [INFO]
[2025-10-08 22:28:25] [INFO] Cluster is ready for fresh deployment
[2025-10-08 22:28:25] [INFO]
[2025-10-08 22:28:25] [INFO] Logs:
[2025-10-08 22:28:25] [INFO]   - /srv/monitoring_data/VMStation/ansible/artifacts/reset-debian.log
[2025-10-08 22:28:25] [INFO]   - /srv/monitoring_data/VMStation/ansible/artifacts/uninstall-rke2.log
[2025-10-08 22:28:25] [INFO]
[2025-10-08 22:28:25] [INFO] Next steps:
[2025-10-08 22:28:25] [INFO]   ./deploy.sh all --with-rke2    # Full deployment
[2025-10-08 22:28:25] [INFO]   ./deploy.sh debian             # Debian only
[2025-10-08 22:28:25] [INFO]   ./deploy.sh rke2               # RKE2 only
[2025-10-08 22:28:25] [INFO]
[2025-10-08 22:28:25] [INFO] ========================================
[2025-10-08 22:28:25] [INFO]  Two-Phase Deployment: Debian + RKE2
[2025-10-08 22:28:25] [INFO] ========================================
[2025-10-08 22:28:25] [INFO]
[2025-10-08 22:28:25] [INFO]
[2025-10-08 22:28:25] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:28:25] [INFO]   PHASE 1: Deploying to Debian Nodes
[2025-10-08 22:28:25] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:28:25] [INFO] ========================================
[2025-10-08 22:28:25] [INFO]  Deploying Kubernetes to Debian Nodes
[2025-10-08 22:28:25] [INFO] ========================================
[2025-10-08 22:28:25] [INFO] Target: monitoring_nodes + storage_nodes
[2025-10-08 22:28:25] [INFO] Playbook: /srv/monitoring_data/VMStation/ansible/playbooks/deploy-cluster.yaml
[2025-10-08 22:28:25] [INFO] Log: /srv/monitoring_data/VMStation/ansible/artifacts/deploy-debian.log
[2025-10-08 22:28:25] [INFO]
[2025-10-08 22:28:25] [INFO] Starting Debian deployment (this may take 10-15 minutes)...
[WARNING]: Collection kubernetes.core does not support Ansible version 2.14.18

PLAY [Phase 0: System Preparation - Install Kubernetes Binaries] ***************

TASK [Gathering Facts] *********************************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Set default flags] *******************************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Display Phase 0 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ok: [storagenodet3500] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Disable swap] ************************************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Remove swap from fstab] **************************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Load kernel modules for containerd] **************************************
ok: [masternode] => (item=overlay)
ok: [storagenodet3500] => (item=overlay)
ok: [masternode] => (item=br_netfilter)
ok: [storagenodet3500] => (item=br_netfilter)

TASK [Ensure modules load on boot] *********************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Configure sysctl for Kubernetes] *****************************************
ok: [masternode] => (item={'name': 'net.bridge.bridge-nf-call-iptables', 'value': '1'})
ok: [storagenodet3500] => (item={'name': 'net.bridge.bridge-nf-call-iptables', 'value': '1'})
ok: [masternode] => (item={'name': 'net.bridge.bridge-nf-call-ip6tables', 'value': '1'})
ok: [storagenodet3500] => (item={'name': 'net.bridge.bridge-nf-call-ip6tables', 'value': '1'})
ok: [masternode] => (item={'name': 'net.ipv4.ip_forward', 'value': '1'})
ok: [storagenodet3500] => (item={'name': 'net.ipv4.ip_forward', 'value': '1'})

TASK [Check if containerd is installed] ****************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Install containerd (try containerd.io first)] ****************************
skipping: [masternode]
skipping: [storagenodet3500]

TASK [Install containerd (fallback to containerd package)] *********************
skipping: [masternode]
skipping: [storagenodet3500]

TASK [Create containerd config directory] **************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Generate containerd default config] **************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Enable SystemdCgroup in containerd] **************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Restart containerd] ******************************************************
changed: [masternode]
changed: [storagenodet3500]

TASK [Configure crictl runtime endpoint] ***************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Add Kubernetes apt key] **************************************************
ok: [storagenodet3500]
ok: [masternode]

TASK [Add Kubernetes apt repository] *******************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Install Kubernetes binaries] *********************************************
ok: [storagenodet3500]
ok: [masternode]

TASK [Hold Kubernetes packages at current version] *****************************
ok: [masternode] => (item=kubelet)
ok: [storagenodet3500] => (item=kubelet)
ok: [masternode] => (item=kubeadm)
ok: [storagenodet3500] => (item=kubeadm)
ok: [masternode] => (item=kubectl)
ok: [storagenodet3500] => (item=kubectl)

TASK [Enable kubelet service] **************************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Create required Kubernetes directories] **********************************
changed: [masternode] => (item=/opt/cni/bin)
changed: [storagenodet3500] => (item=/opt/cni/bin)
changed: [masternode] => (item=/etc/cni/net.d)
changed: [storagenodet3500] => (item=/etc/cni/net.d)
changed: [masternode] => (item=/var/lib/kubelet)
changed: [storagenodet3500] => (item=/var/lib/kubelet)

TASK [Check if CNI plugins are installed] **************************************
ok: [masternode]
ok: [storagenodet3500]

TASK [Download CNI plugins if missing] *****************************************
ok: [storagenodet3500]
ok: [masternode]

TASK [Extract CNI plugins] *****************************************************
changed: [masternode]
changed: [storagenodet3500]

PLAY [Phase 1: Control Plane Initialization] ***********************************

TASK [Display Phase 1 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 1: Control Plane Initialization
    Target: masternode
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Check if cluster is already initialized] *********************************
ok: [masternode]

TASK [Initialize control plane (if not exists)] ********************************
changed: [masternode]

TASK [Regenerate admin.conf with kubeadm (fixes authentication issues)] ********
skipping: [masternode]

TASK [Create .kube directory for root] *****************************************
changed: [masternode]

TASK [Copy admin.conf to /root/.kube/config] ***********************************
changed: [masternode]

TASK [Set KUBECONFIG environment variable globally] ****************************
ok: [masternode]

TASK [Add KUBECONFIG to root's bash profile] ***********************************
ok: [masternode]

PLAY [Phase 2: Control Plane Validation] ***************************************

TASK [Display Phase 2 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 2: Control Plane Validation
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Wait for API server to be ready] *****************************************
ok: [masternode]

TASK [Verify control plane is responding] **************************************
changed: [masternode]

TASK [Display control plane status] ********************************************
ok: [masternode] =>
  msg:
  - "\e[0;32mKubernetes control plane\e[0m is running at \e[0;33mhttps://192.168.4.63:6443\e[0m"
  - "\e[0;32mCoreDNS\e[0m is running at \e[0;33mhttps://192.168.4.63:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\e[0m"
  - ''
  - To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

PLAY [Phase 3: Token Generation] ***********************************************

TASK [Display Phase 3 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 3: Token Generation
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Generate join token] *****************************************************
changed: [masternode]

TASK [Store join command] ******************************************************
ok: [masternode]

TASK [Display join command] ****************************************************
ok: [masternode] =>
  msg: 'Join command: kubeadm join 192.168.4.63:6443 --token rwz9ar.ze7wxt4ksh9156zu --discovery-token-ca-cert-hash sha256:2c1f4296e66c35db7977b57a7165228b917f3d9386b281c3279ee7ae93f35c3c '

PLAY [Phase 4: CNI Deployment - Flannel] ***************************************

TASK [Display Phase 4 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 4: CNI Deployment (Flannel)
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Check if Flannel is already deployed] ************************************
ok: [masternode]

TASK [Deploy Flannel CNI] ******************************************************
changed: [masternode]

TASK [Wait for Flannel DaemonSet to be available] ******************************
changed: [masternode]

TASK [Display Flannel deployment status] ***************************************
ok: [masternode] =>
  msg: Flannel CNI deployed successfully

PLAY [Phase 5: Worker Node Join] ***********************************************

TASK [Display Phase 5 banner] **************************************************
ok: [storagenodet3500] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 5: Worker Node Join
    Target: storagenodet3500
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Check if node is already joined] *****************************************
ok: [storagenodet3500]

TASK [Join worker node to cluster] *********************************************
changed: [storagenodet3500]

TASK [Wait for kubelet to start] ***********************************************
ok: [storagenodet3500]

TASK [Display join status] *****************************************************
ok: [storagenodet3500] =>
  msg: storagenodet3500 successfully joined the cluster

PLAY [Phase 6: Cluster Validation] *********************************************

TASK [Display Phase 6 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 6: Cluster Validation
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Wait for nodes to be Ready] **********************************************
FAILED - RETRYING: [masternode]: Wait for nodes to be Ready (20 retries left).
changed: [masternode]

TASK [Ensure all nodes are schedulable (uncordon)] *****************************
changed: [masternode]

TASK [Get node status] *********************************************************
changed: [masternode]

TASK [Display node status] *****************************************************
ok: [masternode] =>
  msg:
  - NAME               STATUS   ROLES           AGE   VERSION    INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION   CONTAINER-RUNTIME
  - masternode         Ready    control-plane   34s   v1.29.15   192.168.4.63   <none>        Debian GNU/Linux 12 (bookworm)   6.1.0-32-amd64   containerd://1.6.20
  - storagenodet3500   Ready    <none>          13s   v1.29.15   192.168.4.61   <none>        Debian GNU/Linux 12 (bookworm)   6.1.0-34-amd64   containerd://1.6.20

TASK [Wait for CoreDNS to be Running] ******************************************
changed: [masternode]

TASK [Display validation success] **********************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ✅ Cluster Validation Successful
    - Ready Nodes: 2
    - CoreDNS Pods: 1
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY [Phase 7: Application Deployment - Monitoring Stack] **********************

TASK [Display Phase 7 banner] **************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Phase 7: Application Deployment (Monitoring)
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Create monitoring namespace] *********************************************
changed: [masternode]

TASK [Create monitoring data directories on control plane with proper permissions] ***
ok: [masternode] => (item={'path': '/srv/monitoring_data'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/grafana', 'owner': '472', 'group': '472'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/prometheus', 'owner': '65534', 'group': '65534'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/loki', 'owner': '10001', 'group': '10001'})
ok: [masternode] => (item={'path': '/srv/monitoring_data/promtail', 'owner': '0', 'group': '0'})

TASK [Apply monitoring PersistentVolumes and PVCs (prometheus/grafana/loki/promtail)] ***
changed: [masternode]

TASK [Fail if IPMI credentials required but not provided] **********************
skipping: [masternode]

TASK [Ensure ipmi credentials secret exists in monitoring namespace] ***********
skipping: [masternode]

TASK [Deploy Node Exporter (system metrics)] ***********************************
changed: [masternode]

TASK [Deploy Kube State Metrics (Kubernetes object state)] *********************
changed: [masternode]

TASK [Deploy Loki and Promtail (log aggregation and collection)] ***************
changed: [masternode]

TASK [Deploy IPMI Exporter] ****************************************************
changed: [masternode]

TASK [Scale up remote IPMI exporter if credentials are available] **************
skipping: [masternode]

TASK [Deploy Prometheus] *******************************************************
changed: [masternode]

TASK [Deploy Grafana] **********************************************************
changed: [masternode]

TASK [Wait for Node Exporter DaemonSet to be ready] ****************************
changed: [masternode]

TASK [Wait for Kube State Metrics to be ready] *********************************
changed: [masternode]

TASK [Wait for Loki to be ready] ***********************************************
FAILED - RETRYING: [masternode]: Wait for Loki to be ready (3 retries left).
FAILED - RETRYING: [masternode]: Wait for Loki to be ready (2 retries left).
FAILED - RETRYING: [masternode]: Wait for Loki to be ready (1 retries left).
changed: [masternode]

TASK [Wait for Promtail DaemonSet to be ready] *********************************
changed: [masternode]

TASK [Wait for Prometheus to be ready] *****************************************
changed: [masternode]

TASK [Wait for Grafana to be ready] ********************************************
changed: [masternode]

TASK [Wait for Blackbox Exporter to be ready] **********************************
changed: [masternode]

TASK [Deploy Jellyfin media server] ********************************************
changed: [masternode]

TASK [Wait for Jellyfin pod to be ready] ***************************************
changed: [masternode]

TASK [Display monitoring stack status] *****************************************
changed: [masternode]

TASK [Display Jellyfin status] *************************************************
changed: [masternode]

TASK [Display monitoring deployment result] ************************************
ok: [masternode] =>
  msg:
  - NAME                                      READY   STATUS             RESTARTS       AGE
  - pod/blackbox-exporter-5949885fb9-6lsl2    1/1     Running            0              12m
  - pod/grafana-5f879c7654-zszmq              1/1     Running            0              12m
  - pod/kube-state-metrics-5f6f5666cc-wsvc4   1/1     Running            0              12m
  - pod/loki-669bb4f448-42xpp                 0/1     CrashLoopBackOff   7 (2m6s ago)   12m
  - pod/node-exporter-2zrrn                   1/1     Running            0              12m
  - pod/node-exporter-796z9                   1/1     Running            0              12m
  - pod/prometheus-5d89d5fc7f-cfbdx           1/1     Running            0              12m
  - pod/promtail-2cbkf                        1/1     Running            0              12m
  - pod/promtail-nn8rv                        1/1     Running            0              12m
  - ''
  - NAME                           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
  - service/blackbox-exporter      ClusterIP   10.111.169.71    <none>        9115/TCP            12m
  - service/grafana                NodePort    10.105.8.138     <none>        3000:30300/TCP      12m
  - service/ipmi-exporter          ClusterIP   10.106.69.250    <none>        9290/TCP            12m
  - service/ipmi-exporter-remote   ClusterIP   10.104.49.133    <none>        9291/TCP            12m
  - service/kube-state-metrics     ClusterIP   10.108.27.218    <none>        8080/TCP,8081/TCP   12m
  - service/loki                   NodePort    10.111.230.147   <none>        3100:31100/TCP      12m
  - service/node-exporter          ClusterIP   None             <none>        9100/TCP            12m
  - service/prometheus             NodePort    10.97.68.221     <none>        9090:30090/TCP      12m
  - ''
  - NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                      AGE
  - daemonset.apps/ipmi-exporter   0         0         0       0            0           kubernetes.io/os=linux,vmstation.io/role=compute   12m
  - daemonset.apps/node-exporter   2         2         2       2            2           <none>                                             12m
  - daemonset.apps/promtail        2         2         2       2            2           <none>                                             12m
  - ''
  - NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
  - deployment.apps/blackbox-exporter      1/1     1            1           12m
  - deployment.apps/grafana                1/1     1            1           12m
  - deployment.apps/ipmi-exporter-remote   0/0     0            0           12m
  - deployment.apps/kube-state-metrics     1/1     1            1           12m
  - deployment.apps/loki                   0/1     1            0           12m
  - deployment.apps/prometheus             1/1     1            1           12m
  - ''
  - NAME                                              DESIRED   CURRENT   READY   AGE
  - replicaset.apps/blackbox-exporter-5949885fb9      1         1         1       12m
  - replicaset.apps/grafana-5f879c7654                1         1         1       12m
  - replicaset.apps/ipmi-exporter-remote-869b4c8fd5   0         0         0       12m
  - replicaset.apps/kube-state-metrics-5f6f5666cc     1         1         1       12m
  - replicaset.apps/loki-669bb4f448                   1         1         0       12m
  - replicaset.apps/prometheus-5d89d5fc7f             1         1         1       12m

TASK [Display Jellyfin deployment result] **************************************
ok: [masternode] =>
  msg:
  - NAME           READY   STATUS    RESTARTS   AGE    IP            NODE               NOMINATED NODE   READINESS GATES
  - pod/jellyfin   1/1     Running   0          4m1s   10.244.1.30   storagenodet3500   <none>           <none>
  - ''
  - NAME                       TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                         AGE    SELECTOR
  - service/jellyfin-service   NodePort   10.97.11.245   <none>        8096:30096/TCP,8920:30920/TCP   4m1s   app=jellyfin,component=media-server

TASK [Verify Grafana endpoint is accessible] ***********************************
ok: [masternode]

TASK [Verify Prometheus endpoint is accessible] ********************************
ok: [masternode]

TASK [Display endpoint health status] ******************************************
ok: [masternode] =>
  msg: |-
    Monitoring Endpoints Health:
    - Grafana:    OK
    - Prometheus: OK

TASK [Display deployment complete message] *************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ✅ VMStation Kubernetes Cluster Deployment Complete!

    Cluster Status:
    - Control Plane: masternode
    - Worker Nodes: storagenodet3500
    - CNI: Flannel
    - Monitoring: Prometheus, Grafana
    - Media Server: Jellyfin (on storagenodet3500)

    Access URLs:
    - Grafana:    http://192.168.4.63:30300
    - Prometheus: http://192.168.4.63:30090
    - Jellyfin:   http://192.168.4.61:30096

    Next Steps:
    - Check cluster: kubectl get nodes
    - View pods: kubectl get pods -A
    - Test monitoring: curl http://192.168.4.63:30300
    - Test Jellyfin: curl http://192.168.4.61:30096/health
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY [Phase 8: Wake-on-LAN Validation] *****************************************

TASK [Skip WoL tests unless explicitly enabled] ********************************
skipping: [masternode]

TASK [Build wol_targets from inventory when not explicitly provided] ***********
skipping: [masternode]

TASK [Ensure wakeonlan is installed on control node (Debian family)] ***********
ok: [masternode]

TASK [Create sleep helper script on target nodes] ******************************
ok: [masternode]

TASK [Trigger sleep helper on each wol target (run as background)] *************
changed: [masternode -> localhost] => (item={'name': 'storagenodet3500', 'ip': '192.168.4.61', 'mac': 'b8:ac:6f:7e:6c:9d'})
changed: [masternode -> localhost] => (item={'name': 'homelab', 'ip': '192.168.4.62', 'mac': 'd0:94:66:30:d6:63'})

TASK [Send WoL magic packets from control-plane] *******************************
changed: [masternode -> localhost] => (item={'name': 'storagenodet3500', 'ip': '192.168.4.61', 'mac': 'b8:ac:6f:7e:6c:9d'})
changed: [masternode -> localhost] => (item={'name': 'homelab', 'ip': '192.168.4.62', 'mac': 'd0:94:66:30:d6:63'})

TASK [Wait for node SSH to return (timeout 120s)] ******************************
ok: [masternode -> localhost] => (item={'name': 'storagenodet3500', 'ip': '192.168.4.61', 'mac': 'b8:ac:6f:7e:6c:9d'})
ok: [masternode -> localhost] => (item={'name': 'homelab', 'ip': '192.168.4.62', 'mac': 'd0:94:66:30:d6:63'})

TASK [Collect wake results] ****************************************************
ok: [masternode] => (item={'name': 'storagenodet3500', 'ip': '192.168.4.61', 'mac': 'b8:ac:6f:7e:6c:9d'})
ok: [masternode] => (item={'name': 'homelab', 'ip': '192.168.4.62', 'mac': 'd0:94:66:30:d6:63'})

TASK [Show WoL report] *********************************************************
ok: [masternode] =>
  wol_report:
  - ip: 192.168.4.61
    mac: b8:ac:6f:7e:6c:9d
    name: storagenodet3500
    wol_out: Sending magic packet to 255.255.255.255:9 with b8:ac:6f:7e:6c:9d
  - ip: 192.168.4.62
    mac: d0:94:66:30:d6:63
    name: homelab
    wol_out: Sending magic packet to 255.255.255.255:9 with d0:94:66:30:d6:63

PLAY RECAP *********************************************************************
masternode                 : ok=84   changed=35   unreachable=0    failed=0    skipped=8    rescued=0    ignored=0
storagenodet3500           : ok=28   changed=4    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0

[2025-10-08 22:43:14] [INFO]
[2025-10-08 22:43:14] [INFO] ✓ Debian deployment completed successfully
[2025-10-08 22:43:14] [INFO]
[2025-10-08 22:43:14] [INFO] Running post-deployment verification...
[2025-10-08 22:43:19] [INFO] Verifying Debian cluster health...
[2025-10-08 22:43:19] [INFO] ✓ Debian cluster is healthy (2 Debian nodes Ready)
[2025-10-08 22:43:19] [INFO]
[2025-10-08 22:43:19] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:43:19] [INFO]   Debian Kubernetes Cluster Ready!
[2025-10-08 22:43:19] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:43:19] [INFO]
[2025-10-08 22:43:19] [INFO] Verification commands:
[2025-10-08 22:43:19] [INFO]   kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes
[2025-10-08 22:43:19] [INFO]   kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -A
[2025-10-08 22:43:19] [INFO]
[2025-10-08 22:43:19] [INFO] Log saved to: /srv/monitoring_data/VMStation/ansible/artifacts/deploy-debian.log
[2025-10-08 22:43:19] [INFO]
[2025-10-08 22:43:19] [INFO]
[2025-10-08 22:43:19] [INFO] Waiting 10 seconds before Phase 2...
[2025-10-08 22:43:29] [INFO]
[2025-10-08 22:43:29] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:43:29] [INFO]   PHASE 2: Deploying RKE2 to Homelab
[2025-10-08 22:43:29] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:43:29] [INFO] ========================================
[2025-10-08 22:43:29] [INFO]  Deploying RKE2 to Homelab (RHEL10)
[2025-10-08 22:43:29] [INFO] ========================================
[2025-10-08 22:43:29] [INFO] Target: homelab (192.168.4.62)
[2025-10-08 22:43:29] [INFO] Playbook: /srv/monitoring_data/VMStation/ansible/playbooks/install-rke2-homelab.yml
[2025-10-08 22:43:29] [INFO] Log: /srv/monitoring_data/VMStation/ansible/artifacts/install-rke2-homelab.log
[2025-10-08 22:43:29] [INFO]
[2025-10-08 22:43:29] [INFO] Running pre-flight checks...
[2025-10-08 22:43:29] [INFO] Verifying SSH connectivity to homelab...
[2025-10-08 22:43:31] [INFO] ✓ SSH connectivity to homelab verified
[2025-10-08 22:43:31] [INFO] Checking if homelab is clean (no kubeadm artifacts)...
[2025-10-08 22:43:32] [INFO] ✓ homelab appears clean
[2025-10-08 22:43:32] [INFO] Verifying Debian cluster health...
[2025-10-08 22:43:32] [INFO] ✓ Debian cluster is healthy (2 Debian nodes Ready)
[2025-10-08 22:43:32] [INFO] ✓ Debian cluster is healthy - RKE2 federation will work
[2025-10-08 22:43:32] [INFO] Starting RKE2 installation (this may take 15-20 minutes)...

PLAY [RKE2 Installation - Homelab Node] ****************************************

TASK [Gathering Facts] *********************************************************
ok: [homelab]

TASK [Display RKE2 installation banner] ****************************************
ok: [homelab] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    RKE2 Installation on Homelab (RHEL 10)
    Target: homelab
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Check if RKE2 is installed] **********************************************
ok: [homelab]

TASK [Display skip message if already installed] *******************************
skipping: [homelab]

TASK [Download RKE2 installation script] ***************************************
ok: [homelab]

TASK [Install RKE2] ************************************************************
changed: [homelab]

TASK [Log RKE2 installation output] ********************************************
ok: [homelab] =>
  msg:
  - '[INFO]  finding release for channel stable'
  - '[INFO]  using 1.33 series from channel stable'
  - Updating Subscription Management repositories.
  - 'Last metadata expiration check: 0:00:02 ago on Wed 08 Oct 2025 10:43:49 PM.'
  - Package rke2-server-1.33.5~rke2r1-0.el8.x86_64 is already installed.
  - Dependencies resolved.
  - Nothing to do.
  - Complete!

TASK [Create RKE2 config directory] ********************************************
ok: [homelab]

TASK [Create RKE2 configuration] ***********************************************
ok: [homelab]

TASK [Enable and start RKE2 server] ********************************************
ok: [homelab]

TASK [Wait for RKE2 to be ready] ***********************************************
skipping: [homelab]

TASK [Create kubectl symlink] **************************************************
ok: [homelab]

TASK [Set KUBECONFIG environment variable] *************************************
ok: [homelab]

TASK [Wait for kubectl binary to be available] *********************************
ok: [homelab]

TASK [Verify RKE2 is running] **************************************************
changed: [homelab]

TASK [Verify RKE2 pods are running] ********************************************
changed: [homelab]

TASK [Display RKE2 node status] ************************************************
ok: [homelab] =>
  msg:
  - NAME      STATUS   ROLES                       AGE   VERSION
  - homelab   Ready    control-plane,etcd,master   35h   v1.33.5+rke2r1

TASK [Display RKE2 pods status] ************************************************
ok: [homelab] =>
  msg:
  - NAMESPACE     NAME                                                   READY   STATUS      RESTARTS   AGE
  - kube-system   cloud-controller-manager-homelab                       1/1     Running     0          35h
  - kube-system   etcd-homelab                                           1/1     Running     0          35h
  - kube-system   helm-install-rke2-coredns-7n8lh                        0/1     Completed   0          35h
  - kube-system   helm-install-rke2-flannel-bt926                        0/1     Completed   0          35h
  - kube-system   helm-install-rke2-ingress-nginx-hps8d                  0/1     Completed   0          35h
  - kube-system   helm-install-rke2-metrics-server-jtqdr                 0/1     Completed   0          35h
  - kube-system   helm-install-rke2-runtimeclasses-jz8wg                 0/1     Completed   0          35h
  - kube-system   helm-install-rke2-snapshot-controller-crd-m7tnz        0/1     Completed   0          35h
  - kube-system   helm-install-rke2-snapshot-controller-vhqr5            0/1     Completed   0          35h
  - kube-system   kube-apiserver-homelab                                 1/1     Running     0          35h
  - kube-system   kube-controller-manager-homelab                        1/1     Running     0          35h
  - kube-system   kube-flannel-ds-bnkml                                  1/1     Running     0          35h
  - kube-system   kube-proxy-homelab                                     1/1     Running     0          35h
  - kube-system   kube-scheduler-homelab                                 1/1     Running     0          35h
  - kube-system   rke2-coredns-rke2-coredns-6464f98784-5vsfj             1/1     Running     0          35h
  - kube-system   rke2-coredns-rke2-coredns-autoscaler-67bb49dff-ws7zm   1/1     Running     0          35h
  - kube-system   rke2-ingress-nginx-controller-md86g                    1/1     Running     0          35h
  - kube-system   rke2-metrics-server-75d485c65b-tk6kw                   1/1     Running     0          35h
  - kube-system   rke2-snapshot-controller-696989ffdd-d7j9b              1/1     Running     0          35h

TASK [Create local artifacts directory] ****************************************
ok: [homelab -> localhost]

TASK [Fetch RKE2 kubeconfig] ***************************************************
changed: [homelab]

TASK [Update kubeconfig server address] ****************************************
changed: [homelab -> localhost]

TASK [Display installation complete message] ***********************************
ok: [homelab] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ✅ RKE2 Installation Complete

    Node: homelab
    Kubeconfig: /etc/rancher/rke2/rke2.yaml
    Local Kubeconfig: /srv/monitoring_data/VMStation/ansible/playbooks/../artifacts/homelab-rke2-kubeconfig.yaml

    Access cluster from homelab node:
    export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
    kubectl get nodes

    Access cluster remotely:
    export KUBECONFIG=/srv/monitoring_data/VMStation/ansible/playbooks/../artifacts/homelab-rke2-kubeconfig.yaml
    kubectl get nodes
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY RECAP *********************************************************************
homelab                    : ok=20   changed=5    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0

[2025-10-08 22:43:58] [INFO]
[2025-10-08 22:43:58] [INFO] ✓ RKE2 installation completed successfully
[2025-10-08 22:43:58] [INFO]
[2025-10-08 22:43:58] [INFO] Verifying RKE2 cluster...
[2025-10-08 22:43:58] [INFO] ✓ RKE2 cluster has 1 node(s) Ready
./deploy.sh: line 409: [[: 0
0: syntax error in expression (error token is "0")
[2025-10-08 22:43:58] [INFO]
[2025-10-08 22:43:58] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:43:58] [INFO]   RKE2 Cluster Ready on Homelab!
[2025-10-08 22:43:58] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:43:58] [INFO]
[2025-10-08 22:43:58] [INFO] Artifacts:
[2025-10-08 22:43:58] [INFO]   - Kubeconfig: /srv/monitoring_data/VMStation/ansible/artifacts/homelab-rke2-kubeconfig.yaml
[2025-10-08 22:43:58] [INFO]   - Log: /srv/monitoring_data/VMStation/ansible/artifacts/install-rke2-homelab.log
[2025-10-08 22:43:58] [INFO]
[2025-10-08 22:43:58] [INFO] Verification commands:
[2025-10-08 22:43:58] [INFO]   export KUBECONFIG=/srv/monitoring_data/VMStation/ansible/artifacts/homelab-rke2-kubeconfig.yaml
[2025-10-08 22:43:58] [INFO]   kubectl get nodes
[2025-10-08 22:43:58] [INFO]   kubectl get pods -A
[2025-10-08 22:43:58] [INFO]   kubectl get pods -n monitoring-rke2
[2025-10-08 22:43:58] [INFO]
[2025-10-08 22:43:58] [INFO] Monitoring endpoints:
[2025-10-08 22:43:58] [INFO]   - Node Exporter: http://192.168.4.62:9100/metrics
[2025-10-08 22:43:58] [INFO]   - Prometheus: http://192.168.4.62:30090
[2025-10-08 22:43:58] [INFO]   - Federation: http://192.168.4.62:30090/federate
[2025-10-08 22:43:58] [INFO]
[2025-10-08 22:43:58] [INFO] Federation test:
[2025-10-08 22:43:58] [INFO]   curl -s 'http://192.168.4.62:30090/federate?match[]={job=~".+"}' | head -20
[2025-10-08 22:43:58] [INFO]
[2025-10-08 22:43:58] [INFO]
[2025-10-08 22:43:58] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:43:58] [INFO]   TWO-PHASE DEPLOYMENT COMPLETE!
[2025-10-08 22:43:58] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 22:43:58] [INFO]
[2025-10-08 22:43:58] [INFO] Summary:
[2025-10-08 22:43:58] [INFO]   ✓ Debian cluster: monitoring_nodes + storage_nodes
[2025-10-08 22:43:58] [INFO]   ✓ RKE2 cluster: homelab
[2025-10-08 22:43:58] [INFO]
[2025-10-08 22:43:58] [INFO] Logs:
[2025-10-08 22:43:58] [INFO]   - /srv/monitoring_data/VMStation/ansible/artifacts/deploy-debian.log
[2025-10-08 22:43:58] [INFO]   - /srv/monitoring_data/VMStation/ansible/artifacts/install-rke2-homelab.log
[2025-10-08 22:43:58] [INFO]
[2025-10-08 22:43:58] [INFO] Setting up auto-sleep monitoring...

PLAY [Setup Auto-Sleep Monitoring] ********************************************************************************************************************************************************

TASK [Gathering Facts] ********************************************************************************************************************************************************************
ok: [masternode]
ok: [homelab]
ok: [storagenodet3500]

TASK [Display auto-sleep setup banner] ****************************************************************************************************************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Auto-Sleep Monitoring Setup
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ok: [storagenodet3500] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Auto-Sleep Monitoring Setup
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ok: [homelab] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Auto-Sleep Monitoring Setup
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Confirm setup] **********************************************************************************************************************************************************************
[Confirm setup]
This will configure auto-sleep monitoring.
Cluster will sleep after 2 hours of inactivity.
Continue? (yes/no)
:
yes^Mok: [masternode]

TASK [Check confirmation] *****************************************************************************************************************************************************************
skipping: [masternode]
fatal: [storagenodet3500]: FAILED! => changed=false
  msg: Setup cancelled by user
fatal: [homelab]: FAILED! => changed=false
  msg: Setup cancelled by user

TASK [Create auto-sleep monitor script] ***************************************************************************************************************************************************
ok: [masternode]

TASK [Create cluster sleep script] ********************************************************************************************************************************************************
ok: [masternode]

TASK [Create systemd service for auto-sleep monitor] **************************************************************************************************************************************
ok: [masternode]

TASK [Create systemd timer for auto-sleep monitor] ****************************************************************************************************************************************
ok: [masternode]

TASK [Reload systemd] *********************************************************************************************************************************************************************
ok: [masternode]

TASK [Enable and start auto-sleep timer] **************************************************************************************************************************************************
ok: [masternode]

TASK [Display setup complete message] *****************************************************************************************************************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ✅ Auto-Sleep Monitoring Setup Complete

    Cluster will automatically sleep after 2 hours of inactivity

    Monitor status:
    systemctl status vmstation-autosleep.timer

    Disable auto-sleep:
    systemctl stop vmstation-autosleep.timer
    systemctl disable vmstation-autosleep.timer
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY RECAP ********************************************************************************************************************************************************************************
homelab                    : ok=2    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0
masternode                 : ok=10   changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0
storagenodet3500           : ok=2    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0

=========================================
VMStation Security Audit
=========================================

[1/10] Checking for hardcoded secrets...
✅ PASS: Hardcoded passwords

[2/10] Checking SSH key security...
✅ PASS: SSH directory permissions

[3/10] Checking for encrypted sensitive files...
⚠️  WARNING: Ansible vault encryption - secrets.yml not encrypted with ansible-vault

[4/10] Checking Kubernetes security configurations...
⚠️  WARNING: Privileged containers - Found privileged container configurations
  Consider using specific capabilities instead
⚠️  WARNING: Host network usage - Pods using host network detected
  Review necessity of hostNetwork configuration
⚠️  WARNING: Resource limits - Some deployments may be missing resource limits

[5/10] Checking RBAC configurations...
✅ PASS: RBAC ClusterRoles
✅ PASS: RBAC verb specificity

[6/10] Checking script file permissions...
✅ PASS: Script permissions

[7/10] Checking .gitignore for sensitive patterns...
✅ PASS: Gitignore coverage

[8/10] Checking monitoring security configuration...
✅ PASS: Grafana anonymous access
✅ PASS: Grafana anonymous role

[9/10] Checking network security...
✅ PASS: NodePort services
  For production: Consider using Ingress with TLS
✅ PASS: LoadBalancer services

[10/10] Checking container image configurations...
⚠️  WARNING: Container image tags - Using :latest tag detected
  Pin to specific versions for reproducibility
✅ PASS: Container images
  Verify these are from trusted sources:
  - docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1
  - docker.io/flannel/flannel:v0.24.2
  - ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
  - ghcr.io/flannel-io/flannel:v0.27.4
  - grafana/grafana:10.0.0
  - grafana/loki:2.9.2
  - grafana/promtail:2.9.2
  - jellyfin/jellyfin:latest
  - prom/blackbox-exporter:v0.25.0
  - prometheuscommunity/ipmi-exporter:v1.6.1
  - prom/node-exporter:v1.6.1
  - prom/prometheus:v2.45.0
  - registry.k8s.io/coredns/coredns:v1.11.1
  - registry.k8s.io/kube-proxy:v1.29.15
  - registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.0

=========================================
Security Audit Summary
=========================================
Passed:   11
Warnings: 5
Errors:   0

⚠️  Security audit found warnings - review recommended
=========================================
VMStation Complete Validation Suite
=========================================

This test suite validates:
  1. Auto-sleep and wake configuration
  2. Monitoring exporters health
  3. Loki log aggregation
  4. Sleep/wake cycle (optional - requires confirmation)

Test order:
  - Non-destructive tests run first
  - Sleep/wake cycle test is optional (requires user confirmation)

Phase 1: Configuration Validation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Auto-Sleep/Wake Configuration
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Auto-Sleep/Wake Validation
Testing sleep/wake cycle and monitoring
=========================================

[1/10] Testing systemd timer on storagenodet3500...
✅ PASS: Auto-sleep timer is enabled on storagenodet3500
✅ PASS: Auto-sleep timer is active on storagenodet3500

[2/10] Testing systemd timer on homelab (RHEL10)...
✅ PASS: Auto-sleep timer is enabled on homelab
⚠️  WARN: Auto-sleep timer is not active on homelab

[3/10] Testing auto-sleep script existence...
✅ PASS: Auto-sleep monitor script exists on storagenodet3500
✅ PASS: Sleep script exists on storagenodet3500

[4/10] Testing WoL configuration...
✅ PASS: WoL script exists and is executable
⚠️  WARN: WoL systemd service not found on masternode

[5/10] Testing kubectl access...
✅ PASS: kubectl access verified on masternode
ℹ️  INFO: Current cluster status:
  masternode         Ready   control-plane   19m   v1.29.15
  storagenodet3500   Ready   <none>          18m   v1.29.15

[6/10] Testing WoL tool availability...
✅ PASS: wakeonlan tool is available

[7/10] Testing node reachability...
✅ PASS: storagenodet3500 is reachable (192.168.4.61)
✅ PASS: homelab is reachable (192.168.4.62)

[8/10] Testing monitoring service configuration...
✅ PASS: Monitoring namespace exists
✅ PASS: Prometheus pods are running
✅ PASS: Grafana pods are running

[9/10] Testing log file configuration...
✅ PASS: VMStation state directory exists
✅ PASS: Auto-sleep log files exist

[10/10] Testing systemd timer schedules...
✅ PASS: Auto-sleep timer is scheduled
ℹ️  INFO: Timer schedule:
  Wed 2025-10-08 22:56:02 EDT 7min left Wed 2025-10-08 22:41:02 EDT 7min ago vmstation-autosleep.timer vmstation-autosleep.service

=========================================
Test Results Summary
=========================================
Passed:   16
Failed:   0
Warnings: 2

✅ All critical tests passed!

Auto-Sleep/Wake Configuration:
  - Systemd timers configured on both nodes
  - Scripts deployed and executable
  - Monitoring services available

Manual Testing:
  1. Trigger sleep: ssh root@192.168.4.63 'sudo /usr/local/bin/vmstation-sleep.sh'
  2. Check node status: ssh root@192.168.4.63 'kubectl get nodes'
  3. Send WoL: wakeonlan b8:ac:6f:7e:6c:9d
  4. Monitor wake time and verify services

✅ SUITE PASSED: Auto-Sleep/Wake Configuration


Phase 2: Monitoring Health Validation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Monitoring Exporters Health
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Monitoring Exporters Health
Validating exporters, targets, and dashboards
=========================================

[1/8] Testing Prometheus targets...
✅ PASS: Prometheus targets API accessible
ℹ️  INFO: Targets UP: 19, DOWN: 4
⚠️  WARN: Some kubernetes-service-endpoints targets are down (may be normal)
⚠️  WARN: 4 targets are DOWN (optional services)
  Down targets:
    - kubernetes-service-endpoints

[2/8] Testing node-exporter on all nodes...
curl http://192.168.4.63:9100/metrics success
✅ PASS: Node exporter healthy on masternode
curl http://192.168.4.61:9100/metrics success
✅ PASS: Node exporter healthy on storagenodet3500
❌ SUITE FAILED: Monitoring Exporters Health


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Loki Log Aggregation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Loki Log Aggregation Test
Validating Loki connectivity and log ingestion
=========================================

[1/6] Testing Loki pod status...
ℹ️  INFO: Loki pods found:
  loki-669bb4f448-42xpp                 0/1     CrashLoopBackOff   8 (2m45s ago)   18m
❌ FAIL: Loki pods are not running

[2/6] Testing Loki service configuration...
✅ PASS: Loki service exists
ℹ️  INFO: Loki service details:
  loki                   NodePort    10.111.230.147   <none>        3100:31100/TCP      18m
✅ PASS: Loki service has endpoints

[3/6] Testing Loki API connectivity...
curl http://192.168.4.63:31100/ready error
⚠️  WARN: Loki ready endpoint not accessible (may be ClusterIP only)
ℹ️  INFO: Attempting to test via kubectl exec...
❌ FAIL: Loki is not ready

[4/6] Testing Promtail log shipper...
ℹ️  INFO: Promtail pods found:
  promtail-2cbkf                        1/1     Running            0               18m
  promtail-nn8rv                        1/1     Running            0               18m
✅ PASS: Promtail pods are running (2 instances)

[5/6] Testing Loki DNS resolution...
✅ PASS: Loki DNS resolution successful

[6/6] Testing Loki datasource in Grafana...
✅ PASS: Loki datasource is configured in Grafana
⚠️  WARN: Could not check Loki datasource health

=========================================
Test Results Summary
=========================================
Passed:   5
Failed:   2
Warnings: 2

❌ Loki log aggregation has issues.

Common fixes:
  1. Check Loki logs: kubectl logs -n monitoring -l app=loki
  2. Check Promtail logs: kubectl logs -n monitoring -l app=promtail
  3. Verify Loki service: kubectl get svc -n monitoring loki
  4. Check DNS: kubectl run -it --rm dns-test --image=busybox --restart=Never -- nslookup loki.monitoring

Connectivity errors:
  - DNS lookup failures: Check CoreDNS pods
  - 500 status: Check Loki logs for errors
  - Service unavailable: Verify Loki pods are running

❌ SUITE FAILED: Loki Log Aggregation


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Monitoring Access (Updated)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Monitoring Access Test
Testing anonymous access to endpoints
=========================================

[1/8] Testing Grafana Access...
Testing Grafana Web UI... ✅ PASS
  curl http://192.168.4.63:30300 success
Testing Grafana API (anonymous)... ✅ PASS
  curl http://192.168.4.63:30300/api/health success

[2/8] Testing Prometheus Access...
Testing Prometheus Web UI... ✅ PASS
  curl http://192.168.4.63:30090 success
Testing Prometheus Health... ❌ FAIL (unexpected response)
  curl http://192.168.4.63:30090/-/healthy failure
❌ SUITE FAILED: Monitoring Access (Updated)


Phase 3: Sleep/Wake Cycle Test (Optional)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

WARNING: This test will trigger cluster sleep and wake.
This is a destructive test that will:
  - Cordon and drain worker nodes
  - Scale down deployments
  - Send Wake-on-LAN packets
  - Measure wake time and validate service restoration

Run sleep/wake cycle test? [y/N]: N
⚠️  Sleep/wake cycle test skipped by user

=========================================
Complete Validation Summary
=========================================

Test Suites Run:    4
Suites Passed:      1
Suites Failed:      3

❌ Some test suites failed.

Review the output above for details.

Common next steps:
  1. Fix failed tests and re-run: ./tests/test-complete-validation.sh
  2. Deploy missing components: ./deploy.sh setup
  3. Check cluster health: kubectl get pods -A
  4. Review logs: journalctl -u vmstation-autosleep -n 50

=========================================
VMStation Monitoring Exporters Health
Validating exporters, targets, and dashboards
=========================================

[1/8] Testing Prometheus targets...
✅ PASS: Prometheus targets API accessible
ℹ️  INFO: Targets UP: 19, DOWN: 4
⚠️  WARN: Some kubernetes-service-endpoints targets are down (may be normal)
⚠️  WARN: 4 targets are DOWN (optional services)
  Down targets:
    - kubernetes-service-endpoints

[2/8] Testing node-exporter on all nodes...
curl http://192.168.4.63:9100/metrics success
✅ PASS: Node exporter healthy on masternode
curl http://192.168.4.61:9100/metrics success
✅ PASS: Node exporter healthy on storagenodet3500
========================================
Deployment Fixes Validation
========================================

1. Testing Loki readiness probe configuration...
✅ PASS: Loki readiness probe is configured
✅ PASS: Loki liveness probe is configured
✅ PASS: Loki readiness probe has appropriate initialDelaySeconds

2. Testing Loki test port configuration...
✅ PASS: Loki test uses correct NodePort (31100)

3. Testing Prometheus Web UI test pattern...
✅ PASS: Prometheus test has multiple patterns for robust matching

4. Testing monitoring exporters optional targets handling...
✅ PASS: Monitoring exporters test handles optional targets
✅ PASS: RKE2 and IPMI exporters are marked as optional

5. Testing Loki schema configuration (24h period)...
✅ PASS: Loki schema uses 24h index period (boltdb-shipper compatible)

6. Validating YAML syntax...
✅ PASS: All YAML manifests have valid syntax

7. Validating bash script syntax...
✅ PASS: All bash test scripts have valid syntax

========================================
Validation Results
========================================
Passed: 10
Failed: 0

✅ All deployment fixes are correctly applied!

The following issues have been resolved:
  1. Loki readiness/liveness probes configured
  2. Loki test uses correct NodePort (31100)
  3. Prometheus Web UI test is robust
  4. Monitoring exporters test handles optional targets
  5. Loki schema is boltdb-shipper compatible

Ready for deployment!
