All code is run on the masternode 192.168.4.63 is running debian bookworm, it has SSH keys on all machines so it can communicate with them. This is the monitoring node that also runs the dashboards and ingests logs and metrics from the other nodes and pods.
The masternode is the control-plane node.
the storagenodet3500 192.168.4.61 is running debian bookworm the SAMBA server and runs the jellyfin node. Currently this node is only used to stream jellyfin to my families devices. We should avoid any unnecesary pods on this device to ensure optimum bandwidth and computation for streaming.
the homelab 192.168.4.62 is the compute node that is running RHEL10 and currently has the least pods on it. I will eventually add pods that I will use for my homelab. In fact you can come up with some ideas. It will also eventually be my lab for testing VM interconnectivity for praciticng with job interviews.
Then names in the inventory filee have been updated to inventory/group_vars/hosts.yml



I eventually want the coredns node to service all physically connected links while my IPS's modem and router can service wireless connections. but currently the ansible deployment is very unstable due to bad practices and incorrect playbooks.

The kube-proxy and kube-flannel and all other necessary backbone pods should be operating correctly upon deployment and not needing stupid scripts post deployment to attempt to fix it.

I need this to be a clean setup so that i can learn best practices and spin up and spin down whenever I need to maintain cost effectivness. For example there should be a hourly batch process that happens on the masternode that checks to see if the resources are being utilized currently (such as users logged into jellyfin or not) and if they are not it should begin the process of spinning down and sleeping all nodes while ensureing they can be awoken on LAN using magic packets. The masternode is a minipc and uses the least amount of energy so I am fine ensuring 100% uptime on that machine, especially since it contians the coredns node that will be neecessary for wireless devices. I can eventually add in enterprise grade security systems and frameworks to play with as well. 
I also plan to implement other strong netwwork security such as rotating TLS certificates, network wide password managment, 

This is my ansible version. If you think I should upgrade go ahead and do it wherever you see it necessary.
ansible [core 2.14.18]
  config file = None
  configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python3/dist-packages/ansible
  ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections
  executable location = /usr/bin/ansible
  python version = 3.11.2 (main, Apr 28 2025, 14:11:48) [GCC 12.2.0] (/usr/bin/python3)
  jinja version = 3.1.2
  libyaml = True

This is my kubectl version, again upgrade anything wherever you see fit as long as it maintains oeprational
Client Version: v1.34.0
Kustomize Version: v5.7.1
Server Version: v1.29.15
Warning: version difference between client (1.34) and server (1.29) exceeds the supported minor version skew of +/-1


The ansible playbooks must be gold-standard 100% robustness with never-fail idempotent setup. That is 
I should be able to do 'deploy.sh' -> 'deploy.sh reset' -> 'deploy.sh' 100 times in a row with no failures. Do not begin until u understand the nuances between RHEL 10 (NFTABLES BACKEND) and Debian Bookworm (IPTABLES BACKEND)
Thee playbooks should bee short and concise with no errors on any run/deployment.
All the Main and deploy-cluster playbooks are corrupted or wrong. Ensure they are short concise but do exactly what is neded of them
Do not put oveerly long timeouts it just leads to longer wait times for errors to appear, however I am certain you can accomplish this task the first time without errors.

root@masternode:/srv/monitoring_data/VMStation# git pull

# 2. Reset cluster
./deploy.sh reset  # Type 'yes'

# 3. Deploy with fixes
./deploy.sh

# 4. Validate (10 minutes)
watch -n 30 'kubectl get pods -A | grep -v Running'
# Should be empty (all pods Running)

# 5. Check for CrashLoopBackOff
kubectl get pods -A | grep -i crash
# Should be empty (no crashes)
remote: Enumerating objects: 22, done.
remote: Counting objects: 100% (22/22), done.
remote: Compressing objects: 100% (10/10), done.
remote: Total 22 (delta 10), reused 22 (delta 10), pack-reused 0 (from 0)
Unpacking objects: 100% (22/22), 40.16 KiB | 1.34 MiB/s, done.
From https://github.com/JashandeepJustinBains/VMStation
   adc6426..3dd8c43  main       -> origin/main
Updating adc6426..3dd8c43
Fast-forward
 COMPLETE_FIX_IMPLEMENTATION.md             | 488 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 DEPLOYMENT_VALIDATION_CHECKLIST.md         | 398 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 Output_for_Copilot.txt                     | 528 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 README.md                                  | 129 ++++++++++++++++++++++++++++++++----
 ansible/playbooks/deploy-cluster.yaml      | 177 ++++++++++++++++++++++++++++++--------------------
 ansible/roles/cluster-reset/tasks/main.yml |  20 ++++++
 ansible/roles/network-fix/tasks/main.yml   |  52 +++++++++++++++
 manifests/cni/flannel.yaml                 |  31 ++++++++-
 8 files changed, 1738 insertions(+), 85 deletions(-)
 create mode 100644 COMPLETE_FIX_IMPLEMENTATION.md
 create mode 100644 DEPLOYMENT_VALIDATION_CHECKLIST.md
[INFO] Running comprehensive cluster reset playbook: /srv/monitoring_data/VMStation/ansible/playbooks/reset-cluster.yaml
[INFO] This will remove all Kubernetes config and network interfaces
[INFO] SSH keys and physical ethernet interfaces will be preserved
[WARNING]: Collection community.general does not support Ansible version 2.14.18
[DEPRECATION WARNING]: community.general.yaml has been deprecated. The plugin has been superseded by the the option `result_format=yaml` in callback plugin ansible.builtin.default from ansible-core
2.13 onwards. This feature will be removed from community.general in version 12.0.0. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.

PLAY [Pre-reset validation and spin-down] *************************************************************************************************************************************************************

TASK [Confirm reset operation] ************************************************************************************************************************************************************************
[Confirm reset operation]
⚠️  CLUSTER RESET OPERATION ⚠️

This will:
- Stop all Kubernetes workloads
- Run kubeadm reset on all nodes
- Remove all Kubernetes config files
- Delete all Kubernetes network interfaces
- Clean container runtime state

This will NOT affect:
- SSH keys and access
- Physical ethernet interfaces
- Container runtime binaries

Type 'yes' to proceed with reset
:
yes^Mok: [localhost]

TASK [Abort if not confirmed] *************************************************************************************************************************************************************************
skipping: [localhost]

TASK [Generate spin targets from cluster] *************************************************************************************************************************************************************
ok: [localhost]

TASK [Set spin_targets fact] **************************************************************************************************************************************************************************
ok: [localhost]

TASK [Display nodes to be reset] **********************************************************************************************************************************************************************
ok: [localhost] =>
  msg: 'Nodes to reset: homelab, masternode, storagenodet3500'

PLAY [Drain and cordon nodes gracefully] **************************************************************************************************************************************************************

TASK [Cordon nodes to prevent new pods] ***************************************************************************************************************************************************************
changed: [localhost] => (item=homelab)
changed: [localhost] => (item=masternode)
changed: [localhost] => (item=storagenodet3500)

TASK [Drain nodes safely] *****************************************************************************************************************************************************************************
changed: [localhost] => (item=homelab)
changed: [localhost] => (item=masternode)
changed: [localhost] => (item=storagenodet3500)

TASK [Wait for pods to terminate] *********************************************************************************************************************************************************************
Pausing for 10 seconds
(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
ok: [localhost]

PLAY [Reset all worker nodes] *************************************************************************************************************************************************************************

TASK [Gathering Facts] ********************************************************************************************************************************************************************************
ok: [storagenodet3500]

TASK [cluster-reset : Stop kubelet service before reset] **********************************************************************************************************************************************
changed: [storagenodet3500]

TASK [cluster-reset : Run kubeadm reset with force flag] **********************************************************************************************************************************************
changed: [storagenodet3500]

TASK [cluster-reset : Display kubeadm reset output] ***************************************************************************************************************************************************
ok: [storagenodet3500] =>
  kubeadm_reset_result.stdout_lines:
  - '[preflight] Running pre-flight checks'
  - '[reset] Deleted contents of the etcd data directory: /var/lib/etcd'
  - '[reset] Stopping the kubelet service'
  - '[reset] Unmounting mounted directories in "/var/lib/kubelet"'
  - '[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]'
  - '[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]'
  - ''
  - The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d
  - ''
  - The reset process does not reset or clean up iptables rules or IPVS tables.
  - If you wish to reset iptables, you must do so manually by using the "iptables" command.
  - ''
  - If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
  - to reset your system's IPVS tables.
  - ''
  - The reset process does not clean your kubeconfig files and you must remove them manually.
  - Please, check the contents of the $HOME/.kube/config file.

TASK [cluster-reset : Remove all CNI config files (cni/cbr cleanup)] **********************************************************************************************************************************
changed: [storagenodet3500]

TASK [cluster-reset : Remove Kubernetes config directories (except /etc/cni/net.d)] *******************************************************************************************************************
changed: [storagenodet3500] => (item=/etc/kubernetes)
changed: [storagenodet3500] => (item=/var/lib/kubelet)
ok: [storagenodet3500] => (item=/var/lib/etcd)
changed: [storagenodet3500] => (item=/var/lib/cni)
changed: [storagenodet3500] => (item=/run/flannel)
ok: [storagenodet3500] => (item=/var/lib/flannel)
ok: [storagenodet3500] => (item=/var/run/flannel)
changed: [storagenodet3500] => (item=/opt/cni/bin)

TASK [cluster-reset : Identify Kubernetes-related network interfaces] *********************************************************************************************************************************
ok: [storagenodet3500]

TASK [cluster-reset : Display identified Kubernetes interfaces] ***************************************************************************************************************************************
ok: [storagenodet3500] =>
  msg: 'Kubernetes interfaces to remove: [''flannel.1'', ''cni0'']'

TASK [cluster-reset : Recreate /etc/kubernetes/manifests directory (empty)] ***************************************************************************************************************************
changed: [storagenodet3500]

TASK [cluster-reset : Bring down Kubernetes network interfaces] ***************************************************************************************************************************************
changed: [storagenodet3500] => (item=flannel.1)
changed: [storagenodet3500] => (item=cni0)

TASK [cluster-reset : Delete Kubernetes network interfaces] *******************************************************************************************************************************************
changed: [storagenodet3500] => (item=flannel.1)
changed: [storagenodet3500] => (item=cni0)

TASK [cluster-reset : Remove iptables rules created by Kubernetes (flush all chains)] *****************************************************************************************************************
changed: [storagenodet3500]

TASK [cluster-reset : Flush nftables rules on RHEL 10+ (cleanup nftables tables)] *********************************************************************************************************************
skipping: [storagenodet3500]

TASK [cluster-reset : Remove ipvs rules if present] ***************************************************************************************************************************************************
changed: [storagenodet3500]

TASK [cluster-reset : Clean container runtime state (containerd)] *************************************************************************************************************************************
changed: [storagenodet3500]

TASK [cluster-reset : Restart containerd after cleanup] ***********************************************************************************************************************************************
changed: [storagenodet3500]

TASK [cluster-reset : Remove any remaining Kubernetes processes] **************************************************************************************************************************************
changed: [storagenodet3500]

TASK [cluster-reset : Find all authorized_keys files] *************************************************************************************************************************************************
ok: [storagenodet3500]

TASK [cluster-reset : Assert at least one SSH authorized_keys file exists] ****************************************************************************************************************************
ok: [storagenodet3500] => changed=false
  msg: SSH keys preserved successfully

TASK [cluster-reset : Verify physical ethernet interfaces are preserved] ******************************************************************************************************************************
ok: [storagenodet3500]

TASK [cluster-reset : Assert physical interfaces still exist] *****************************************************************************************************************************************
ok: [storagenodet3500] => changed=false
  msg: Physical ethernet interfaces preserved successfully

TASK [cluster-reset : Display reset completion summary] ***********************************************************************************************************************************************
ok: [storagenodet3500] =>
  msg: |-
    Kubernetes cluster reset completed successfully:
    - kubeadm reset: OK
    - Kubernetes interfaces removed: 2
    - SSH keys: preserved
    - Physical interfaces: preserved
    - Node is ready for fresh cluster deployment

PLAY [Reset all worker nodes] *************************************************************************************************************************************************************************

TASK [Gathering Facts] ********************************************************************************************************************************************************************************
ok: [homelab]

TASK [cluster-reset : Stop kubelet service before reset] **********************************************************************************************************************************************
changed: [homelab]

TASK [cluster-reset : Run kubeadm reset with force flag] **********************************************************************************************************************************************
changed: [homelab]

TASK [cluster-reset : Display kubeadm reset output] ***************************************************************************************************************************************************
ok: [homelab] =>
  kubeadm_reset_result.stdout_lines:
  - '[preflight] Running pre-flight checks'
  - '[reset] Deleted contents of the etcd data directory: /var/lib/etcd'
  - '[reset] Stopping the kubelet service'
  - '[reset] Unmounting mounted directories in "/var/lib/kubelet"'
  - '[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]'
  - '[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]'
  - ''
  - The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d
  - ''
  - The reset process does not reset or clean up iptables rules or IPVS tables.
  - If you wish to reset iptables, you must do so manually by using the "iptables" command.
  - ''
  - If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
  - to reset your system's IPVS tables.
  - ''
  - The reset process does not clean your kubeconfig files and you must remove them manually.
  - Please, check the contents of the $HOME/.kube/config file.

TASK [cluster-reset : Remove all CNI config files (cni/cbr cleanup)] **********************************************************************************************************************************
changed: [homelab]

TASK [cluster-reset : Remove Kubernetes config directories (except /etc/cni/net.d)] *******************************************************************************************************************
changed: [homelab] => (item=/etc/kubernetes)
changed: [homelab] => (item=/var/lib/kubelet)
ok: [homelab] => (item=/var/lib/etcd)
changed: [homelab] => (item=/var/lib/cni)
changed: [homelab] => (item=/run/flannel)
ok: [homelab] => (item=/var/lib/flannel)
ok: [homelab] => (item=/var/run/flannel)
changed: [homelab] => (item=/opt/cni/bin)

TASK [cluster-reset : Identify Kubernetes-related network interfaces] *********************************************************************************************************************************
ok: [homelab]

TASK [cluster-reset : Display identified Kubernetes interfaces] ***************************************************************************************************************************************
ok: [homelab] =>
  msg: 'Kubernetes interfaces to remove: [''flannel.1'']'

TASK [cluster-reset : Recreate /etc/kubernetes/manifests directory (empty)] ***************************************************************************************************************************
changed: [homelab]

TASK [cluster-reset : Bring down Kubernetes network interfaces] ***************************************************************************************************************************************
changed: [homelab] => (item=flannel.1)

TASK [cluster-reset : Delete Kubernetes network interfaces] *******************************************************************************************************************************************
changed: [homelab] => (item=flannel.1)

TASK [cluster-reset : Remove iptables rules created by Kubernetes (flush all chains)] *****************************************************************************************************************
changed: [homelab]

TASK [cluster-reset : Flush nftables rules on RHEL 10+ (cleanup nftables tables)] *********************************************************************************************************************
changed: [homelab]

TASK [cluster-reset : Remove ipvs rules if present] ***************************************************************************************************************************************************
changed: [homelab]

TASK [cluster-reset : Clean container runtime state (containerd)] *************************************************************************************************************************************
changed: [homelab]

TASK [cluster-reset : Restart containerd after cleanup] ***********************************************************************************************************************************************
changed: [homelab]

TASK [cluster-reset : Remove any remaining Kubernetes processes] **************************************************************************************************************************************
changed: [homelab]

TASK [cluster-reset : Find all authorized_keys files] *************************************************************************************************************************************************
ok: [homelab]

TASK [cluster-reset : Assert at least one SSH authorized_keys file exists] ****************************************************************************************************************************
ok: [homelab] => changed=false
  msg: SSH keys preserved successfully

TASK [cluster-reset : Verify physical ethernet interfaces are preserved] ******************************************************************************************************************************
ok: [homelab]

TASK [cluster-reset : Assert physical interfaces still exist] *****************************************************************************************************************************************
ok: [homelab] => changed=false
  msg: Physical ethernet interfaces preserved successfully

TASK [cluster-reset : Display reset completion summary] ***********************************************************************************************************************************************
ok: [homelab] =>
  msg: |-
    Kubernetes cluster reset completed successfully:
    - kubeadm reset: OK
    - Kubernetes interfaces removed: 1
    - SSH keys: preserved
    - Physical interfaces: preserved
    - Node is ready for fresh cluster deployment

PLAY [Reset control plane node] ***********************************************************************************************************************************************************************

TASK [Gathering Facts] ********************************************************************************************************************************************************************************
ok: [masternode]

TASK [cluster-reset : Stop kubelet service before reset] **********************************************************************************************************************************************
changed: [masternode]

TASK [cluster-reset : Run kubeadm reset with force flag] **********************************************************************************************************************************************
changed: [masternode]

TASK [cluster-reset : Display kubeadm reset output] ***************************************************************************************************************************************************
ok: [masternode] =>
  kubeadm_reset_result.stdout_lines:
  - '[reset] Reading configuration from the cluster...'
  - '[reset] FYI: You can look at this config file with ''kubectl -n kube-system get cm kubeadm-config -o yaml'''
  - '[preflight] Running pre-flight checks'
  - '[reset] Deleted contents of the etcd data directory: /var/lib/etcd'
  - '[reset] Stopping the kubelet service'
  - '[reset] Unmounting mounted directories in "/var/lib/kubelet"'
  - '[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki]'
  - '[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]'
  - ''
  - The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d
  - ''
  - The reset process does not reset or clean up iptables rules or IPVS tables.
  - If you wish to reset iptables, you must do so manually by using the "iptables" command.
  - ''
  - If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
  - to reset your system's IPVS tables.
  - ''
  - The reset process does not clean your kubeconfig files and you must remove them manually.
  - Please, check the contents of the $HOME/.kube/config file.

TASK [cluster-reset : Remove all CNI config files (cni/cbr cleanup)] **********************************************************************************************************************************
changed: [masternode]

TASK [cluster-reset : Remove Kubernetes config directories (except /etc/cni/net.d)] *******************************************************************************************************************
changed: [masternode] => (item=/etc/kubernetes)
changed: [masternode] => (item=/var/lib/kubelet)
changed: [masternode] => (item=/var/lib/etcd)
changed: [masternode] => (item=/var/lib/cni)
changed: [masternode] => (item=/run/flannel)
ok: [masternode] => (item=/var/lib/flannel)
ok: [masternode] => (item=/var/run/flannel)
changed: [masternode] => (item=/opt/cni/bin)

TASK [cluster-reset : Identify Kubernetes-related network interfaces] *********************************************************************************************************************************
ok: [masternode]

TASK [cluster-reset : Display identified Kubernetes interfaces] ***************************************************************************************************************************************
ok: [masternode] =>
  msg: 'Kubernetes interfaces to remove: [''flannel.1'', ''cni0'']'

TASK [cluster-reset : Recreate /etc/kubernetes/manifests directory (empty)] ***************************************************************************************************************************
changed: [masternode]

TASK [cluster-reset : Bring down Kubernetes network interfaces] ***************************************************************************************************************************************
changed: [masternode] => (item=flannel.1)
changed: [masternode] => (item=cni0)

TASK [cluster-reset : Delete Kubernetes network interfaces] *******************************************************************************************************************************************
changed: [masternode] => (item=flannel.1)
changed: [masternode] => (item=cni0)

TASK [cluster-reset : Remove iptables rules created by Kubernetes (flush all chains)] *****************************************************************************************************************
changed: [masternode]

TASK [cluster-reset : Flush nftables rules on RHEL 10+ (cleanup nftables tables)] *********************************************************************************************************************
skipping: [masternode]

TASK [cluster-reset : Remove ipvs rules if present] ***************************************************************************************************************************************************
changed: [masternode]

TASK [cluster-reset : Clean container runtime state (containerd)] *************************************************************************************************************************************
changed: [masternode]

TASK [cluster-reset : Restart containerd after cleanup] ***********************************************************************************************************************************************
changed: [masternode]

TASK [cluster-reset : Remove any remaining Kubernetes processes] **************************************************************************************************************************************
changed: [masternode]

TASK [cluster-reset : Find all authorized_keys files] *************************************************************************************************************************************************
ok: [masternode]

TASK [cluster-reset : Assert at least one SSH authorized_keys file exists] ****************************************************************************************************************************
ok: [masternode] => changed=false
  msg: SSH keys preserved successfully

TASK [cluster-reset : Verify physical ethernet interfaces are preserved] ******************************************************************************************************************************
ok: [masternode]

TASK [cluster-reset : Assert physical interfaces still exist] *****************************************************************************************************************************************
ok: [masternode] => changed=false
  msg: Physical ethernet interfaces preserved successfully

TASK [cluster-reset : Display reset completion summary] ***********************************************************************************************************************************************
ok: [masternode] =>
  msg: |-
    Kubernetes cluster reset completed successfully:
    - kubeadm reset: OK
    - Kubernetes interfaces removed: 2
    - SSH keys: preserved
    - Physical interfaces: preserved
    - Node is ready for fresh cluster deployment

PLAY [Post-reset validation] **************************************************************************************************************************************************************************

TASK [Verify kubelet is stopped] **********************************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [Verify no Kubernetes config remains] ************************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [Assert clean state] *****************************************************************************************************************************************************************************
ok: [masternode] => changed=false
  msg: Clean reset verified on masternode
ok: [storagenodet3500] => changed=false
  msg: Clean reset verified on storagenodet3500
ok: [homelab] => changed=false
  msg: Clean reset verified on homelab

TASK [Verify SSH connectivity after reset] ************************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [Display final reset summary] ********************************************************************************************************************************************************************
ok: [masternode] =>
  msg: |-
    ✅ Cluster reset completed successfully on masternode
    - Kubernetes config removed
    - Network interfaces cleaned
    - SSH access preserved
    - Ready for fresh deployment
ok: [storagenodet3500] =>
  msg: |-
    ✅ Cluster reset completed successfully on storagenodet3500
    - Kubernetes config removed
    - Network interfaces cleaned
    - SSH access preserved
    - Ready for fresh deployment
ok: [homelab] =>
  msg: |-
    ✅ Cluster reset completed successfully on homelab
    - Kubernetes config removed
    - Network interfaces cleaned
    - SSH access preserved
    - Ready for fresh deployment

PLAY [Final summary] **********************************************************************************************************************************************************************************

TASK [Display completion message] *********************************************************************************************************************************************************************
ok: [localhost] =>
  msg: |2-

    ╔════════════════════════════════════════════════════════════╗
    ║         CLUSTER RESET COMPLETED SUCCESSFULLY              ║
    ╚════════════════════════════════════════════════════════════╝

    All nodes have been reset and are ready for deployment.

    Next steps:
    1. Run deployment: ./deploy.sh
    2. Or manually: ansible-playbook -i ansible/inventory/hosts ansible/playbooks/deploy-cluster.yaml

    All nodes are now in a clean state with:
    ✅ No Kubernetes configuration
    ✅ No CNI network interfaces
    ✅ SSH access preserved
    ✅ Physical interfaces intact

PLAY RECAP ********************************************************************************************************************************************************************************************
homelab                    : ok=27   changed=13   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
localhost                  : ok=8    changed=2    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0
masternode                 : ok=26   changed=12   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0
storagenodet3500           : ok=26   changed=12   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0

[INFO] Reset completed successfully
[INFO] Cluster is ready for fresh deployment
[INFO] Running deploy playbook: /srv/monitoring_data/VMStation/ansible/playbooks/deploy-cluster.yaml
[WARNING]: Collection community.general does not support Ansible version 2.14.18
[WARNING]: Collection ansible.posix does not support Ansible version 2.14.18
[WARNING]: Collection kubernetes.core does not support Ansible version 2.14.18
[DEPRECATION WARNING]: community.general.yaml has been deprecated. The plugin has been superseded by the the option `result_format=yaml` in callback plugin ansible.builtin.default from ansible-core
2.13 onwards. This feature will be removed from community.general in version 12.0.0. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.

PLAY [Phase 1 - System preparation on all nodes] ******************************************************************************************************************************************************

TASK [Gathering Facts] ********************************************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [system-prep : Debug become password variable] ***************************************************************************************************************************************************
ok: [masternode] =>
  msg: 'masternode: ansible_become_pass is '
ok: [storagenodet3500] =>
  msg: 'storagenodet3500: ansible_become_pass is hidden'
ok: [homelab] =>
  msg: 'homelab: ansible_become_pass is hidden'

TASK [system-prep : Ensure /etc/hosts has all cluster nodes] ******************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [system-prep : Check kubectl version] ************************************************************************************************************************************************************
changed: [masternode]
changed: [storagenodet3500]
changed: [homelab]

TASK [system-prep : Check kubelet version] ************************************************************************************************************************************************************
changed: [masternode]
changed: [storagenodet3500]
changed: [homelab]

TASK [system-prep : Gather package manager info (apt)] ************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [system-prep : Warn about version skew between client and server if known] ***********************************************************************************************************************
ok: [masternode] =>
  msg: |-
    kubectl: Client Version: v1.34.0
    Kustomize Version: v5.7.1
    kubelet: Kubernetes v1.29.15
ok: [storagenodet3500] =>
  msg: |-
    kubectl: Client Version: v1.29.15
    Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
    kubelet: Kubernetes v1.29.15
ok: [homelab] =>
  msg: |-
    kubectl: Client Version: v1.29.15
    Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
    kubelet: Kubernetes v1.29.15

TASK [preflight : Check for connectivity to all hosts] ************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [preflight : Ensure required kernel modules present (overlay, br_netfilter)] *********************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [preflight : Verify sysctl parameters] ***********************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [preflight : Fail if kubelet not present] ********************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [preflight : Abort when kubelet missing] *********************************************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
skipping: [homelab]

TASK [network-fix : Ensure swap is disabled (immediate, before kubelet)] ******************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Disable swap in /etc/fstab (persistent across reboots)] ***************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Load all required kernel modules (immediate, before kubelet)] *********************************************************************************************************************
ok: [masternode] => (item=br_netfilter)
ok: [storagenodet3500] => (item=br_netfilter)
ok: [homelab] => (item=br_netfilter)
ok: [masternode] => (item=overlay)
ok: [storagenodet3500] => (item=overlay)
ok: [masternode] => (item=nf_conntrack)
ok: [homelab] => (item=overlay)
ok: [storagenodet3500] => (item=nf_conntrack)
ok: [masternode] => (item=vxlan)
ok: [storagenodet3500] => (item=vxlan)
ok: [homelab] => (item=nf_conntrack)
ok: [homelab] => (item=vxlan)

TASK [network-fix : Persist kernel modules for boot] **************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Set all required sysctl parameters (immediate, before kubelet)] *******************************************************************************************************************
ok: [masternode] => (item={'name': 'net.bridge.bridge-nf-call-iptables', 'value': '1'})
ok: [storagenodet3500] => (item={'name': 'net.bridge.bridge-nf-call-iptables', 'value': '1'})
ok: [homelab] => (item={'name': 'net.bridge.bridge-nf-call-iptables', 'value': '1'})
ok: [masternode] => (item={'name': 'net.bridge.bridge-nf-call-ip6tables', 'value': '1'})
ok: [storagenodet3500] => (item={'name': 'net.bridge.bridge-nf-call-ip6tables', 'value': '1'})
ok: [masternode] => (item={'name': 'net.ipv4.ip_forward', 'value': '1'})
ok: [homelab] => (item={'name': 'net.bridge.bridge-nf-call-ip6tables', 'value': '1'})
ok: [storagenodet3500] => (item={'name': 'net.ipv4.ip_forward', 'value': '1'})
ok: [homelab] => (item={'name': 'net.ipv4.ip_forward', 'value': '1'})

TASK [network-fix : Persist sysctl settings for boot] *************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Apply all sysctl settings] ********************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Ensure /etc/cni/net.d exists with correct permissions] ****************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Remove all conflicting CNI configs (keep only Flannel)] ***************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Delete conflicting CNI configs] ***************************************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
skipping: [homelab]

TASK [network-fix : Remove stale CNI bridge interfaces (prevents pod sandbox issues)] *****************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Install required network packages (RHEL/CentOS)] **********************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Install iptables-nft and nftables for RHEL 10+ (kube-proxy compatibility)] ********************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Install required network packages (Debian/Ubuntu)] ********************************************************************************************************************************
skipping: [homelab]
ok: [masternode]
ok: [storagenodet3500]

TASK [network-fix : Set iptables FORWARD policy to ACCEPT] ********************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Check if ufw exists] **************************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Stop and disable ufw if present] **************************************************************************************************************************************************
skipping: [homelab]
ok: [masternode]
ok: [storagenodet3500]

TASK [network-fix : Stop and disable firewalld on RHEL (Flannel VXLAN requires open communication)] ***************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Ensure NetworkManager conf.d directory exists] ************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Configure NetworkManager to ignore CNI interfaces] ********************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Ensure nftables is installed and enabled (RHEL 10+)] ******************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Ensure nftables service is running (RHEL 10+)] ************************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Configure nftables to allow Flannel VXLAN and pod traffic (RHEL 10+)] *************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Check if iptables-nft binary exists (RHEL 10+)] ***********************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Check if iptables alternatives entry exists (RHEL 10+)] ***************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Install iptables alternatives if missing (RHEL 10+)] ******************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
skipping: [homelab]

TASK [network-fix : Configure iptables to use nftables backend (RHEL 10+)] ****************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Check if ip6tables-nft binary exists (RHEL 10+)] **********************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Check if ip6tables alternatives entry exists (RHEL 10+)] **************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Install ip6tables alternatives if missing (RHEL 10+)] *****************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
skipping: [homelab]

TASK [network-fix : Configure ip6tables to use nftables backend (RHEL 10+)] ***************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Ensure iptables lock file exists (RHEL 10+)] **************************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
changed: [homelab]

TASK [network-fix : Set SELinux to permissive mode (RHEL, for CNI compatibility)] *********************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Ensure SELinux permissive persists on reboot (RHEL)] ******************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Disable systemd-oomd interference with containers (RHEL 10+)] *********************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Ensure containerd cgroup driver is systemd (RHEL 10+)] ****************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Check if kubelet config exists (RHEL 10+)] ****************************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [network-fix : Ensure kubelet uses systemd cgroup driver (RHEL 10+)] *****************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
skipping: [homelab]

TASK [network-fix : Pre-create all iptables chains for kube-proxy (RHEL 10+)] *************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

PLAY [Phase 2 - Install CNI plugins on all nodes] *****************************************************************************************************************************************************

TASK [Ensure /opt/cni/bin directory exists] ***********************************************************************************************************************************************************
changed: [masternode]
changed: [storagenodet3500]
changed: [homelab]

TASK [Check if CNI plugins are already installed] *****************************************************************************************************************************************************
ok: [masternode]
ok: [storagenodet3500]
ok: [homelab]

TASK [Download CNI plugins (if not present)] **********************************************************************************************************************************************************
skipping: [homelab]
ok: [storagenodet3500]
ok: [masternode]

TASK [Download CNI plugins with curl (RHEL fallback)] *************************************************************************************************************************************************
skipping: [masternode]
skipping: [storagenodet3500]
ok: [homelab]

TASK [Extract CNI plugins] ****************************************************************************************************************************************************************************
changed: [masternode]
changed: [storagenodet3500]
changed: [homelab]

TASK [Ensure CNI plugins are executable] **************************************************************************************************************************************************************
changed: [masternode]
changed: [storagenodet3500]
changed: [homelab]

PLAY [Phase 3 - Initialize Kubernetes control plane] **************************************************************************************************************************************************

TASK [Check if control plane is already initialized] **************************************************************************************************************************************************
ok: [masternode]

TASK [Initialize control plane with kubeadm] **********************************************************************************************************************************************************
changed: [masternode]

TASK [Display kubeadm init output] ********************************************************************************************************************************************************************
ok: [masternode] =>
  kubeadm_init.stdout_lines:
  - '[init] Using Kubernetes version: v1.29.15'
  - '[preflight] Running pre-flight checks'
  - '[preflight] Pulling images required for setting up a Kubernetes cluster'
  - '[preflight] This might take a minute or two, depending on the speed of your internet connection'
  - '[preflight] You can also perform this action in beforehand using ''kubeadm config images pull'''
  - '[certs] Using certificateDir folder "/etc/kubernetes/pki"'
  - '[certs] Generating "ca" certificate and key'
  - '[certs] Generating "apiserver" certificate and key'
  - '[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local masternode] and IPs [10.96.0.1 192.168.4.63]'
  - '[certs] Generating "apiserver-kubelet-client" certificate and key'
  - '[certs] Generating "front-proxy-ca" certificate and key'
  - '[certs] Generating "front-proxy-client" certificate and key'
  - '[certs] Generating "etcd/ca" certificate and key'
  - '[certs] Generating "etcd/server" certificate and key'
  - '[certs] etcd/server serving cert is signed for DNS names [localhost masternode] and IPs [192.168.4.63 127.0.0.1 ::1]'
  - '[certs] Generating "etcd/peer" certificate and key'
  - '[certs] etcd/peer serving cert is signed for DNS names [localhost masternode] and IPs [192.168.4.63 127.0.0.1 ::1]'
  - '[certs] Generating "etcd/healthcheck-client" certificate and key'
  - '[certs] Generating "apiserver-etcd-client" certificate and key'
  - '[certs] Generating "sa" key and public key'
  - '[kubeconfig] Using kubeconfig folder "/etc/kubernetes"'
  - '[kubeconfig] Writing "admin.conf" kubeconfig file'
  - '[kubeconfig] Writing "super-admin.conf" kubeconfig file'
  - '[kubeconfig] Writing "kubelet.conf" kubeconfig file'
  - '[kubeconfig] Writing "controller-manager.conf" kubeconfig file'
  - '[kubeconfig] Writing "scheduler.conf" kubeconfig file'
  - '[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"'
  - '[control-plane] Using manifest folder "/etc/kubernetes/manifests"'
  - '[control-plane] Creating static Pod manifest for "kube-apiserver"'
  - '[control-plane] Creating static Pod manifest for "kube-controller-manager"'
  - '[control-plane] Creating static Pod manifest for "kube-scheduler"'
  - '[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"'
  - '[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"'
  - '[kubelet-start] Starting the kubelet'
  - '[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s'
  - '[apiclient] All control plane components are healthy after 5.502231 seconds'
  - '[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace'
  - '[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster'
  - '[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace'
  - '[upload-certs] Using certificate key:'
  - f97ab809778002bdc8ccd3f9cab9f860b11abb62fc96d74cadecf24be1e558ad
  - '[mark-control-plane] Marking the node masternode as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]'
  - '[mark-control-plane] Marking the node masternode as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]'
  - '[bootstrap-token] Using token: omoqpw.t3xgjstcklbun274'
  - '[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles'
  - '[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes'
  - '[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials'
  - '[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token'
  - '[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster'
  - '[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace'
  - '[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key'
  - '[addons] Applied essential addon: CoreDNS'
  - '[addons] Applied essential addon: kube-proxy'
  - ''
  - Your Kubernetes control-plane has initialized successfully!
  - ''
  - 'To start using your cluster, you need to run the following as a regular user:'
  - ''
  - '  mkdir -p $HOME/.kube'
  - '  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config'
  - '  sudo chown $(id -u):$(id -g) $HOME/.kube/config'
  - ''
  - 'Alternatively, if you are the root user, you can run:'
  - ''
  - '  export KUBECONFIG=/etc/kubernetes/admin.conf'
  - ''
  - You should now deploy a pod network to the cluster.
  - 'Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:'
  - '  https://kubernetes.io/docs/concepts/cluster-administration/addons/'
  - ''
  - 'You can now join any number of the control-plane node running the following command on each as root:'
  - ''
  - '  kubeadm join 192.168.4.63:6443 --token omoqpw.t3xgjstcklbun274 \'
  - "\t--discovery-token-ca-cert-hash sha256:1d7b2ec4947e9ef53ce510201e46d34282c73186a4262f5b1843739cbf753e60 \\"
  - "\t--control-plane --certificate-key f97ab809778002bdc8ccd3f9cab9f860b11abb62fc96d74cadecf24be1e558ad"
  - ''
  - Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
  - As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
  - '"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.'
  - ''
  - 'Then you can join any number of worker nodes by running the following on each as root:'
  - ''
  - kubeadm join 192.168.4.63:6443 --token omoqpw.t3xgjstcklbun274 \
  - "\t--discovery-token-ca-cert-hash sha256:1d7b2ec4947e9ef53ce510201e46d34282c73186a4262f5b1843739cbf753e60 "

TASK [Wait for API server to be ready] ****************************************************************************************************************************************************************
changed: [masternode]

TASK [Ensure KUBECONFIG is set for root] **************************************************************************************************************************************************************
ok: [masternode]

TASK [Set KUBECONFIG for current session] *************************************************************************************************************************************************************
ok: [masternode]

PLAY [Phase 4 - Join worker nodes to cluster] *********************************************************************************************************************************************************

TASK [Check if node is already joined] ****************************************************************************************************************************************************************
ok: [storagenodet3500]
ok: [homelab]

TASK [Generate join token on control plane] ***********************************************************************************************************************************************************
changed: [storagenodet3500 -> masternode(192.168.4.63)]

TASK [Join worker node to cluster] ********************************************************************************************************************************************************************
changed: [homelab]
changed: [storagenodet3500]

TASK [Wait for kubelet to start] **********************************************************************************************************************************************************************
ok: [storagenodet3500]
ok: [homelab]

PLAY [Phase 5 - Deploy Flannel CNI] *******************************************************************************************************************************************************************

TASK [Ensure all nodes have stable networking before Flannel deployment] ******************************************************************************************************************************
changed: [masternode]

TASK [Check if Flannel is already deployed] ***********************************************************************************************************************************************************
ok: [masternode]

TASK [Apply Flannel CNI manifest] *********************************************************************************************************************************************************************
changed: [masternode]

TASK [Wait for Flannel DaemonSet to be ready] *********************************************************************************************************************************************************
FAILED - RETRYING: [masternode]: Wait for Flannel DaemonSet to be ready (2 retries left).
FAILED - RETRYING: [masternode]: Wait for Flannel DaemonSet to be ready (1 retries left).
fatal: [masternode]: FAILED! => changed=true
  attempts: 2
  cmd: |-
    kubectl --kubeconfig=/etc/kubernetes/admin.conf  -n kube-flannel rollout status daemonset/kube-flannel-ds --timeout=240s
  delta: '0:04:00.077394'
  end: '2025-10-04 15:56:31.398051'
  msg: non-zero return code
  rc: 1
  start: '2025-10-04 15:52:31.320657'
  stderr: 'error: timed out waiting for the condition'
  stderr_lines: <omitted>
  stdout: 'Waiting for daemon set "kube-flannel-ds" rollout to finish: 0 of 3 updated pods are available...'
  stdout_lines: <omitted>

PLAY RECAP ********************************************************************************************************************************************************************************************
homelab                    : ok=52   changed=7    unreachable=0    failed=0    skipped=8    rescued=0    ignored=0
masternode                 : ok=41   changed=9    unreachable=0    failed=1    skipped=25   rescued=0    ignored=0
storagenodet3500           : ok=36   changed=7    unreachable=0    failed=0    skipped=25   rescued=0    ignored=0

[ERROR] Deployment failed - check logs above for details
Every 30.0s: kubectl get pods -A | grep -v Running                                                                                                                 masternode: Sat Oct  4 16:04:32 2025

NAMESPACE      NAME                                 READY   STATUS             RESTARTS        AGE
kube-system    coredns-76f75df574-pkd4w             0/1     CrashLoopBackOff   6 (3m11s ago)   20m
kube-system    coredns-76f75df574-t6rpg             0/1     CrashLoopBackOff   5 (2m27s ago)   20m
kube-system    kube-proxy-wknbr                     0/1     CrashLoopBackOff   7 (22s ago)     20m




