root@masternode:/srv/monitoring_data/VMStation# ./deploy.sh
=== VMStation Simplified Deployment ===
Timestamp: Thu 11 Sep 2025 10:29:10 PM EDT

[INFO] Simplified VMStation deployment starting...
[INFO] Deploying complete VMStation stack...
[WARNING]: Collection community.general does not support Ansible version 2.14.18
[WARNING]: Collection ansible.posix does not support Ansible version 2.14.18
[WARNING]: Collection kubernetes.core does not support Ansible version 2.14.18

PLAY [VMStation Simplified Deployment] ********************************************************************************************************************************************************

TASK [Display deployment overview] ************************************************************************************************************************************************************
ok: [localhost] => {
    "msg": "=== VMStation Simplified Deployment ===\n\nThis playbook deploys:\n1. Kubernetes cluster (if not exists)\n2. Essential monitoring stack (Prometheus, Grafana, Loki)\n3. Jellyfin media server\n4. Additional applications (Dashboard, MongoDB, etc.)\n\nTarget Infrastructure:\n- Control plane: 192.168.4.63 (monitoring_nodes)\n- Storage node: 192.168.4.61 (storage_nodes)  \n- Compute node: 192.168.4.62 (compute_nodes)\n"
}

PLAY [Setup Kubernetes Cluster - All Nodes] ***************************************************************************************************************************************************

TASK [Gathering Facts] ************************************************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Update package cache (Debian/Ubuntu)] ***************************************************************************************************************************************************
skipping: [192.168.4.62]
ok: [192.168.4.63]
ok: [192.168.4.61]

TASK [Install required packages (Debian/Ubuntu)] **********************************************************************************************************************************************
skipping: [192.168.4.62]
ok: [192.168.4.63]
ok: [192.168.4.61]

TASK [Add Kubernetes GPG key] *****************************************************************************************************************************************************************
skipping: [192.168.4.62]
ok: [192.168.4.63]
ok: [192.168.4.61]

TASK [Add Kubernetes repository] **************************************************************************************************************************************************************
skipping: [192.168.4.62]
ok: [192.168.4.63]
ok: [192.168.4.61]

TASK [Install Kubernetes packages] ************************************************************************************************************************************************************
skipping: [192.168.4.62]
ok: [192.168.4.63]
ok: [192.168.4.61]

TASK [Add Kubernetes repository (RHEL/CentOS)] ************************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
ok: [192.168.4.62]

TASK [Install Kubernetes packages (RHEL/CentOS)] **********************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
ok: [192.168.4.62]

TASK [Hold Kubernetes packages (Debian/Ubuntu)] ***********************************************************************************************************************************************
skipping: [192.168.4.62] => (item=kubelet)
skipping: [192.168.4.62] => (item=kubeadm)
skipping: [192.168.4.62] => (item=kubectl)
skipping: [192.168.4.62]
ok: [192.168.4.63] => (item=kubelet)
ok: [192.168.4.61] => (item=kubelet)
ok: [192.168.4.63] => (item=kubeadm)
ok: [192.168.4.63] => (item=kubectl)
ok: [192.168.4.61] => (item=kubeadm)
ok: [192.168.4.61] => (item=kubectl)

TASK [Verify Kubernetes binaries are installed and executable] ********************************************************************************************************************************
changed: [192.168.4.63]
changed: [192.168.4.61]
changed: [192.168.4.62]

TASK [Display package verification results] ***************************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "=== Verifying Kubernetes Package Installation ===\n✓ kubelet is installed: Kubernetes v1.29.15\n✓ kubelet is executable\n✓ kubeadm is installed: \n✓ kubeadm is executable\n✓ kubectl is installed: \n✓ kubectl is executable\n✓ containerd is installed\n✓ containerd is executable\n✅ All Kubernetes packages verified successfully"
}
ok: [192.168.4.61] => {
    "msg": "=== Verifying Kubernetes Package Installation ===\n✓ kubelet is installed: Kubernetes v1.29.15\n✓ kubelet is executable\n✓ kubeadm is installed: \n✓ kubeadm is executable\n✓ kubectl is installed: \n✓ kubectl is executable\n✓ containerd is installed\n✓ containerd is executable\n✅ All Kubernetes packages verified successfully"
}
ok: [192.168.4.62] => {
    "msg": "=== Verifying Kubernetes Package Installation ===\n✓ kubelet is installed: Kubernetes v1.29.15\n✓ kubelet is executable\n✓ kubeadm is installed: \n✓ kubeadm is executable\n✓ kubectl is installed: \n✓ kubectl is executable\n✓ containerd is installed\n✓ containerd is executable\n✅ All Kubernetes packages verified successfully"
}

TASK [Verify kubelet systemd service unit exists] *********************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Alternative kubelet service unit location check] ****************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Third alternative kubelet service unit location check] **********************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Search for kubelet service unit in all locations] ***************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Attempt to reinstall kubelet package when service unit missing] *************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Attempt to reinstall kubelet package when service unit missing (RHEL/CentOS)] ***********************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Reload systemd daemon after package reinstallation] *************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Re-verify kubelet service unit after remediation] ***************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Alternative kubelet service unit location re-check] *************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Final kubelet service unit verification after remediation] ******************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Display kubelet service unit location] **************************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "kubelet service unit found at: \n/lib/systemd/system/kubelet.service\n"
}
ok: [192.168.4.61] => {
    "msg": "kubelet service unit found at: \n/lib/systemd/system/kubelet.service\n"
}
ok: [192.168.4.62] => {
    "msg": "kubelet service unit found at: \n/lib/systemd/system/kubelet.service\n"
}

TASK [Verify containerd systemd service unit exists] ******************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Alternative containerd service unit location check] *************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Search for containerd service unit in all locations] ************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Verify containerd service unit was found] ***********************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Disable swap] ***************************************************************************************************************************************************************************
changed: [192.168.4.63]
changed: [192.168.4.61]
changed: [192.168.4.62]

TASK [Load kernel modules] ********************************************************************************************************************************************************************
ok: [192.168.4.63] => (item=overlay)
ok: [192.168.4.61] => (item=overlay)
ok: [192.168.4.63] => (item=br_netfilter)
ok: [192.168.4.62] => (item=overlay)
ok: [192.168.4.61] => (item=br_netfilter)
ok: [192.168.4.62] => (item=br_netfilter)

TASK [Create kernel modules config] ***********************************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Set sysctl parameters] ******************************************************************************************************************************************************************
ok: [192.168.4.63] => (item={'key': 'net.bridge.bridge-nf-call-iptables', 'value': '1'})
ok: [192.168.4.61] => (item={'key': 'net.bridge.bridge-nf-call-iptables', 'value': '1'})
ok: [192.168.4.63] => (item={'key': 'net.bridge.bridge-nf-call-ip6tables', 'value': '1'})
ok: [192.168.4.62] => (item={'key': 'net.bridge.bridge-nf-call-iptables', 'value': '1'})
ok: [192.168.4.63] => (item={'key': 'net.ipv4.ip_forward', 'value': '1'})
ok: [192.168.4.61] => (item={'key': 'net.bridge.bridge-nf-call-ip6tables', 'value': '1'})
ok: [192.168.4.61] => (item={'key': 'net.ipv4.ip_forward', 'value': '1'})
ok: [192.168.4.62] => (item={'key': 'net.bridge.bridge-nf-call-ip6tables', 'value': '1'})
ok: [192.168.4.62] => (item={'key': 'net.ipv4.ip_forward', 'value': '1'})

TASK [Create containerd config directory] *****************************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Generate containerd config] *************************************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Configure containerd cgroup driver] *****************************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Create CNI directories before containerd starts] ****************************************************************************************************************************************
ok: [192.168.4.63] => (item=/opt/cni/bin)
ok: [192.168.4.61] => (item=/opt/cni/bin)
ok: [192.168.4.63] => (item=/etc/cni/net.d)
ok: [192.168.4.62] => (item=/opt/cni/bin)
ok: [192.168.4.63] => (item=/var/lib/cni/networks)
ok: [192.168.4.61] => (item=/etc/cni/net.d)
ok: [192.168.4.63] => (item=/run/flannel)
ok: [192.168.4.61] => (item=/var/lib/cni/networks)
ok: [192.168.4.62] => (item=/etc/cni/net.d)
ok: [192.168.4.61] => (item=/run/flannel)
ok: [192.168.4.62] => (item=/var/lib/cni/networks)
ok: [192.168.4.62] => (item=/run/flannel)

TASK [Create placeholder CNI configuration before containerd starts] **************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Start and enable containerd] ************************************************************************************************************************************************************
changed: [192.168.4.63]
changed: [192.168.4.61]
changed: [192.168.4.62]

TASK [Configure crictl for containerd] ********************************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Create containerd group for socket access] **********************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Add root user to containerd group for socket access] ************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Set up containerd socket permissions monitoring] ****************************************************************************************************************************************
changed: [192.168.4.63]
changed: [192.168.4.61]
changed: [192.168.4.62]

TASK [Test crictl communication with containerd] **********************************************************************************************************************************************
changed: [192.168.4.63]
changed: [192.168.4.61]
changed: [192.168.4.62]

TASK [Display crictl communication status] ****************************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "Crictl Communication Status:\n✓ crictl communication successful\\n                  Version:  0.1.0\\n                  RuntimeName:  containerd\\n                  RuntimeVersion:  1.6.20~ds1\\n                  RuntimeApiVersion:  v1\n\nSocket permissions properly configured for worker node join.\n"
}
ok: [192.168.4.61] => {
    "msg": "Crictl Communication Status:\n✓ crictl communication successful\\n                  Version:  0.1.0\\n                  RuntimeName:  containerd\\n                  RuntimeVersion:  1.6.20~ds1\\n                  RuntimeApiVersion:  v1\n\nSocket permissions properly configured for worker node join.\n"
}
ok: [192.168.4.62] => {
    "msg": "Crictl Communication Status:\n✓ crictl communication successful\\n                  Version:  0.1.0\\n                  RuntimeName:  containerd\\n                  RuntimeVersion:  1.7.27\\n                  RuntimeApiVersion:  v1\n\nSocket permissions properly configured for worker node join.\n"
}

TASK [Wait for containerd to fully initialize] ************************************************************************************************************************************************
Pausing for 10 seconds
(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
ok: [192.168.4.63]

TASK [Initialize containerd image filesystem] *************************************************************************************************************************************************
changed: [192.168.4.63]
changed: [192.168.4.61]
changed: [192.168.4.62]

TASK [Enable kubelet] *************************************************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Check kubelet enable result] ************************************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "kubelet enable result: True\nkubelet enable failed: False\n"
}
ok: [192.168.4.61] => {
    "msg": "kubelet enable result: True\nkubelet enable failed: False\n"
}
ok: [192.168.4.62] => {
    "msg": "kubelet enable result: True\nkubelet enable failed: False\n"
}

TASK [Reload systemd daemon] ******************************************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Retry kubelet enable after daemon reload] ***********************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Final kubelet enable verification] ******************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]
skipping: [192.168.4.63]

TASK [Verify kubelet service is properly enabled] *********************************************************************************************************************************************
changed: [192.168.4.63]
changed: [192.168.4.61]
changed: [192.168.4.62]

TASK [Ensure kubelet service directory exists] ************************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Download and install Flannel CNI plugin binary] *****************************************************************************************************************************************
ok: [192.168.4.63]
ok: [192.168.4.62]
ok: [192.168.4.61]

TASK [Fallback: Download Flannel CNI plugin with curl] ****************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Enhanced fallback: Download Flannel CNI plugin with wget] *******************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Verify Flannel CNI plugin download succeeded] *******************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Post-download verification: Ensure flannel binary is executable and valid] **************************************************************************************************************
changed: [192.168.4.61]
changed: [192.168.4.62]
changed: [192.168.4.63]

TASK [Diagnose flannel binary issue] **********************************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Attempt to fix flannel binary permissions] **********************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Retry flannel download when file missing or corrupted] **********************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Final flannel binary verification after remediation] ************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Display flannel remediation results] ****************************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Download and install additional CNI plugins] ********************************************************************************************************************************************
skipping: [192.168.4.63]
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Verify essential CNI plugins are installed] *********************************************************************************************************************************************
changed: [192.168.4.63]
changed: [192.168.4.61]
changed: [192.168.4.62]

TASK [Display CNI plugin verification results] ************************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "=== Verifying CNI Plugin Installation ===\n✓ bridge plugin installed and executable\n✓ host-local plugin installed and executable\n✓ loopback plugin installed and executable\n✓ flannel plugin installed and executable\n✅ All required CNI plugins verified successfully\n\nAll installed CNI plugins:\ntotal 79484\ndrwxr-xr-x 2 root root     4096 Sep 11 14:40 .\ndrwxr-xr-x 3 root root     4096 Aug 25 22:18 ..\n-rwxr-xr-x 1 root root  4016001 Sep 11  2023 bandwidth\n-rwxr-xr-x 1 root root  4531309 Sep 11  2023 bridge\n-rwxr-xr-x 1 root root 10816051 Sep 11  2023 dhcp\n-rwxr-xr-x 1 root root  4171248 Sep 11  2023 dummy\n-rwxr-xr-x 1 root root  4649749 Sep 11  2023 firewall\n-rwxr-xr-x 1 root root  2907995 Sep 11 14:40 flannel\n-rwxr-xr-x 1 root root  4059321 Sep 11  2023 host-device\n-rwxr-xr-x 1 root root  3444776 Sep 11  2023 host-local\n-rwxr-xr-x 1 root root  4193323 Sep 11  2023 ipvlan\n-rwxr-xr-x 1 root root  3514598 Sep 11  2023 loopback\n-rwxr-xr-x 1 root root  4227193 Sep 11  2023 macvlan\n-rwxr-xr-x 1 root root  3955775 Sep 11  2023 portmap\n-rwxr-xr-x 1 root root  4348835 Sep 11  2023 ptp\n-rwxr-xr-x 1 root root  3716095 Sep 11  2023 sbr\n-rwxr-xr-x 1 root root  2984504 Sep 11  2023 static\n-rwxr-xr-x 1 root root  4258344 Sep 11  2023 tap\n-rwxr-xr-x 1 root root  3603365 Sep 11  2023 tuning\n-rwxr-xr-x 1 root root  4187498 Sep 11  2023 vlan\n-rwxr-xr-x 1 root root  3754911 Sep 11  2023 vrf"
}
ok: [192.168.4.61] => {
    "msg": "=== Verifying CNI Plugin Installation ===\n✓ bridge plugin installed and executable\n✓ host-local plugin installed and executable\n✓ loopback plugin installed and executable\n✓ flannel plugin installed and executable\n✅ All required CNI plugins verified successfully\n\nAll installed CNI plugins:\ntotal 79484\ndrwxr-xr-x 2 root root     4096 Sep 11 19:03 .\ndrwxr-xr-x 3 root root     4096 Sep 11 14:40 ..\n-rwxr-xr-x 1 root root  4016001 May  9  2023 bandwidth\n-rwxr-xr-x 1 root root  4531309 May  9  2023 bridge\n-rwxr-xr-x 1 root root 10816051 May  9  2023 dhcp\n-rwxr-xr-x 1 root root  4171248 May  9  2023 dummy\n-rwxr-xr-x 1 root root  4649749 May  9  2023 firewall\n-rwxr-xr-x 1 root root  2907995 Sep 11 19:03 flannel\n-rwxr-xr-x 1 root root  4059321 May  9  2023 host-device\n-rwxr-xr-x 1 root root  3444776 May  9  2023 host-local\n-rwxr-xr-x 1 root root  4193323 May  9  2023 ipvlan\n-rwxr-xr-x 1 root root  3514598 May  9  2023 loopback\n-rwxr-xr-x 1 root root  4227193 May  9  2023 macvlan\n-rwxr-xr-x 1 root root  3955775 May  9  2023 portmap\n-rwxr-xr-x 1 root root  4348835 May  9  2023 ptp\n-rwxr-xr-x 1 root root  3716095 May  9  2023 sbr\n-rwxr-xr-x 1 root root  2984504 May  9  2023 static\n-rwxr-xr-x 1 root root  4258344 May  9  2023 tap\n-rwxr-xr-x 1 root root  3603365 May  9  2023 tuning\n-rwxr-xr-x 1 root root  4187498 May  9  2023 vlan\n-rwxr-xr-x 1 root root  3754911 May  9  2023 vrf"
}
ok: [192.168.4.62] => {
    "msg": "=== Verifying CNI Plugin Installation ===\n✓ bridge plugin installed and executable\n✓ host-local plugin installed and executable\n✓ loopback plugin installed and executable\n✓ flannel plugin installed and executable\n✅ All required CNI plugins verified successfully\n\nAll installed CNI plugins:\ntotal 57800\ndrwxr-xr-x. 2 root root    4096 Sep 11 22:24 .\ndrwxr-xr-x. 3 root root      17 Sep 11 14:40 ..\n-rwxr-xr-x. 1 root root 2868856 Sep 11  2023 bandwidth\n-rwxr-xr-x. 1 root root 3248936 Sep 11  2023 bridge\n-rwxr-xr-x. 1 root root 8021392 Sep 11  2023 dhcp\n-rwxr-xr-x. 1 root root 2971752 Sep 11  2023 dummy\n-rwxr-xr-x. 1 root root 3333560 Sep 11  2023 firewall\n-rwxr-xr-x. 1 root root 2907995 Sep 11 22:24 flannel\n-rwxr-xr-x. 1 root root 2888056 Sep 11  2023 host-device\n-rwxr-xr-x. 1 root root 2426584 Sep 11  2023 host-local\n-rwxr-xr-x. 1 root root 2989072 Sep 11  2023 ipvlan\n-rwxr-xr-x. 1 root root 2492304 Sep 11  2023 loopback\n-rwxr-xr-x. 1 root root 3015104 Sep 11  2023 macvlan\n-rwxr-xr-x. 1 root root 2820904 Sep 11  2023 portmap\n-rwxr-xr-x. 1 root root 3112536 Sep 11  2023 ptp\n-rwxr-xr-x. 1 root root 2642160 Sep 11  2023 sbr\n-rwxr-xr-x. 1 root root 2158744 Sep 11  2023 static\n-rwxr-xr-x. 1 root root 3035352 Sep 11  2023 tap\n-rwxr-xr-x. 1 root root 2556368 Sep 11  2023 tuning\n-rwxr-xr-x. 1 root root 2984672 Sep 11  2023 vlan\n-rwxr-xr-x. 1 root root 2665880 Sep 11  2023 vrf"
}

PLAY [Comprehensive Worker Node Installation Verification] ************************************************************************************************************************************

TASK [Gathering Facts] ************************************************************************************************************************************************************************
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Check all required Kubernetes services and packages] ************************************************************************************************************************************
changed: [192.168.4.62]
changed: [192.168.4.61]

TASK [Display worker node verification results] ***********************************************************************************************************************************************
ok: [192.168.4.61] => {
    "msg": "=== Worker Node Installation Verification ===\nNode: storagenodeT3500 (192.168.4.61)\nTimestamp: Thu Sep 11 22:30:13 EDT 2025\n\n=== Package Verification ===\n✓ kubelet: Kubernetes v1.29.15\n✓ kubeadm: \n✓ kubectl: \n✓ containerd: containerd github.com/containerd/containerd 1.6.20~ds1 1.6.20~ds1-1+deb12u1\n\n=== Service Unit Verification ===\n✓ kubelet.service unit exists (enabled: enabled)\n✓ containerd.service unit exists (enabled: enabled)\n\n=== CNI Plugin Verification ===\n✓ CNI plugin: bridge\n✓ CNI plugin: host-local\n✓ CNI plugin: loopback\n✓ CNI plugin: flannel\n\n=== Directory Structure Verification ===\n✓ Directory exists: /opt/cni/bin\n✓ Directory exists: /etc/cni/net.d\n✓ Directory exists: /var/lib/kubelet\n✓ Directory exists: /etc/systemd/system/kubelet.service.d\n\n=== Containerd Configuration Verification ===\n✓ containerd config exists\n✓ containerd cgroup driver configured\n\n=== Overall Status ===\n✅ Worker node installation verification PASSED\nNode is ready for cluster join"
}
ok: [192.168.4.62] => {
    "msg": "=== Worker Node Installation Verification ===\nNode: homelab (192.168.4.62)\nTimestamp: Thu 11 Sep 2025 10:30:13 PM EDT\n\n=== Package Verification ===\n✓ kubelet: Kubernetes v1.29.15\n✓ kubeadm: \n✓ kubectl: \n✓ containerd: containerd containerd.io 1.7.27 05044ec0a9a75232cad458027ca83437aae3f4da\n\n=== Service Unit Verification ===\n✓ kubelet.service unit exists (enabled: enabled)\n✓ containerd.service unit exists (enabled: enabled)\n\n=== CNI Plugin Verification ===\n✓ CNI plugin: bridge\n✓ CNI plugin: host-local\n✓ CNI plugin: loopback\n✓ CNI plugin: flannel\n\n=== Directory Structure Verification ===\n✓ Directory exists: /opt/cni/bin\n✓ Directory exists: /etc/cni/net.d\n✓ Directory exists: /var/lib/kubelet\n✓ Directory exists: /etc/systemd/system/kubelet.service.d\n\n=== Containerd Configuration Verification ===\n✓ containerd config exists\n⚠ containerd cgroup driver may not be properly configured\n\n=== Overall Status ===\n✅ Worker node installation verification PASSED\nNode is ready for cluster join"
}

PLAY [Initialize Kubernetes Control Plane] ****************************************************************************************************************************************************

TASK [Gathering Facts] ************************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Check if cluster exists] ****************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Initialize cluster with secure authorization mode] **************************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Initialize cluster with AlwaysAllow fallback (if secure mode failed)] *******************************************************************************************************************
skipping: [192.168.4.63]

TASK [Display authorization mode warning if fallback was used] ********************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "WARNING: Cluster initialized with --authorization-mode=AlwaysAllow\nThis is less secure and should only be used for troubleshooting.\nConsider investigating why Node,RBAC mode failed.\n"
}

TASK [Create .kube directory] *****************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Copy admin.conf] ************************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Open firewall ports for Kubernetes] *****************************************************************************************************************************************************
skipping: [192.168.4.63] => (item=6443/tcp)
skipping: [192.168.4.63] => (item=10250/tcp)
skipping: [192.168.4.63] => (item=10251/tcp)
skipping: [192.168.4.63] => (item=10252/tcp)
skipping: [192.168.4.63] => (item=8472/udp)
skipping: [192.168.4.63]

TASK [Check if Flannel CNI is already installed] **********************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Install Flannel CNI] ********************************************************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Wait for Flannel DaemonSet to be created] ***********************************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Ensure CoreDNS has correct replica count and proper scheduling] *************************************************************************************************************************
changed: [192.168.4.63]

TASK [Check Flannel namespace and resources] **************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Check CNI plugins availability] *********************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Check containerd CNI configuration] *****************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Analyze CNI runtime status] *************************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Reapply Flannel when CNI shows only loopback interface] *********************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Check Flannel DaemonSet status] *********************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Display comprehensive CNI readiness status] *********************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "=== CNI Readiness Status ===\n\nControl Plane Flannel Status:\nChecking Flannel deployment status:\nNAME           STATUS   AGE\nkube-flannel   Active   30h\nNAME                        READY   STATUS    RESTARTS        AGE\npod/kube-flannel-ds-9xsgk   1/1     Running   7 (5m50s ago)   21m\npod/kube-flannel-ds-bjwls   1/1     Running   0               3h27m\npod/kube-flannel-ds-n2cln   1/1     Running   1 (7h49m ago)   29h\n\nNAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/kube-flannel-ds   3         3         3       3            3           <none>          30h\nNode status:\nNAME               STATUS   ROLES           AGE     VERSION\nhomelab            Ready    <none>          3h25m   v1.29.15\nmasternode         Ready    control-plane   30h     v1.29.15\nstoragenodet3500   Ready    <none>          3h27m   v1.29.15\n\nCNI Plugins Status:\n=== CNI Plugins Status ===\n-rwxr-xr-x 1 root root  4531309 Sep 11  2023 bridge\n-rwxr-xr-x 1 root root  2907995 Sep 11 14:40 flannel\n-rwxr-xr-x 1 root root  3444776 Sep 11  2023 host-local\n-rwxr-xr-x 1 root root  3514598 Sep 11  2023 loopback\n\n=== CNI Configuration Directory ===\ntotal 16\ndrwxr-xr-x 2 root root 4096 Sep 11 14:40 .\ndrwxr-xr-x 3 root root 4096 Sep  9 14:27 ..\n-rw-r--r-- 1 root root  268 Sep 10 19:32 00-placeholder.conflist\n-rw-r--r-- 1 root root  292 Sep 11 14:40 10-flannel.conflist\n\nContainerd CNI Configuration:\n=== Containerd CNI Configuration ===\n    stream_server_port = \"0\"\n    systemd_cgroup = false\n    tolerate_missing_hugetlb_controller = true\n    unset_seccomp_profile = \"\"\n\n    [plugins.\"io.containerd.grpc.v1.cri\".cni]\n      bin_dir = \"/opt/cni/bin\"\n      conf_dir = \"/etc/cni/net.d\"\n      conf_template = \"\"\n      ip_pref = \"\"\n      max_conf_num = 1\n\n    [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n--\n      no_pivot = false\n      snapshotter = \"overlayfs\"\n\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.default_runtime]\n        base_runtime_spec = \"\"\n        cni_conf_dir = \"\"\n        cni_max_conf_num = 0\n        container_annotations = []\n        pod_annotations = []\n        privileged_without_host_devices = false\n        runtime_engine = \"\"\n        runtime_path = \"\"\n--\n\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes]\n\n        [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n          base_runtime_spec = \"\"\n          cni_conf_dir = \"\"\n          cni_max_conf_num = 0\n          container_annotations = []\n          pod_annotations = []\n          privileged_without_host_devices = false\n          runtime_engine = \"\"\n          runtime_path = \"\"\n--\n            ShimCgroup = \"\"\n            SystemdCgroup = true\n\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.untrusted_workload_runtime]\n        base_runtime_spec = \"\"\n        cni_conf_dir = \"\"\n        cni_max_conf_num = 0\n        container_annotations = []\n        pod_annotations = []\n        privileged_without_host_devices = false\n        runtime_engine = \"\"\n        runtime_path = \"\"\n\nCNI Runtime Analysis:\ncni_has_real_network=true\n\nFlannel DaemonSet Status:\n=== Flannel DaemonSet and Pod Status ===\nNAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE   CONTAINERS     IMAGES                               SELECTOR\ndaemonset.apps/kube-flannel-ds   3         3         3       3            3           <none>          30h   kube-flannel   ghcr.io/flannel-io/flannel:v0.27.3   app=flannel,k8s-app=flannel\n\nNAME                        READY   STATUS    RESTARTS        AGE     IP             NODE               NOMINATED NODE   READINESS GATES\npod/kube-flannel-ds-9xsgk   1/1     Running   7 (5m51s ago)   21m     192.168.4.62   homelab            <none>           <none>\npod/kube-flannel-ds-bjwls   1/1     Running   0               3h27m   192.168.4.61   storagenodet3500   <none>           <none>\npod/kube-flannel-ds-n2cln   1/1     Running   1 (7h49m ago)   29h     192.168.4.63   masternode         <none>           <none>\n\n=== Flannel Readiness Check ===\n3\n\n\nNote: Flannel pods may show CrashLoopBackOff until worker nodes join.\nThis is expected behavior in a single-node control plane setup.\n\nCNI Configuration Ready: Worker nodes can now join the cluster.\n"
}

TASK [Wait for CoreDNS to stabilize after Flannel setup] **************************************************************************************************************************************
Pausing for 30 seconds
(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
ok: [192.168.4.63]

TASK [Check CoreDNS pod status and fix if needed] *********************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Display CoreDNS fix results] ************************************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "CoreDNS Post-Flannel Validation Results:\n=== Post-Flannel CoreDNS Validation ===\\n              CoreDNS pods status:\\n                coredns-68444cf7cd-vq79r: Status=Pending, IP=<none>\\n                  ↳ CNI bridge IP conflict detected for this pod\\n              === CNI Bridge Conflict Detected ===\\n              CoreDNS pods cannot start due to CNI bridge IP conflict.\\n              This requires CNI bridge reset, which will be handled by post-deployment fixes.\\n              Skipping CoreDNS rollout wait to avoid hanging the deployment.\\n              \\n              The deployment will complete and run fix_cni_bridge_conflict.sh automatically.\\n              === Final CoreDNS Status ===\\n              NAME                       READY   STATUS              RESTARTS   AGE   IP       NODE         NOMINATED NODE   READINESS GATES\\n              coredns-68444cf7cd-vq79r   0/1     ContainerCreating   0          32m   <none>   masternode   <none>           <none>\n\n\n"
}

TASK [Wait for API server to be ready] ********************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Verify API server accessibility using kubectl] ******************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Validate kubernetes-admin RBAC permissions] *********************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Check current authorization mode] *******************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Display current authorization mode] *****************************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "Current Kubernetes authorization mode: "
}

TASK [Fix kubernetes-admin RBAC if needed] ****************************************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Backup API server manifest] *************************************************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Update authorization mode from AlwaysAllow to Node,RBAC] ********************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Wait for API server to restart after authorization fix] *********************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Verify API server health after authorization fix] ***************************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Apply RBAC fix after authorization mode change] *****************************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Skip RBAC fix for AlwaysAllow mode (no longer needed after fix)] ************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Wait for API server pod to be Ready] ****************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Check cluster-info configmap RBAC permissions] ******************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Create RBAC rule for anonymous access to cluster-info configmap] ************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Verify cluster-info configmap accessibility] ********************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Check existing tokens and clean up old ones if needed] **********************************************************************************************************************************
changed: [192.168.4.63]

TASK [Generate fresh join command with enhanced validation] ***********************************************************************************************************************************
changed: [192.168.4.63]

TASK [Validate join command contains correct control-plane IP] ********************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Save enhanced join command for wiped workers] *******************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Display join command info] **************************************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "Join Command Generated:\n- Timestamp: 2025-09-12T02:30:16Z\n- Control-plane: 192.168.4.63:6443\n- Token TTL: 2 hours\n- Ready for wiped workers: Yes\n"
}

PLAY [Join Worker Nodes] **********************************************************************************************************************************************************************

TASK [Gathering Facts] ************************************************************************************************************************************************************************
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Check if node is joined] ****************************************************************************************************************************************************************
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Check for existing cluster artifacts] ***************************************************************************************************************************************************
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Check for clean post-wipe state indicators] *********************************************************************************************************************************************
ok: [192.168.4.61] => (item=/etc/kubernetes)
ok: [192.168.4.62] => (item=/etc/kubernetes)
ok: [192.168.4.61] => (item=/var/lib/kubelet)
ok: [192.168.4.61] => (item=/etc/cni/net.d)
ok: [192.168.4.62] => (item=/var/lib/kubelet)
ok: [192.168.4.61] => (item=/var/lib/containerd)
ok: [192.168.4.62] => (item=/etc/cni/net.d)
ok: [192.168.4.62] => (item=/var/lib/containerd)

TASK [Detect if worker was aggressively wiped] ************************************************************************************************************************************************
ok: [192.168.4.61]
ok: [192.168.4.62]

TASK [Display worker state detection] *********************************************************************************************************************************************************
ok: [192.168.4.61] => {
    "msg": "Worker Node State Analysis:\n- Kubelet config exists: True\n- Cluster artifacts exist: True\n- Post-wipe state detected: False\n- Node requires fresh join: False\n"
}
ok: [192.168.4.62] => {
    "msg": "Worker Node State Analysis:\n- Kubelet config exists: True\n- Cluster artifacts exist: True\n- Post-wipe state detected: False\n- Node requires fresh join: False\n"
}

TASK [Check control-plane API server accessibility] *******************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Verify control-plane cluster status] ****************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Ensure control-plane has proper RBAC configuration] *************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Display control-plane readiness status] *************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Stop and disable kubelet service] *******************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Stop containerd temporarily for thorough preparation] ***********************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Reset kubeadm configuration (idempotent for wiped workers)] *****************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Clean up iptables rules (idempotent)] ***************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Ensure complete removal of any residual Kubernetes state] *******************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Reset systemd services for clean slate] *************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Restart containerd after cleanup] *******************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Display worker preparation status] ******************************************************************************************************************************************************
ok: [192.168.4.61] => {
    "msg": "Worker Preparation Complete:\n- Worker was previously wiped: False\n- Required reset performed: False\n- Ready for fresh join: Yes\n"
}
ok: [192.168.4.62] => {
    "msg": "Worker Preparation Complete:\n- Worker was previously wiped: False\n- Required reset performed: False\n- Ready for fresh join: Yes\n"
}

TASK [Open firewall ports for worker nodes] ***************************************************************************************************************************************************
skipping: [192.168.4.61] => (item=10250/tcp)
skipping: [192.168.4.61] => (item=8472/udp)
skipping: [192.168.4.61]
skipping: [192.168.4.62] => (item=10250/tcp)
skipping: [192.168.4.62] => (item=8472/udp)
skipping: [192.168.4.62]

TASK [Test connectivity to control plane API server] ******************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Copy join command from control plane] ***************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Write join command to worker] ***********************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Copy enhanced join scripts to worker nodes] *********************************************************************************************************************************************
skipping: [192.168.4.61] => (item=../../scripts/validate_join_prerequisites.sh)
skipping: [192.168.4.61] => (item=../../scripts/enhanced_kubeadm_join.sh)
skipping: [192.168.4.61]
skipping: [192.168.4.62] => (item=../../scripts/validate_join_prerequisites.sh)
skipping: [192.168.4.62] => (item=../../scripts/enhanced_kubeadm_join.sh)
skipping: [192.168.4.62]

TASK [Check if kubelet config already exists] *************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Validate existing kubelet config if present] ********************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Backup invalid kubelet configuration] ***************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Reset invalid kubelet configuration] ****************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Skip join if kubelet already properly joined] *******************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Copy pre-join validation script to worker] **********************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Pre-join validation for wiped workers] **************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Wait for pre-join validation to complete] ***********************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Display pre-join validation results] ****************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Execute enhanced join process for wiped worker] *****************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Display join skip message] **************************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Display enhanced join results for wiped worker] *****************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Gather detailed failure diagnostics] ****************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Create comprehensive failure report] ****************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Wait for kubelet to stabilize after join] ***********************************************************************************************************************************************
skipping: [192.168.4.61]

TASK [Enhanced kubelet validation for post-wipe workers] **************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Display kubelet validation results] *****************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Verify node appears in cluster from control plane] **************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Display cluster integration status] *****************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Wait for Flannel DaemonSet to be ready on control plane] ********************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Check CNI configuration file exists] ****************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Validate CNI configuration syntax] ******************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Enhanced pre-join CNI preparation] ******************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Create CNI directories on worker nodes] *************************************************************************************************************************************************
skipping: [192.168.4.61] => (item=/opt/cni/bin)
skipping: [192.168.4.61] => (item=/etc/cni/net.d)
skipping: [192.168.4.61] => (item=/var/lib/cni/networks)
skipping: [192.168.4.61] => (item=/var/lib/cni/results)
skipping: [192.168.4.61]
skipping: [192.168.4.62] => (item=/opt/cni/bin)
skipping: [192.168.4.62] => (item=/etc/cni/net.d)
skipping: [192.168.4.62] => (item=/var/lib/cni/networks)
skipping: [192.168.4.62] => (item=/var/lib/cni/results)
skipping: [192.168.4.62]

TASK [Download and install Flannel CNI plugin binary on worker nodes] *************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Fallback: Download Flannel CNI plugin with curl on worker nodes] ************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Enhanced fallback: Download Flannel CNI plugin with wget on worker nodes] ***************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Verify Flannel CNI plugin download succeeded on worker nodes] ***************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Post-download verification: Ensure flannel binary is executable and valid on worker nodes] **********************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Download and install additional CNI plugins on worker nodes] ****************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Create basic CNI configuration for worker nodes BEFORE containerd restart] **************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Create Flannel subnet environment directory] ********************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Restart containerd AFTER CNI configuration is ready] ************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Wait for containerd to fully initialize] ************************************************************************************************************************************************
skipping: [192.168.4.61]

TASK [Initialize containerd image filesystem for kubelet] *************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Display containerd image filesystem status] *********************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Verify containerd is ready for kubelet] *************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Starting kubeadm join process] **********************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Stop kubelet service before join] *******************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Remove any stale kubelet configuration files] *******************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Ensure kubelet service is enabled for post-join management] *****************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Wait for kubelet to fully stop] *********************************************************************************************************************************************************
skipping: [192.168.4.61]

TASK [Join cluster with retry logic] **********************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Capture kubelet logs for troubleshooting] ***********************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Display failure diagnostics] ************************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Cleaning up after failed join] **********************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Stop services for comprehensive cleanup] ************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Clean up networking rules] **************************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Remove Kubernetes state directories] ****************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Reset systemd services] *****************************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Restart containerd and prepare for retry] ***********************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Wait for containerd to be fully ready after restart] ************************************************************************************************************************************
skipping: [192.168.4.61]

TASK [Reinitialize containerd image filesystem after cleanup] *********************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Recreate CNI directories] ***************************************************************************************************************************************************************
skipping: [192.168.4.61] => (item=/etc/cni/net.d)
skipping: [192.168.4.61] => (item=/run/flannel)
skipping: [192.168.4.61]
skipping: [192.168.4.62] => (item=/etc/cni/net.d)
skipping: [192.168.4.62] => (item=/run/flannel)
skipping: [192.168.4.62]

TASK [Recreate basic CNI configuration] *******************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Enable kubelet for kubeadm join] ********************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Verify containerd is running before retry] **********************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Wait before retry to ensure system stability] *******************************************************************************************************************************************
skipping: [192.168.4.61]

TASK [Stop kubelet service before retry] ******************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Remove any stale kubelet configuration files before retry] ******************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Wait for kubelet to fully stop before retry] ********************************************************************************************************************************************
skipping: [192.168.4.61]

TASK [Retry join after thorough cleanup] ******************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Display join result] ********************************************************************************************************************************************************************
skipping: [192.168.4.61]
skipping: [192.168.4.62]

TASK [Remove join command and cleanup temporary files] ****************************************************************************************************************************************
skipping: [192.168.4.61] => (item=/tmp/kubeadm-join.sh)
skipping: [192.168.4.61] => (item=/tmp/validate_join_prerequisites.sh)
skipping: [192.168.4.61] => (item=/tmp/enhanced_kubeadm_join.sh)
skipping: [192.168.4.61]
skipping: [192.168.4.62] => (item=/tmp/kubeadm-join.sh)
skipping: [192.168.4.62] => (item=/tmp/validate_join_prerequisites.sh)
skipping: [192.168.4.62] => (item=/tmp/enhanced_kubeadm_join.sh)
skipping: [192.168.4.62]

PLAY [Post-Wipe Worker Integration Validation] ************************************************************************************************************************************************

TASK [Gathering Facts] ************************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Final cluster status check after post-wipe worker joins] ********************************************************************************************************************************
changed: [192.168.4.63]

TASK [Display final cluster integration summary] **********************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "=== Final Cluster Status After Post-Wipe Worker Integration ===\\n          Timestamp: Thu 11 Sep 2025 10:31:14 PM EDT\\n          \\n          Cluster Nodes:\\n          NAME               STATUS   ROLES           AGE     VERSION    INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                                   KERNEL-VERSION                CONTAINER-RUNTIME\\n          homelab            Ready    <none>          3h26m   v1.29.15   192.168.4.62   <none>        Red Hat Enterprise Linux 10.0 (Coughlan)   6.12.0-55.9.1.el10_0.x86_64   containerd://1.7.27\\n          masternode         Ready    control-plane   30h     v1.29.15   192.168.4.63   <none>        Debian GNU/Linux 12 (bookworm)             6.1.0-32-amd64                containerd://1.6.20\\n          storagenodet3500   Ready    <none>          3h28m   v1.29.15   192.168.4.61   <none>        Debian GNU/Linux 12 (bookworm)             6.1.0-34-amd64                containerd://1.6.20\\n          \\n          Node Readiness Status:\\n          NAME               STATUS        READY\\n          homelab            PIDPressure   False\\n          masternode         PIDPressure   False\\n          storagenodet3500   PIDPressure   False\\n          \\n          Flannel DaemonSet Status:\\n          NAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE   CONTAINERS     IMAGES                               SELECTOR\\n          kube-flannel-ds   3         3         2       3            2           <none>          30h   kube-flannel   ghcr.io/flannel-io/flannel:v0.27.3   app=flannel,k8s-app=flannel\\n          \\n          Pod Network Status:\\n          NAME                    READY   STATUS             RESTARTS        AGE     IP             NODE               NOMINATED NODE   READINESS GATES\\n          kube-flannel-ds-9xsgk   0/1     CrashLoopBackOff   7 (30s ago)     21m     192.168.4.62   homelab            <none>           <none>\\n          kube-flannel-ds-bjwls   1/1     Running            0               3h28m   192.168.4.61   storagenodet3500   <none>           <none>\\n          kube-flannel-ds-n2cln   1/1     Running            1 (7h50m ago)   30h     192.168.4.63   masternode         <none>           <none>\\n          \\n          Cluster Summary:\\n          - Total nodes: 3\\n          - Ready nodes: 3\\n          - Control-plane: 1\\n          - Storage nodes: 1\\n          - Compute nodes: 1\\n          \\n          ✅ POST-WIPE WORKER INTEGRATION SUCCESSFUL\\n          All wiped workers successfully joined control-plane cluster\n\n====================================\nPOST-WIPE WORKER JOIN PROCESS COMPLETE\n====================================\n\nThe enhanced worker join process has:\n✓ Detected post-wipe worker states\n✓ Validated control-plane readiness\n✓ Generated fresh join tokens (2h TTL)\n✓ Performed enhanced reset and cleanup\n✓ Successfully joined workers to control-plane\n✓ Verified kubelet cluster connectivity (NOT standalone)\n✓ Confirmed node registration in cluster\n\nWorkers are now managed by the control-plane using TLS certificates.\nNo standalone mode detected - cluster formation successful!\n"
}

PLAY [Cluster Readiness Validation] ***********************************************************************************************************************************************************

TASK [Check cluster node status] **************************************************************************************************************************************************************
[WARNING]: kubernetes<24.2.0 is not supported or tested. Some features may not work.
ok: [192.168.4.63]

TASK [Check flannel pod status] ***************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Display cluster status] *****************************************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "=== Cluster Status Check ===\nTotal nodes: 3\n- homelab: True\n- masternode: True\n- storagenodet3500: True\n\nFlannel pods: 3\n- kube-flannel-ds-9xsgk on homelab: Running\n- kube-flannel-ds-bjwls on storagenodet3500: Running\n- kube-flannel-ds-n2cln on masternode: Running\n"
}

TASK [Check for CNI bridge conflicts before flannel wait] *************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Wait for flannel DaemonSet with timeout protection] *************************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Handle CNI conflicts detected during flannel wait] **************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "=== CNI Bridge Conflicts Detected ===\nFlannel cannot be ready due to CNI bridge IP conflicts.\nSetting flag for post-deployment fixes.\nDeployment will continue and apply CNI bridge fix automatically.\n"
}

TASK [Set CNI conflict flag for post-deployment handling] *************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Handle flannel wait timeout] ************************************************************************************************************************************************************
skipping: [192.168.4.63]

TASK [Display flannel readiness status] *******************************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "Flannel Readiness Check Complete:\n- CNI conflicts detected: Yes\n- Flannel wait completed: Yes\n- Post-deployment fixes required: Yes\n"
}

TASK [Check if any critical pods are failing] *************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Display system pod issues] **************************************************************************************************************************************************************
ok: [192.168.4.63] => {
    "msg": "=== System Pod Status ===\n- coredns-68444cf7cd-vq79r: Pending\n  Container coredns: ContainerCreating\n"
}

PLAY [Deploy VMStation Applications] **********************************************************************************************************************************************************

TASK [Gathering Facts] ************************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Create monitoring namespace] ************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Get flannel pod status] *****************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Restart crashlooping flannel pods] ******************************************************************************************************************************************************
changed: [192.168.4.63] => (item={'metadata': {'name': 'kube-flannel-ds-9xsgk', 'generateName': 'kube-flannel-ds-', 'namespace': 'kube-flannel', 'uid': 'b99aece8-3f9c-4f79-817f-e474f62aef41', 'resourceVersion': '139644', 'creationTimestamp': '2025-09-12T02:09:16Z', 'labels': {'app': 'flannel', 'controller-revision-hash': '8f698f8cd', 'k8s-app': 'flannel', 'pod-template-generation': '1', 'tier': 'node'}, 'ownerReferences': [{'apiVersion': 'apps/v1', 'kind': 'DaemonSet', 'name': 'kube-flannel-ds', 'uid': '498c66ad-8d8f-4b82-abce-7eff1439f0f1', 'controller': True, 'blockOwnerDeletion': True}], 'managedFields': [{'manager': 'kube-controller-manager', 'operation': 'Update', 'apiVersion': 'v1', 'time': '2025-09-12T02:09:16Z', 'fieldsType': 'FieldsV1', 'fieldsV1': {'f:metadata': {'f:generateName': {}, 'f:labels': {'.': {}, 'f:app': {}, 'f:controller-revision-hash': {}, 'f:k8s-app': {}, 'f:pod-template-generation': {}, 'f:tier': {}}, 'f:ownerReferences': {'.': {}, 'k:{"uid":"498c66ad-8d8f-4b82-abce-7eff1439f0f1"}': {}}}, 'f:spec': {'f:affinity': {'.': {}, 'f:nodeAffinity': {'.': {}, 'f:requiredDuringSchedulingIgnoredDuringExecution': {}}}, 'f:containers': {'k:{"name":"kube-flannel"}': {'.': {}, 'f:args': {}, 'f:command': {}, 'f:env': {'.': {}, 'k:{"name":"CONT_WHEN_CACHE_NOT_READY"}': {'.': {}, 'f:name': {}, 'f:value': {}}, 'k:{"name":"EVENT_QUEUE_DEPTH"}': {'.': {}, 'f:name': {}, 'f:value': {}}, 'k:{"name":"POD_NAME"}': {'.': {}, 'f:name': {}, 'f:valueFrom': {'.': {}, 'f:fieldRef': {}}}, 'k:{"name":"POD_NAMESPACE"}': {'.': {}, 'f:name': {}, 'f:valueFrom': {'.': {}, 'f:fieldRef': {}}}}, 'f:image': {}, 'f:imagePullPolicy': {}, 'f:name': {}, 'f:resources': {'.': {}, 'f:requests': {'.': {}, 'f:cpu': {}, 'f:memory': {}}}, 'f:securityContext': {'.': {}, 'f:capabilities': {'.': {}, 'f:add': {}}, 'f:privileged': {}}, 'f:terminationMessagePath': {}, 'f:terminationMessagePolicy': {}, 'f:volumeMounts': {'.': {}, 'k:{"mountPath":"/etc/kube-flannel/"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}, 'k:{"mountPath":"/run/flannel"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}, 'k:{"mountPath":"/run/xtables.lock"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}}}}, 'f:dnsPolicy': {}, 'f:enableServiceLinks': {}, 'f:hostNetwork': {}, 'f:initContainers': {'.': {}, 'k:{"name":"install-cni"}': {'.': {}, 'f:args': {}, 'f:command': {}, 'f:image': {}, 'f:imagePullPolicy': {}, 'f:name': {}, 'f:resources': {}, 'f:terminationMessagePath': {}, 'f:terminationMessagePolicy': {}, 'f:volumeMounts': {'.': {}, 'k:{"mountPath":"/etc/cni/net.d"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}, 'k:{"mountPath":"/etc/kube-flannel/"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}}}, 'k:{"name":"install-cni-plugin"}': {'.': {}, 'f:args': {}, 'f:command': {}, 'f:image': {}, 'f:imagePullPolicy': {}, 'f:name': {}, 'f:resources': {}, 'f:terminationMessagePath': {}, 'f:terminationMessagePolicy': {}, 'f:volumeMounts': {'.': {}, 'k:{"mountPath":"/opt/cni/bin"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}}}}, 'f:priorityClassName': {}, 'f:restartPolicy': {}, 'f:schedulerName': {}, 'f:securityContext': {}, 'f:serviceAccount': {}, 'f:serviceAccountName': {}, 'f:terminationGracePeriodSeconds': {}, 'f:tolerations': {}, 'f:volumes': {'.': {}, 'k:{"name":"cni"}': {'.': {}, 'f:hostPath': {'.': {}, 'f:path': {}, 'f:type': {}}, 'f:name': {}}, 'k:{"name":"cni-plugin"}': {'.': {}, 'f:hostPath': {'.': {}, 'f:path': {}, 'f:type': {}}, 'f:name': {}}, 'k:{"name":"flannel-cfg"}': {'.': {}, 'f:configMap': {'.': {}, 'f:defaultMode': {}, 'f:name': {}}, 'f:name': {}}, 'k:{"name":"run"}': {'.': {}, 'f:hostPath': {'.': {}, 'f:path': {}, 'f:type': {}}, 'f:name': {}}, 'k:{"name":"xtables-lock"}': {'.': {}, 'f:hostPath': {'.': {}, 'f:path': {}, 'f:type': {}}, 'f:name': {}}}}}}, {'manager': 'kubelet', 'operation': 'Update', 'apiVersion': 'v1', 'time': '2025-09-12T02:31:03Z', 'fieldsType': 'FieldsV1', 'fieldsV1': {'f:status': {'f:conditions': {'k:{"type":"ContainersReady"}': {'.': {}, 'f:lastProbeTime': {}, 'f:lastTransitionTime': {}, 'f:message': {}, 'f:reason': {}, 'f:status': {}, 'f:type': {}}, 'k:{"type":"Initialized"}': {'.': {}, 'f:lastProbeTime': {}, 'f:lastTransitionTime': {}, 'f:status': {}, 'f:type': {}}, 'k:{"type":"PodReadyToStartContainers"}': {'.': {}, 'f:lastProbeTime': {}, 'f:lastTransitionTime': {}, 'f:status': {}, 'f:type': {}}, 'k:{"type":"Ready"}': {'.': {}, 'f:lastProbeTime': {}, 'f:lastTransitionTime': {}, 'f:message': {}, 'f:reason': {}, 'f:status': {}, 'f:type': {}}}, 'f:containerStatuses': {}, 'f:hostIP': {}, 'f:hostIPs': {}, 'f:initContainerStatuses': {}, 'f:phase': {}, 'f:podIP': {}, 'f:podIPs': {'.': {}, 'k:{"ip":"192.168.4.62"}': {'.': {}, 'f:ip': {}}}, 'f:startTime': {}}}, 'subresource': 'status'}]}, 'spec': {'volumes': [{'name': 'run', 'hostPath': {'path': '/run/flannel', 'type': ''}}, {'name': 'cni-plugin', 'hostPath': {'path': '/opt/cni/bin', 'type': ''}}, {'name': 'cni', 'hostPath': {'path': '/etc/cni/net.d', 'type': ''}}, {'name': 'flannel-cfg', 'configMap': {'name': 'kube-flannel-cfg', 'defaultMode': 420}}, {'name': 'xtables-lock', 'hostPath': {'path': '/run/xtables.lock', 'type': 'FileOrCreate'}}, {'name': 'kube-api-access-jsvb9', 'projected': {'sources': [{'serviceAccountToken': {'expirationSeconds': 3607, 'path': 'token'}}, {'configMap': {'name': 'kube-root-ca.crt', 'items': [{'key': 'ca.crt', 'path': 'ca.crt'}]}}, {'downwardAPI': {'items': [{'path': 'namespace', 'fieldRef': {'apiVersion': 'v1', 'fieldPath': 'metadata.namespace'}}]}}], 'defaultMode': 420}}], 'initContainers': [{'name': 'install-cni-plugin', 'image': 'ghcr.io/flannel-io/flannel-cni-plugin:v1.7.1-flannel1', 'command': ['cp'], 'args': ['-f', '/flannel', '/opt/cni/bin/flannel'], 'resources': {}, 'volumeMounts': [{'name': 'cni-plugin', 'mountPath': '/opt/cni/bin'}, {'name': 'kube-api-access-jsvb9', 'readOnly': True, 'mountPath': '/var/run/secrets/kubernetes.io/serviceaccount'}], 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'imagePullPolicy': 'IfNotPresent'}, {'name': 'install-cni', 'image': 'ghcr.io/flannel-io/flannel:v0.27.3', 'command': ['cp'], 'args': ['-f', '/etc/kube-flannel/cni-conf.json', '/etc/cni/net.d/10-flannel.conflist'], 'resources': {}, 'volumeMounts': [{'name': 'cni', 'mountPath': '/etc/cni/net.d'}, {'name': 'flannel-cfg', 'mountPath': '/etc/kube-flannel/'}, {'name': 'kube-api-access-jsvb9', 'readOnly': True, 'mountPath': '/var/run/secrets/kubernetes.io/serviceaccount'}], 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'imagePullPolicy': 'IfNotPresent'}], 'containers': [{'name': 'kube-flannel', 'image': 'ghcr.io/flannel-io/flannel:v0.27.3', 'command': ['/opt/bin/flanneld'], 'args': ['--ip-masq', '--kube-subnet-mgr'], 'env': [{'name': 'POD_NAME', 'valueFrom': {'fieldRef': {'apiVersion': 'v1', 'fieldPath': 'metadata.name'}}}, {'name': 'POD_NAMESPACE', 'valueFrom': {'fieldRef': {'apiVersion': 'v1', 'fieldPath': 'metadata.namespace'}}}, {'name': 'EVENT_QUEUE_DEPTH', 'value': '5000'}, {'name': 'CONT_WHEN_CACHE_NOT_READY', 'value': 'false'}], 'resources': {'requests': {'cpu': '100m', 'memory': '50Mi'}}, 'volumeMounts': [{'name': 'run', 'mountPath': '/run/flannel'}, {'name': 'flannel-cfg', 'mountPath': '/etc/kube-flannel/'}, {'name': 'xtables-lock', 'mountPath': '/run/xtables.lock'}, {'name': 'kube-api-access-jsvb9', 'readOnly': True, 'mountPath': '/var/run/secrets/kubernetes.io/serviceaccount'}], 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'imagePullPolicy': 'IfNotPresent', 'securityContext': {'capabilities': {'add': ['NET_ADMIN', 'NET_RAW']}, 'privileged': False}}], 'restartPolicy': 'Always', 'terminationGracePeriodSeconds': 30, 'dnsPolicy': 'ClusterFirst', 'serviceAccountName': 'flannel', 'serviceAccount': 'flannel', 'nodeName': 'homelab', 'hostNetwork': True, 'securityContext': {}, 'affinity': {'nodeAffinity': {'requiredDuringSchedulingIgnoredDuringExecution': {'nodeSelectorTerms': [{'matchFields': [{'key': 'metadata.name', 'operator': 'In', 'values': ['homelab']}]}]}}}, 'schedulerName': 'default-scheduler', 'tolerations': [{'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/not-ready', 'operator': 'Exists', 'effect': 'NoExecute'}, {'key': 'node.kubernetes.io/unreachable', 'operator': 'Exists', 'effect': 'NoExecute'}, {'key': 'node.kubernetes.io/disk-pressure', 'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/memory-pressure', 'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/pid-pressure', 'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/unschedulable', 'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/network-unavailable', 'operator': 'Exists', 'effect': 'NoSchedule'}], 'priorityClassName': 'system-node-critical', 'priority': 2000001000, 'enableServiceLinks': True, 'preemptionPolicy': 'PreemptLowerPriority'}, 'status': {'phase': 'Running', 'conditions': [{'type': 'PodReadyToStartContainers', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-12T02:30:46Z'}, {'type': 'Initialized', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-12T02:09:18Z'}, {'type': 'Ready', 'status': 'False', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-12T02:30:45Z', 'reason': 'ContainersNotReady', 'message': 'containers with unready status: [kube-flannel]'}, {'type': 'ContainersReady', 'status': 'False', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-12T02:30:45Z', 'reason': 'ContainersNotReady', 'message': 'containers with unready status: [kube-flannel]'}, {'type': 'PodScheduled', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-12T02:09:16Z'}], 'hostIP': '192.168.4.62', 'hostIPs': [{'ip': '192.168.4.62'}], 'podIP': '192.168.4.62', 'podIPs': [{'ip': '192.168.4.62'}], 'startTime': '2025-09-12T02:09:16Z', 'initContainerStatuses': [{'name': 'install-cni-plugin', 'state': {'terminated': {'exitCode': 0, 'reason': 'Completed', 'startedAt': '2025-09-12T02:30:45Z', 'finishedAt': '2025-09-12T02:30:45Z', 'containerID': 'containerd://24a566129c167cfc71adc4953b52313fdcc4537f247f277a2b3c3118980efd2b'}}, 'lastState': {}, 'ready': True, 'restartCount': 1, 'image': 'ghcr.io/flannel-io/flannel-cni-plugin:v1.7.1-flannel1', 'imageID': 'ghcr.io/flannel-io/flannel-cni-plugin@sha256:cb3176a2c9eae5fa0acd7f45397e706eacb4577dac33cad89f93b775ff5611df', 'containerID': 'containerd://24a566129c167cfc71adc4953b52313fdcc4537f247f277a2b3c3118980efd2b', 'started': False}, {'name': 'install-cni', 'state': {'terminated': {'exitCode': 0, 'reason': 'Completed', 'startedAt': '2025-09-12T02:30:46Z', 'finishedAt': '2025-09-12T02:30:46Z', 'containerID': 'containerd://997e6e5f2ab25d43857fab8bb36af7175ca0345462648eed2afcf9f99818f83b'}}, 'lastState': {}, 'ready': True, 'restartCount': 0, 'image': 'ghcr.io/flannel-io/flannel:v0.27.3', 'imageID': 'ghcr.io/flannel-io/flannel@sha256:8cc0cf9e94df48e98be84bce3e61984bbd46c3c44ad35707ec7ef40e96b009d1', 'containerID': 'containerd://997e6e5f2ab25d43857fab8bb36af7175ca0345462648eed2afcf9f99818f83b', 'started': False}], 'containerStatuses': [{'name': 'kube-flannel', 'state': {'waiting': {'reason': 'CrashLoopBackOff', 'message': 'back-off 5m0s restarting failed container=kube-flannel pod=kube-flannel-ds-9xsgk_kube-flannel(b99aece8-3f9c-4f79-817f-e474f62aef41)'}}, 'lastState': {'terminated': {'exitCode': 0, 'reason': 'Completed', 'startedAt': '2025-09-12T02:29:36Z', 'finishedAt': '2025-09-12T02:30:45Z', 'containerID': 'containerd://b9130cc9e96206326022d52ca67a89704d67fc80350a4d815bdfdda431856fa8'}}, 'ready': False, 'restartCount': 7, 'image': 'ghcr.io/flannel-io/flannel:v0.27.3', 'imageID': 'ghcr.io/flannel-io/flannel@sha256:8cc0cf9e94df48e98be84bce3e61984bbd46c3c44ad35707ec7ef40e96b009d1', 'containerID': 'containerd://b9130cc9e96206326022d52ca67a89704d67fc80350a4d815bdfdda431856fa8', 'started': False}], 'qosClass': 'Burstable'}, 'apiVersion': 'v1', 'kind': 'Pod'})
skipping: [192.168.4.63] => (item={'metadata': {'name': 'kube-flannel-ds-bjwls', 'generateName': 'kube-flannel-ds-', 'namespace': 'kube-flannel', 'uid': '23ca8404-101e-4844-9a68-633fb02266c9', 'resourceVersion': '119088', 'creationTimestamp': '2025-09-11T23:02:36Z', 'labels': {'app': 'flannel', 'controller-revision-hash': '8f698f8cd', 'k8s-app': 'flannel', 'pod-template-generation': '1', 'tier': 'node'}, 'ownerReferences': [{'apiVersion': 'apps/v1', 'kind': 'DaemonSet', 'name': 'kube-flannel-ds', 'uid': '498c66ad-8d8f-4b82-abce-7eff1439f0f1', 'controller': True, 'blockOwnerDeletion': True}], 'managedFields': [{'manager': 'kube-controller-manager', 'operation': 'Update', 'apiVersion': 'v1', 'time': '2025-09-11T23:02:36Z', 'fieldsType': 'FieldsV1', 'fieldsV1': {'f:metadata': {'f:generateName': {}, 'f:labels': {'.': {}, 'f:app': {}, 'f:controller-revision-hash': {}, 'f:k8s-app': {}, 'f:pod-template-generation': {}, 'f:tier': {}}, 'f:ownerReferences': {'.': {}, 'k:{"uid":"498c66ad-8d8f-4b82-abce-7eff1439f0f1"}': {}}}, 'f:spec': {'f:affinity': {'.': {}, 'f:nodeAffinity': {'.': {}, 'f:requiredDuringSchedulingIgnoredDuringExecution': {}}}, 'f:containers': {'k:{"name":"kube-flannel"}': {'.': {}, 'f:args': {}, 'f:command': {}, 'f:env': {'.': {}, 'k:{"name":"CONT_WHEN_CACHE_NOT_READY"}': {'.': {}, 'f:name': {}, 'f:value': {}}, 'k:{"name":"EVENT_QUEUE_DEPTH"}': {'.': {}, 'f:name': {}, 'f:value': {}}, 'k:{"name":"POD_NAME"}': {'.': {}, 'f:name': {}, 'f:valueFrom': {'.': {}, 'f:fieldRef': {}}}, 'k:{"name":"POD_NAMESPACE"}': {'.': {}, 'f:name': {}, 'f:valueFrom': {'.': {}, 'f:fieldRef': {}}}}, 'f:image': {}, 'f:imagePullPolicy': {}, 'f:name': {}, 'f:resources': {'.': {}, 'f:requests': {'.': {}, 'f:cpu': {}, 'f:memory': {}}}, 'f:securityContext': {'.': {}, 'f:capabilities': {'.': {}, 'f:add': {}}, 'f:privileged': {}}, 'f:terminationMessagePath': {}, 'f:terminationMessagePolicy': {}, 'f:volumeMounts': {'.': {}, 'k:{"mountPath":"/etc/kube-flannel/"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}, 'k:{"mountPath":"/run/flannel"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}, 'k:{"mountPath":"/run/xtables.lock"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}}}}, 'f:dnsPolicy': {}, 'f:enableServiceLinks': {}, 'f:hostNetwork': {}, 'f:initContainers': {'.': {}, 'k:{"name":"install-cni"}': {'.': {}, 'f:args': {}, 'f:command': {}, 'f:image': {}, 'f:imagePullPolicy': {}, 'f:name': {}, 'f:resources': {}, 'f:terminationMessagePath': {}, 'f:terminationMessagePolicy': {}, 'f:volumeMounts': {'.': {}, 'k:{"mountPath":"/etc/cni/net.d"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}, 'k:{"mountPath":"/etc/kube-flannel/"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}}}, 'k:{"name":"install-cni-plugin"}': {'.': {}, 'f:args': {}, 'f:command': {}, 'f:image': {}, 'f:imagePullPolicy': {}, 'f:name': {}, 'f:resources': {}, 'f:terminationMessagePath': {}, 'f:terminationMessagePolicy': {}, 'f:volumeMounts': {'.': {}, 'k:{"mountPath":"/opt/cni/bin"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}}}}, 'f:priorityClassName': {}, 'f:restartPolicy': {}, 'f:schedulerName': {}, 'f:securityContext': {}, 'f:serviceAccount': {}, 'f:serviceAccountName': {}, 'f:terminationGracePeriodSeconds': {}, 'f:tolerations': {}, 'f:volumes': {'.': {}, 'k:{"name":"cni"}': {'.': {}, 'f:hostPath': {'.': {}, 'f:path': {}, 'f:type': {}}, 'f:name': {}}, 'k:{"name":"cni-plugin"}': {'.': {}, 'f:hostPath': {'.': {}, 'f:path': {}, 'f:type': {}}, 'f:name': {}}, 'k:{"name":"flannel-cfg"}': {'.': {}, 'f:configMap': {'.': {}, 'f:defaultMode': {}, 'f:name': {}}, 'f:name': {}}, 'k:{"name":"run"}': {'.': {}, 'f:hostPath': {'.': {}, 'f:path': {}, 'f:type': {}}, 'f:name': {}}, 'k:{"name":"xtables-lock"}': {'.': {}, 'f:hostPath': {'.': {}, 'f:path': {}, 'f:type': {}}, 'f:name': {}}}}}}, {'manager': 'kubelet', 'operation': 'Update', 'apiVersion': 'v1', 'time': '2025-09-11T23:03:20Z', 'fieldsType': 'FieldsV1', 'fieldsV1': {'f:status': {'f:conditions': {'k:{"type":"ContainersReady"}': {'.': {}, 'f:lastProbeTime': {}, 'f:lastTransitionTime': {}, 'f:status': {}, 'f:type': {}}, 'k:{"type":"Initialized"}': {'.': {}, 'f:lastProbeTime': {}, 'f:lastTransitionTime': {}, 'f:status': {}, 'f:type': {}}, 'k:{"type":"PodReadyToStartContainers"}': {'.': {}, 'f:lastProbeTime': {}, 'f:lastTransitionTime': {}, 'f:status': {}, 'f:type': {}}, 'k:{"type":"Ready"}': {'.': {}, 'f:lastProbeTime': {}, 'f:lastTransitionTime': {}, 'f:status': {}, 'f:type': {}}}, 'f:containerStatuses': {}, 'f:hostIP': {}, 'f:hostIPs': {}, 'f:initContainerStatuses': {}, 'f:phase': {}, 'f:podIP': {}, 'f:podIPs': {'.': {}, 'k:{"ip":"192.168.4.61"}': {'.': {}, 'f:ip': {}}}, 'f:startTime': {}}}, 'subresource': 'status'}]}, 'spec': {'volumes': [{'name': 'run', 'hostPath': {'path': '/run/flannel', 'type': ''}}, {'name': 'cni-plugin', 'hostPath': {'path': '/opt/cni/bin', 'type': ''}}, {'name': 'cni', 'hostPath': {'path': '/etc/cni/net.d', 'type': ''}}, {'name': 'flannel-cfg', 'configMap': {'name': 'kube-flannel-cfg', 'defaultMode': 420}}, {'name': 'xtables-lock', 'hostPath': {'path': '/run/xtables.lock', 'type': 'FileOrCreate'}}, {'name': 'kube-api-access-mm7qj', 'projected': {'sources': [{'serviceAccountToken': {'expirationSeconds': 3607, 'path': 'token'}}, {'configMap': {'name': 'kube-root-ca.crt', 'items': [{'key': 'ca.crt', 'path': 'ca.crt'}]}}, {'downwardAPI': {'items': [{'path': 'namespace', 'fieldRef': {'apiVersion': 'v1', 'fieldPath': 'metadata.namespace'}}]}}], 'defaultMode': 420}}], 'initContainers': [{'name': 'install-cni-plugin', 'image': 'ghcr.io/flannel-io/flannel-cni-plugin:v1.7.1-flannel1', 'command': ['cp'], 'args': ['-f', '/flannel', '/opt/cni/bin/flannel'], 'resources': {}, 'volumeMounts': [{'name': 'cni-plugin', 'mountPath': '/opt/cni/bin'}, {'name': 'kube-api-access-mm7qj', 'readOnly': True, 'mountPath': '/var/run/secrets/kubernetes.io/serviceaccount'}], 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'imagePullPolicy': 'IfNotPresent'}, {'name': 'install-cni', 'image': 'ghcr.io/flannel-io/flannel:v0.27.3', 'command': ['cp'], 'args': ['-f', '/etc/kube-flannel/cni-conf.json', '/etc/cni/net.d/10-flannel.conflist'], 'resources': {}, 'volumeMounts': [{'name': 'cni', 'mountPath': '/etc/cni/net.d'}, {'name': 'flannel-cfg', 'mountPath': '/etc/kube-flannel/'}, {'name': 'kube-api-access-mm7qj', 'readOnly': True, 'mountPath': '/var/run/secrets/kubernetes.io/serviceaccount'}], 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'imagePullPolicy': 'IfNotPresent'}], 'containers': [{'name': 'kube-flannel', 'image': 'ghcr.io/flannel-io/flannel:v0.27.3', 'command': ['/opt/bin/flanneld'], 'args': ['--ip-masq', '--kube-subnet-mgr'], 'env': [{'name': 'POD_NAME', 'valueFrom': {'fieldRef': {'apiVersion': 'v1', 'fieldPath': 'metadata.name'}}}, {'name': 'POD_NAMESPACE', 'valueFrom': {'fieldRef': {'apiVersion': 'v1', 'fieldPath': 'metadata.namespace'}}}, {'name': 'EVENT_QUEUE_DEPTH', 'value': '5000'}, {'name': 'CONT_WHEN_CACHE_NOT_READY', 'value': 'false'}], 'resources': {'requests': {'cpu': '100m', 'memory': '50Mi'}}, 'volumeMounts': [{'name': 'run', 'mountPath': '/run/flannel'}, {'name': 'flannel-cfg', 'mountPath': '/etc/kube-flannel/'}, {'name': 'xtables-lock', 'mountPath': '/run/xtables.lock'}, {'name': 'kube-api-access-mm7qj', 'readOnly': True, 'mountPath': '/var/run/secrets/kubernetes.io/serviceaccount'}], 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'imagePullPolicy': 'IfNotPresent', 'securityContext': {'capabilities': {'add': ['NET_ADMIN', 'NET_RAW']}, 'privileged': False}}], 'restartPolicy': 'Always', 'terminationGracePeriodSeconds': 30, 'dnsPolicy': 'ClusterFirst', 'serviceAccountName': 'flannel', 'serviceAccount': 'flannel', 'nodeName': 'storagenodet3500', 'hostNetwork': True, 'securityContext': {}, 'affinity': {'nodeAffinity': {'requiredDuringSchedulingIgnoredDuringExecution': {'nodeSelectorTerms': [{'matchFields': [{'key': 'metadata.name', 'operator': 'In', 'values': ['storagenodet3500']}]}]}}}, 'schedulerName': 'default-scheduler', 'tolerations': [{'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/not-ready', 'operator': 'Exists', 'effect': 'NoExecute'}, {'key': 'node.kubernetes.io/unreachable', 'operator': 'Exists', 'effect': 'NoExecute'}, {'key': 'node.kubernetes.io/disk-pressure', 'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/memory-pressure', 'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/pid-pressure', 'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/unschedulable', 'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/network-unavailable', 'operator': 'Exists', 'effect': 'NoSchedule'}], 'priorityClassName': 'system-node-critical', 'priority': 2000001000, 'enableServiceLinks': True, 'preemptionPolicy': 'PreemptLowerPriority'}, 'status': {'phase': 'Running', 'conditions': [{'type': 'PodReadyToStartContainers', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-11T23:03:03Z'}, {'type': 'Initialized', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-11T23:03:19Z'}, {'type': 'Ready', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-11T23:03:20Z'}, {'type': 'ContainersReady', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-11T23:03:20Z'}, {'type': 'PodScheduled', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-11T23:02:36Z'}], 'hostIP': '192.168.4.61', 'hostIPs': [{'ip': '192.168.4.61'}], 'podIP': '192.168.4.61', 'podIPs': [{'ip': '192.168.4.61'}], 'startTime': '2025-09-11T23:02:38Z', 'initContainerStatuses': [{'name': 'install-cni-plugin', 'state': {'terminated': {'exitCode': 0, 'reason': 'Completed', 'startedAt': '2025-09-11T23:03:02Z', 'finishedAt': '2025-09-11T23:03:02Z', 'containerID': 'containerd://ca059b7e1c6e658bd270a757bd96209a983633789e4acf6ad50c3c5b52355ad7'}}, 'lastState': {}, 'ready': True, 'restartCount': 0, 'image': 'ghcr.io/flannel-io/flannel-cni-plugin:v1.7.1-flannel1', 'imageID': 'ghcr.io/flannel-io/flannel-cni-plugin@sha256:cb3176a2c9eae5fa0acd7f45397e706eacb4577dac33cad89f93b775ff5611df', 'containerID': 'containerd://ca059b7e1c6e658bd270a757bd96209a983633789e4acf6ad50c3c5b52355ad7', 'started': False}, {'name': 'install-cni', 'state': {'terminated': {'exitCode': 0, 'reason': 'Completed', 'startedAt': '2025-09-11T23:03:15Z', 'finishedAt': '2025-09-11T23:03:15Z', 'containerID': 'containerd://db58802e880a12fcd93ce3797a521c5fa04d04f516608db22f4cf5ee6b104217'}}, 'lastState': {}, 'ready': True, 'restartCount': 0, 'image': 'ghcr.io/flannel-io/flannel:v0.27.3', 'imageID': 'ghcr.io/flannel-io/flannel@sha256:8cc0cf9e94df48e98be84bce3e61984bbd46c3c44ad35707ec7ef40e96b009d1', 'containerID': 'containerd://db58802e880a12fcd93ce3797a521c5fa04d04f516608db22f4cf5ee6b104217', 'started': False}], 'containerStatuses': [{'name': 'kube-flannel', 'state': {'running': {'startedAt': '2025-09-11T23:03:19Z'}}, 'lastState': {}, 'ready': True, 'restartCount': 0, 'image': 'ghcr.io/flannel-io/flannel:v0.27.3', 'imageID': 'ghcr.io/flannel-io/flannel@sha256:8cc0cf9e94df48e98be84bce3e61984bbd46c3c44ad35707ec7ef40e96b009d1', 'containerID': 'containerd://155ed7af1d8aba3b67a10d7ee04b1e80c07f6a64e9696ccb3e3ae66086c41a42', 'started': True}], 'qosClass': 'Burstable'}, 'apiVersion': 'v1', 'kind': 'Pod'})
skipping: [192.168.4.63] => (item={'metadata': {'name': 'kube-flannel-ds-n2cln', 'generateName': 'kube-flannel-ds-', 'namespace': 'kube-flannel', 'uid': 'ab34ad2b-acc0-44b8-b2db-053cb7b18e00', 'resourceVersion': '97993', 'creationTimestamp': '2025-09-10T20:30:28Z', 'labels': {'app': 'flannel', 'controller-revision-hash': '8f698f8cd', 'k8s-app': 'flannel', 'pod-template-generation': '1', 'tier': 'node'}, 'ownerReferences': [{'apiVersion': 'apps/v1', 'kind': 'DaemonSet', 'name': 'kube-flannel-ds', 'uid': '498c66ad-8d8f-4b82-abce-7eff1439f0f1', 'controller': True, 'blockOwnerDeletion': True}], 'managedFields': [{'manager': 'kube-controller-manager', 'operation': 'Update', 'apiVersion': 'v1', 'time': '2025-09-10T20:30:28Z', 'fieldsType': 'FieldsV1', 'fieldsV1': {'f:metadata': {'f:generateName': {}, 'f:labels': {'.': {}, 'f:app': {}, 'f:controller-revision-hash': {}, 'f:k8s-app': {}, 'f:pod-template-generation': {}, 'f:tier': {}}, 'f:ownerReferences': {'.': {}, 'k:{"uid":"498c66ad-8d8f-4b82-abce-7eff1439f0f1"}': {}}}, 'f:spec': {'f:affinity': {'.': {}, 'f:nodeAffinity': {'.': {}, 'f:requiredDuringSchedulingIgnoredDuringExecution': {}}}, 'f:containers': {'k:{"name":"kube-flannel"}': {'.': {}, 'f:args': {}, 'f:command': {}, 'f:env': {'.': {}, 'k:{"name":"CONT_WHEN_CACHE_NOT_READY"}': {'.': {}, 'f:name': {}, 'f:value': {}}, 'k:{"name":"EVENT_QUEUE_DEPTH"}': {'.': {}, 'f:name': {}, 'f:value': {}}, 'k:{"name":"POD_NAME"}': {'.': {}, 'f:name': {}, 'f:valueFrom': {'.': {}, 'f:fieldRef': {}}}, 'k:{"name":"POD_NAMESPACE"}': {'.': {}, 'f:name': {}, 'f:valueFrom': {'.': {}, 'f:fieldRef': {}}}}, 'f:image': {}, 'f:imagePullPolicy': {}, 'f:name': {}, 'f:resources': {'.': {}, 'f:requests': {'.': {}, 'f:cpu': {}, 'f:memory': {}}}, 'f:securityContext': {'.': {}, 'f:capabilities': {'.': {}, 'f:add': {}}, 'f:privileged': {}}, 'f:terminationMessagePath': {}, 'f:terminationMessagePolicy': {}, 'f:volumeMounts': {'.': {}, 'k:{"mountPath":"/etc/kube-flannel/"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}, 'k:{"mountPath":"/run/flannel"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}, 'k:{"mountPath":"/run/xtables.lock"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}}}}, 'f:dnsPolicy': {}, 'f:enableServiceLinks': {}, 'f:hostNetwork': {}, 'f:initContainers': {'.': {}, 'k:{"name":"install-cni"}': {'.': {}, 'f:args': {}, 'f:command': {}, 'f:image': {}, 'f:imagePullPolicy': {}, 'f:name': {}, 'f:resources': {}, 'f:terminationMessagePath': {}, 'f:terminationMessagePolicy': {}, 'f:volumeMounts': {'.': {}, 'k:{"mountPath":"/etc/cni/net.d"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}, 'k:{"mountPath":"/etc/kube-flannel/"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}}}, 'k:{"name":"install-cni-plugin"}': {'.': {}, 'f:args': {}, 'f:command': {}, 'f:image': {}, 'f:imagePullPolicy': {}, 'f:name': {}, 'f:resources': {}, 'f:terminationMessagePath': {}, 'f:terminationMessagePolicy': {}, 'f:volumeMounts': {'.': {}, 'k:{"mountPath":"/opt/cni/bin"}': {'.': {}, 'f:mountPath': {}, 'f:name': {}}}}}, 'f:priorityClassName': {}, 'f:restartPolicy': {}, 'f:schedulerName': {}, 'f:securityContext': {}, 'f:serviceAccount': {}, 'f:serviceAccountName': {}, 'f:terminationGracePeriodSeconds': {}, 'f:tolerations': {}, 'f:volumes': {'.': {}, 'k:{"name":"cni"}': {'.': {}, 'f:hostPath': {'.': {}, 'f:path': {}, 'f:type': {}}, 'f:name': {}}, 'k:{"name":"cni-plugin"}': {'.': {}, 'f:hostPath': {'.': {}, 'f:path': {}, 'f:type': {}}, 'f:name': {}}, 'k:{"name":"flannel-cfg"}': {'.': {}, 'f:configMap': {'.': {}, 'f:defaultMode': {}, 'f:name': {}}, 'f:name': {}}, 'k:{"name":"run"}': {'.': {}, 'f:hostPath': {'.': {}, 'f:path': {}, 'f:type': {}}, 'f:name': {}}, 'k:{"name":"xtables-lock"}': {'.': {}, 'f:hostPath': {'.': {}, 'f:path': {}, 'f:type': {}}, 'f:name': {}}}}}}, {'manager': 'kubelet', 'operation': 'Update', 'apiVersion': 'v1', 'time': '2025-09-11T18:40:48Z', 'fieldsType': 'FieldsV1', 'fieldsV1': {'f:status': {'f:conditions': {'k:{"type":"ContainersReady"}': {'.': {}, 'f:lastProbeTime': {}, 'f:lastTransitionTime': {}, 'f:status': {}, 'f:type': {}}, 'k:{"type":"Initialized"}': {'.': {}, 'f:lastProbeTime': {}, 'f:lastTransitionTime': {}, 'f:status': {}, 'f:type': {}}, 'k:{"type":"PodReadyToStartContainers"}': {'.': {}, 'f:lastProbeTime': {}, 'f:lastTransitionTime': {}, 'f:status': {}, 'f:type': {}}, 'k:{"type":"Ready"}': {'.': {}, 'f:lastProbeTime': {}, 'f:lastTransitionTime': {}, 'f:status': {}, 'f:type': {}}}, 'f:containerStatuses': {}, 'f:hostIP': {}, 'f:hostIPs': {}, 'f:initContainerStatuses': {}, 'f:phase': {}, 'f:podIP': {}, 'f:podIPs': {'.': {}, 'k:{"ip":"192.168.4.63"}': {'.': {}, 'f:ip': {}}}, 'f:startTime': {}}}, 'subresource': 'status'}]}, 'spec': {'volumes': [{'name': 'run', 'hostPath': {'path': '/run/flannel', 'type': ''}}, {'name': 'cni-plugin', 'hostPath': {'path': '/opt/cni/bin', 'type': ''}}, {'name': 'cni', 'hostPath': {'path': '/etc/cni/net.d', 'type': ''}}, {'name': 'flannel-cfg', 'configMap': {'name': 'kube-flannel-cfg', 'defaultMode': 420}}, {'name': 'xtables-lock', 'hostPath': {'path': '/run/xtables.lock', 'type': 'FileOrCreate'}}, {'name': 'kube-api-access-sx8sh', 'projected': {'sources': [{'serviceAccountToken': {'expirationSeconds': 3607, 'path': 'token'}}, {'configMap': {'name': 'kube-root-ca.crt', 'items': [{'key': 'ca.crt', 'path': 'ca.crt'}]}}, {'downwardAPI': {'items': [{'path': 'namespace', 'fieldRef': {'apiVersion': 'v1', 'fieldPath': 'metadata.namespace'}}]}}], 'defaultMode': 420}}], 'initContainers': [{'name': 'install-cni-plugin', 'image': 'ghcr.io/flannel-io/flannel-cni-plugin:v1.7.1-flannel1', 'command': ['cp'], 'args': ['-f', '/flannel', '/opt/cni/bin/flannel'], 'resources': {}, 'volumeMounts': [{'name': 'cni-plugin', 'mountPath': '/opt/cni/bin'}, {'name': 'kube-api-access-sx8sh', 'readOnly': True, 'mountPath': '/var/run/secrets/kubernetes.io/serviceaccount'}], 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'imagePullPolicy': 'IfNotPresent'}, {'name': 'install-cni', 'image': 'ghcr.io/flannel-io/flannel:v0.27.3', 'command': ['cp'], 'args': ['-f', '/etc/kube-flannel/cni-conf.json', '/etc/cni/net.d/10-flannel.conflist'], 'resources': {}, 'volumeMounts': [{'name': 'cni', 'mountPath': '/etc/cni/net.d'}, {'name': 'flannel-cfg', 'mountPath': '/etc/kube-flannel/'}, {'name': 'kube-api-access-sx8sh', 'readOnly': True, 'mountPath': '/var/run/secrets/kubernetes.io/serviceaccount'}], 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'imagePullPolicy': 'IfNotPresent'}], 'containers': [{'name': 'kube-flannel', 'image': 'ghcr.io/flannel-io/flannel:v0.27.3', 'command': ['/opt/bin/flanneld'], 'args': ['--ip-masq', '--kube-subnet-mgr'], 'env': [{'name': 'POD_NAME', 'valueFrom': {'fieldRef': {'apiVersion': 'v1', 'fieldPath': 'metadata.name'}}}, {'name': 'POD_NAMESPACE', 'valueFrom': {'fieldRef': {'apiVersion': 'v1', 'fieldPath': 'metadata.namespace'}}}, {'name': 'EVENT_QUEUE_DEPTH', 'value': '5000'}, {'name': 'CONT_WHEN_CACHE_NOT_READY', 'value': 'false'}], 'resources': {'requests': {'cpu': '100m', 'memory': '50Mi'}}, 'volumeMounts': [{'name': 'run', 'mountPath': '/run/flannel'}, {'name': 'flannel-cfg', 'mountPath': '/etc/kube-flannel/'}, {'name': 'xtables-lock', 'mountPath': '/run/xtables.lock'}, {'name': 'kube-api-access-sx8sh', 'readOnly': True, 'mountPath': '/var/run/secrets/kubernetes.io/serviceaccount'}], 'terminationMessagePath': '/dev/termination-log', 'terminationMessagePolicy': 'File', 'imagePullPolicy': 'IfNotPresent', 'securityContext': {'capabilities': {'add': ['NET_ADMIN', 'NET_RAW']}, 'privileged': False}}], 'restartPolicy': 'Always', 'terminationGracePeriodSeconds': 30, 'dnsPolicy': 'ClusterFirst', 'serviceAccountName': 'flannel', 'serviceAccount': 'flannel', 'nodeName': 'masternode', 'hostNetwork': True, 'securityContext': {}, 'affinity': {'nodeAffinity': {'requiredDuringSchedulingIgnoredDuringExecution': {'nodeSelectorTerms': [{'matchFields': [{'key': 'metadata.name', 'operator': 'In', 'values': ['masternode']}]}]}}}, 'schedulerName': 'default-scheduler', 'tolerations': [{'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/not-ready', 'operator': 'Exists', 'effect': 'NoExecute'}, {'key': 'node.kubernetes.io/unreachable', 'operator': 'Exists', 'effect': 'NoExecute'}, {'key': 'node.kubernetes.io/disk-pressure', 'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/memory-pressure', 'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/pid-pressure', 'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/unschedulable', 'operator': 'Exists', 'effect': 'NoSchedule'}, {'key': 'node.kubernetes.io/network-unavailable', 'operator': 'Exists', 'effect': 'NoSchedule'}], 'priorityClassName': 'system-node-critical', 'priority': 2000001000, 'enableServiceLinks': True, 'preemptionPolicy': 'PreemptLowerPriority'}, 'status': {'phase': 'Running', 'conditions': [{'type': 'PodReadyToStartContainers', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-11T18:40:46Z'}, {'type': 'Initialized', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-10T20:30:31Z'}, {'type': 'Ready', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-11T18:40:48Z'}, {'type': 'ContainersReady', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-11T18:40:48Z'}, {'type': 'PodScheduled', 'status': 'True', 'lastProbeTime': None, 'lastTransitionTime': '2025-09-10T20:30:28Z'}], 'hostIP': '192.168.4.63', 'hostIPs': [{'ip': '192.168.4.63'}], 'podIP': '192.168.4.63', 'podIPs': [{'ip': '192.168.4.63'}], 'startTime': '2025-09-10T20:30:28Z', 'initContainerStatuses': [{'name': 'install-cni-plugin', 'state': {'terminated': {'exitCode': 0, 'reason': 'Completed', 'startedAt': '2025-09-11T18:40:46Z', 'finishedAt': '2025-09-11T18:40:46Z', 'containerID': 'containerd://d402dfe14ef11f93a30c8ebeb0dd22e12e25fbbb20bc9b214f123f694dbd1316'}}, 'lastState': {}, 'ready': True, 'restartCount': 1, 'image': 'ghcr.io/flannel-io/flannel-cni-plugin:v1.7.1-flannel1', 'imageID': 'ghcr.io/flannel-io/flannel-cni-plugin@sha256:cb3176a2c9eae5fa0acd7f45397e706eacb4577dac33cad89f93b775ff5611df', 'containerID': 'containerd://d402dfe14ef11f93a30c8ebeb0dd22e12e25fbbb20bc9b214f123f694dbd1316', 'started': False}, {'name': 'install-cni', 'state': {'terminated': {'exitCode': 0, 'reason': 'Completed', 'startedAt': '2025-09-11T18:40:46Z', 'finishedAt': '2025-09-11T18:40:46Z', 'containerID': 'containerd://6009607a45fbe978aa97dfe10a556ed746a6f919a2954b09f3d1e4e081b0d95b'}}, 'lastState': {}, 'ready': True, 'restartCount': 0, 'image': 'ghcr.io/flannel-io/flannel:v0.27.3', 'imageID': 'ghcr.io/flannel-io/flannel@sha256:8cc0cf9e94df48e98be84bce3e61984bbd46c3c44ad35707ec7ef40e96b009d1', 'containerID': 'containerd://6009607a45fbe978aa97dfe10a556ed746a6f919a2954b09f3d1e4e081b0d95b', 'started': False}], 'containerStatuses': [{'name': 'kube-flannel', 'state': {'running': {'startedAt': '2025-09-11T18:40:47Z'}}, 'lastState': {'terminated': {'exitCode': 255, 'reason': 'Unknown', 'startedAt': '2025-09-10T20:30:31Z', 'finishedAt': '2025-09-11T18:40:40Z', 'containerID': 'containerd://b7c0bb899fb0722c43a73f48ec5036abda55f986ea1acad76e3c19caf694e4fb'}}, 'ready': True, 'restartCount': 1, 'image': 'ghcr.io/flannel-io/flannel:v0.27.3', 'imageID': 'ghcr.io/flannel-io/flannel@sha256:8cc0cf9e94df48e98be84bce3e61984bbd46c3c44ad35707ec7ef40e96b009d1', 'containerID': 'containerd://945604aa75ef9808a0b607ee49c27cc239f982dfc9a216ab230fbcd852e56ef3', 'started': True}], 'qosClass': 'Burstable'}, 'apiVersion': 'v1', 'kind': 'Pod'})

TASK [Wait for flannel to stabilize] **********************************************************************************************************************************************************
Pausing for 30 seconds
(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
ok: [192.168.4.63]

TASK [Deploy Prometheus] **********************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Create Prometheus ConfigMap] ************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Deploy Prometheus Service] **************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Deploy Grafana] *************************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Deploy Grafana Service] *****************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Deploy Loki] ****************************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Deploy Loki Service] ********************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Deploy Kubernetes Dashboard] ************************************************************************************************************************************************************
changed: [192.168.4.63]

TASK [Patch Kubernetes Dashboard to run on monitoring node] ***********************************************************************************************************************************
changed: [192.168.4.63]

TASK [Patch Dashboard Metrics Scraper to run on monitoring node] ******************************************************************************************************************************
changed: [192.168.4.63]

TASK [Create Dashboard Admin User] ************************************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Create Dashboard Admin ClusterRoleBinding] **********************************************************************************************************************************************
ok: [192.168.4.63]

TASK [Check if cluster networking is stable before waiting] ***********************************************************************************************************************************
ok: [192.168.4.63]

TASK [Verify at least one CoreDNS pod is running] *********************************************************************************************************************************************
fatal: [192.168.4.63]: FAILED! => {"changed": false, "msg": "CoreDNS is not running - cluster networking is unstable. Run: ./scripts/fix_homelab_node_issues.sh"}

PLAY RECAP ************************************************************************************************************************************************************************************
192.168.4.61               : ok=47   changed=10   unreachable=0    failed=0    skipped=106  rescued=0    ignored=0
192.168.4.62               : ok=43   changed=10   unreachable=0    failed=0    skipped=104  rescued=0    ignored=0
192.168.4.63               : ok=96   changed=33   unreachable=0    failed=1    skipped=43   rescued=0    ignored=0
localhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0

root@masternode:/srv/monitoring_data/VMStation# ./scripts/fix_homelab_node_issues.sh
=== Homelab Node Issue Remediation ===
Timestamp: Thu 11 Sep 2025 10:32:16 PM EDT

[INFO] Step 0: Checking for CNI bridge IP conflicts
[WARN] Found 11 pods stuck in ContainerCreating - checking for CNI bridge conflicts
[ERROR] CNI bridge IP conflict detected!
Error: kube-system            32m         Warning   FailedCreatePodSandBox   pod/coredns-68444cf7cd-vq79r                     Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "d2fca323a070fffdaa5fa9e425a955acaf71b88d3630d0490eb63ca3640ff635": plugin type="bridge" failed (add): failed to set bridge addr: "cni0" already has an IP address different from 10.244.0.1/16

[WARN] Applying CNI bridge fix before proceeding with other fixes...
=== CNI Bridge Conflict Fix ===
Timestamp: Thu 11 Sep 2025 10:32:17 PM EDT
Fixing: cni0 bridge IP address conflicts preventing pod creation

[INFO] Step 1: Diagnosing CNI bridge configuration on all nodes

=== Node: homelab ===
Worker node - will be handled by CNI reset on control plane

=== Node: masternode ===
Checking CNI bridge configuration on control plane node...
Current cni0 bridge configuration:
77: cni0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    inet 10.244.0.1/24 brd 10.244.0.255 scope global cni0
    inet6 fe80::a8dd:8fff:fe82:88eb/64 scope link
Current cni0 IP: 10.244.0.1/24
[INFO] cni0 bridge IP is in correct Flannel subnet

=== Node: storagenodet3500 ===
Worker node - will be handled by CNI reset on control plane
[INFO] Step 2: Checking current pod status
Pods stuck in ContainerCreating:
kube-system            coredns-68444cf7cd-vq79r                     0/1     ContainerCreating   0                33m
kubernetes-dashboard   dashboard-metrics-scraper-597744f4cf-lmvlj   0/1     ContainerCreating   0                9h
kubernetes-dashboard   dashboard-metrics-scraper-fbd9c767-8pzxd     0/1     ContainerCreating   0                9h
kubernetes-dashboard   kubernetes-dashboard-547749b7d9-zcs4w        0/1     ContainerCreating   0                9h
kubernetes-dashboard   kubernetes-dashboard-7fc5595d85-cgh9p        0/1     ContainerCreating   0                9h
monitoring             grafana-554bf5687f-l8snn                     0/1     ContainerCreating   0                9h
monitoring             grafana-79db5b584f-jdbgf                     0/1     ContainerCreating   0                117m
monitoring             loki-564bd8dfb7-wfjzk                        0/1     ContainerCreating   0                9h
monitoring             loki-85d467fb56-xt466                        0/1     ContainerCreating   0                117m
monitoring             prometheus-54d6cfcf7d-rs7vj                  0/1     ContainerCreating   0                9h
monitoring             prometheus-74887c8bb6-rqdl5                  0/1     ContainerCreating   0                117m

Recent pod creation errors:
kube-system            33m         Warning   FailedCreatePodSandBox   pod/coredns-68444cf7cd-vq79r                     Failed to create pod sandbox: rpc error: code = Unknown desc = failed to reserve sandbox name "coredns-68444cf7cd-vq79r_kube-system_c002f635-edb4-4087-8d49-097dc080494f_1": name "coredns-68444cf7cd-vq79r_kube-system_c002f635-edb4-4087-8d49-097dc080494f_1" is reserved for "809d4801a440ea859f79317215423b7b10de8368f17503ff427196b681b3b9cf"
kube-system            33m         Warning   FailedCreatePodSandBox   pod/coredns-68444cf7cd-vq79r                     Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "bf6ca3e732f901f0ee0a23652201c41859344395b4372b4ffe5481abc12019d5": plugin type="bridge" failed (add): failed to set bridge addr: "cni0" already has an IP address different from 10.244.0.1/16
kube-system            32m         Warning   FailedCreatePodSandBox   pod/coredns-68444cf7cd-vq79r                     Failed to create pod sandbox: rpc error: code = Unknown desc = failed to reserve sandbox name "coredns-68444cf7cd-vq79r_kube-system_c002f635-edb4-4087-8d49-097dc080494f_1": name "coredns-68444cf7cd-vq79r_kube-system_c002f635-edb4-4087-8d49-097dc080494f_1" is reserved for "bf6ca3e732f901f0ee0a23652201c41859344395b4372b4ffe5481abc12019d5"
kube-system            32m         Warning   FailedCreatePodSandBox   pod/coredns-68444cf7cd-vq79r                     Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "d2fca323a070fffdaa5fa9e425a955acaf71b88d3630d0490eb63ca3640ff635": plugin type="bridge" failed (add): failed to set bridge addr: "cni0" already has an IP address different from 10.244.0.1/16
kube-system            32m         Warning   FailedCreatePodSandBox   pod/coredns-68444cf7cd-vq79r                     Failed to create pod sandbox: rpc error: code = Unknown desc = failed to reserve sandbox name "coredns-68444cf7cd-vq79r_kube-system_c002f635-edb4-4087-8d49-097dc080494f_1": name "coredns-68444cf7cd-vq79r_kube-system_c002f635-edb4-4087-8d49-097dc080494f_1" is reserved for "d2fca323a070fffdaa5fa9e425a955acaf71b88d3630d0490eb63ca3640ff635"
[INFO] Step 3: Applying CNI bridge fix
[WARN] Found 1 routes using cni0 - will need to reset networking
[WARN] Deleting existing cni0 bridge to fix IP conflict
[INFO] Deleted existing cni0 bridge
[WARN] Cleaning up potentially conflicting CNI configurations
Remaining CNI configurations:
total 12
drwxr-xr-x 2 root root 4096 Sep 11 22:32 .
drwxr-xr-x 3 root root 4096 Sep  9 14:27 ..
-rw-r--r-- 1 root root  292 Sep 11 14:40 10-flannel.conflist
[INFO] Step 4: Restarting containerd to apply CNI changes
[INFO] Step 5: Restarting Flannel pods to recreate CNI bridge
Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
pod "kube-flannel-ds-6jqxl" force deleted from kube-flannel namespace
pod "kube-flannel-ds-bjwls" force deleted from kube-flannel namespace
pod "kube-flannel-ds-n2cln" force deleted from kube-flannel namespace
Waiting for Flannel pods to recreate...
[INFO] Waiting for Flannel DaemonSet to be ready...
daemon set "kube-flannel-ds" successfully rolled out
[INFO] Flannel DaemonSet is ready
[INFO] Step 6: Verifying CNI bridge fix
New cni0 bridge configuration:
3450: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
    inet 10.244.0.1/24 brd 10.244.0.255 scope global cni0
    inet6 fe80::b0b0:cfff:fe89:3b29/64 scope link
New cni0 IP: 10.244.0.1/24
[INFO] ✓ cni0 bridge now has correct Flannel subnet IP
[INFO] Step 7: Checking if stuck pods can now start
Current pod status:
kube-system            coredns-68444cf7cd-vq79r                     0/1     ContainerCreating   0                34m
kubernetes-dashboard   kubernetes-dashboard-547749b7d9-zcs4w        0/1     ContainerCreating   0                9h
kubernetes-dashboard   kubernetes-dashboard-7fc5595d85-cgh9p        0/1     ContainerCreating   0                9h
monitoring             grafana-554bf5687f-l8snn                     0/1     ContainerCreating   0                9h
monitoring             grafana-79db5b584f-jdbgf                     0/1     ContainerCreating   0                118m
monitoring             loki-564bd8dfb7-wfjzk                        0/1     ContainerCreating   0                9h
monitoring             loki-85d467fb56-xt466                        0/1     ContainerCreating   0                118m
monitoring             prometheus-74887c8bb6-rqdl5                  0/1     ContainerCreating   0                118m
[INFO] Testing pod creation with clean CNI bridge...
pod/cni-test created
[INFO] ✓ Test pod created successfully - CNI bridge fix worked
Test pod IP: 10.244.0.14
[INFO] ✓ Test pod received IP from correct Flannel subnet
pod "cni-test" deleted from kube-system namespace
root@masternode:/srv/monitoring_data/VMStation# kubectl get pods -o wide --all-namespaces
NAMESPACE              NAME                                       READY   STATUS             RESTARTS         AGE     IP             NODE               NOMINATED NODE   READINESS GATES
jellyfin               jellyfin                                   0/1     Running            12 (7m25s ago)   59m     10.244.0.2     storagenodet3500   <none>           <none>
kube-flannel           kube-flannel-ds-cz2k8                      1/1     Running            1 (17s ago)      92s     192.168.4.62   homelab            <none>           <none>
kube-flannel           kube-flannel-ds-vs2k8                      1/1     Running            0                92s     192.168.4.61   storagenodet3500   <none>           <none>
kube-flannel           kube-flannel-ds-x4gnh                      1/1     Running            0                92s     192.168.4.63   masternode         <none>           <none>
kube-system            coredns-68444cf7cd-vq79r                   1/1     Running            0                35m     10.244.0.27    masternode         <none>           <none>
kube-system            etcd-masternode                            1/1     Running            4 (7h53m ago)    30h     192.168.4.63   masternode         <none>           <none>
kube-system            kube-apiserver-masternode                  1/1     Running            11 (7h53m ago)   30h     192.168.4.63   masternode         <none>           <none>
kube-system            kube-controller-manager-masternode         1/1     Running            32 (7h53m ago)   30h     192.168.4.63   masternode         <none>           <none>
kube-system            kube-proxy-2zfsw                           1/1     Running            1 (7h53m ago)    30h     192.168.4.63   masternode         <none>           <none>
kube-system            kube-proxy-nh8vb                           0/1     CrashLoopBackOff   37 (3m36s ago)   3h29m   192.168.4.62   homelab            <none>           <none>
kube-system            kube-proxy-r8tlw                           1/1     Running            0                3h31m   192.168.4.61   storagenodet3500   <none>           <none>
kube-system            kube-scheduler-masternode                  1/1     Running            32 (7h53m ago)   30h     192.168.4.63   masternode         <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-fbd9c767-8pzxd   1/1     Running            0                9h      10.244.0.21    masternode         <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-547749b7d9-zcs4w      1/1     Running            0                9h      10.244.0.28    masternode         <none>           <none>
monitoring             grafana-79db5b584f-jdbgf                   1/1     Running            0                119m    10.244.0.29    masternode         <none>           <none>
monitoring             loki-564bd8dfb7-wfjzk                      0/1     Terminating        0                9h      10.244.0.26    masternode         <none>           <none>
monitoring             loki-85d467fb56-xt466                      1/1     Running            0                119m    10.244.0.25    masternode         <none>           <none>
monitoring             prometheus-74887c8bb6-rqdl5                1/1     Running            0                119m    10.244.0.23    masternode         <none>           <none>
