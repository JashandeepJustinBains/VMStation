
TASK [Show WoL report] *********************************************************
ok: [masternode] =>
  wol_report:
  - ip: 192.168.4.61
    mac: b8:ac:6f:7e:6c:9d
    name: storagenodet3500
    wol_out: Sending magic packet to 255.255.255.255:9 with b8:ac:6f:7e:6c:9d
  - ip: 192.168.4.62
    mac: d0:94:66:30:d6:63
    name: homelab
    wol_out: Sending magic packet to 255.255.255.255:9 with d0:94:66:30:d6:63

PLAY RECAP *********************************************************************
masternode                 : ok=84   changed=35   unreachable=0    failed=0    skipped=8    rescued=0    ignored=0
storagenodet3500           : ok=28   changed=4    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0

[2025-10-08 20:35:46] [INFO]
[2025-10-08 20:35:46] [INFO] ✓ Debian deployment completed successfully
[2025-10-08 20:35:46] [INFO]
[2025-10-08 20:35:46] [INFO] Running post-deployment verification...
[2025-10-08 20:35:51] [INFO] Verifying Debian cluster health...
[2025-10-08 20:35:51] [INFO] ✓ Debian cluster is healthy (2 Debian nodes Ready)
[2025-10-08 20:35:51] [INFO]
[2025-10-08 20:35:51] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 20:35:51] [INFO]   Debian Kubernetes Cluster Ready!
[2025-10-08 20:35:51] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 20:35:51] [INFO]
[2025-10-08 20:35:51] [INFO] Verification commands:
[2025-10-08 20:35:51] [INFO]   kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes
[2025-10-08 20:35:51] [INFO]   kubectl --kubeconfig=/etc/kubernetes/admin.conf get pods -A
[2025-10-08 20:35:51] [INFO]
[2025-10-08 20:35:51] [INFO] Log saved to: /srv/monitoring_data/VMStation/ansible/artifacts/deploy-debian.log
[2025-10-08 20:35:51] [INFO]
[2025-10-08 20:35:51] [INFO]
[2025-10-08 20:35:51] [INFO] Waiting 10 seconds before Phase 2...
[2025-10-08 20:36:01] [INFO]
[2025-10-08 20:36:01] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 20:36:01] [INFO]   PHASE 2: Deploying RKE2 to Homelab
[2025-10-08 20:36:01] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 20:36:01] [INFO] ========================================
[2025-10-08 20:36:01] [INFO]  Deploying RKE2 to Homelab (RHEL10)
[2025-10-08 20:36:01] [INFO] ========================================
[2025-10-08 20:36:01] [INFO] Target: homelab (192.168.4.62)
[2025-10-08 20:36:01] [INFO] Playbook: /srv/monitoring_data/VMStation/ansible/playbooks/install-rke2-homelab.yml
[2025-10-08 20:36:01] [INFO] Log: /srv/monitoring_data/VMStation/ansible/artifacts/install-rke2-homelab.log
[2025-10-08 20:36:01] [INFO]
[2025-10-08 20:36:01] [INFO] Running pre-flight checks...
[2025-10-08 20:36:01] [INFO] Verifying SSH connectivity to homelab...
[2025-10-08 20:36:03] [INFO] ✓ SSH connectivity to homelab verified
[2025-10-08 20:36:03] [INFO] Checking if homelab is clean (no kubeadm artifacts)...
[2025-10-08 20:36:04] [INFO] ✓ homelab appears clean
[2025-10-08 20:36:04] [INFO] Verifying Debian cluster health...
[2025-10-08 20:36:05] [INFO] ✓ Debian cluster is healthy (2 Debian nodes Ready)
[2025-10-08 20:36:05] [INFO] ✓ Debian cluster is healthy - RKE2 federation will work
[2025-10-08 20:36:05] [INFO] Starting RKE2 installation (this may take 15-20 minutes)...

PLAY [RKE2 Installation - Homelab Node] ****************************************

TASK [Gathering Facts] *********************************************************
ok: [homelab]

TASK [Display RKE2 installation banner] ****************************************
ok: [homelab] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    RKE2 Installation on Homelab (RHEL 10)
    Target: homelab
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Check if RKE2 is installed] **********************************************
ok: [homelab]

TASK [Display skip message if already installed] *******************************
skipping: [homelab]

TASK [Download RKE2 installation script] ***************************************
ok: [homelab]

TASK [Install RKE2] ************************************************************
changed: [homelab]

TASK [Log RKE2 installation output] ********************************************
ok: [homelab] =>
  msg:
  - '[INFO]  finding release for channel stable'
  - '[INFO]  using 1.33 series from channel stable'
  - Updating Subscription Management repositories.
  - 'Last metadata expiration check: 0:00:02 ago on Wed 08 Oct 2025 08:36:18 PM.'
  - Package rke2-server-1.33.5~rke2r1-0.el8.x86_64 is already installed.
  - Dependencies resolved.
  - Nothing to do.
  - Complete!

TASK [Create RKE2 config directory] ********************************************
ok: [homelab]

TASK [Create RKE2 configuration] ***********************************************
ok: [homelab]

TASK [Enable and start RKE2 server] ********************************************
ok: [homelab]

TASK [Wait for RKE2 to be ready] ***********************************************
skipping: [homelab]

TASK [Create kubectl symlink] **************************************************
ok: [homelab]

TASK [Set KUBECONFIG environment variable] *************************************
ok: [homelab]

TASK [Wait for kubectl binary to be available] *********************************
ok: [homelab]

TASK [Verify RKE2 is running] **************************************************
changed: [homelab]

TASK [Verify RKE2 pods are running] ********************************************
changed: [homelab]

TASK [Display RKE2 node status] ************************************************
ok: [homelab] =>
  msg:
  - NAME      STATUS   ROLES                       AGE   VERSION
  - homelab   Ready    control-plane,etcd,master   33h   v1.33.5+rke2r1

TASK [Display RKE2 pods status] ************************************************
ok: [homelab] =>
  msg:
  - NAMESPACE     NAME                                                   READY   STATUS      RESTARTS   AGE
  - kube-system   cloud-controller-manager-homelab                       1/1     Running     0          33h
  - kube-system   etcd-homelab                                           1/1     Running     0          33h
  - kube-system   helm-install-rke2-coredns-7n8lh                        0/1     Completed   0          33h
  - kube-system   helm-install-rke2-flannel-bt926                        0/1     Completed   0          33h
  - kube-system   helm-install-rke2-ingress-nginx-hps8d                  0/1     Completed   0          33h
  - kube-system   helm-install-rke2-metrics-server-jtqdr                 0/1     Completed   0          33h
  - kube-system   helm-install-rke2-runtimeclasses-jz8wg                 0/1     Completed   0          33h
  - kube-system   helm-install-rke2-snapshot-controller-crd-m7tnz        0/1     Completed   0          33h
  - kube-system   helm-install-rke2-snapshot-controller-vhqr5            0/1     Completed   0          33h
  - kube-system   kube-apiserver-homelab                                 1/1     Running     0          33h
  - kube-system   kube-controller-manager-homelab                        1/1     Running     0          33h
  - kube-system   kube-flannel-ds-bnkml                                  1/1     Running     0          33h
  - kube-system   kube-proxy-homelab                                     1/1     Running     0          33h
  - kube-system   kube-scheduler-homelab                                 1/1     Running     0          33h
  - kube-system   rke2-coredns-rke2-coredns-6464f98784-5vsfj             1/1     Running     0          33h
  - kube-system   rke2-coredns-rke2-coredns-autoscaler-67bb49dff-ws7zm   1/1     Running     0          33h
  - kube-system   rke2-ingress-nginx-controller-md86g                    1/1     Running     0          32h
  - kube-system   rke2-metrics-server-75d485c65b-tk6kw                   1/1     Running     0          33h
  - kube-system   rke2-snapshot-controller-696989ffdd-d7j9b              1/1     Running     0          32h

TASK [Create local artifacts directory] ****************************************
ok: [homelab -> localhost]

TASK [Fetch RKE2 kubeconfig] ***************************************************
changed: [homelab]

TASK [Update kubeconfig server address] ****************************************
changed: [homelab -> localhost]

TASK [Display installation complete message] ***********************************
ok: [homelab] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ✅ RKE2 Installation Complete

    Node: homelab
    Kubeconfig: /etc/rancher/rke2/rke2.yaml
    Local Kubeconfig: /srv/monitoring_data/VMStation/ansible/playbooks/../artifacts/homelab-rke2-kubeconfig.yaml

    Access cluster from homelab node:
    export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
    kubectl get nodes

    Access cluster remotely:
    export KUBECONFIG=/srv/monitoring_data/VMStation/ansible/playbooks/../artifacts/homelab-rke2-kubeconfig.yaml
    kubectl get nodes
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY RECAP *********************************************************************
homelab                    : ok=20   changed=5    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0

[2025-10-08 20:36:27] [INFO]
[2025-10-08 20:36:27] [INFO] ✓ RKE2 installation completed successfully
[2025-10-08 20:36:27] [INFO]
[2025-10-08 20:36:27] [INFO] Verifying RKE2 cluster...
[2025-10-08 20:36:27] [INFO] ✓ RKE2 cluster has 1 node(s) Ready
./deploy.sh: line 409: [[: 0
0: syntax error in expression (error token is "0")
[2025-10-08 20:36:27] [INFO]
[2025-10-08 20:36:27] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 20:36:27] [INFO]   RKE2 Cluster Ready on Homelab!
[2025-10-08 20:36:27] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 20:36:27] [INFO]
[2025-10-08 20:36:27] [INFO] Artifacts:
[2025-10-08 20:36:27] [INFO]   - Kubeconfig: /srv/monitoring_data/VMStation/ansible/artifacts/homelab-rke2-kubeconfig.yaml
[2025-10-08 20:36:27] [INFO]   - Log: /srv/monitoring_data/VMStation/ansible/artifacts/install-rke2-homelab.log
[2025-10-08 20:36:27] [INFO]
[2025-10-08 20:36:27] [INFO] Verification commands:
[2025-10-08 20:36:27] [INFO]   export KUBECONFIG=/srv/monitoring_data/VMStation/ansible/artifacts/homelab-rke2-kubeconfig.yaml
[2025-10-08 20:36:27] [INFO]   kubectl get nodes
[2025-10-08 20:36:27] [INFO]   kubectl get pods -A
[2025-10-08 20:36:27] [INFO]   kubectl get pods -n monitoring-rke2
[2025-10-08 20:36:27] [INFO]
[2025-10-08 20:36:27] [INFO] Monitoring endpoints:
[2025-10-08 20:36:27] [INFO]   - Node Exporter: http://192.168.4.62:9100/metrics
[2025-10-08 20:36:27] [INFO]   - Prometheus: http://192.168.4.62:30090
[2025-10-08 20:36:27] [INFO]   - Federation: http://192.168.4.62:30090/federate
[2025-10-08 20:36:27] [INFO]
[2025-10-08 20:36:27] [INFO] Federation test:
[2025-10-08 20:36:27] [INFO]   curl -s 'http://192.168.4.62:30090/federate?match[]={job=~".+"}' | head -20
[2025-10-08 20:36:27] [INFO]
[2025-10-08 20:36:27] [INFO]
[2025-10-08 20:36:27] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 20:36:27] [INFO]   TWO-PHASE DEPLOYMENT COMPLETE!
[2025-10-08 20:36:27] [INFO] ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
[2025-10-08 20:36:27] [INFO]
[2025-10-08 20:36:27] [INFO] Summary:
[2025-10-08 20:36:27] [INFO]   ✓ Debian cluster: monitoring_nodes + storage_nodes
[2025-10-08 20:36:27] [INFO]   ✓ RKE2 cluster: homelab
[2025-10-08 20:36:27] [INFO]
[2025-10-08 20:36:27] [INFO] Logs:
[2025-10-08 20:36:27] [INFO]   - /srv/monitoring_data/VMStation/ansible/artifacts/deploy-debian.log
[2025-10-08 20:36:27] [INFO]   - /srv/monitoring_data/VMStation/ansible/artifacts/install-rke2-homelab.log
[2025-10-08 20:36:27] [INFO]
[2025-10-08 20:36:27] [INFO] Setting up auto-sleep monitoring...

PLAY [Setup Auto-Sleep Monitoring] ********************************************************************************************************************************************************

TASK [Gathering Facts] ********************************************************************************************************************************************************************
ok: [masternode]
ok: [homelab]
ok: [storagenodet3500]

TASK [Display auto-sleep setup banner] ****************************************************************************************************************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Auto-Sleep Monitoring Setup
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ok: [storagenodet3500] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Auto-Sleep Monitoring Setup
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ok: [homelab] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    Auto-Sleep Monitoring Setup
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

TASK [Confirm setup] **********************************************************************************************************************************************************************
[Confirm setup]
This will configure auto-sleep monitoring.
Cluster will sleep after 2 hours of inactivity.
Continue? (yes/no)
:
yes^Mok: [masternode]

TASK [Check confirmation] *****************************************************************************************************************************************************************
skipping: [masternode]
fatal: [storagenodet3500]: FAILED! => changed=false
  msg: Setup cancelled by user
fatal: [homelab]: FAILED! => changed=false
  msg: Setup cancelled by user

TASK [Create auto-sleep monitor script] ***************************************************************************************************************************************************
ok: [masternode]

TASK [Create cluster sleep script] ********************************************************************************************************************************************************
ok: [masternode]

TASK [Create systemd service for auto-sleep monitor] **************************************************************************************************************************************
ok: [masternode]

TASK [Create systemd timer for auto-sleep monitor] ****************************************************************************************************************************************
ok: [masternode]

TASK [Reload systemd] *********************************************************************************************************************************************************************
ok: [masternode]

TASK [Enable and start auto-sleep timer] **************************************************************************************************************************************************
ok: [masternode]

TASK [Display setup complete message] *****************************************************************************************************************************************************
ok: [masternode] =>
  msg: |-
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ✅ Auto-Sleep Monitoring Setup Complete

    Cluster will automatically sleep after 2 hours of inactivity

    Monitor status:
    systemctl status vmstation-autosleep.timer

    Disable auto-sleep:
    systemctl stop vmstation-autosleep.timer
    systemctl disable vmstation-autosleep.timer
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

PLAY RECAP ********************************************************************************************************************************************************************************
homelab                    : ok=2    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0
masternode                 : ok=10   changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0
storagenodet3500           : ok=2    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0

=========================================
VMStation Security Audit
=========================================

[1/10] Checking for hardcoded secrets...
✅ PASS: Hardcoded passwords

[2/10] Checking SSH key security...
✅ PASS: SSH directory permissions

[3/10] Checking for encrypted sensitive files...
⚠️  WARNING: Ansible vault encryption - secrets.yml not encrypted with ansible-vault

[4/10] Checking Kubernetes security configurations...
⚠️  WARNING: Privileged containers - Found privileged container configurations
  Consider using specific capabilities instead
⚠️  WARNING: Host network usage - Pods using host network detected
  Review necessity of hostNetwork configuration
⚠️  WARNING: Resource limits - Some deployments may be missing resource limits

[5/10] Checking RBAC configurations...
✅ PASS: RBAC ClusterRoles
✅ PASS: RBAC verb specificity

[6/10] Checking script file permissions...
✅ PASS: Script permissions

[7/10] Checking .gitignore for sensitive patterns...
✅ PASS: Gitignore coverage

[8/10] Checking monitoring security configuration...
✅ PASS: Grafana anonymous access
✅ PASS: Grafana anonymous role

[9/10] Checking network security...
✅ PASS: NodePort services
  For production: Consider using Ingress with TLS
✅ PASS: LoadBalancer services

[10/10] Checking container image configurations...
⚠️  WARNING: Container image tags - Using :latest tag detected
  Pin to specific versions for reproducibility
✅ PASS: Container images
  Verify these are from trusted sources:
  - docker.io/flannel/flannel-cni-plugin:v1.4.0-flannel1
  - docker.io/flannel/flannel:v0.24.2
  - ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
  - ghcr.io/flannel-io/flannel:v0.27.4
  - grafana/grafana:10.0.0
  - grafana/loki:2.9.2
  - grafana/promtail:2.9.2
  - jellyfin/jellyfin:latest
  - prom/blackbox-exporter:v0.25.0
  - prometheuscommunity/ipmi-exporter:v1.6.1
  - prom/node-exporter:v1.6.1
  - prom/prometheus:v2.45.0
  - registry.k8s.io/coredns/coredns:v1.11.1
  - registry.k8s.io/kube-proxy:v1.29.15
  - registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.0

=========================================
Security Audit Summary
=========================================
Passed:   11
Warnings: 5
Errors:   0

⚠️  Security audit found warnings - review recommended
=========================================
VMStation Complete Validation Suite
=========================================

This test suite validates:
  1. Auto-sleep and wake configuration
  2. Monitoring exporters health
  3. Loki log aggregation
  4. Sleep/wake cycle (optional - requires confirmation)

Test order:
  - Non-destructive tests run first
  - Sleep/wake cycle test is optional (requires user confirmation)

Phase 1: Configuration Validation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Auto-Sleep/Wake Configuration
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Auto-Sleep/Wake Validation
Testing sleep/wake cycle and monitoring
=========================================

[1/10] Testing systemd timer on storagenodet3500...
✅ PASS: Auto-sleep timer is enabled on storagenodet3500
✅ PASS: Auto-sleep timer is active on storagenodet3500

[2/10] Testing systemd timer on homelab (RHEL10)...
✅ PASS: Auto-sleep timer is enabled on homelab
⚠️  WARN: Auto-sleep timer is not active on homelab

[3/10] Testing auto-sleep script existence...
✅ PASS: Auto-sleep monitor script exists on storagenodet3500
✅ PASS: Sleep script exists on storagenodet3500

[4/10] Testing WoL configuration...
✅ PASS: WoL script exists and is executable
⚠️  WARN: WoL systemd service not found on masternode

[5/10] Testing kubectl access...
✅ PASS: kubectl access verified on masternode
ℹ️  INFO: Current cluster status:
  masternode         Ready   control-plane   8m52s   v1.29.15
  storagenodet3500   Ready   <none>          8m31s   v1.29.15

[6/10] Testing WoL tool availability...
✅ PASS: wakeonlan tool is available

[7/10] Testing node reachability...
✅ PASS: storagenodet3500 is reachable (192.168.4.61)
✅ PASS: homelab is reachable (192.168.4.62)

[8/10] Testing monitoring service configuration...
✅ PASS: Monitoring namespace exists
✅ PASS: Prometheus pods are running
✅ PASS: Grafana pods are running

[9/10] Testing log file configuration...
✅ PASS: VMStation state directory exists
✅ PASS: Auto-sleep log files exist

[10/10] Testing systemd timer schedules...
✅ PASS: Auto-sleep timer is scheduled
ℹ️  INFO: Timer schedule:
  Wed 2025-10-08 20:40:18 EDT 1min 12s left Wed 2025-10-08 20:25:18 EDT 13min ago vmstation-autosleep.timer vmstation-autosleep.service

=========================================
Test Results Summary
=========================================
Passed:   16
Failed:   0
Warnings: 2

✅ All critical tests passed!

Auto-Sleep/Wake Configuration:
  - Systemd timers configured on both nodes
  - Scripts deployed and executable
  - Monitoring services available

Manual Testing:
  1. Trigger sleep: ssh root@192.168.4.63 'sudo /usr/local/bin/vmstation-sleep.sh'
  2. Check node status: ssh root@192.168.4.63 'kubectl get nodes'
  3. Send WoL: wakeonlan b8:ac:6f:7e:6c:9d
  4. Monitor wake time and verify services

✅ SUITE PASSED: Auto-Sleep/Wake Configuration


Phase 2: Monitoring Health Validation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Monitoring Exporters Health
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Monitoring Exporters Health
Validating exporters, targets, and dashboards
=========================================

[1/8] Testing Prometheus targets...
✅ PASS: Prometheus targets API accessible
ℹ️  INFO: Targets UP: 19, DOWN: 4
❌ FAIL: 4 targets are DOWN
  - kubernetes-service-endpoints

[2/8] Testing node-exporter on all nodes...
curl http://192.168.4.63:9100/metrics success
✅ PASS: Node exporter healthy on masternode
curl http://192.168.4.61:9100/metrics success
✅ PASS: Node exporter healthy on storagenodet3500
❌ SUITE FAILED: Monitoring Exporters Health


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Loki Log Aggregation
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Loki Log Aggregation Test
Validating Loki connectivity and log ingestion
=========================================

[1/6] Testing Loki pod status...
ℹ️  INFO: Loki pods found:
  loki-74577b9557-2d8rm                 0/1     CrashLoopBackOff   6 (2m33s ago)   8m16s
❌ FAIL: Loki pods are not running

[2/6] Testing Loki service configuration...
✅ PASS: Loki service exists
ℹ️  INFO: Loki service details:
  loki                   NodePort    10.105.117.71    <none>        3100:31100/TCP      8m16s
✅ PASS: Loki service has endpoints

[3/6] Testing Loki API connectivity...
curl http://192.168.4.63:3100/ready error
⚠️  WARN: Loki ready endpoint not accessible (may be ClusterIP only)
ℹ️  INFO: Attempting to test via kubectl exec...
❌ FAIL: Loki is not ready

[4/6] Testing Promtail log shipper...
ℹ️  INFO: Promtail pods found:
  promtail-86czm                        1/1     Running            0               8m18s
  promtail-sg792                        1/1     Running            0               8m18s
✅ PASS: Promtail pods are running (2 instances)

[5/6] Testing Loki DNS resolution...
✅ PASS: Loki DNS resolution successful

[6/6] Testing Loki datasource in Grafana...
✅ PASS: Loki datasource is configured in Grafana
⚠️  WARN: Could not check Loki datasource health

=========================================
Test Results Summary
=========================================
Passed:   5
Failed:   2
Warnings: 2

❌ Loki log aggregation has issues.

Common fixes:
  1. Check Loki logs: kubectl logs -n monitoring -l app=loki
  2. Check Promtail logs: kubectl logs -n monitoring -l app=promtail
  3. Verify Loki service: kubectl get svc -n monitoring loki
  4. Check DNS: kubectl run -it --rm dns-test --image=busybox --restart=Never -- nslookup loki.monitoring

Connectivity errors:
  - DNS lookup failures: Check CoreDNS pods
  - 500 status: Check Loki logs for errors
  - Service unavailable: Verify Loki pods are running

❌ SUITE FAILED: Loki Log Aggregation


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Monitoring Access (Updated)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Monitoring Access Test
Testing anonymous access to endpoints
=========================================

[1/8] Testing Grafana Access...
Testing Grafana Web UI... ✅ PASS
  curl http://192.168.4.63:30300 success
Testing Grafana API (anonymous)... ✅ PASS
  curl http://192.168.4.63:30300/api/health success

[2/8] Testing Prometheus Access...
Testing Prometheus Web UI... ❌ FAIL (unexpected response)
  curl http://192.168.4.63:30090 failure
❌ SUITE FAILED: Monitoring Access (Updated)


Phase 3: Sleep/Wake Cycle Test (Optional)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

WARNING: This test will trigger cluster sleep and wake.
This is a destructive test that will:
  - Cordon and drain worker nodes
  - Scale down deployments
  - Send Wake-on-LAN packets
  - Measure wake time and validate service restoration

Run sleep/wake cycle test? [y/N]: y
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Test Suite: Sleep/Wake Cycle
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

=========================================
VMStation Sleep/Wake Cycle Test
Automated testing of full sleep/wake cycle
=========================================

⚠️  WARNING: This test will:
  1. Trigger cluster sleep (drain nodes, cordon)
  2. Send Wake-on-LAN packets to worker nodes
  3. Measure wake time and validate services

Continue with sleep/wake cycle test? [y/N]: y

[1/7] Recording initial cluster state...
✅ PASS: Cluster is accessible
ℹ️  INFO: Initial node status:
  masternode         Ready   control-plane   9m15s   v1.29.15
  storagenodet3500   Ready   <none>          8m54s   v1.29.15
ℹ️  INFO: Ready nodes: 2

[2/7] Triggering cluster sleep...
ℹ️  INFO: Running vmstation-sleep.sh on masternode...
[2025-10-08 20:39:28] ==========================================
[2025-10-08 20:39:28] Initiating cluster sleep sequence
[2025-10-08 20:39:28] ==========================================
[2025-10-08 20:39:28] Step 1: Cordoning and draining worker nodes...
[2025-10-08 20:39:28] Skipping control-plane node: masternode
[2025-10-08 20:39:28] Cordoning node: storagenodet3500
node/storagenodet3500 cordoned
[2025-10-08 20:39:28] Draining node: storagenodet3500
node/storagenodet3500 already cordoned
Warning: deleting Pods that declare no controller: jellyfin/jellyfin; ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-5gdm8, kube-system/kube-proxy-stjww, monitoring/node-exporter-sxfwj, monitoring/promtail-86czm
evicting pod jellyfin/jellyfin
pod/jellyfin evicted
node/storagenodet3500 drained
[2025-10-08 20:39:31] Step 2: Scaling down deployments...
[2025-10-08 20:39:31] No deployments found in default namespace
[2025-10-08 20:39:31] ==========================================
[2025-10-08 20:39:31] Cluster sleep sequence completed
[2025-10-08 20:39:31] ==========================================
✅ PASS: Sleep script executed

[3/7] Verifying node status after sleep...
ℹ️  INFO: Node status after sleep:
  masternode         Ready                      control-plane   9m30s   v1.29.15
  storagenodet3500   Ready,SchedulingDisabled   <none>          9m9s    v1.29.15
✅ PASS: Worker nodes are cordoned (1 nodes)

[4/7] Sending Wake-on-LAN packets...
ℹ️  INFO: Waking storagenodet3500 (b8:ac:6f:7e:6c:9d)...
Sending magic packet to 255.255.255.255:9 with b8:ac:6f:7e:6c:9d
✅ PASS: WoL packet sent to storagenodet3500 (wakeonlan)
ℹ️  INFO: Waking homelab (d0:94:66:30:d6:63)...
Sending magic packet to 255.255.255.255:9 with d0:94:66:30:d6:63
✅ PASS: WoL packet sent to homelab (wakeonlan)

[5/7] Measuring wake time...
ℹ️  INFO: Waiting for storagenodet3500 to respond (timeout: 120s)...
✅ PASS: storagenodet3500 responded after 0s
ℹ️  INFO: Waiting for homelab to respond (timeout: 120s)...
✅ PASS: homelab responded after 0s

[6/7] Validating service restoration...
ℹ️  INFO: Waiting 30s for services to stabilize...
✅ PASS: kubelet is active on storagenodet3500
❌ FAIL: node-exporter is not responding on storagenodet3500
✅ PASS: rke2 service is active on homelab
ℹ️  INFO: Checking cluster node status...
ℹ️  INFO: Node status after wake:
  masternode         Ready                      control-plane   10m     v1.29.15
  storagenodet3500   Ready,SchedulingDisabled   <none>          9m40s   v1.29.15
ℹ️  INFO: Note: Nodes remain cordoned until manually uncordoned with 'kubectl uncordon <node>'

[7/7] Validating monitoring stack...
curl http://192.168.4.63:30090/-/healthy error
❌ FAIL: Prometheus is not healthy after wake
curl http://192.168.4.63:30300/api/health ok
✅ PASS: Grafana is healthy after wake

=========================================
Sleep/Wake Cycle Test Results
=========================================
Passed: 10
Failed: 2

Wake Time Summary:
  storagenodet3500: 0s
  homelab:          0s

❌ Sleep/wake cycle test encountered failures.

Review details above for troubleshooting.
❌ SUITE FAILED: Sleep/Wake Cycle


=========================================
Complete Validation Summary
=========================================

Test Suites Run:    5
Suites Passed:      1
Suites Failed:      4

❌ Some test suites failed.

Review the output above for details.

Common next steps:
  1. Fix failed tests and re-run: ./tests/test-complete-validation.sh
  2. Deploy missing components: ./deploy.sh setup
  3. Check cluster health: kubectl get pods -A
  4. Review logs: journalctl -u vmstation-autosleep -n 50

=========================================
VMStation Auto-Sleep/Wake Validation
Testing sleep/wake cycle and monitoring
=========================================

[1/10] Testing systemd timer on storagenodet3500...
✅ PASS: Auto-sleep timer is enabled on storagenodet3500
✅ PASS: Auto-sleep timer is active on storagenodet3500

[2/10] Testing systemd timer on homelab (RHEL10)...
✅ PASS: Auto-sleep timer is enabled on homelab
⚠️  WARN: Auto-sleep timer is not active on homelab

[3/10] Testing auto-sleep script existence...
✅ PASS: Auto-sleep monitor script exists on storagenodet3500
✅ PASS: Sleep script exists on storagenodet3500

[4/10] Testing WoL configuration...
✅ PASS: WoL script exists and is executable
⚠️  WARN: WoL systemd service not found on masternode

[5/10] Testing kubectl access...
✅ PASS: kubectl access verified on masternode
ℹ️  INFO: Current cluster status:
  masternode         Ready                      control-plane   10m     v1.29.15
  storagenodet3500   Ready,SchedulingDisabled   <none>          9m43s   v1.29.15

[6/10] Testing WoL tool availability...
✅ PASS: wakeonlan tool is available

[7/10] Testing node reachability...
✅ PASS: storagenodet3500 is reachable (192.168.4.61)
✅ PASS: homelab is reachable (192.168.4.62)

[8/10] Testing monitoring service configuration...
✅ PASS: Monitoring namespace exists
✅ PASS: Prometheus pods are running
✅ PASS: Grafana pods are running

[9/10] Testing log file configuration...
✅ PASS: VMStation state directory exists
✅ PASS: Auto-sleep log files exist

[10/10] Testing systemd timer schedules...
✅ PASS: Auto-sleep timer is scheduled
ℹ️  INFO: Timer schedule:
  Wed 2025-10-08 20:55:18 EDT 14min left Wed 2025-10-08 20:40:18 EDT 930ms ago vmstation-autosleep.timer vmstation-autosleep.service

=========================================
Test Results Summary
=========================================
Passed:   16
Failed:   0
Warnings: 2

✅ All critical tests passed!

Auto-Sleep/Wake Configuration:
  - Systemd timers configured on both nodes
  - Scripts deployed and executable
  - Monitoring services available

Manual Testing:
  1. Trigger sleep: ssh root@192.168.4.63 'sudo /usr/local/bin/vmstation-sleep.sh'
  2. Check node status: ssh root@192.168.4.63 'kubectl get nodes'
  3. Send WoL: wakeonlan b8:ac:6f:7e:6c:9d
  4. Monitor wake time and verify services

=========================================
VMStation Monitoring Exporters Health
Validating exporters, targets, and dashboards
=========================================

[1/8] Testing Prometheus targets...
✅ PASS: Prometheus targets API accessible
ℹ️  INFO: Targets UP: 19, DOWN: 4
❌ FAIL: 4 targets are DOWN
  - kubernetes-service-endpoints

[2/8] Testing node-exporter on all nodes...
curl http://192.168.4.63:9100/metrics success
✅ PASS: Node exporter healthy on masternode
curl http://192.168.4.61:9100/metrics success
✅ PASS: Node exporter healthy on storagenodet3500
=========================================
VMStation Loki Log Aggregation Test
Validating Loki connectivity and log ingestion
=========================================

[1/6] Testing Loki pod status...
ℹ️  INFO: Loki pods found:
  loki-74577b9557-2d8rm                 0/1     CrashLoopBackOff   6 (3m46s ago)   9m29s
❌ FAIL: Loki pods are not running

[2/6] Testing Loki service configuration...
✅ PASS: Loki service exists
ℹ️  INFO: Loki service details:
  loki                   NodePort    10.105.117.71    <none>        3100:31100/TCP      9m30s
✅ PASS: Loki service has endpoints

[3/6] Testing Loki API connectivity...
curl http://192.168.4.63:3100/ready error
⚠️  WARN: Loki ready endpoint not accessible (may be ClusterIP only)
ℹ️  INFO: Attempting to test via kubectl exec...
❌ FAIL: Loki is not ready

[4/6] Testing Promtail log shipper...
ℹ️  INFO: Promtail pods found:
  promtail-86czm                        1/1     Running            0               9m31s
  promtail-sg792                        1/1     Running            0               9m31s
✅ PASS: Promtail pods are running (2 instances)

[5/6] Testing Loki DNS resolution...
⚠️  WARN: Could not verify Loki DNS resolution

[6/6] Testing Loki datasource in Grafana...
✅ PASS: Loki datasource is configured in Grafana
⚠️  WARN: Could not check Loki datasource health

=========================================
Test Results Summary
=========================================
Passed:   4
Failed:   2
Warnings: 3

❌ Loki log aggregation has issues.

Common fixes:
  1. Check Loki logs: kubectl logs -n monitoring -l app=loki
  2. Check Promtail logs: kubectl logs -n monitoring -l app=promtail
  3. Verify Loki service: kubectl get svc -n monitoring loki
  4. Check DNS: kubectl run -it --rm dns-test --image=busybox --restart=Never -- nslookup loki.monitoring

Connectivity errors:
  - DNS lookup failures: Check CoreDNS pods
  - 500 status: Check Loki logs for errors
  - Service unavailable: Verify Loki pods are running

