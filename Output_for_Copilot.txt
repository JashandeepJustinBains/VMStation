root@masternode:~# kubectl get pods -o wide --all-namespaces
NAMESPACE      NAME                                 READY   STATUS              RESTARTS         AGE     IP             NODE         NOMINATED NODE   READINESS GATES
kube-flannel   kube-flannel-ds-gjh88                1/1     Running             0                6m4s    192.168.4.63   masternode   <none>           <none>
kube-system    coredns-68444cf7cd-85vlv             0/1     ContainerCreating   0                6m4s    <none>         masternode   <none>           <none>
kube-system    etcd-masternode                      1/1     Running             5                6m18s   192.168.4.63   masternode   <none>           <none>
kube-system    kube-apiserver-masternode            1/1     Running             13 (3m40s ago)   6m18s   192.168.4.63   masternode   <none>           <none>
kube-system    kube-controller-manager-masternode   1/1     Running             34 (4m ago)      6m18s   192.168.4.63   masternode   <none>           <none>
kube-system    kube-proxy-hfcqt                     1/1     Running             0                6m4s    192.168.4.63   masternode   <none>           <none>
kube-system    kube-scheduler-masternode            1/1     Running             34 (3m58s ago)   6m18s   192.168.4.63   masternode   <none>           <none>
root@masternode:~# kubectl describe pod coredns-68444cf7cd-85vlv -n kube-system
Name:                 coredns-68444cf7cd-85vlv
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Service Account:      coredns
Node:                 masternode/192.168.4.63
Start Time:           Fri, 12 Sep 2025 11:43:36 -0400
Labels:               k8s-app=kube-dns
                      pod-template-hash=68444cf7cd
Annotations:          <none>
Status:               Pending
IP:
IPs:                  <none>
Controlled By:        ReplicaSet/coredns-68444cf7cd
Containers:
  coredns:
    Container ID:
    Image:         registry.k8s.io/coredns/coredns:v1.11.1
    Image ID:
    Ports:         53/UDP (dns), 53/TCP (dns-tcp), 9153/TCP (metrics)
    Host Ports:    0/UDP (dns), 0/TCP (dns-tcp), 0/TCP (metrics)
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dnqcs (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-dnqcs:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason                  Age                    From               Message
  ----     ------                  ----                   ----               -------
  Normal   Scheduled               6m27s                  default-scheduler  Successfully assigned kube-system/coredns-68444cf7cd-85vlv to masternode
  Warning  FailedCreatePodSandBox  6m27s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "c8cc0013df5ac741509983227706f458b8b6409d1f0fbe02571bcd480b392bb5": plugin type="bridge" failed (add): failed to set bridge addr: "cni0" already has an IP address different from 10.244.0.1/16
  Warning  FailedCreatePodSandBox  6m26s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "b73f871d4f69ca23a7104aa500f5d3843a5781abc62b0ea2105675bcff25f8ac": plugin type="bridge" failed (add): failed to set bridge addr: "cni0" already has an IP address different from 10.244.0.1/16
  Warning  FailedCreatePodSandBox  5m49s (x4 over 6m26s)  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to reserve sandbox name "coredns-68444cf7cd-85vlv_kube-system_1bca2941-ba01-4d20-ae9e-1b1debd95dee_1": name "coredns-68444cf7cd-85vlv_kube-system_1bca2941-ba01-4d20-ae9e-1b1debd95dee_1" is reserved for "b73f871d4f69ca23a7104aa500f5d3843a5781abc62b0ea2105675bcff25f8ac"
  Warning  FailedCreatePodSandBox  5m37s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "6c134e2ab6aef24516dd89c9cb715076d547adb37c4eb0c79c567acde12fa970": plugin type="bridge" failed (add): failed to set bridge addr: "cni0" already has an IP address different from 10.244.0.1/16
  Warning  FailedCreatePodSandBox  4m44s (x5 over 5m37s)  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to reserve sandbox name "coredns-68444cf7cd-85vlv_kube-system_1bca2941-ba01-4d20-ae9e-1b1debd95dee_1": name "coredns-68444cf7cd-85vlv_kube-system_1bca2941-ba01-4d20-ae9e-1b1debd95dee_1" is reserved for "6c134e2ab6aef24516dd89c9cb715076d547adb37c4eb0c79c567acde12fa970"
  Warning  FailedCreatePodSandBox  4m29s                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "e73e9c478e6f007d8d46116dc5d0fc111582fc377867205263344237560e902a": plugin type="bridge" failed (add): failed to set bridge addr: "cni0" already has an IP address different from 10.244.0.1/16
  Normal   SandboxChanged          84s (x31 over 6m27s)   kubelet            Pod sandbox changed, it will be killed and re-created.



TASK [Wait for CoreDNS to stabilize after Flannel setup] **************************************************************************************************************************************
Pausing for 30 seconds
(ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
ok: [masternode]

TASK [Check CoreDNS pod status and fix if needed] *********************************************************************************************************************************************
changed: [masternode]

TASK [Display CoreDNS fix results] ************************************************************************************************************************************************************
ok: [masternode] => {
    "msg": "CoreDNS Post-Flannel Validation Results:\n=== Post-Flannel CoreDNS Validation ===\\n              CoreDNS pods status:\\n                coredns-68444cf7cd-85vlv: Status=Pending, IP=<none>\\n                  â†³ CNI bridge IP conflict detected for this pod\\n              === CNI Bridge Conflict Detected ===\\n              CoreDNS pods cannot start due to CNI bridge IP conflict.\\n              This requires CNI bridge reset, which will be handled by post-deployment fixes.\\n              Skipping CoreDNS rollout wait to avoid hanging the deployment.\\n              \\n              The deployment will complete and run fix_cni_bridge_conflict.sh automatically.\\n              === Final CoreDNS Status ===\\n              NAME                       READY   STATUS              RESTARTS   AGE     IP       NODE         NOMINATED NODE   READINESS GATES\\n              coredns-68444cf7cd-85vlv   0/1     ContainerCreating   0          5m36s   <none>   masternode   <none>           <none>\n\n\n"
}

TASK [Wait for API server to be ready] ********************************************************************************************************************************************************
ok: [masternode]

TASK [Verify API server accessibility using kubectl] ******************************************************************************************************************************************
changed: [masternode]

TASK [Validate kubernetes-admin RBAC permissions] *********************************************************************************************************************************************
changed: [masternode]

TASK [Check current authorization mode] *******************************************************************************************************************************************************
changed: [masternode]

TASK [Display current authorization mode] *****************************************************************************************************************************************************
ok: [masternode] => {
    "msg": "Current Kubernetes authorization mode: "
}

TASK [Fix kubernetes-admin RBAC if needed] ****************************************************************************************************************************************************
skipping: [masternode]

TASK [Backup API server manifest] *************************************************************************************************************************************************************
skipping: [masternode]

TASK [Update authorization mode from AlwaysAllow to Node,RBAC] ********************************************************************************************************************************
skipping: [masternode]

TASK [Wait for API server to restart after authorization fix] *********************************************************************************************************************************
skipping: [masternode]

TASK [Verify API server health after authorization fix] ***************************************************************************************************************************************
skipping: [masternode]

TASK [Apply RBAC fix after authorization mode change] *****************************************************************************************************************************************
skipping: [masternode]

TASK [Skip RBAC fix for AlwaysAllow mode (no longer needed after fix)] ************************************************************************************************************************
skipping: [masternode]

TASK [Wait for API server pod to be Ready] ****************************************************************************************************************************************************
changed: [masternode]

TASK [Check cluster-info configmap RBAC permissions] ******************************************************************************************************************************************
changed: [masternode]

TASK [Create RBAC rule for anonymous access to cluster-info configmap] ************************************************************************************************************************
skipping: [masternode]

TASK [Verify cluster-info configmap accessibility] ********************************************************************************************************************************************
changed: [masternode]

TASK [Check existing tokens and clean up old ones if needed] **********************************************************************************************************************************
changed: [masternode]

TASK [Generate fresh join command with enhanced validation] ***********************************************************************************************************************************
changed: [masternode]

TASK [Validate join command contains correct control-plane IP] ********************************************************************************************************************************
skipping: [masternode]

TASK [Save enhanced join command for wiped workers] *******************************************************************************************************************************************
changed: [masternode]

TASK [Display join command info] **************************************************************************************************************************************************************
ok: [masternode] => {
    "msg": "Join Command Generated:\n- Timestamp: 2025-09-12T15:48:37Z\n- Control-plane: 192.168.4.63:6443\n- Token TTL: 2 hours\n- Ready for wiped workers: Yes\n"
}

PLAY [Join Worker Nodes] **********************************************************************************************************************************************************************

TASK [Gathering Facts] ************************************************************************************************************************************************************************
ok: [storagenodet3500]
ok: [homelab]

TASK [Check if node is joined] ****************************************************************************************************************************************************************
ok: [storagenodet3500]
ok: [homelab]

TASK [Check for existing cluster artifacts] ***************************************************************************************************************************************************
ok: [storagenodet3500]
ok: [homelab]

TASK [Check for clean post-wipe state indicators] *********************************************************************************************************************************************
ok: [storagenodet3500] => (item=/etc/kubernetes)
ok: [homelab] => (item=/etc/kubernetes)
ok: [storagenodet3500] => (item=/var/lib/kubelet)
ok: [storagenodet3500] => (item=/etc/cni/net.d)
ok: [homelab] => (item=/var/lib/kubelet)
ok: [storagenodet3500] => (item=/var/lib/containerd)
ok: [homelab] => (item=/etc/cni/net.d)
ok: [homelab] => (item=/var/lib/containerd)

TASK [Detect if worker was aggressively wiped] ************************************************************************************************************************************************
ok: [storagenodet3500]
ok: [homelab]

TASK [Display worker state detection] *********************************************************************************************************************************************************
ok: [storagenodet3500] => {
    "msg": "Worker Node State Analysis:\n- Kubelet config exists: False\n- Cluster artifacts exist: False\n- Post-wipe state detected: False\n- Node requires fresh join: True\n"
}
ok: [homelab] => {
    "msg": "Worker Node State Analysis:\n- Kubelet config exists: False\n- Cluster artifacts exist: False\n- Post-wipe state detected: False\n- Node requires fresh join: True\n"
}

TASK [Check control-plane API server accessibility] *******************************************************************************************************************************************
ok: [storagenodet3500 -> localhost]
ok: [homelab -> localhost]

TASK [Verify control-plane cluster status] ****************************************************************************************************************************************************
changed: [storagenodet3500 -> masternode(192.168.4.63)]
changed: [homelab -> masternode(192.168.4.63)]

TASK [Ensure control-plane has proper RBAC configuration] *************************************************************************************************************************************
changed: [storagenodet3500 -> masternode(192.168.4.63)]
changed: [homelab -> masternode(192.168.4.63)]

TASK [Display control-plane readiness status] *************************************************************************************************************************************************
fatal: [storagenodet3500]: FAILED! => {"msg": "The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'status'. 'dict object' has no attribute 'status'\n\nThe error appears to be in '/srv/monitoring_data/VMStation/ansible/plays/setup-cluster.yaml': line 1495, column 11, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n        - name: \"Display control-plane readiness status\"\n          ^ here\n"}
fatal: [homelab]: FAILED! => {"msg": "The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'status'. 'dict object' has no attribute 'status'\n\nThe error appears to be in '/srv/monitoring_data/VMStation/ansible/plays/setup-cluster.yaml': line 1495, column 11, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n        - name: \"Display control-plane readiness status\"\n          ^ here\n"}

